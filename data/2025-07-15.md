<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 本文介绍了Spatial ModernBERT，这是一种基于Transformer的模型，通过空间嵌入进行增强，可以准确地检测和提取复杂财务文档中的表格数据和键值字段。


<details>
  <summary>Details</summary>
Motivation: 从财务文档中提取表格和键值对对于审计、数据分析和自动发票处理等业务工作流程至关重要。

Method: 我们提出了一个后处理方法，使用B-I-IB标记合并令牌，重建表格布局，并提取键值对。实证评估表明，Spatial ModernBERT有效地利用了文本和空间线索，有助于在实际财务文件中进行高度准确的表格和键值提取。

Result: Spatial ModernBERT在财务文档数据集上进行了微调，通过每个分类头的交叉熵损失实现了稳健的性能。

Conclusion: Spatial ModernBERT模型能够有效利用文本和空间线索，从而在实际财务文件中实现高度准确的表格和键值提取。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [2] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: SEALGuard是一种多语言guardrail，通过在包含十种语言的SEALSBench基准测试中，使用LoRA调整多语言语言模型，提高了LLM系统在多语言环境下的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM guardrail方法在处理多语言不安全输入时存在困难，尤其是在东南亚等低资源语言中。

Method: 使用低秩适应（LoRA）将通用多语言语言模型调整为多语言guardrail。

Result: SEALGuard在检测多语言不安全和jailbreak提示方面优于现有guardrail，与LlamaGuard相比，DSR提高了48%，并实现了最佳的DSR、精确率和F1分数。多语言不安全和jailbreak提示会显著降低LlamaGuard的性能，与仅英语提示相比，防御成功率（DSR）分别下降了9%和18%。

Conclusion: SEALGuard通过引入有效的多语言guardrail，提高了LLM系统的安全性对齐。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [3] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: Existing datasets for evaluating LLMs in medicine lack clinical realism and rigor. A standardized framework and collaborative efforts are needed.


<details>
  <summary>Details</summary>
Motivation: To evaluate the current limitations of large language models (LLMs) in medical question answering, focusing on the quality of datasets used for their evaluation.

Method: Widely-used benchmark datasets, including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor, transparency, and relevance to clinical scenarios. Alternatives, such as challenge questions in medical journals, were also analyzed to identify their potential as unbiased evaluation tools.

Result: Most existing datasets lack clinical realism, transparency, and robust validation processes. Publicly available challenge questions offer some benefits but are limited by their small size, narrow scope, and exposure to LLM training. These gaps highlight the need for secure, comprehensive, and representative datasets.

Conclusion: A standardized framework is critical for evaluating LLMs in medicine. Collaborative efforts among institutions and policymakers are needed to ensure datasets and methodologies are rigorous, unbiased, and reflective of clinical complexities.

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [4] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: Introduces two Korean expert-level benchmarks (KMMLU-Redux, KMMLU-Pro) to evaluate LLMs in real-world industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: The development of LLMs requires robust benchmarks that encompass both academic and industrial fields to evaluate real-world applicability.

Method: Reconstruction of existing KMMLU and creation of new benchmark based on Korean National Professional Licensure exams.

Result: The experiments demonstrate that the benchmarks comprehensively represent industrial knowledge in Korea.

Conclusion: The paper introduces two new Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, and makes the dataset publicly available.

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [5] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS is the first self-improving model-steering framework that operates without relying on external supervision, and it outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional model-steering methods rely heavily on externally annotated data, not only limiting their adaptability to varying contexts but also tethering their effectiveness to annotation quality.

Method: SIMS autonomously generates and refines contrastive samples through iterative self-improvement cycles, enabling adaptive, context-specific steering. Additionally, SIMS employs novel strategies, including prompt ranking and contrast sampling, to further enhance steering efficacy.

Result: SIMS substantially outperforms existing methods in steering effectiveness and adaptability.

Conclusion: SIMS substantially outperforms existing methods in steering effectiveness and adaptability, highlighting self-improving model steering as a promising direction for future research on inference-time LLM alignment.

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [6] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: The study found that stigmatizing language in electronic health records disproportionately affects historically stigmatized patients and is used by multiple provider types.


<details>
  <summary>Details</summary>
Motivation: Electronic health records (EHR) are a critical medium through which patient stigmatization is perpetuated among healthcare teams.

Method: We identified linguistic features of doubt markers and stigmatizing labels in MIMIC-III EHR via expanded lexicon matching and supervised learning classifiers. Predictors of rates of linguistic features were assessed using Poisson regression models.

Result: We found higher rates of stigmatizing labels per chart among patients who were Black or African American (RR: 1.16), patients with Medicare/Medicaid or government-run insurance (RR: 2.46), self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and mental health conditions. Patterns among doubt markers were similar, though male patients had higher rates of doubt markers (RR: 1.25). We found increased stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25), with similar patterns of doubt markers.

Conclusion: Stigmatizing language occurred at higher rates among historically stigmatized patients, perpetuated by multiple provider types.

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [7] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: Strong imagers of Ganzflicker-induced hallucinations reported complex, naturalistic content, while weak imagers reported simple geometric patterns.


<details>
  <summary>Details</summary>
Motivation: Recent proposals regarding the imagery spectrum suggest differences should impact the complexity of other internally generated visual experiences.

Method: used tools from natural language processing to analyze free-text descriptions of hallucinations from over 4,000 participants

Result: Embeddings from vision language models better captured these differences than text-only language models, and participants with stronger imagery used language with richer sensorimotor associations.

Conclusion: Strong imagers reported complex, naturalistic content, while weak imagers reported simple geometric patterns. These findings may reflect individual variation in coordination between early visual areas and higher-order regions relevant for the imagery spectrum.

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [8] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一种线性化框架，可以将基于Transformer的预训练大型语言模型(LLM)转换为灵活的、亚二次架构，用于无限上下文生成。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的LLM随着上下文长度的增加，面临着显著的内存和计算瓶颈，这是由于softmax注意力的二次复杂性和不断增长的键值(KV)缓存。

Method: Lizard结合了用于全局上下文压缩的门控线性注意力与通过元记忆增强的滑动窗口注意力，形成了一种混合机制，可以捕获远程依赖关系和细粒度的局部交互。此外，我们还引入了一种硬件感知算法，可以加速我们模型的训练速度。

Result: Lizard改进了先前的模型18分，并在联想召回任务上显示出显著的改进。

Conclusion: Lizard在标准语言建模任务中实现了接近无损的教师模型性能恢复，同时显著优于之前的线性化方法。在5-shot MMLU基准测试中，Lizard比之前的模型提高了18个点，并在联想召回任务中显示出显著的改进。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [9] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: ALIGN系统专注于基于LLM的决策者的动态个性化，通过基于prompt的对齐到一组细粒度的属性。


<details>
  <summary>Details</summary>
Motivation: 用户有不同的价值观和偏好，会影响他们的决策，这需要LLM对齐和个性化的新方法。

Method: prompt-based alignment

Result: 定性和定量分析比较了两种不同领域中的对齐方法：用于公众意见调查的人口统计对齐和用于医疗分诊决策的价值对齐。

Conclusion: ALIGN框架是开源的，可以支持对基于LLM的可靠、负责和个性化的决策者的研究。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [10] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 本文提出了VIL，一种视角不变的后训练策略，提高了现有导航策略对相机视角变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大多数导航策略对视角变化敏感，即相机高度和视角的变化会改变智能体的观察结果。因此，本文介绍了一个广义的场景V2-VLNCE（具有不同视角的VLNCE）。

Method: VIL，一种视角不变的后训练策略，它采用对比学习框架来学习稀疏和视角不变的特征。此外，我们还为航点预测器模块引入了一个师生框架，其中一个依赖于视角的教师模型将知识提炼成一个视角不变的学生模型。我们采用端到端的训练范式来共同优化这些组件。

Result: VIL在V2-VLNCE上优于现有技术水平的方法8-15%。在更具挑战性的RxR-CE数据集上，我们的方法在所有指标上也实现了最先进的性能。

Conclusion: VIL在V2-VLNCE上优于现有技术水平的方法8-15%，在RxR-CE数据集上实现了最先进的性能，并且可以作为一种即插即用的后训练方法。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [11] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 提出了一种新的用于检测deepfake视频的取证机器学习技术，该技术利用面部生物特征中的非自然模式，并在大型数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 逼真的声音克隆和引人注目的头像、换脸或唇形同步deepfake视频生成技术的结合，使得制作任何人说任何话的视频相对容易。如今，这种deepfake模仿经常被用于诈骗、欺诈和政治虚假信息。

Method: 利用面部生物特征中的非自然模式。

Result: 对大型deepfake技术和模仿数据集评估了该技术，并评估了其对视频清洗的可靠性及其对以前未见过的视频deepfake生成器的泛化能力。

Conclusion: 提出了一种新的用于检测deepfake视频伪装的取证机器学习技术。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [12] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM is a data-free and task-agnostic solution for bias mitigation in VLMs that outperforms current methods.


<details>
  <summary>Details</summary>
Motivation: VLMs often inherit and amplify biases in their training data, leading to skewed predictions.

Method: A contrastive-style debiasing loss is used to learn a projection that maps embeddings onto a latent space minimizing spurious correlations while preserving image-text alignment.

Result: A new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP is introduced.

Conclusion: PRISM outperforms current debiasing methods on Waterbirds and CelebA datasets.

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [13] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: HMR-ViT uses both temporal and kinematic information with a Vision Transformer to improve Human Mesh Recovery.


<details>
  <summary>Details</summary>
Motivation: Existing HMR methods utilized either temporal information or kinematic relationships, but not both, leading to potential limitations in accuracy.

Method: A Temporal-kinematic Feature Image is constructed using feature vectors obtained from video frames by an image encoder. A Channel Rearranging Matrix (CRM) is used to locate similar kinematic features spatially close together. The feature image is then further encoded using Vision Transformer, and the SMPL pose and shape parameters are finally inferred using a regression network.

Result: The HMR-ViT method achieves competitive performance in HMR.

Conclusion: The proposed HMR-ViT method achieves competitive performance in Human Mesh Recovery (HMR) on the 3DPW and Human3.6M datasets.

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [14] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: integrates NeRF with MPM simulation to infer granular material properties from visual observations


<details>
  <summary>Details</summary>
Motivation: infer granular material properties from visual observations

Method: integrate Neural Radiance Fields (NeRF) with Material Point Method (MPM) simulation

Result: friction angle can be estimated with an error within 2 degrees

Conclusion: The friction angle can be estimated with an error within 2 degrees, highlighting the effectiveness of inverse analysis through purely visual observations. This approach offers a promising solution for characterizing granular materials in real-world scenarios where direct measurement is impractical or impossible.

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [15] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA是一个视觉分析框架，通过结合多阶段数据验证和人工专业知识来提高多模态模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于数据数量而非质量，并且缺乏全面的视角来识别问题数据，或者仅对一小部分数据进行人工验证，无法解决所有潜在问题。

Method: VISTA，一个视觉分析框架，集成了多阶段数据验证策略与人类专业知识。

Result: 在两个基准数据集和专家评审的详细用例中，从定量和定性的角度证明了VISTA的有效性。

Conclusion: VISTA通过整合多阶段数据验证策略与人类专业知识，能够识别、理解和纠正FM生成标签中隐藏的问题，从而提高数据质量并增强多模态模型的性能。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [16] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个用于构建脑病灶图像分析流程的Python工具包。


<details>
  <summary>Details</summary>
Motivation: BrainLesion Suite是一个多功能的工具包，用于在Python中构建模块化脑病灶图像分析流程。

Method: BrainLesion Suite利用来自BraTS challenge的算法来合成缺失的模态，修复病灶，并且生成特定于病理的肿瘤分割。

Result: BrainLesion Suite也能够量化分割模型的性能，使用诸如panoptica之类的工具来计算病灶方面的指标。

Conclusion: BrainLesion Suite可以被调整用于其他的生物医学图像分析应用。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [17] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 针对类别不平衡的扩散模型，我们提出了一种对比学习框架，该框架通过增加合成图像之间的差异来提高尾部类别图像的多样性。


<details>
  <summary>Details</summary>
Motivation: 类别条件图像合成的训练数据通常呈现长尾分布，尾部类别的图像有限。这种不平衡会导致模式崩溃并降低尾部类别的合成图像的多样性。对于在不平衡数据上训练的类别条件扩散模型，我们的目标是提高尾部类别图像的多样性，而不影响头部类别图像的保真度和多样性。

Method: 我们通过引入两个看似简单但非常有效的对比损失函数来实现这一目标。首先，我们采用无监督的 InfoNCE 损失，利用负样本来增加合成图像之间的距离/差异，特别是对于尾部类别。为了进一步提高尾部类别的多样性，我们的第二个损失是一个 MSE 损失，它对比了大型时间步长的有条件生成和无条件生成。

Result: 条件-非条件对齐已被证明可以提高长尾 GAN 的性能。我们是第一个将这种对齐适应于扩散模型的人。

Conclusion: 我们成功地将对比学习用于类别不平衡的扩散模型。我们的对比学习框架易于实现，并且在各种数据集（包括 CIFAR10/100-LT、PlacesLT、TinyImageNetLT 和 ImageNetLT）上优于标准 DDPM 和类别不平衡扩散模型的替代方法。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 通过移除推理过程中的冗余来提高大型语言模型的性能，尤其是在数学竞赛等需要大量推理的基准测试中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长程推理中表现出潜力，但推理路径往往包含大量冗余，注意力模式分散，尤其是不正确的答案表现出更高的注意力稀疏性。

Method: 通过测量token级别对特殊end-of-thinking token的注意力分数来系统地识别推理冗余，并提出结构感知的剪枝方法，优先移除低贡献的推理块中的token。

Result: 在不需要任何训练的情况下，显著提高了推理密集型基准测试的整体准确性。

Conclusion: 通过移除推理过程中的冗余，显著提高了性能，尤其是在需要大量推理的基准测试中，如AIME和AMC等数学竞赛。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [19] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: Proposes a new Multi-Criteria Assessment (MCA) method using Virtual Gap Analysis (VGA) to improve decision-making by addressing limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Modern MCA methods rely on assumptions and can be influenced by subjective judgment. The homogeneity assumption is often employed, significantly affecting evaluations. Real-world scenarios require incorporating both quantitative and qualitative criteria.

Method: The paper proposes a novel MCA approach that combines two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear programming, is pivotal in the MCA methodology.

Result: Two comprehensive numerical examples demonstrate the accuracy and transparency of the proposed method. The approach improves efficiency and fairness, ensuring that evaluations are both comprehensive and dependable.

Conclusion: The paper introduces a novel MCA approach combining two Virtual Gap Analysis (VGA) models to improve efficiency and fairness in evaluations, offering a strong and adaptive solution.

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [20] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: Uses the Entity-Component pattern, inspired by TTRPGs, to allow the construction of a flexible environment for diverse generative AI use-cases.


<details>
  <summary>Details</summary>
Motivation: Generative AI is used in multi-actor environments for social science modeling, interactive narrative, and AI evaluation, demanding a flexible scenario definition framework.

Method: Inspired by tabletop role-playing games (TTRPGs), uses the Entity-Component architectural pattern.

Result: The approach allows separation between implementation details, reusable components, and their composition, ensuring rapid iteration, modularity, and scalability.

Conclusion: The Concordia library's evolution, based on the Entity-Component pattern, enables users to configure scenarios aligning with their specific goals.

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [21] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: Introduces BioAnalyst, the first AI Foundation Model tailored for biodiversity analysis and conservation planning, demonstrating improved performance in ecological forecasting, especially in data-scarce situations.


<details>
  <summary>Details</summary>
Motivation: The accelerating loss of biodiversity presents critical challenges for ecological research and conservation strategies.

Method: transformer-based architecture, pre-trained on extensive multi-modal datasets

Result: demonstrates its generalisability compared to existing methods, particularly in data-scarce scenarios for two distinct use-cases

Conclusion: BioAnalyst establishes a new accuracy baseline for ecological forecasting and fosters collaborative efforts in biodiversity modeling.

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [22] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: AI tools unexpectedly slowed down experienced developers in open-source projects.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of AI tools on software development productivity.

Method: Randomized controlled trial with 16 experienced open-source developers completing 246 tasks.

Result: AI tools slowed down developers by 19%, contrary to expectations.

Conclusion: AI tools increased completion time by 19%, contradicting predictions.

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [23] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: uses a Multi-Agent Reinforcement Learning (MARL) framework for decentralized manipulation detection


<details>
  <summary>Details</summary>
Motivation: Decentralized finance (DeFi) has introduced a new era of permissionless financial innovation but also led to unprecedented market manipulation.

Method: a Multi-Agent Reinforcement Learning (MARL) framework

Result: achieves top performance in detection accuracy and causal attribution

Conclusion: This work bridges multi-agent systems with financial surveillance, advancing a new paradigm for decentralized market intelligence. All resources are available at the Hide-and-Shill GitHub repository to promote open research and reproducibility.

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [24] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: This paper evaluates the security of LLM-based coding agents, finding that they often introduce security vulnerabilities. The authors also propose mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: The security implications of LLM-based coding agents are poorly understood, and these agents may inadvertently introduce insecure practices.

Method: The authors conducted a systematic security evaluation of autonomous coding agents, analyzing over 12,000 actions across five state-of-the-art models on 93 real-world software setup tasks. They also developed a high-precision detection system and evaluated mitigation strategies.

Result: 21% of agent trajectories contained insecure actions, with information exposure being the most prevalent vulnerability. GPT-4.1 demonstrated exceptional security awareness with 96.8% mitigation success.

Conclusion: This paper provides a comprehensive framework for evaluating the security of coding agents, highlights the need for security-aware design, and evaluates mitigation strategies.

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [25] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: The report presents a taxonomy and examples of potential omnicidal events resulting from AI, and hopes to support preventive measures against catastrophic risks from AI by presenting these possibilities in public.


<details>
  <summary>Details</summary>
Motivation: The report aims to present potential omnicidal events resulting from AI.

Method: The report presents a taxonomy and examples of potential omnicidal events resulting from AI.

Result: The report presents scenarios where all or almost all humans are killed.

Conclusion: The report hopes to support preventive measures against catastrophic risks from AI by presenting these possibilities in public.

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [26] [Orchestration for Domain-specific Edge-Cloud Language Models](https://arxiv.org/abs/2507.09003)
*Prasoon Patidar,Alex Crown,Kevin Hsieh,Yifei Xu,Tusher Chakraborty,Ranveer Chandra,Yuvraj Agarwal*

Main category: cs.DB

TL;DR: ECO-LLM is a system that jointly optimizes LLM components in edge-cloud collaboration, outperforming existing methods in accuracy, cost, and latency by dynamically selecting optimal strategies at the query level.


<details>
  <summary>Details</summary>
Motivation: Traditional methods primarily focus on selecting the best LLM model for optimizing performance, while neglecting the critical interplay between the components of the LLM serving pipeline or the changing latency and cost constraints. The remarkable performance of Large Language Models (LLMs) has inspired many applications, which often necessitate edge-cloud collaboration due to connectivity, privacy, and cost considerations.

Method: ECO-LLM consists of two components: (1) the ECO-LLM Emulator, which efficiently explores the vast configuration space utilizing query clustering and pareto-optimal path selection, gathering domain-specific performance metrics without exhaustive evaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to dynamically select optimal resolution strategies for user queries while meeting user-defined Service Level Objectives (SLOs).

Result: ECO-LLM outperforms cloud-based models like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing costs by 90% and latency by 55%. In practical deployment for previously unseen queries, ECO-LLM selects configurations that reduce costs by 62% or improve response times by 62% on average compared to state-of-the-art model routing approaches, while maintaining higher accuracy and consistently adhering to specified latency and cost constraints.

Conclusion: ECO-LLM outperforms cloud-based models like GPT-4o in terms of accuracy while reducing costs and latency. In practical deployment for previously unseen queries, ECO-LLM selects configurations that reduce costs or improve response times on average compared to state-of-the-art model routing approaches, while maintaining higher accuracy and consistently adhering to specified latency and cost constraints.

Abstract: The remarkable performance of Large Language Models (LLMs) has inspired many
applications, which often necessitate edge-cloud collaboration due to
connectivity, privacy, and cost considerations. Traditional methods primarily
focus on selecting the best LLM model for optimizing performance, while
neglecting the critical interplay between the components of the LLM serving
pipeline (context retrieval, query preprocessing, etc.) or the changing latency
and cost constraints. We introduce ECO-LLM (Edge-Cloud Orchestrator for LLMs),
a novel system that reframes this problem as a joint optimization challenge and
solves it by systematically exploring component configurations and dynamically
selecting optimal strategies at the query level. ECO-LLM consists of two
components: (1) the ECO-LLM Emulator, which efficiently explores the vast
configuration space utilizing query clustering and pareto-optimal path
selection, gathering domain-specific performance metrics without exhaustive
evaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to
dynamically select optimal resolution strategies for user queries while meeting
user-defined Service Level Objectives (SLOs). We evaluate ECO-LLM on a smart
home and a smart car assistant scenarios. With an exhaustive exploration of all
possible configurations for seen queries, ECO-LLM outperforms cloud-based
models like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing
costs by 90% and latency by 55%, demonstrating the value of its joint
optimization at the query level. In practical deployment for previously unseen
queries, ECO-LLM selects configurations that reduce costs by 62% or improve
response times by 62% on average compared to state-of-the-art model routing
approaches, while maintaining higher accuracy and consistently adhering to
specified latency and cost constraints.

</details>


### [27] [HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving](https://arxiv.org/abs/2507.09138)
*Zhengding Hu,Vibha Murthy,Zaifeng Pan,Wanlu Li,Xiaoyi Fang,Yufei Ding,Yuke Wang*

Main category: cs.DB

TL;DR: Addresses challenges in heterogeneous RAG serving with HedraRAG, a graph-based runtime system that optimizes execution through dynamic graph transformations and hybrid CPU-GPU pipelines, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: emerging system-level challenges in heterogeneous retrieval-augmented generation (RAG) serving, where complex multi-stage workflows and diverse request patterns complicate efficient execution

Method: a runtime system built on a graph-based abstraction that exposes optimization opportunities across stage-level parallelism, intra-request similarity, and inter-request skewness. These opportunities are realized through dynamic graph transformations, such as node splitting, reordering, edge addition, and dependency rewiring, applied to wavefronts of subgraphs spanning concurrent requests. The resulting execution plans are mapped onto hybrid CPU-GPU pipelines

Result: speedups exceeding 1.5x and reaching up to 5x over existing frameworks

Conclusion: Evaluations across a wide range of RAG workflows demonstrate speedups exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the effectiveness of coordinated generation and retrieval in serving environments.

Abstract: This paper addresses emerging system-level challenges in heterogeneous
retrieval-augmented generation (RAG) serving, where complex multi-stage
workflows and diverse request patterns complicate efficient execution. We
present HedraRAG, a runtime system built on a graph-based abstraction that
exposes optimization opportunities across stage-level parallelism,
intra-request similarity, and inter-request skewness. These opportunities are
realized through dynamic graph transformations, such as node splitting,
reordering, edge addition, and dependency rewiring, applied to wavefronts of
subgraphs spanning concurrent requests. The resulting execution plans are
mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce
latency. Evaluations across a wide range of RAG workflows demonstrate speedups
exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the
effectiveness of coordinated generation and retrieval in serving environments.

</details>


### [28] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer 是一种新型 VDBMS，它使用自适应查询处理框架来高效地处理 Re-ID 查询。它优于现有的跨摄像头分析系统，平均提高了 3.9 倍。


<details>
  <summary>Details</summary>
Motivation: 在摄像头网络中高效地重新识别和跟踪对象对于交通监控等应用至关重要。然而，最先进的视频数据库管理系统 (VDBMS) Spatula 存在两个局限性：时空过滤方案在大型摄像头网络上的准确性有限，并且由于缺乏对自适应查询处理的支持，因此不适用于需要高召回率的关键视频分析应用。

Method: Tracer 通过训练一个循环网络来模拟长期历史相关性，从而在每个时间步选择最佳摄像头进行处理。Tracer 包含一个概率自适应搜索模型，该模型在增量搜索窗口中处理摄像头馈送，并使用探索-利用策略动态更新采样概率。

Result: Tracer 的评估表明，其性能优于当前最优的跨摄像头分析系统，平均提高了 3.9 倍。

Conclusion: Tracer在各种数据集上的表现优于当前最优的跨摄像头分析系统，平均提高了 3.9 倍。

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>


### [29] [THOR: Transformer Heuristics for On-Demand Retrieval](https://arxiv.org/abs/2507.09592)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.DB

TL;DR: THOR模块是一个安全、可扩展的引擎，可以将自然语言问题转换为经过验证的只读SQL分析，从而使非技术用户能够轻松安全地访问企业数据。


<details>
  <summary>Details</summary>
Motivation: 为了使非技术用户能够访问实时数据，并简化企业数据库的查询过程。

Method: Transformer Heuristics for On-Demand Retrieval (THOR) Module

Result: 在财务、销售和运营场景中的测试表明，THOR模块能够实现可靠的即席查询和自动定期报告。

Conclusion: THOR模块通过嵌入模式感知、容错执行和合规性保护措施，使非技术用户能够以零SQL的简易性和企业级的安全性访问实时数据。

Abstract: We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)
Module, designed and implemented by eSapiens, a secure, scalable engine that
transforms natural-language questions into verified, read-only SQL analytics
for enterprise databases. The Text-to-SQL module follows a decoupled
orchestration/execution architecture: a Supervisor Agent routes queries, Schema
Retrieval dynamically injects table and column metadata, and a SQL Generation
Agent emits single-statement SELECT queries protected by a read-only guardrail.
An integrated Self-Correction & Rating loop captures empty results, execution
errors, or low-quality outputs and triggers up to five LLM-driven regeneration
attempts. Finally, a Result Interpretation Agent produces concise,
human-readable insights and hands raw rows to the Insight & Intelligence engine
for visualization or forecasting.
  Smoke tests across finance, sales, and operations scenarios demonstrate
reliable ad-hoc querying and automated periodic reporting. By embedding schema
awareness, fault-tolerant execution, and compliance guardrails, the THOR Module
empowers non-technical users to access live data with zero-SQL simplicity and
enterprise-grade safety.

</details>


### [30] [Rethinking LSM-tree based Key-Value Stores: A Survey](https://arxiv.org/abs/2507.09642)
*Yina Lv,Qiao Li,Quanqing Xu,Congming Gao,Chuanhui Yang,Xiaoli Wang,Chun Jason Xue*

Main category: cs.DB

TL;DR: This survey reviews LSM-tree optimizations over the past five years, focusing on mitigating compaction issues, improving key-value operations, and addressing challenges in modern distributed key-value stores. It provides future research directions.


<details>
  <summary>Details</summary>
Motivation: LSM-tree compaction introduces significant challenges, including performance variability during peak workloads and in resource-constrained environments, write amplification, read amplification, trade-off between read and write performance, as well as efficient space utilization. Prior studies on LSM-tree optimizations have addressed the above challenges; however, in recent years, research on LSM-tree optimization has continued to propose.

Method: reviewing LSM-tree optimization, focusing on representative works in the past five years

Result: a detailed discussion of the state-of-the-art work on LSM-tree optimizations and gives future research directions

Conclusion: This survey reviews LSM-tree optimization, focusing on representative works in the past five years. It studies existing solutions on how to mitigate the performance impact of LSM-tree flush and compaction and how to improve basic key-value operations. It also analyzes the new challenges and opportunities in modern architectures and across various application scenarios. The survey provides a detailed discussion of the state-of-the-art work on LSM-tree optimizations and gives future research directions.

Abstract: LSM-tree is a widely adopted data structure in modern key-value store systems
that optimizes write performance in write-heavy applications by using append
writes to achieve sequential writes. However, the unpredictability of LSM-tree
compaction introduces significant challenges, including performance variability
during peak workloads and in resource-constrained environments, write
amplification caused by data rewriting during compactions, read amplification
from multi-level queries, trade-off between read and write performance, as well
as efficient space utilization to mitigate space amplification. Prior studies
on LSM-tree optimizations have addressed the above challenges; however, in
recent years, research on LSM-tree optimization has continued to propose. The
goal of this survey is to review LSM-tree optimization, focusing on
representative works in the past five years. This survey first studies existing
solutions on how to mitigate the performance impact of LSM-tree flush and
compaction and how to improve basic key-value operations. In addition,
distributed key-value stores serve multi-tenants, ranging from tens of
thousands to millions of users with diverse requirements. We then analyze the
new challenges and opportunities in these modern architectures and across
various application scenarios. Unlike the existing survey papers, this survey
provides a detailed discussion of the state-of-the-art work on LSM-tree
optimizations and gives future research directions.

</details>


### [31] [Efficient Temporal Simple Path Graph Generation](https://arxiv.org/abs/2507.10017)
*Zhiyang Tang,Yanping Wu,Xiangjun Zai,Chen Chen,Xiaoyang Wang,Ying Zhang*

Main category: cs.DB

TL;DR: This paper introduces the temporal simple path graph (tspG) problem and proposes an efficient method to construct it using a verification in the upper-bound graph approach, which is validated through experiments.


<details>
  <summary>Details</summary>
Motivation: Exploring the relationships between vertices based on temporal paths is one of the fundamental tasks. In this paper, we conduct the first research to propose and investigate the problem of generating the temporal simple path graph (tspG), which is the subgraph consisting of all temporal simple paths from the source vertex to the target vertex within the given time interval. Directly enumerating all temporal simple paths and constructing the tspG is computationally expensive.

Method: The paper proposes an efficient method named Verification in Upper-bound Graph. It first incorporates the temporal path constraint and simple path constraint to exclude unpromising edges from the original graph, which obtains a tight upper-bound graph as a high-quality approximation of the tspG in polynomial time. Then, an Escape Edges Verification algorithm is further applied in the upper-bound graph to construct the exact tspG without exhaustively enumerating all temporal simple paths between given vertices.

Result: The paper proposes the problem of generating the temporal simple path graph (tspG).

Conclusion: The paper conducts comprehensive experiments on 10 real-world graphs to demonstrate the efficiency and effectiveness of the proposed techniques.

Abstract: Interactions between two entities often occur at specific timestamps, which
can be modeled as a temporal graph. Exploring the relationships between
vertices based on temporal paths is one of the fundamental tasks. In this
paper, we conduct the first research to propose and investigate the problem of
generating the temporal simple path graph (tspG), which is the subgraph
consisting of all temporal simple paths from the source vertex to the target
vertex within the given time interval. Directly enumerating all temporal simple
paths and constructing the tspG is computationally expensive. To accelerate the
processing, we propose an efficient method named Verification in Upper-bound
Graph. It first incorporates the temporal path constraint and simple path
constraint to exclude unpromising edges from the original graph, which obtains
a tight upper-bound graph as a high-quality approximation of the tspG in
polynomial time. Then, an Escape Edges Verification algorithm is further
applied in the upper-bound graph to construct the exact tspG without
exhaustively enumerating all temporal simple paths between given vertices.
Finally, comprehensive experiments on 10 real-world graphs are conducted to
demonstrate the efficiency and effectiveness of the proposed techniques.

</details>


### [32] [Breaking the Storage-Compute Bottleneck in Billion-Scale ANNS: A GPU-Driven Asynchronous I/O Framework](https://arxiv.org/abs/2507.10070)
*Yang Xiao,Mo Sun,Ziyu Song,Bing Tian,Jie Zhang,Jie Sun,Zeke Wang*

Main category: cs.DB

TL;DR: FlashANNS is a GPU-accelerated out-of-core graph-based ANNS system that improves throughput by overlapping I/O and computation.


<details>
  <summary>Details</summary>
Motivation: Existing disk-based ANNS systems suffer from suboptimal performance due to failing to overlap SSD accesses with distance computation and extended I/O latency caused by suboptimal I/O Stack.

Method: FlashANNS, a GPU-accelerated out-of-core graph-based ANNS system through I/O-compute overlapping, featuring Dependency-Relaxed asynchronous pipeline, Warp-Level concurrent SSD access, and Computation-I/O balanced graph degree Selection.

Result: FlashANNS achieves 2.3-5.9x higher throughput compared to existing SOTA methods with a single SSD, and further attains 2.7-12.2x throughput improvement in multi-SSD configurations at $\geq$95% recall@10 accuracy.

Conclusion: FlashANNS achieves 2.3-5.9x higher throughput compared to existing SOTA methods with a single SSD, and further attains 2.7-12.2x throughput improvement in multi-SSD configurations at $\geq$95% recall@10 accuracy.

Abstract: With the advancement of information retrieval, recommendation systems, and
Retrieval-Augmented Generation (RAG), Approximate Nearest Neighbor Search
(ANNS) gains widespread applications due to its higher performance and
accuracy. While several disk-based ANNS systems have emerged to handle
exponentially growing vector datasets, they suffer from suboptimal performance
due to two inherent limitations: 1) failing to overlap SSD accesses with
distance computation processes and 2) extended I/O latency caused by suboptimal
I/O Stack. To address these challenges, we present FlashANNS, a GPU-accelerated
out-of-core graph-based ANNS system through I/O-compute overlapping. Our core
insight lies in the synchronized orchestration of I/O and computation through
three key innovations: 1) Dependency-Relaxed asynchronous pipeline: FlashANNS
decouples I/O-computation dependencies to fully overlap between GPU distance
calculations and SSD data transfers. 2) Warp-Level concurrent SSD access:
FlashANNS implements a lock-free I/O stack with warp-level concurrency control,
to reduce the latency-induced time overhead. 3) Computation-I/O balanced graph
degree Selection: FlashANNS selects graph degrees via lightweight
compute-to-I/O ratio sampling, ensuring optimal balance between computational
load and storage access latency across different I/O bandwidth configurations.
We implement FlashANNS and compare it with state-of-the-art out-of-core ANNS
systems (SPANN, DiskANN) and a GPU-accelerated out-of-core ANNS system
(FusionANNS). Experimental results demonstrate that at $\geq$95\% recall@10
accuracy, our method achieves 2.3-5.9$\times$ higher throughput compared to
existing SOTA methods with a single SSD, and further attains 2.7-12.2$\times$
throughput improvement in multi-SSD configurations.

</details>


### [33] [LogLite: Lightweight Plug-and-Play Streaming Log Compression](https://arxiv.org/abs/2507.10337)
*Benzhao Tang,Shiyu Yang,Zhitao Shen,Wenjie Zhang,Xuemin Lin,Zhihong Tian*

Main category: cs.DB

TL;DR: This paper introduces LogLite, a new lossless log compression algorithm that is lightweight, adaptable, and achieves better compression ratio and speed compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The daily volume of log generation has surged to tens of petabytes, leading to significant collection and storage costs.

Method: propose LogLite, a lightweight, plug-and-play, streaming lossless compression algorithm designed to handle both TEXT and JSON logs throughout their life cycle. LogLite requires no predefined rules or pre-training and is inherently adaptable to evolving log structures.

Result: LogLite achieves Pareto optimality in most scenarios, delivering an average improvement of up to 67.8% in compression ratio and up to 2.7$\times$ in compression speed.

Conclusion: LogLite achieves Pareto optimality in most scenarios, delivering an average improvement of up to 67.8% in compression ratio and up to 2.7$\times$ in compression speed compared to state-of-the-art baselines.

Abstract: Log data is a vital resource for capturing system events and states. With the
increasing complexity and widespread adoption ofmodern software systems and IoT
devices, the daily volume of log generation has surged to tens of petabytes,
leading to significant collection and storage costs. To address this challenge,
lossless log compression has emerged as an effective solution, enabling
substantial resource savings without compromising log information. In this
paper, we first conduct a characterization study on extensive public log
datasets and identify four key observations. Building on these insights, we
propose LogLite, a lightweight, plug-and-play, streaming lossless compression
algorithm designed to handle both TEXT and JSON logs throughout their life
cycle. LogLite requires no predefined rules or pre-training and is inherently
adaptable to evolving log structures. Our evaluation shows that, compared to
state-of-the-art baselines, LogLite achieves Pareto optimality in most
scenarios, delivering an average improvement of up to 67.8% in compression
ratio and up to 2.7 $\times$ in compression speed.

</details>


### [34] [Instance-Optimized String Fingerprints](https://arxiv.org/abs/2507.10391)
*Mihail Stoian,Johannes Thürauf,Andreas Zimmerer,Alexander van Renen,Andreas Kipf*

Main category: cs.DB

TL;DR: String fingerprints are introduced as a lightweight secondary index structure to improve the efficiency of processing string columns in cloud data warehouses, achieving table-scan speedups.


<details>
  <summary>Details</summary>
Motivation: Cloud data warehouses are text-heavy, but their capabilities for efficiently processing string columns remain limited.

Method: Introduction of string fingerprints - a lightweight secondary index structure designed to approximate LIKE predicates.

Result: On an IMDb column evaluated in DuckDB v1.3, string fingerprints yield table-scan speedups of up to 1.36x.

Conclusion: String fingerprints can be optimized for specific workloads and generalize to unseen table predicates, yielding table-scan speedups.

Abstract: Recent research found that cloud data warehouses are text-heavy. However,
their capabilities for efficiently processing string columns remain limited,
relying primarily on techniques like dictionary encoding and prefix-based
partition pruning. In recent work, we introduced string fingerprints - a
lightweight secondary index structure designed to approximate LIKE predicates,
albeit with false positives. This approach is particularly compelling for
columnar query engines, where fingerprints can help reduce both compute and I/O
overhead. We show that string fingerprints can be optimized for specific
workloads using mixed-integer optimization, and that they can generalize to
unseen table predicates. On an IMDb column evaluated in DuckDB v1.3, this
yields table-scan speedups of up to 1.36$\times$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [35] [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Hossein A. Rahmani,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: The TREC Deep Learning track's fifth year found that Large Language Model prompting outperformed previous best approaches. Synthetic queries provided similar evaluation results to human queries.


<details>
  <summary>Details</summary>
Motivation: To get another matching test set, based on the larger, cleaner, less-biased v2 passage and document set, with passage ranking as primary and document ranking as a secondary task. The approach yields a more difficult test with more headroom for improvement.

Method: Leveraged the MS MARCO datasets and repeated last year's design, sampling from MS MARCO queries that were completely held out. Generated synthetic queries using a fine-tuned T5 model and using a GPT-4 prompt.

Result: Runs using LLM prompting outperformed runs using the "nnlm" approach. Evaluation using synthetic queries gave similar results to human queries, with system ordering agreement of 0.8487. No clear evidence of bias was observed.

Conclusion: Runs using Large Language Model (LLM) prompting outperformed runs that use the "nnlm" approach. Evaluation using synthetic queries gave similar results to human queries.

Abstract: This is the fifth year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of
human-annotated training labels available for both passage and document ranking
tasks. We mostly repeated last year's design, to get another matching test set,
based on the larger, cleaner, less-biased v2 passage and document set, with
passage ranking as primary and document ranking as a secondary task (using
labels inferred from passage). As we did last year, we sample from MS MARCO
queries that were completely held out, unused in corpus construction, unlike
the test queries in the first three years. This approach yields a more
difficult test with more headroom for improvement. Alongside the usual MS MARCO
(human) queries from MS MARCO, this year we generated synthetic queries using a
fine-tuned T5 model and using a GPT-4 prompt.
  The new headline result this year is that runs using Large Language Model
(LLM) prompting in some way outperformed runs that use the "nnlm" approach,
which was the best approach in the previous four years. Since this is the last
year of the track, future iterations of prompt-based ranking can happen in
other tracks. Human relevance assessments were applied to all query types, not
just human MS MARCO queries. Evaluation using synthetic queries gave similar
results to human queries, with system ordering agreement of $\tau=0.8487$.
However, human effort was needed to select a subset of the synthetic queries
that were usable. We did not see clear evidence of bias, where runs using GPT-4
were favored when evaluated using synthetic GPT-4 queries, or where runs using
T5 were favored when evaluated on synthetic T5 queries.

</details>


### [36] [GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval](https://arxiv.org/abs/2507.08945)
*Savini Kashmira,Jayanaka L. Dantanarayana,Krisztián Flautner,Lingjia Tang,Jason Mars*

Main category: cs.IR

TL;DR: GraphRunner, a novel graph-based retrieval framework, outperforms existing approaches by reducing reasoning errors and hallucinations through planning, verification, and execution, leading to significant performance improvements and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Conventional Retrieval Augmented Generation (RAG) approaches struggle with structured, interconnected datasets like knowledge graphs, where understanding underlying relationships is crucial for accurate retrieval. Existing iterative methods typically combine reasoning with single hop traversal at each step, making them vulnerable to LLM reasoning errors and hallucinations that ultimately hinder the retrieval of relevant information.

Method: a novel graph-based retrieval framework that operates in three distinct stages: planning, verification, and execution. This introduces high-level traversal actions that enable multi-hop exploration in a single step. It also generates a holistic traversal plan, which is verified against the graph structure and pre-defined traversal actions

Result: GraphRunner significantly reduces LLM reasoning errors and detects hallucinations through validation. Our evaluation using the GRBench dataset shows that GraphRunner consistently outperforms existing approaches, achieving 10-50% performance improvements over the strongest baseline while reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x

Conclusion: GraphRunner consistently outperforms existing approaches, achieving 10-50% performance improvements over the strongest baseline while reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x, making it significantly more robust and efficient for graph-based retrieval tasks.

Abstract: Conventional Retrieval Augmented Generation (RAG) approaches are common in
text-based applications. However, they struggle with structured, interconnected
datasets like knowledge graphs, where understanding underlying relationships is
crucial for accurate retrieval. A common direction in graph-based retrieval
employs iterative, rule-based traversal guided by Large Language Models (LLMs).
Such existing iterative methods typically combine reasoning with single hop
traversal at each step, making them vulnerable to LLM reasoning errors and
hallucinations that ultimately hinder the retrieval of relevant information.
  To address these limitations, we propose GraphRunner, a novel graph-based
retrieval framework that operates in three distinct stages: planning,
verification, and execution. This introduces high-level traversal actions that
enable multi-hop exploration in a single step. It also generates a holistic
traversal plan, which is verified against the graph structure and pre-defined
traversal actions, reducing reasoning errors and detecting hallucinations
before execution. GraphRunner significantly reduces LLM reasoning errors and
detects hallucinations through validation. Our evaluation using the GRBench
dataset shows that GraphRunner consistently outperforms existing approaches,
achieving 10-50% performance improvements over the strongest baseline while
reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,
making it significantly more robust and efficient for graph-based retrieval
tasks.

</details>


### [37] [DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
*Anthony Miyaguchi,Conor Johnston,Aaryan Potdar*

Main category: cs.IR

TL;DR: LLMs can debate well with related arguments but are verbose and consistent in evaluation.


<details>
  <summary>Details</summary>
Motivation: Study Large Language Models (LLMs) in the context of debating.

Method: Retrieval-Augmented Debate and Evaluation with six leading publicly available models from three providers

Result: LLMs perform well in debates when given related arguments, they tend to be verbose in responses yet consistent in evaluation.

Conclusion: LLMs perform well in debates when given related arguments, they tend to be verbose in responses yet consistent in evaluation.

Abstract: Large Language Models (LLMs) demonstrate strong conversational abilities. In
this Working Paper, we study them in the context of debating in two ways: their
ability to perform in a structured debate along with a dataset of arguments to
use and their ability to evaluate utterances throughout the debate. We deploy
six leading publicly available models from three providers for the
Retrieval-Augmented Debate and Evaluation. The evaluation is performed by
measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout
this task, we found that although LLMs perform well in debates when given
related arguments, they tend to be verbose in responses yet consistent in
evaluation. The accompanying source code for this paper is located at
https://github.com/dsgt-arc/touche-2025-rad.

</details>


### [38] [Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation](https://arxiv.org/abs/2507.09188)
*Bangcheng Sun,Yazhe Chen,Jilin Yang,Xiaodong Li,Hui Li*

Main category: cs.IR

TL;DR: 提出 REXHA，一种用于推荐解释生成的分层聚合检索增强方法，优于现有方法高达 12.6%，同时实现了高检索效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 LLM 的 ExRec 模型存在画像偏差和高检索开销的问题，阻碍了它们的部署。

Method: 提出了一种基于分层聚合的检索增强推荐解释生成方法 (REXHA)。设计了一个基于分层聚合的画像模块，全面考虑用户和项目的评论信息，分层总结和构建整体画像。此外，引入了一个高效的检索模块，使用两种类型的伪文档查询来检索相关评论，以增强推荐解释的生成，有效减少检索延迟并提高相关评论的召回率。

Result: 实验表明，该方法在解释质量方面优于现有方法高达 12.6%，同时实现了高检索效率。

Conclusion: 该方法在解释质量方面优于现有方法高达 12.6%，同时实现了高检索效率。

Abstract: Explainable Recommender System (ExRec) provides transparency to the
recommendation process, increasing users' trust and boosting the operation of
online services. With the rise of large language models (LLMs), whose extensive
world knowledge and nuanced language understanding enable the generation of
human-like, contextually grounded explanations, LLM-powered ExRec has gained
great momentum. However, existing LLM-based ExRec models suffer from profile
deviation and high retrieval overhead, hindering their deployment. To address
these issues, we propose Retrieval-Augmented Recommendation Explanation
Generation with Hierarchical Aggregation (REXHA). Specifically, we design a
hierarchical aggregation based profiling module that comprehensively considers
user and item review information, hierarchically summarizing and constructing
holistic profiles. Furthermore, we introduce an efficient retrieval module
using two types of pseudo-document queries to retrieve relevant reviews to
enhance the generation of recommendation explanations, effectively reducing
retrieval latency and improving the recall of relevant reviews. Extensive
experiments demonstrate that our method outperforms existing approaches by up
to 12.6% w.r.t. the explanation quality while achieving high retrieval
efficiency.

</details>


### [39] [Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval](https://arxiv.org/abs/2507.09331)
*Kirill Khrylchenko,Vladimir Baikalov,Sergei Makeev,Artem Matveev,Sergei Liamaev*

Main category: cs.IR

TL;DR: 这篇论文提出了一种改进的logQ校正方法，以减少在推荐系统中使用batch内负采样时引入的偏差，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，双塔神经网络常用于检索阶段，但当项目目录非常大时，全softmax变得不可行。常用的解决方法是采样softmax，但使用batch内负样本会引入偏差，即在batch中出现频率较高的项目（即热门项目）会受到更重的惩罚。为了解决这个问题，工业界广泛采用一种称为logQ校正的技术，但这种方法并不能完全消除偏差。

Method: 通过重新审视logQ校正的推导过程，发现其忽略了一个重要的细节：分母中的正样本不是蒙特卡洛采样的，而是以概率1存在的。基于此，提出了一个改进的校正公式。

Result: 在公共和私有数据集上的评估表明，该方法相对于标准的logQ校正具有持续的改进。

Conclusion: 这篇论文提出了一种改进的logQ校正公式，该公式考虑了正样本在分母中的特殊性，并引入了一个可解释的样本权重，该权重反映了模型的不确定性。

Abstract: Two-tower neural networks are a popular architecture for the retrieval stage
in recommender systems. These models are typically trained with a softmax loss
over the item catalog. However, in web-scale settings, the item catalog is
often prohibitively large, making full softmax infeasible. A common solution is
sampled softmax, which approximates the full softmax using a small number of
sampled negatives.
  One practical and widely adopted approach is to use in-batch negatives, where
negatives are drawn from items in the current mini-batch. However, this
introduces a bias: items that appear more frequently in the batch (i.e.,
popular items) are penalized more heavily.
  To mitigate this issue, a popular industry technique known as logQ correction
adjusts the logits during training by subtracting the log-probability of an
item appearing in the batch. This correction is derived by analyzing the bias
in the gradient and applying importance sampling, effectively twice, using the
in-batch distribution as a proposal distribution. While this approach improves
model quality, it does not fully eliminate the bias.
  In this work, we revisit the derivation of logQ correction and show that it
overlooks a subtle but important detail: the positive item in the denominator
is not Monte Carlo-sampled - it is always present with probability 1. We
propose a refined correction formula that accounts for this. Notably, our loss
introduces an interpretable sample weight that reflects the model's uncertainty
- the probability of misclassification under the current parameters. We
evaluate our method on both public and proprietary datasets, demonstrating
consistent improvements over the standard logQ correction.

</details>


### [40] [Balancing Semantic Relevance and Engagement in Related Video Recommendations](https://arxiv.org/abs/2507.09403)
*Amit Jaspal,Feng Zhang,Wei Chang,Sumit Kumar,Yubo Wang,Roni Mittleman,Qifan Wang,Weize Mao*

Main category: cs.IR

TL;DR: 提出了一种用于相关视频推荐的多目标检索框架，该框架通过多任务学习、多模态特征融合和非策略校正来平衡语义相关性和用户参与度，从而提高语义一致性并减少热门偏差。


<details>
  <summary>Details</summary>
Motivation: 相关视频推荐通常使用协作过滤 (CF)，由共同参与信号驱动，通常导致推荐缺乏语义一致性并表现出很强的受欢迎程度偏差。

Method: 一种新颖的多目标检索框架，增强了标准双塔模型，以显式地平衡语义相关性和用户参与度，结合了多任务学习 (MTL)、多模态内容特征融合（文本和视觉嵌入）和通过逆倾向加权 (OPC) 进行的非策略校正。

Result: 在工业规模数据和为期两周的实际 A/B 测试中，语义相关性显着提高（主题匹配率从 51% 提高到 63%），热门项目分布减少（热门视频推荐减少 -13.8%），并且我们的首要用户参与度指标提高了 +0.04%。

Conclusion: 该方法成功实现了更好的语义一致性、平衡的参与度以及实际部署的实用可扩展性。

Abstract: Related video recommendations commonly use collaborative filtering (CF)
driven by co-engagement signals, often resulting in recommendations lacking
semantic coherence and exhibiting strong popularity bias. This paper introduces
a novel multi-objective retrieval framework, enhancing standard two-tower
models to explicitly balance semantic relevance and user engagement. Our
approach uniquely combines: (a) multi-task learning (MTL) to jointly optimize
co-engagement and semantic relevance, explicitly prioritizing topical
coherence; (b) fusion of multimodal content features (textual and visual
embeddings) for richer semantic understanding; and (c) off-policy correction
(OPC) via inverse propensity weighting to effectively mitigate popularity bias.
Evaluation on industrial-scale data and a two-week live A/B test reveals our
framework's efficacy. We observed significant improvements in semantic
relevance (from 51% to 63% topic match rate), a reduction in popular item
distribution (-13.8% popular video recommendations), and a +0.04% improvement
in our topline user engagement metric. Our method successfully achieves better
semantic coherence, balanced engagement, and practical scalability for
real-world deployment.

</details>


### [41] [Item-centric Exploration for Cold Start Problem](https://arxiv.org/abs/2507.09423)
*Dong Wang,Junyi Jiao,Arnab Bhadury,Yaping Zhang,Mingyan Gao,Onkar Dalal*

Main category: cs.IR

TL;DR: This paper proposes an item-centric recommendation method to improve cold-start performance by finding the optimal users for new items, leading to better user satisfaction and exploration efficiency.


<details>
  <summary>Details</summary>
Motivation: Recommender systems struggle with the item cold-start problem, limiting content diversity and exacerbating popularity bias. Traditional user-centric approaches can obscure the ideal audience for new content.

Method: An item-centric control integrated into an exploration system, employing a Bayesian model with Beta distributions.

Result: Online evaluations show improved cold-start targeting efficacy, enhanced user satisfaction with newly explored content, and increased overall exploration efficiency.

Conclusion: This paper introduces an item-centric recommendation approach to address the item cold-start problem. An item-centric control using a Bayesian model with Beta distributions is proposed.

Abstract: Recommender systems face a critical challenge in the item cold-start problem,
which limits content diversity and exacerbates popularity bias by struggling to
recommend new items. While existing solutions often rely on auxiliary data, but
this paper illuminates a distinct, yet equally pressing, issue stemming from
the inherent user-centricity of many recommender systems. We argue that in
environments with large and rapidly expanding item inventories, the traditional
focus on finding the "best item for a user" can inadvertently obscure the ideal
audience for nascent content. To counter this, we introduce the concept of
item-centric recommendations, shifting the paradigm to identify the optimal
users for new items. Our initial realization of this vision involves an
item-centric control integrated into an exploration system. This control
employs a Bayesian model with Beta distributions to assess candidate items
based on a predicted balance between user satisfaction and the item's inherent
quality. Empirical online evaluations reveal that this straightforward control
markedly improves cold-start targeting efficacy, enhances user satisfaction
with newly explored content, and significantly increases overall exploration
efficiency.

</details>


### [42] [Does UMBRELA Work on Other LLMs?](https://arxiv.org/abs/2507.09483)
*Naghmeh Farzi,Laura Dietz*

Main category: cs.IR

TL;DR: UMBRELA框架在不同LLM上的表现有差异，DeepSeek V3最佳。


<details>
  <summary>Details</summary>
Motivation: 评估UMBRELA LLM Judge评估框架的通用性，考察LLM的选择如何影响相关性评估的准确性。

Method: 在多个LLM上重现UMBRELA LLM Judge评估框架。

Result: DeepSeek V3性能与GPT-4o相当，LLaMA-3.3-70B性能稍低，更小的LLM性能更差。

Conclusion: UMBRELA框架在不同LLM上的泛化能力：DeepSeek V3与GPT-4o性能相当，LLaMA-3.3-70B略逊，更小的LLM性能下降。

Abstract: We reproduce the UMBRELA LLM Judge evaluation framework across a range of
large language models (LLMs) to assess its generalizability beyond the original
study. Our investigation evaluates how LLM choice affects relevance assessment
accuracy, focusing on leaderboard rank correlation and per-label agreement
metrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very
comparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we
obtain slightly lower performance, which further degrades with smaller LLMs.

</details>


### [43] [Criteria-Based LLM Relevance Judgments](https://arxiv.org/abs/2507.09488)
*Naghmeh Farzi,Laura Dietz*

Main category: cs.IR

TL;DR: Proposes a Multi-Criteria framework for LLM-based relevance judgments to improve the robustness and interpretability of retrieval evaluations compared to direct grading methods.


<details>
  <summary>Details</summary>
Motivation: Traditional human-annotated labels are time-consuming and expensive. As a result, many researchers turn to automatic alternatives to accelerate method development. Among these, Large Language Models (LLMs) provide a scalable solution by generating relevance labels directly through prompting. However, prompting an LLM for a relevance label without constraints often results in not only incorrect predictions but also outputs that are difficult for humans to interpret.

Method: Multi-Criteria framework for LLM-based relevance judgments, decomposing the notion of relevance into multiple criteria--such as exactness, coverage, topicality, and contextual fit

Result: Results demonstrate that Multi-Criteria judgments enhance the system ranking/leaderboard performance. Moreover, highlight the strengths and limitations of this approach relative to direct grading approaches, offering insights that can guide the development of future automatic evaluation frameworks in information retrieval.

Conclusion: Multi-Criteria judgments enhance the system ranking/leaderboard performance.

Abstract: Relevance judgments are crucial for evaluating information retrieval systems,
but traditional human-annotated labels are time-consuming and expensive. As a
result, many researchers turn to automatic alternatives to accelerate method
development. Among these, Large Language Models (LLMs) provide a scalable
solution by generating relevance labels directly through prompting. However,
prompting an LLM for a relevance label without constraints often results in not
only incorrect predictions but also outputs that are difficult for humans to
interpret. We propose the Multi-Criteria framework for LLM-based relevance
judgments, decomposing the notion of relevance into multiple criteria--such as
exactness, coverage, topicality, and contextual fit--to improve the robustness
and interpretability of retrieval evaluations compared to direct grading
methods. We validate this approach on three datasets: the TREC Deep Learning
tracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our
results demonstrate that Multi-Criteria judgments enhance the system
ranking/leaderboard performance. Moreover, we highlight the strengths and
limitations of this approach relative to direct grading approaches, offering
insights that can guide the development of future automatic evaluation
frameworks in information retrieval.

</details>


### [44] [Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems](https://arxiv.org/abs/2507.09566)
*Timo Wilm,Philipp Normann*

Main category: cs.IR

TL;DR: 该论文提出了一种用于识别与在线影响相一致的离线指标的实用策略，并通过 OTTO 电子商务平台上的大规模在线实验验证了该策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的一个关键挑战是建立可靠的离线和在线指标之间的关系，以预测真实世界的性能。受 Pareto 前沿逼近的最新进展的推动。

Method: 引入了一种实用的策略，用于识别与在线影响相一致的离线指标。该方法对于具有神经网络主干的系统是模型无关的，从而实现了跨架构和领域的广泛适用性。

Result: 在线实验确定了离线指标与真实点击率、点击后转化率和已售单位之间的显着一致性。

Conclusion: 该策略为行业从业者提供了一个有价值的工具，可以理解离线到在线的指标关系，并做出明智的、数据驱动的决策。

Abstract: A critical challenge in recommender systems is to establish reliable
relationships between offline and online metrics that predict real-world
performance. Motivated by recent advances in Pareto front approximation, we
introduce a pragmatic strategy for identifying offline metrics that align with
online impact. A key advantage of this approach is its ability to
simultaneously serve multiple test groups, each with distinct offline
performance metrics, in an online experiment controlled by a single model. The
method is model-agnostic for systems with a neural network backbone, enabling
broad applicability across architectures and domains. We validate the strategy
through a large-scale online experiment in the field of session-based
recommender systems on the OTTO e-commerce platform. The online experiment
identifies significant alignments between offline metrics and real-word
click-through rate, post-click conversion rate and units sold. Our strategy
provides industry practitioners with a valuable tool for understanding
offline-to-online metric relationships and making informed, data-driven
decisions.

</details>


### [45] [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
*Tuan-Luc Huynh,Thuy-Trang Vu,Weiqing Wang,Trung Le,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: MixLoRA-DSI 通过结合 LoRA 专家混合和 OOD 驱动的扩展策略，实现了高效的模型更新，优于全模型更新，且参数开销和训练成本更低。


<details>
  <summary>Details</summary>
Motivation: 在生成式检索中，在资源约束下，由于完全重新训练计算成本高且不切实际，因此使用新文档不断更新基于模型的索引仍然具有挑战性。

Method: 结合了可扩展的低秩适应专家混合和分层异分布（OOD）驱动的扩展策略。

Result: 在 NQ320k 和 MS MARCO Passage 上的实验表明，MixLoRA-DSI 优于全模型更新基线。

Conclusion: MixLoRA-DSI在参数开销最小和训练成本大幅降低的情况下，优于全模型更新基线。

Abstract: Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

</details>


### [46] [Non-parametric Graph Convolution for Re-ranking in Recommendation Systems](https://arxiv.org/abs/2507.09969)
*Zhongyu Ouyang,Mingxuan Ju,Soroush Vosoughi,Yanfang Ye*

Main category: cs.IR

TL;DR: 该论文提出了一种在测试时使用图卷积进行重新排序的非参数策略，以提高推荐系统的性能，同时最大限度地减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 尽管在推荐系统中，图知识已被证明在提高项目排名方面有效，特别是在检索阶段，但它在排名阶段的应用，尤其是在用户-项目交互中提供更丰富的上下文信息时，仍未得到充分探索。一个主要的挑战在于从存储在分布式系统中的数十亿个项目中重复检索邻域信息相关的巨大计算成本。这种资源密集型需求使得在实际 RecSys 中扩展基于图的方法变得困难。

Method: 该论文提出了一种非参数策略，该策略利用图卷积在测试时进行重新排序。该策略避免了训练期间图卷积带来的巨大计算开销，并在测试期间动态地利用图中隐藏的结构知识。它可以作为一个即插即用模块使用，并且可以很容易地用于提高具有显着降低的计算开销的真实 RecSys 的各种排序层的排序能力。

Result: 在具有不同稀疏程度的四个基准数据集上进行的综合实验表明，该策略在测试时产生了显着改进（即平均 8.1%），而几乎没有额外的计算开销（即平均 0.5%）。

Conclusion: 该论文提出了一种非参数策略，该策略仅在测试时使用图卷积进行重新排序，从而在几乎没有额外计算开销的情况下，在测试时产生了显着的改进（平均 8.1%）。

Abstract: Graph knowledge has been proven effective in enhancing item rankings in
recommender systems (RecSys), particularly during the retrieval stage. However,
its application in the ranking stage, especially when richer contextual
information in user-item interactions is available, remains underexplored. A
major challenge lies in the substantial computational cost associated with
repeatedly retrieving neighborhood information from billions of items stored in
distributed systems. This resource-intensive requirement makes it difficult to
scale graph-based methods in practical RecSys. To bridge this gap, we first
demonstrate that incorporating graphs in the ranking stage improves ranking
qualities. Notably, while the improvement is evident, we show that the
substantial computational overheads entailed by graphs are prohibitively
expensive for real-world recommendations. In light of this, we propose a
non-parametric strategy that utilizes graph convolution for re-ranking only
during test time. Our strategy circumvents the notorious computational
overheads from graph convolution during training, and utilizes structural
knowledge hidden in graphs on-the-fly during testing. It can be used as a
plug-and-play module and easily employed to enhance the ranking ability of
various ranking layers of a real-world RecSys with significantly reduced
computational overhead. Through comprehensive experiments across four benchmark
datasets with varying levels of sparsity, we demonstrate that our strategy
yields noticeable improvements (i.e., 8.1% on average) during testing time with
little to no additional computational overheads (i.e., 0.5 on average). Code:
https://github.com/zyouyang/RecSys2025_NonParamGC.git

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: RE is a new learning paradigm that advances beyond conventional ML and DL by learning from the evolving behavior of models themselves, enabling iterative self-improvement.


<details>
  <summary>Details</summary>
Motivation: conventional Machine Learning (ML) and Deep Learning (DL) focuses on learning from static data representations

Method: Recurrent Expansion (RE), Multiverse RE (MVRE), Heterogeneous MVRE (HMVRE), Scalable and adaptive variant, Sc-HMVRE

Result: RE enables iterative self-improvement, allowing each model version to gain insight from its predecessors.

Conclusion: RE presents a shift in DL: from purely representational learning to behavior-aware, self-evolving systems. It lays the groundwork for a new class of intelligent models capable of reasoning over their own learning dynamics, offering a path toward scalable, introspective, and adaptive artificial intelligence.

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [48] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 提出了一种高效的 TMR 方法，该方法使用可解释人工智能 (XAI) 方法来增强 DNN 抵抗位翻转错误的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，深度神经网络 (DNN) 得到了广泛应用，确保其可靠性至关重要。三重模块冗余 (TMR) 是一种有效的技术，可以增强 DNN 在存在位翻转故障时的可靠性。为了处理 TMR 的巨大开销，它有选择地应用于模型输出中贡献最高的参数和组件。

Method: 利用一种低成本的、基于梯度的 XAI 技术，即逐层相关性传播 (LRP) 来计算 DNN 参数的重要性得分。

Result: 该方法在两个 DNN 模型 VGG16 和 AlexNet 上进行了评估，使用了 MNIST 和 CIFAR-10 等数据集。结果表明，该方法能够在 10-4 的误码率下保护 AlexNet 模型，在保持与现有技术相同开销的情况下，可靠性提高 60% 以上。

Conclusion: 该方法能够在保持与现有技术相同开销的情况下，将 AlexNet 模型在 10-4 的误码率下，可靠性提高 60% 以上。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [49] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: A voice-based decision support system using machine learning models helps farmers in Karnataka, India, to make profitable crop choices despite market and climate volatility.


<details>
  <summary>Details</summary>
Motivation: Farmers in developing regions face market and climate volatility and are excluded from the digital revolution due to literacy barriers.

Method: A hybrid recommendation engine that integrates a Random Forest classifier for agronomic suitability and a Long Short-Term Memory (LSTM) network to forecast market prices, delivered through a voice-based interface in the local Kannada language.

Result: The Random Forest model achieves 98.5% accuracy in suitability prediction, while the LSTM model forecasts harvest-time prices with a low margin of error.

Conclusion: This work provides a scalable solution to enhance financial resilience of marginalized farming communities by delivering data-driven, economically optimized recommendations through an inclusive interface.

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [50] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: This paper analyzes LoRA's inconsistent speed improvements, proposes more efficient fine-tuning methods, and demonstrates their superior performance.


<details>
  <summary>Details</summary>
Motivation: LoRA does not consistently provide speed improvements across all model architectures and training setups.

Method: The paper conducts a comprehensive analysis of LoRA's performance and investigates the underlying factors limiting its speedup. Based on these findings, the paper proposes several methods for more efficient fine-tuning of LLMs. These methods are then empirically evaluated and compared to LoRA.

Result: The proposed approach achieves comparable or superior performance to LoRA while delivering more consistent training speed improvements.

Conclusion: The paper proposes several methods for more efficient fine-tuning of LLMs, achieving comparable or superior performance to LoRA with more consistent training speed improvements.

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [51] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: This paper introduces a PINN framework to simulate pollutant dispersion, offering a scalable and flexible alternative to traditional solvers.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods often struggle with the complexity and scale of modeling pollutant transport across vast and dynamic oceanic domains.

Method: A Physics-Informed Neural Network (PINN) framework is used to simulate the dispersion of pollutants governed by the 2D advection-diffusion equation.

Result: The model achieves physically consistent predictions by embedding physical laws and fitting to noisy synthetic data.

Conclusion: The PINN framework offers a scalable and flexible alternative to traditional solvers for simulating pollutant dispersion.

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [52] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer神经网络的洗钱检测新方法，该方法通过对比学习时间序列表示，并使用双阈值方法控制误报率，实验结果表明该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决洗钱检测问题。

Method: 利用Transformer神经网络，通过对比学习学习时间序列的表示，并生成洗钱评分。

Result: 实验表明，Transformer能够生成通用表示，成功利用洗钱模式，且在最小化领域专家监督的情况下，检测非欺诈者和欺诈者的能力更强，同时保持误报率在可控范围内。

Conclusion: 该论文提出的方法能够有效检测洗钱活动，同时控制误报率，优于基于规则或LSTM架构的方法。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [53] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: 本研究评估了 CompactifAI 压缩 Llama 3.1 8B 模型的性能，发现该方法在显著降低计算资源消耗的同时，保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: 评估 Multiverse Computing 开发的名为 CompactifAI 的压缩方法应用于大型语言模型 Llama 3.1 8B 的性能。

Method: 使用 Codecarbon 和 Ragas 框架分别评估模型效率（能源消耗）和准确性。比较了使用 CompactifAI 压缩的模型及其完整版本。

Result: 压缩后的模型在使用 CompactifAI 后，显著降低了计算资源，同时保持了模型精度。

Conclusion: 压缩后的模型在使用 CompactifAI 后，显著减少了计算资源，同时保持了模型精度，使模型更高效、可扩展且更具成本效益。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [54] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: wd1 是一种用于 dLLM 推理的强化学习新方法，无需监督微调即可实现更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 通过强化学习 (RL) 提高基于扩散的大型语言模型 (dLLM) 的推理能力仍然是一个开放的问题。dLLM 似然函数的难处理性需要在每个策略优化步骤中近似当前、旧的和参考策略似然。这种依赖性带来了额外的计算开销，并可能导致较大的偏差——尤其是在重要性采样的策略比率的 denominator 中出现近似误差时。

Method: 提出了一种新的策略优化方法，将目标重新定义为加权似然，只需要对当前参数化策略似然进行一次近似。

Result: 在广泛使用的推理基准上进行的实验表明，wd1 在没有监督微调 (SFT) 或任何监督数据的情况下，优于现有的 dLLM 的 RL 方法，实现了高达 16% 的准确率提升。wd1 提供了额外的计算增益，包括减少了训练时间和每个梯度步骤的函数评估次数 (NFE)。

Conclusion: wd1 是一种更有效和高效的将 RL 应用于 dLLMs 推理的方法，因为它实现简单，并且类似于 R1-Zero 的训练（没有 SFT）。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [55] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 该研究提出了一种新的Transformer模型，利用阿兹海默症数据来帮助诊断路易体病，解决了数据少和数据差异大的问题。


<details>
  <summary>Details</summary>
Motivation: 路易体病（LBD）是一种常见但研究不足的痴呆症，对公共健康构成重大负担。LBD诊断的一个主要障碍是数据稀缺，这限制了深度学习的有效性。相比之下，AD数据集更为丰富，为知识转移提供了潜力。然而，LBD和AD数据通常使用不同的机器和协议从不同的站点收集，从而导致明显的领域偏移。

Method: 该方法提出了一种可转移性感知Transformer（TAT），它基于注意力机制，自适应地为疾病可转移特征分配更大的权重，同时抑制特定于领域的特征，从而减少领域偏移。

Result: 实验结果表明了TAT的有效性。

Conclusion: 该研究提出了一种新的领域自适应方法，用于在数据稀缺和领域偏移的情况下，利用阿尔茨海默病（AD）数据来改善路易体病（LBD）的诊断，为罕见疾病的领域自适应诊断提供了一个有前景的框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>
