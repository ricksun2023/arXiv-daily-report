<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 29]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 17]
- [cs.LG](#cs.LG) [Total: 29]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Confidence Estimation for Text-to-SQL in Large Language Models](https://arxiv.org/abs/2508.14056)
*Sepideh Entezari Maleki,Mohammadreza Pourreza,Davood Rafiei*

Main category: cs.CL

TL;DR: Confidence estimation for text-to-SQL using LLMs, explores black-box and white-box strategies, consistency-based methods work better for black-box, SQL-syntax-aware for white-box, execution-based grounding helps both.


<details>
  <summary>Details</summary>
Motivation: assess the reliability of model-generated SQL queries without having access to gold answers

Method: black-box and white-box confidence estimation strategies

Result: consistency-based methods among black-box models have superior performance, SQL-syntax-aware approaches for interpreting LLM logits in white-box settings have advantage

Conclusion: Execution-based grounding of queries provides a valuable supplementary signal, improving the effectiveness of both approaches.

Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of
model-generated SQL queries without having access to gold answers. We study
this problem in the context of large language models (LLMs), where access to
model weights and gradients is often constrained. We explore both black-box and
white-box confidence estimation strategies, evaluating their effectiveness on
cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior
performance of consistency-based methods among black-box models and the
advantage of SQL-syntax-aware approaches for interpreting LLM logits in
white-box settings. Furthermore, we show that execution-based grounding of
queries provides a valuable supplementary signal, improving the effectiveness
of both approaches.

</details>


### [2] [From Image Captioning to Visual Storytelling](https://arxiv.org/abs/2508.14045)
*Admitos Passadakis,Yingjin Song,Albert Gatt*

Main category: cs.CL

TL;DR: This paper proposes a new framework for visual storytelling that integrates image captioning and language-to-language methods. The results show that this approach improves the quality of the generated stories, accelerates training time, and is more reusable and reproducible. The paper also introduces a new metric, ideality, to evaluate the human-likeness of visual storytelling results.


<details>
  <summary>Details</summary>
Motivation: Visual Storytelling is a challenging multimodal task between Vision & Language, where the purpose is to generate a story for a stream of images. Its difficulty lies on the fact that the story should be both grounded to the image sequence but also narrative and coherent. The aim of this work is to balance between these aspects, by treating Visual Storytelling as a superset of Image Captioning

Method: employ a vision-to-language model for obtaining captions of the input images, and then, these captions are transformed into coherent narratives using language-to-language methods

Result: integrating captioning and storytelling under a unified framework, has a positive impact on the quality of the produced stories. compared to numerous previous studies, this approach accelerates training time and makes our framework readily reusable and reproducible by anyone interested

Conclusion: Integrating captioning and storytelling under a unified framework, has a positive impact on the quality of the produced stories. This approach accelerates training time and makes our framework readily reusable and reproducible. A new metric/tool, named ideality, can be used to simulate how far some results are from an oracle model, and we apply it to emulate human-likeness in visual storytelling.

Abstract: Visual Storytelling is a challenging multimodal task between Vision &
Language, where the purpose is to generate a story for a stream of images. Its
difficulty lies on the fact that the story should be both grounded to the image
sequence but also narrative and coherent. The aim of this work is to balance
between these aspects, by treating Visual Storytelling as a superset of Image
Captioning, an approach quite different compared to most of prior relevant
studies. This means that we firstly employ a vision-to-language model for
obtaining captions of the input images, and then, these captions are
transformed into coherent narratives using language-to-language methods. Our
multifarious evaluation shows that integrating captioning and storytelling
under a unified framework, has a positive impact on the quality of the produced
stories. In addition, compared to numerous previous studies, this approach
accelerates training time and makes our framework readily reusable and
reproducible by anyone interested. Lastly, we propose a new metric/tool, named
ideality, that can be used to simulate how far some results are from an oracle
model, and we apply it to emulate human-likeness in visual storytelling.

</details>


### [3] [Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach](https://arxiv.org/abs/2508.14051)
*Kezia Oketch,John P. Lalor,Ahmed Abbasi*

Main category: cs.CL

TL;DR: This paper presents a new Swahili NLP dataset and a taxonomy-guided evaluation method, revealing the impact of sociolinguistic variation on model performance.


<details>
  <summary>Details</summary>
Motivation: This paper introduces the first taxonomy-guided evaluation of Swahili NLP, addressing gaps in sociolinguistic diversity.

Method: The study develops a structured taxonomy and uses it as a lens for examining model prediction errors across pre-trained and instruction-tuned language models.

Result: The data exhibits tribal influences, urban vernacular, code-mixing, and loanwords.

Conclusion: The study advances culturally grounded evaluation frameworks and highlights the role of sociolinguistic variation in shaping model performance.

Abstract: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing
gaps in sociolinguistic diversity. Drawing on health-related psychometric
tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers.
The data exhibits tribal influences, urban vernacular, code-mixing, and
loanwords. We develop a structured taxonomy and use it as a lens for examining
model prediction errors across pre-trained and instruction-tuned language
models. Our findings advance culturally grounded evaluation frameworks and
highlight the role of sociolinguistic variation in shaping model performance.

</details>


### [4] [Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach](https://arxiv.org/abs/2508.14054)
*Yiran Rex Ma*

Main category: cs.CL

TL;DR: 本文研究了英汉新闻语料中状语功能块的位置差异，发现英语后置，汉语前置，且语序会根据信息和语用目的进行调整。


<details>
  <summary>Details</summary>
Motivation: 探讨英汉新闻在状语功能块的成分顺序上的差异。

Method: 基于大型语言模型（LLM）标注的可比英汉新闻语料库，探讨英汉新闻在状语功能块的成分顺序上的差异，并分析其典型的语序偏好和分布模式。

Result: （1）英语新闻偏爱先核心信息的线性叙述，功能块大多后置，而汉语新闻偏爱先背景的整体呈现模式，功能块通常前置；（2）在SVO结构中，英语和汉语新闻在功能块的分布上都表现出差异，但汉语前置的倾向更为显著，而英语后置的倾向相对温和；（3）当功能块共现时，英语和汉语新闻都表现出高度的灵活性，语序调整由信息和语用目的驱动。

Conclusion: 英语和汉语新闻在功能块的位置选择上既有系统偏好又有动态适应性，为英汉信息结构的对比研究提供了新的经验支持。

Abstract: Based on comparable English-Chinese news corpora annotated by Large Language
Model (LLM), this paper attempts to explore the differences in constituent
order of English-Chinese news from the perspective of functional chunks with
adverbial roles, and analyze their typical positional preferences and
distribution patterns. It is found that: (1) English news prefers linear
narrative of core information first, and functional chunks are mostly
post-positioned, while Chinese news prefers overall presentation mode of
background first, and functional chunks are often pre-positioned; (2) In SVO
structure, both English and Chinese news show differences in the distribution
of functional chunks, but the tendency of Chinese pre-positioning is more
significant, while that of English post-positioning is relatively mild; (3)
When function blocks are co-occurring, both English and Chinese news show high
flexibility, and the order adjustment is driven by information and pragmatic
purposes. The study reveals that word order has both systematic preference and
dynamic adaptability, providing new empirical support for contrastive study of
English-Chinese information structure.

</details>


### [5] [T-REX: Table -- Refute or Entail eXplainer](https://arxiv.org/abs/2508.14055)
*Tim Luka Horstmann,Baptiste Geisenberger,Mehwish Alam*

Main category: cs.CL

TL;DR: T-REX: a tool for claim verification over tables using LLMs, designed for non-experts, and openly available.


<details>
  <summary>Details</summary>
Motivation: Verifying textual claims against structured tabular data is a critical yet challenging task and current solutions remain inaccessible to non-experts.

Method: a live, interactive tool for claim verification over multimodal, multilingual tables using state-of-the-art instruction-tuned reasoning LLMs

Result: T-REX empowers non-experts by providing access to advanced fact-checking technology.

Conclusion: T-REX is openly available online.

Abstract: Verifying textual claims against structured tabular data is a critical yet
challenging task in Natural Language Processing with broad real-world impact.
While recent advances in Large Language Models (LLMs) have enabled significant
progress in table fact-checking, current solutions remain inaccessible to
non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer),
the first live, interactive tool for claim verification over multimodal,
multilingual tables using state-of-the-art instruction-tuned reasoning LLMs.
Designed for accuracy and transparency, T-REX empowers non-experts by providing
access to advanced fact-checking technology. The system is openly available
online.

</details>


### [6] [Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models](https://arxiv.org/abs/2508.14062)
*Badrinath Ramakrishnan,Akshaya Balaji*

Main category: cs.CL

TL;DR: This paper analyzes data memorization in fine-tuned LLMs, proposes a privacy protection framework, and demonstrates that the techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.


<details>
  <summary>Details</summary>
Motivation: LLMs' tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes.

Method: The paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. It uses controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2.

Result: Fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. The proposed techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.

Conclusion: The paper introduces and evaluates four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. These techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks, but their tendency to memorize
training data poses significant privacy risks, particularly during fine-tuning
processes. This paper presents a comprehensive empirical analysis of data
memorization in fine-tuned LLMs and introduces a novel multi-layered privacy
protection framework. Through controlled experiments on modern LLM
architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that
fine-tuning with repeated sensitive data increases privacy leakage rates from
baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across
tested models. We propose and rigorously evaluate four complementary privacy
protection methods: semantic data deduplication, differential privacy during
generation, entropy-based filtering, and pattern-based content filtering. Our
experimental results show that these techniques can reduce data leakage to 0%
while maintaining 94.7% of original model utility.

</details>


### [7] [Punctuation and Predicates in Language Models](https://arxiv.org/abs/2508.14067)
*Sonakshi Chauhan,Maheep Chaudhary,Koby Choy,Samuel Nellessen,Nandi Schoots*

Main category: cs.CL

TL;DR: This paper analyzes how LLMs process information across layers, focusing on punctuation and reasoning rules, revealing model-specific differences and insights into interpretability.


<details>
  <summary>Details</summary>
Motivation: This paper explores where information is collected and how it is propagated throughout layers in large language models (LLMs).

Method: The paper uses intervention-based techniques, interchange intervention, and layer-swapping experiments.

Result: The results show stark model-specific differences: for GPT-2, punctuation is both necessary and sufficient in multiple layers, while this holds far less in DeepSeek and not at all in Gemma. Conditional statements (if, then), and universal quantification (for all) are processed very differently.

Conclusion: The paper's findings offer new insight into the internal mechanisms of punctuation usage and reasoning in LLMs and have implications for interpretability.

Abstract: In this paper we explore where information is collected and how it is
propagated throughout layers in large language models (LLMs). We begin by
examining the surprising computational importance of punctuation tokens which
previous work has identified as attention sinks and memory aids. Using
intervention-based techniques, we evaluate the necessity and sufficiency (for
preserving model performance) of punctuation tokens across layers in GPT-2,
DeepSeek, and Gemma. Our results show stark model-specific differences: for
GPT-2, punctuation is both necessary and sufficient in multiple layers, while
this holds far less in DeepSeek and not at all in Gemma. Extending beyond
punctuation, we ask whether LLMs process different components of input (e.g.,
subjects, adjectives, punctuation, full sentences) by forming early static
summaries reused across the network, or if the model remains sensitive to
changes in these components across layers. Extending beyond punctuation, we
investigate whether different reasoning rules are processed differently by
LLMs. In particular, through interchange intervention and layer-swapping
experiments, we find that conditional statements (if, then), and universal
quantification (for all) are processed very differently. Our findings offer new
insight into the internal mechanisms of punctuation usage and reasoning in LLMs
and have implications for interpretability.

</details>


### [8] [DLLMQuant: Quantizing Diffusion-based Large Language Models](https://arxiv.org/abs/2508.14090)
*Chen Xu,Dawei Yang*

Main category: cs.CL

TL;DR: This paper proposes DLLMQuant, a PTQ framework tailored for DLLMs, to address the accuracy degradation issues when applying PTQ to DLLMs. It incorporates three novel techniques: TMAS, IA-AQ, and CGQ. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.


<details>
  <summary>Details</summary>
Motivation: Post-training quantization (PTQ) suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs. This paper explores how DLLMs' key mechanisms clash with quantization, identifying three core issues related to iterative generation, dynamic masking, and feature distribution.

Method: a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs.

Result: DLLMQuant achieves significant performance gains while enhancing efficiency.

Conclusion: DLLMQuant achieves significant performance gains while enhancing efficiency.

Abstract: Diffusion-based large language models (DLLMs) have shown promise for
non-autoregressive text generation, but their deployment is constrained by
large model sizes and heavy computational costs. Post-training quantization
(PTQ), a widely used method for compressing and accelerating Large Language
Models (LLMs), suffers from severe accuracy degradation and reduced
generalization performance when directly applied to DLLMs (e.g., AWQ suffers a
16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key
mechanisms - dynamic masking, iterative generation, bidirectional attention -
clash with quantization. We identify three core issues: 1) Iterative generation
and dynamic masking ratios lead to distinct token distributions across decoding
steps, which are not adequately captured by existing PTQ calibration methods;
2) Quantization errors are accumulated and amplified progressively during
iteration in DLLMs, causing quantized models to perform worse as decoding steps
progress; 3) Unmasked tokens stabilize while masked remain probabilistic,
making overall feature distribution incompatible with existing PTQ methods. To
address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,
which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling
(TMAS), a calibration method that accounts for both time and mask factors, with
the capacity to capture distributions across timesteps. 2) Interaction-Aware
Activation Quantization (IA-AQ), which utilizes bidirectional attention's
interaction signals to dynamically allocate quantization resources. 3)
Certainty-Guided Quantization (CGQ), which integrates mask status and token
scores as key weighting criteria into error compensation, making weight
quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves
significant performance gains while enhancing efficiency.

</details>


### [9] [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
*Xian Gao,Jiacheng Ruan,Zongyun Zhang,Jingsheng Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.CL

TL;DR: MMReview: A comprehensive benchmark for evaluating LLMs and MLLMs in multimodal peer review, addressing the lack of unified evaluation in the field.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables.

Method: We propose MMReview, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories.

Result: Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark.

Conclusion: MMReview is a critical step toward establishing a standardized foundation for the development of automated peer review systems.

Abstract: With the rapid growth of academic publications, peer review has become an
essential yet time-consuming responsibility within the research community.
Large Language Models (LLMs) have increasingly been adopted to assist in the
generation of review comments; however, current LLM-based review tasks lack a
unified evaluation benchmark to rigorously assess the models' ability to
produce comprehensive, accurate, and human-aligned assessments, particularly in
scenarios involving multimodal content such as figures and tables. To address
this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans
multiple disciplines and modalities. MMReview includes multimodal content and
expert-written review comments for 240 papers across 17 research domains within
four major academic disciplines: Artificial Intelligence, Natural Sciences,
Engineering Sciences, and Social Sciences. We design a total of 13 tasks
grouped into four core categories, aimed at evaluating the performance of LLMs
and Multimodal LLMs (MLLMs) in step-wise review generation, outcome
formulation, alignment with human preferences, and robustness to adversarial
input manipulation. Extensive experiments conducted on 16 open-source models
and 5 advanced closed-source models demonstrate the thoroughness of the
benchmark. We envision MMReview as a critical step toward establishing a
standardized foundation for the development of automated peer review systems.

</details>


### [10] [DPad: Efficient Diffusion Language Models with Suffix Dropout](https://arxiv.org/abs/2508.14148)
*Xinhua Chen,Sitao Huang,Cong Guo,Chiyue Wei,Yintao He,Jianyi Zhang,Hai "Hellen" Li,Yiran Chen*

Main category: cs.CL

TL;DR: DPad通过限制注意力到附近的后缀token，显著加速了基于扩散的大型语言模型的推理速度，同时保持了精度。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的大型语言模型（dLLM）通过将解码构建为去噪过程来并行化文本生成，但由于它们在每一步预测所有未来的后缀token，同时只保留一小部分，因此遭受高计算开销。

Method: DPad，一种无训练方法，它将注意力限制在一小组附近的后缀token上，包括一个滑动窗口和一个距离衰减dropout。

Result: 在LLaDA-1.5和Dream模型上的多个基准测试中进行的综合评估表明，DPad比原始dLLM提供了高达61.4倍的加速，同时保持了相当的精度。

Conclusion: DPad通过在保持相当精度的同时提供高达61.4倍的加速，展示了其在高效和可扩展的长序列推理方面的潜力。

Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by
framing decoding as a denoising process, but suffer from high computational
overhead since they predict all future suffix tokens at each step while
retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a
training-free method that restricts attention to a small set of nearby suffix
tokens, preserving fidelity while eliminating redundancy. DPad integrates two
strategies: (i) a sliding window, which maintains a fixed-length suffix window,
and (ii) distance-decay dropout, which deterministically removes distant suffix
tokens before attention computation. This simple design is compatible with
existing optimizations such as prefix caching and can be implemented with only
a few lines of code. Comprehensive evaluations across multiple benchmarks on
LLaDA-1.5 and Dream models demonstrate that DPad delivers up to
$\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable
accuracy, highlighting its potential for efficient and scalable long-sequence
inference. Our code is available at https://github.com/Crys-Chen/DPad.

</details>


### [11] [Comparing energy consumption and accuracy in text classification inference](https://arxiv.org/abs/2508.14170)
*Johannes Zschache,Tilman Hartwig*

Main category: cs.CL

TL;DR: This study evaluates the trade-offs between model accuracy and energy consumption in text classification inference. The best-performing model in terms of accuracy can also be energy-efficient. Inference energy consumption is influenced by model type, model size, and hardware specifications. There is a strong correlation between inference energy consumption and model runtime.


<details>
  <summary>Details</summary>
Motivation: The increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainability. Inference phase has received comparatively less attention than model training.

Method: This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations. Empirical analysis is performed.

Result: Observed substantial variability in inference energy consumption, influenced by model type, model size, and hardware specifications.

Conclusion: The best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. There is a strong correlation between inference energy consumption and model runtime.

Abstract: The increasing deployment of large language models (LLMs) in natural language
processing (NLP) tasks raises concerns about energy efficiency and
sustainability. While prior research has largely focused on energy consumption
during model training, the inference phase has received comparatively less
attention. This study systematically evaluates the trade-offs between model
accuracy and energy consumption in text classification inference across various
model architectures and hardware configurations. Our empirical analysis shows
that the best-performing model in terms of accuracy can also be
energy-efficient, while larger LLMs tend to consume significantly more energy
with lower classification accuracy. We observe substantial variability in
inference energy consumption ($<$mWh to $>$kWh), influenced by model type,
model size, and hardware specifications. Additionally, we find a strong
correlation between inference energy consumption and model runtime, indicating
that execution time can serve as a practical proxy for energy usage in settings
where direct measurement is not feasible. These findings have implications for
sustainable AI development, providing actionable insights for researchers,
industry practitioners, and policymakers seeking to balance performance and
resource efficiency in NLP applications.

</details>


### [12] [Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper](https://arxiv.org/abs/2508.14273)
*Krishna Garg,Firoz Shaikh,Sambaran Bandyopadhyay,Cornelia Caragea*

Main category: cs.CL

TL;DR: 提出了 SciIG 任务，评估 LLM 生成论文引言的能力。LLaMA-4 Maverick 表现最好，三样本提示更有效。


<details>
  <summary>Details</summary>
Motivation: 随着研究人员越来越多地采用 LLM 作为写作助手，生成高质量的研究论文引言仍然具有挑战性且至关重要。

Method: 提出了 Scientific Introduction Generation (SciIG) 任务，该任务评估 LLM 从标题、摘要和相关工作中生成连贯的介绍的能力。从 NAACL 2025 和 ICLR 2025 论文中整理新的数据集，我们评估了五种最先进的模型，包括开源和闭源系统。

Result: LLaMA-4 Maverick 在大多数指标上表现出色，尤其是在语义相似性和忠实性方面。此外，三样本提示始终优于少样本方法。

Conclusion: LLaMA-4 Maverick 在语义相似性和忠实性方面表现出色，三样本提示优于少样本方法。研究结果为开发有效的研究写作助手提供了实践见解，并为 LLM 辅助学术写作设定了现实的期望。为了促进可重复性和未来研究，我们将公开发布所有代码和数据集。

Abstract: As researchers increasingly adopt LLMs as writing assistants, generating
high-quality research paper introductions remains both challenging and
essential. We introduce Scientific Introduction Generation (SciIG), a task that
evaluates LLMs' ability to produce coherent introductions from titles,
abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR
2025 papers, we assess five state-of-the-art models, including both open-source
(DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and
closed-source GPT-4o systems, across multiple dimensions: lexical overlap,
semantic similarity, content coverage, faithfulness, consistency, citation
correctness, and narrative quality. Our comprehensive framework combines
automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4
Maverick's superior performance on most metrics, particularly in semantic
similarity and faithfulness. Moreover, three-shot prompting consistently
outperforms fewer-shot approaches. These findings provide practical insights
into developing effective research writing assistants and set realistic
expectations for LLM-assisted academic writing. To foster reproducibility and
future research, we will publicly release all code and datasets.

</details>


### [13] [Disentangling concept semantics via multilingual averaging in Sparse Autoencoders](https://arxiv.org/abs/2508.14275)
*Cliff O'Reilly,Ernesto Jimenez-Ruiz,Tillman Weyde*

Main category: cs.CL

TL;DR: Isolates concept semantics in LLMs by averaging concept activations derived via Sparse Autoencoders from multiple languages. Conceptual average aligns better with true relationships between classes compared to single languages.


<details>
  <summary>Details</summary>
Motivation: Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information.

Method: Averages concept activations derived via Sparse Autoencoders from English, French and Chinese text representations of OWL ontology classes.

Result: The conceptual average aligns to the true relationship between classes when compared with a single language by itself.

Conclusion: The conceptual average aligns to the true relationship between classes when compared with a single language by itself, hinting at a new technique for mechanistic interpretation of internal network states with higher accuracy.

Abstract: Connecting LLMs with formal knowledge representation and reasoning is a
promising approach to address their shortcomings. Embeddings and sparse
autoencoders are widely used to represent textual content, but the semantics
are entangled with syntactic and language-specific information. We propose a
method that isolates concept semantics in Large Langue Models by averaging
concept activations derived via Sparse Autoencoders. We create English text
representations from OWL ontology classes, translate the English into French
and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the
open source Gemma Scope suite of Sparse Autoencoders, we obtain concept
activations for each class and language version. We average the different
language activations to derive a conceptual average. We then correlate the
conceptual averages with a ground truth mapping between ontology classes. Our
results give a strong indication that the conceptual average aligns to the true
relationship between classes when compared with a single language by itself.
The result hints at a new technique which enables mechanistic interpretation of
internal network states with higher accuracy.

</details>


### [14] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: The paper introduces GRILE, a new benchmark for Romanian, and finds that while some LLMs perform well, many struggle with accuracy and generating correct explanations.


<details>
  <summary>Details</summary>
Motivation: The pedagogical value of LLMs for low-resource languages remains unclear.

Method: The paper introduces GRILE, a new open benchmark of 1,151 multiple-choice questions from Romanian high-stakes exams, and uses it to probe the abilities of seven LLMs.

Result: Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws. The error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms.

Conclusion: This paper exposes open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


### [15] [Tokens with Meaning: A Hybrid Tokenization Approach for NLP](https://arxiv.org/abs/2508.14292)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım,Demircan Çelik*

Main category: cs.CL

TL;DR: A hybrid tokenization method improves NLP for morphologically rich languages by combining linguistic rules and statistical subword segmentation, achieving better results on Turkish compared to existing tokenizers.


<details>
  <summary>Details</summary>
Motivation: Subword tokenization methods often struggle with morphologically rich languages because they rely on frequency rather than linguistic structure.

Method: The method combines rule-based morphological analysis with statistical subword segmentation, using phonological normalization, root-affix dictionaries, and a novel algorithm balancing morpheme preservation with vocabulary efficiency. It also integrates BPE for out-of-vocabulary coverage and uses special tokens for whitespace and case.

Result: The tokenizer achieves the highest Turkish Token Percentage (90.29%) and Pure Token Percentage (85.8%) on the TR-MMLU benchmark, outperforming tokenizers from LLaMA, Gemma, and GPT in linguistic meaningfulness and coherence.

Conclusion: The hybrid tokenization framework offers a language-independent approach to improve interpretability and effectiveness in multilingual NLP systems, as demonstrated by its strong performance on Turkish and potential adaptability to other languages.

Abstract: Tokenization plays a pivotal role in natural language processing (NLP),
shaping how text is segmented and interpreted by language models. While subword
methods such as Byte Pair Encoding (BPE) and WordPiece have been effective,
they often struggle with morphologically rich and agglutinative languages
because they rely on frequency rather than linguistic structure. We introduce a
hybrid tokenization framework that combines rule-based morphological analysis
with statistical subword segmentation. The method uses phonological
normalization, root-affix dictionaries, and a novel algorithm that balances
morpheme preservation with vocabulary efficiency. It assigns shared identifiers
to phonologically variant affixes (e.g., -ler and -lar) and altered root forms
(e.g., kitap vs. kitab{\i}), reducing redundancy while maintaining semantic
integrity. Special tokens are added for whitespace and case, including an
UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is
integrated for out-of-vocabulary coverage without harming morphological
coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish
Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with
tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and
coherent tokens. Although demonstrated on Turkish, the approach is
language-independent and adaptable to other languages, offering a practical
path toward more interpretable and effective multilingual NLP systems.

</details>


### [16] [A Joint Multitask Model for Morpho-Syntactic Parsing](https://arxiv.org/abs/2508.14307)
*Demian Inostroza,Mel Mistica,Ekaterina Vylomova,Chris Guest,Kemal Kurniawan*

Main category: cs.CL

TL;DR: A joint multitask model achieves state-of-the-art performance on morpho-syntactic parsing but has difficulty with grammatical cases and nominal features.


<details>
  <summary>Details</summary>
Motivation: To predict both morphological and syntactic analyses following a novel UD annotation scheme.

Method: A joint multitask model with a shared XLM-RoBERTa encoder and three specialized decoders.

Result: Achieves an average MSLAS score of 78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Matching the task's gold tokenization and content word identification are crucial to model performance.

Conclusion: The model achieves the best overall performance on the UniDive 2025 Morpho-Syntactic Parsing shared task, but struggles with core grammatical cases and nominal features.

Abstract: We present a joint multitask model for the UniDive 2025 Morpho-Syntactic
Parsing shared task, where systems predict both morphological and syntactic
analyses following novel UD annotation scheme. Our system uses a shared
XLM-RoBERTa encoder with three specialized decoders for content word
identification, dependency parsing, and morphosyntactic feature prediction. Our
model achieves the best overall performance on the shared task's leaderboard
covering nine typologically diverse languages, with an average MSLAS score of
78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation
studies show that matching the task's gold tokenization and content word
identification are crucial to model performance. Error analysis reveals that
our model struggles with core grammatical cases (particularly Nom-Acc) and
nominal features across languages.

</details>


### [17] [Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency](https://arxiv.org/abs/2508.14314)
*Aman Goel,Daniel Schwartz,Yanjun Qi*

Main category: cs.CL

TL;DR: Finch-Zk是一个黑盒框架，通过跨模型一致性检测和修正来减少大型语言模型的幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易产生幻觉，生成看似合理但包含事实错误的内容。

Method: 提出Finch-Zk框架，利用细粒度的跨模型一致性来检测和减轻LLM输出中的幻觉。Finch-Zk引入了跨模型一致性检查策略和有针对性的缓解技术。

Result: 在FELM数据集上，Finch-Zk将幻觉检测的F1分数提高了6-39%。在GPQA-diamond数据集上，Finch-Zk在答案准确率方面提高了7-8个百分点。

Conclusion: Finch-Zk通过跨模型一致性检查和有针对性的修正技术，提高了大型语言模型的事实可靠性，并且可以实际部署。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, but they remain susceptible to hallucinations--generating
content that appears plausible but contains factual inaccuracies. We present
Finch-Zk, a black-box framework that leverages FINe-grained Cross-model
consistency to detect and mitigate Hallucinations in LLM outputs without
requiring external knowledge sources. Finch-Zk introduces two key innovations:
1) a cross-model consistency checking strategy that reveals fine-grained
inaccuracies by comparing responses generated by diverse models from
semantically-equivalent prompts, and 2) a targeted mitigation technique that
applies precise corrections to problematic segments while preserving accurate
content. Experiments on the FELM dataset show Finch-Zk improves hallucination
detection F1 scores by 6-39\% compared to existing approaches. For mitigation,
Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy
on the GPQA-diamond dataset when applied to state-of-the-art models like Llama
4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models
demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for
enhancing factual reliability in production LLM systems.

</details>


### [18] [SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing](https://arxiv.org/abs/2508.14317)
*Jing Chen,Zhiheng Yang,Yixian Shen,Jie Liu,Adam Belloum,Chrysa Papagainni,Paola Grosso*

Main category: cs.CL

TL;DR: SurveyGen-I: an automatic survey generation framework


<details>
  <summary>Details</summary>
Motivation: existing LLM-based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage

Method: an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation

Result: Experiments across four scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage.

Conclusion: SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage.

Abstract: Survey papers play a critical role in scientific communication by
consolidating progress across a field. Recent advances in Large Language Models
(LLMs) offer a promising solution by automating key steps in the
survey-generation pipeline, such as retrieval, structuring, and summarization.
However, existing LLM-based approaches often struggle with maintaining
coherence across long, multi-section surveys and providing comprehensive
citation coverage. To address these limitations, we introduce SurveyGen-I, an
automatic survey generation framework that combines coarse-to-fine retrieval,
adaptive planning, and memory-guided generation. SurveyGen-I first performs
survey-level retrieval to construct the initial outline and writing plan, and
then dynamically refines both during generation through a memory mechanism that
stores previously written content and terminology, ensuring coherence across
subsections. When the system detects insufficient context, it triggers
fine-grained subsection-level retrieval. During generation, SurveyGen-I
leverages this memory mechanism to maintain coherence across subsections.
Experiments across four scientific domains demonstrate that SurveyGen-I
consistently outperforms previous works in content quality, consistency, and
citation coverage.

</details>


### [19] [Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever](https://arxiv.org/abs/2508.14323)
*Yixin Chen,Ying Xiong,Shangyu Wu,Yufei Cui,Xue Liu,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 训练了一个行为对齐检索器(BAR)，它提供行为一致的演示，以帮助llm做出更准确的工具使用决策。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型(llm)利用外部函数来扩展它们的能力，但是不准确的函数调用会导致效率低下和成本增加。现有的方法通过微调llm或使用基于演示的提示来解决这一挑战，但它们通常存在训练开销高的问题，并且不能解释不一致的演示样本，这些样本会误导模型的调用行为。

Method: 训练了一个行为对齐检索器(BAR)，它提供行为一致的演示，以帮助llm做出更准确的工具使用决策。使用对比学习框架训练BAR，定制正/负对和双负对比损失，确保可靠地检索行为一致的示例。

Result: 该方法显著减少了错误的函数调用，同时保持了较高的任务性能。

Conclusion: 该方法显著减少了错误的函数调用，同时保持了较高的任务性能，为工具增强的LLM提供了一种经济高效的解决方案。

Abstract: Tool-augmented large language models (LLMs) leverage external functions to
extend their capabilities, but inaccurate function calls can lead to
inefficiencies and increased costs.Existing methods address this challenge by
fine-tuning LLMs or using demonstration-based prompting, yet they often suffer
from high training overhead and fail to account for inconsistent demonstration
samples, which misguide the model's invocation behavior. In this paper, we
trained a behavior-aligned retriever (BAR), which provides behaviorally
consistent demonstrations to help LLMs make more accurate tool-using decisions.
To train the BAR, we construct a corpus including different function-calling
behaviors, i.e., calling or non-calling.We use the contrastive learning
framework to train the BAR with customized positive/negative pairs and a
dual-negative contrastive loss, ensuring robust retrieval of behaviorally
consistent examples.Experiments demonstrate that our approach significantly
reduces erroneous function calls while maintaining high task performance,
offering a cost-effective and efficient solution for tool-augmented LLMs.

</details>


### [20] [ISCA: A Framework for Interview-Style Conversational Agents](https://arxiv.org/abs/2508.14344)
*Charles Welch,Allison Lahnala,Vasudha Varadarajan,Lucie Flek,Rada Mihalcea,J. Lomax Boyd,João Sedoc*

Main category: cs.CL

TL;DR: Presents a low-compute conversational agent system for controlled interviews, useful for tracking attitude/behavior changes. Open-source and adaptable via online panel.


<details>
  <summary>Details</summary>
Motivation: The motivation is to facilitate qualitative data collection through controlled interactions and quantitative analysis, particularly for tracking attitude formation or behavior change.

Method: The paper introduces a system adjustable through an online administrative panel to create new interviews without coding.

Result: The system is demonstrated through two case studies: one on COVID-19 and another on neurotechnology public opinion.

Conclusion: The paper presents an open-source, low-compute, non-generative system for interview-style conversational agents, demonstrated through two case studies, and highlights its adaptability and accessibility for creating new interviews.

Abstract: We present a low-compute non-generative system for implementing
interview-style conversational agents which can be used to facilitate
qualitative data collection through controlled interactions and quantitative
analysis. Use cases include applications to tracking attitude formation or
behavior change, where control or standardization over the conversational flow
is desired. We show how our system can be easily adjusted through an online
administrative panel to create new interviews, making the tool accessible
without coding. Two case studies are presented as example applications, one
regarding the Expressive Interviewing system for COVID-19 and the other a
semi-structured interview to survey public opinion on emerging neurotechnology.
Our code is open-source, allowing others to build off of our work and develop
extensions for additional functionality.

</details>


### [21] [ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities](https://arxiv.org/abs/2508.14377)
*Wenhan Dong,Zhen Sun,Yuemeng Zhao,Zifan Peng,Jun Wu,Jingyi Zheng,Yule Liu,Xinlei He,Yu Wang,Ruiming Wang,Xinyi Huang,Lei Mo*

Main category: cs.CL

TL;DR: LLMs在评估阅读难度方面有潜力，但需要改进以更好地匹配学生的认知能力，尤其是在中文教育中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育应用中具有潜力，但其准确评估阅读材料与学生发展阶段认知对齐的能力仍未得到充分探索。尤其是在中文语言教育中，缺乏对LLM评估不同年龄段学生阅读理解难度的研究。

Method: 引入ZPD-SCA，这是一个专门用于评估中文阅读理解难度的基准，由60位特级教师进行标注。

Result: LLMs在zero-shot场景下表现不佳，但在提供上下文示例后，性能显着提高。这表明LLM具有评估阅读难度的能力，但也暴露了其在教育对齐判断方面的局限性。

Conclusion: LLMs表现出评估阅读难度的能力，但也存在局限性，尤其是在与学生认知能力对齐方面。即使是表现最好的模型也显示出系统性的方向偏差，并且不同类型的文本中模型表现差异很大。

Abstract: Large language models (LLMs) have demonstrated potential in educational
applications, yet their capacity to accurately assess the cognitive alignment
of reading materials with students' developmental stages remains insufficiently
explored. This gap is particularly critical given the foundational educational
principle of the Zone of Proximal Development (ZPD), which emphasizes the need
to match learning resources with Students' Cognitive Abilities (SCA). Despite
the importance of this alignment, there is a notable absence of comprehensive
studies investigating LLMs' ability to evaluate reading comprehension
difficulty across different student age groups, especially in the context of
Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel
benchmark specifically designed to assess stage-level Chinese reading
comprehension difficulty. The benchmark is annotated by 60 Special Grade
teachers, a group that represents the top 0.15% of all in-service teachers
nationwide. Experimental results reveal that LLMs perform poorly in zero-shot
learning scenarios, with Qwen-max and GLM even falling below the probability of
random guessing. When provided with in-context examples, LLMs performance
improves substantially, with some models achieving nearly double the accuracy
of their zero-shot baselines. These results reveal that LLMs possess emerging
abilities to assess reading difficulty, while also exposing limitations in
their current training for educationally aligned judgment. Notably, even the
best-performing models display systematic directional biases, suggesting
difficulties in accurately aligning material difficulty with SCA. Furthermore,
significant variations in model performance across different genres underscore
the complexity of task. We envision that ZPD-SCA can provide a foundation for
evaluating and improving LLMs in cognitively aligned educational applications.

</details>


### [22] [Credence Calibration Game? Calibrating Large Language Models through Structured Play](https://arxiv.org/abs/2508.14390)
*Ke Fang,Tianyi Zhao,Lu Cheng*

Main category: cs.CL

TL;DR: This paper introduces a game-based prompting framework to improve the calibration of LLMs by providing feedback on their confidence and correctness alignment, achieving consistent improvements in experiments.


<details>
  <summary>Details</summary>
Motivation: Ensuring that LLMs' confidence estimates faithfully correspond to their actual correctness is essential as they are increasingly deployed in decision-critical domains. Existing calibration methods have limitations such as requiring additional supervision or parameter updates.

Method: The paper proposes a novel prompt-based calibration framework inspired by the Credence Calibration Game, which establishes a structured interaction loop where LLMs receive feedback based on the alignment of their predicted confidence with correctness. The framework dynamically improves model calibration through feedback-driven prompting and natural language summaries of prior performance.

Result: The experiments across models and game configurations show consistent improvements in evaluation metrics.

Conclusion: This paper demonstrates the effectiveness of game-based prompting as a strategy for LLM calibration through experiments and consistent improvements in evaluation metrics.

Abstract: As Large Language Models (LLMs) are increasingly deployed in
decision-critical domains, it becomes essential to ensure that their confidence
estimates faithfully correspond to their actual correctness. Existing
calibration methods have primarily focused on post-hoc adjustments or auxiliary
model training; however, many of these approaches necessitate additional
supervision or parameter updates. In this work, we propose a novel prompt-based
calibration framework inspired by the Credence Calibration Game. Our method
establishes a structured interaction loop wherein LLMs receive feedback based
on the alignment of their predicted confidence with correctness. Through
feedback-driven prompting and natural language summaries of prior performance,
our framework dynamically improves model calibration. Extensive experiments
across models and game configurations demonstrate consistent improvements in
evaluation metrics. Our results highlight the potential of game-based prompting
as an effective strategy for LLM calibration. Code and data are available at
https://anonymous.4open.science/r/LLM-Calibration/.

</details>


### [23] [DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement](https://arxiv.org/abs/2508.14391)
*Yupei Yang,Fan Feng,Lin Yang,Wanxi Deng,Lin Qu,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.CL

TL;DR: DEPTH框架通过依赖感知句子简化和双层分层细化来减少关系抽取中的幻觉，并在多个基准测试中提高了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在关系抽取中难以可靠地确定关系是否存在，尤其是在涉及复杂句子结构或复杂语义的情况下，这导致了虚假预测。这种幻觉会在知识图中引入噪声边缘，从而损害结构化知识的完整性和下游可靠性。

Method: 该框架集成了依赖感知句子简化和双层分层细化到关系提取流程中。它包含两个阶段：Grounding模块和Refinement模块。还引入了一个因果驱动的奖励模型，通过解开虚假相关性来减轻奖励攻击，从而通过强化学习和人工反馈实现稳健的微调。

Result: DEPTH将平均幻觉率降低到7.0％，同时在平均F1得分上实现了比最先进的基线高17.2％的改进。

Conclusion: DEPTH通过减少幻觉率和提高F1分数，在六个基准数据集上优于现有技术水平。

Abstract: Relation extraction enables the construction of structured knowledge for many
downstream applications. While large language models (LLMs) have shown great
promise in this domain, most existing methods concentrate on relation
classification, which predicts the semantic relation type between a related
entity pair. However, we observe that LLMs often struggle to reliably determine
whether a relation exists, especially in cases involving complex sentence
structures or intricate semantics, which leads to spurious predictions. Such
hallucinations can introduce noisy edges in knowledge graphs, compromising the
integrity of structured knowledge and downstream reliability. To address these
challenges, we propose DEPTH, a framework that integrates Dependency-aware
sEntence simPlification and Two-tiered Hierarchical refinement into the
relation extraction pipeline. Given a sentence and its candidate entity pairs,
DEPTH operates in two stages: (1) the Grounding module extracts relations for
each pair by leveraging their shortest dependency path, distilling the sentence
into a minimal yet coherent relational context that reduces syntactic noise
while preserving key semantics; (2) the Refinement module aggregates all local
predictions and revises them based on a holistic understanding of the sentence,
correcting omissions and inconsistencies. We further introduce a
causality-driven reward model that mitigates reward hacking by disentangling
spurious correlations, enabling robust fine-tuning via reinforcement learning
with human feedback. Experiments on six benchmarks demonstrate that DEPTH
reduces the average hallucination rate to 7.0\% while achieving a 17.2\%
improvement in average F1 score over state-of-the-art baselines.

</details>


### [24] [Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs](https://arxiv.org/abs/2508.14408)
*Yinghan Zhou,Weifeng Zhu,Juan Wen,Wanli Peng,Zhengxian Wu,Yiming Xue*

Main category: cs.CL

TL;DR: This paper analyzes why LLMs struggle to identify their own generated text under IPP and introduces Cognitive Surgery (CoSur) to improve performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to distinguish self- from other-generated text under Individual Presentation Paradigm (IPP), and the underlying causes have not been systematically analyzed.

Method: The paper proposes Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing to awaken the ITA of LLMs.

Result: The proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.

Conclusion: The paper introduces Cognitive Surgery (CoSur), a novel framework that improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.

Abstract: Large language models (LLMs) have been shown to possess a degree of
self-recognition capability-the ability to identify whether a given text was
generated by themselves. Prior work has demonstrated that this capability is
reliably expressed under the Pair Presentation Paradigm (PPP), where the model
is presented with two texts and asked to choose which one it authored. However,
performance deteriorates sharply under the Individual Presentation Paradigm
(IPP), where the model is given a single text to judge authorship. Although
this phenomenon has been observed, its underlying causes have not been
systematically analyzed. In this paper, we first replicate existing findings to
confirm that LLMs struggle to distinguish self- from other-generated text under
IPP. We then investigate the reasons for this failure and attribute it to a
phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent
ability to distinguish self- and other-texts in representational space, which
remains unexpressed in its output behavior. To awaken the ITA of LLMs, we
propose Cognitive Surgery (CoSur), a novel framework comprising four main
modules: representation extraction, territory construction, authorship
discrimination and cognitive editing. Experimental results demonstrate that our
proposed method improves the performance of three different LLMs in the IPP
scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,
respectively.

</details>


### [25] [Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models](https://arxiv.org/abs/2508.14427)
*Wuyang Zhang,Yexin Tian,Xiandong Meng,Mengjie Wang,Junliang Du*

Main category: cs.CL

TL;DR: This paper introduces a knowledge graph injection method to improve language models' reasoning and semantic understanding by fine-tuning with structured graph information and a dynamic gating mechanism.


<details>
  <summary>Details</summary>
Motivation: Large language models suffer from missing reasoning chains and insufficient entity-level semantic understanding when dealing with tasks requiring structured knowledge.

Method: A fine-tuning algorithm framework based on knowledge graph injection, building on pretrained language models and introducing structured graph information for auxiliary learning. A graph neural network encodes entities and relations, and a fusion mechanism jointly models knowledge graph embeddings with contextual representations. A gating mechanism dynamically balances linguistic semantics and structural knowledge.

Result: The proposed method demonstrates effectiveness and stability across tasks such as entity recognition, question answering, and language generation. Systematic sensitivity experiments validate the effects of learning rate, graph coverage, and structural perturbations on model performance.

Conclusion: The proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units, demonstrating better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction.

Abstract: This paper addresses the problems of missing reasoning chains and
insufficient entity-level semantic understanding in large language models when
dealing with tasks that require structured knowledge. It proposes a fine-tuning
algorithm framework based on knowledge graph injection. The method builds on
pretrained language models and introduces structured graph information for
auxiliary learning. A graph neural network is used to encode entities and their
relations, constructing a graph-based semantic representation. A fusion
mechanism is then designed to jointly model the knowledge graph embeddings with
the contextual representations from the language model. To enhance the
robustness of knowledge integration, a gating mechanism is introduced to
dynamically balance the contributions of linguistic semantics and structural
knowledge. This effectively mitigates conflicts between different
representational spaces. During training, a joint loss function is constructed
to account for both task performance and structural alignment objectives. This
helps improve the accuracy of entity prediction and semantic reasoning. The
study also includes a series of systematic sensitivity experiments. It
evaluates the effects of learning rate, graph coverage, and structural
perturbations on model performance. The results further validate the
effectiveness and stability of the proposed method across tasks such as entity
recognition, question answering, and language generation. Experimental findings
show that the proposed structure-aware fine-tuning framework significantly
enhances the model's ability to represent complex semantic units. It
demonstrates better semantic consistency and contextual logic modeling in
scenarios involving structural reasoning and entity extraction.

</details>


### [26] [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
*NVIDIA,:,Aarti Basant,Abhijit Khairnar,Abhijit Paithankar,Abhinav Khattar,Adi Renduchintala,Adithya Renduchintala,Aditya Malte,Akhiad Bercovich,Akshay Hazare,Alejandra Rico,Aleksander Ficek,Alex Kondratenko,Alex Shaposhnikov,Ali Taghibakhshi,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amy Shen,Andrew Tao,Ann Guan,Anna Shors,Anubhav Mandarwal,Arham Mehta,Arun Venkatesan,Ashton Sharabiani,Ashwath Aithal,Ashwin Poojary,Ayush Dattagupta,Balaram Buddharaju,Banghua Zhu,Barnaby Simkin,Bilal Kartal,Bita Darvish Rouhani,Bobby Chen,Boris Ginsburg,Brandon Norick,Brian Yu,Bryan Catanzaro,Charles Wang,Charlie Truong,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christian Munley,Christopher Parisien,Dan Su,Daniel Afrimi,Daniel Korzekwa,Daniel Rohrer,Daria Gitman,David Mosallanezhad,Deepak Narayanan,Dima Rekesh,Dina Yared,Dmytro Pykhtar,Dong Ahn,Duncan Riach,Eileen Long,Elliott Ning,Eric Chung,Erick Galinkin,Evelina Bakhturina,Gargi Prasad,Gerald Shen,Haim Elisha,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Hoo Chang Shin,Hua Huang,Iain Cunningham,Igor Gitman,Ivan Moshkov,Jaehun Jung,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jimmy Zhang,Jinze Xue,Jocelyn Huang,Joey Conway,John Kamalu,Jonathan Cohen,Joseph Jennings,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kezhi Kong,Krzysztof Pawelec,Kumar Anik,Kunlun Li,Kushan Ahmadian,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Luis Vega,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Marcin Chochowski,Mark Cai,Markus Kliegl,Marta Stepniewska-Dziubinska,Matvei Novikov,Mehrzad Samadi,Meredith Price,Meriem Boubdir,Michael Boone,Michael Evans,Michal Bien,Michal Zawalski,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Namit Dhameja,Nave Assaf,Negar Habibi,Nidhi Bhatia,Nikki Pope,Nima Tajbakhsh,Nirmal Kumar Juluru,Oleg Rybakov,Oleksii Hrinchuk,Oleksii Kuchaiev,Oluwatobi Olabiyi,Pablo Ribalta,Padmavathy Subramanian,Parth Chadha,Pavlo Molchanov,Peter Dykas,Peter Jin,Piotr Bialecki,Piotr Januszewski,Pradeep Thalasta,Prashant Gaikwad,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Rabeeh Karimi Mahabadi,Rajen Patel,Ran El-Yaniv,Ranjit Rajan,Ria Cheruvu,Rima Shahbazyan,Ritika Borkar,Ritu Gala,Roger Waleffe,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Sahil Jain,Samuel Kriman,Sanjeev Satheesh,Saori Kaji,Sarah Yurick,Saurav Muralidharan,Sean Narenthiran,Seonmyeong Bak,Sepehr Sameni,Seungju Han,Shanmugam Ramasamy,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shizhe Diao,Shreya Gopal,Shrimai Prabhumoye,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Siddhartha Jain,Somshubra Majumdar,Stefania Alborghetti,Syeda Nahida Akter,Terry Kong,Tim Moon,Tomasz Hliwiak,Tomer Asida,Tony Wang,Twinkle Vashishth,Tyler Poon,Udi Karpas,Vahid Noroozi,Venkat Srinivasan,Vijay Korthikanti,Vikram Fugro,Vineeth Kalluru,Vitaly Kurin,Vitaly Lavrukhin,Wasi Uddin Ahmad,Wei Du,Wonmin Byeon,Ximing Lu,Xin Dong,Yashaswi Karnati,Yejin Choi,Yian Zhang,Ying Lin,Yonggan Fu,Yoshi Suhara,Zhen Dong,Zhiyu Li,Zhongbo Zhu,Zijia Chen*

Main category: cs.CL

TL;DR: Nemotron-Nano-9B-v2 is a hybrid Mamba-Transformer language model that increases throughput for reasoning workloads and achieves state-of-the-art accuracy. It is being released on Hugging Face.


<details>
  <summary>Details</summary>
Motivation: increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models

Method: hybrid Mamba-Transformer language model, FP8 training recipe, Minitron strategy to compress and distill the model

Result: releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face

Conclusion: Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings compared to existing similarly-sized models.

Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model
designed to increase throughput for reasoning workloads while achieving
state-of-the-art accuracy compared to similarly-sized models.
Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the
majority of the self-attention layers in the common Transformer architecture
are replaced with Mamba-2 layers, to achieve improved inference speed when
generating the long thinking traces needed for reasoning. We create
Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model
(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.
After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to
compress and distill the model with the goal of enabling inference on up to
128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).
Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that
Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks
while achieving up to 6x higher inference throughput in reasoning settings like
8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,
Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with
the majority of our pre- and post-training datasets on Hugging Face.

</details>


### [27] [In2x at WMT25 Translation Task](https://arxiv.org/abs/2508.14472)
*Lei Pang,Hanyi Mao,Quanjia Xiao,HaiXiao Liu,Xiangyi Li*

Main category: cs.CL

TL;DR: In2x research team's WMT25 submission focuses on extending LLMs to Japanese and low-resource languages using data construction and reward models.


<details>
  <summary>Details</summary>
Motivation: explore a generalizable paradigm for extending large language models to other languages

Method: data construction methods and reward model design

Result: N/A

Conclusion: This paper aims to achieve exceptional performance in low-resource languages by extending LLMs.

Abstract: This paper presents the open-system submission by the In2x research team for
the WMT25 General Machine Translation Shared Task. Our submission focuses on
Japanese-related translation tasks, aiming to explore a generalizable paradigm
for extending large language models (LLMs) to other languages. This paradigm
encompasses aspects such as data construction methods and reward model design.
The ultimate goal is to enable large language model systems to achieve
exceptional performance in low-resource or less commonly spoken languages.

</details>


### [28] [Reasoning is about giving reasons](https://arxiv.org/abs/2508.14488)
*Krunal Shah,Dan Roth*

Main category: cs.CL

TL;DR: This paper introduces a new method for understanding the logical structure of arguments, which improves interpretability and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current approaches to chaining rules suffer in terms of their interpretability and their ability to accommodate extensions to theoretically equivalent reasoning tasks.

Method: The paper identifies an intermediate representation (Representation of the Logical Structure (RLS)) of the argument that possesses an understanding of the logical structure of a natural language argument.

Result: The paper can identify and extract the logical structure of natural language arguments in three popular reasoning datasets with high accuracies.

Conclusion: The paper shows that the logical structure of natural language arguments can be identified and extracted with high accuracies, supporting explanation generation and extending reasoning capabilities.

Abstract: Convincing someone of the truth value of a premise requires understanding and
articulating the core logical structure of the argument which proves or
disproves the premise. Understanding the logical structure of an argument
refers to understanding the underlying "reasons" which make up the proof or
disproof of the premise - as a function of the "logical atoms" in the argument.
While it has been shown that transformers can "chain" rules to derive simple
arguments, the challenge of articulating the "reasons" remains. Not only do
current approaches to chaining rules suffer in terms of their interpretability,
they are also quite constrained in their ability to accommodate extensions to
theoretically equivalent reasoning tasks - a model trained to chain rules
cannot support abduction or identify contradictions. In this work we suggest
addressing these shortcomings by identifying an intermediate representation
(which we call the Representation of the Logical Structure (RLS) of the
argument) that possesses an understanding of the logical structure of a natural
language argument - the logical atoms in the argument and the rules
incorporating them. Given the logical structure, reasoning is deterministic and
easy to compute. Therefore, our approach supports all forms of reasoning that
depend on the logical structure of the natural language argument, including
arbitrary depths of reasoning, on-the-fly mistake rectification and interactive
discussion with respect to an argument. We show that we can identify and
extract the logical structure of natural language arguments in three popular
reasoning datasets with high accuracies, thus supporting explanation generation
and extending the reasoning capabilities significantly.

</details>


### [29] [EmoTale: An Enacted Speech-emotion Dataset in Danish](https://arxiv.org/abs/2508.14548)
*Maja J. Hjuler,Harald V. Skat-Rørdam,Line H. Clemmensen,Sneha Das*

Main category: cs.CL

TL;DR: 提出了EmoTale，一个包含丹麦语和英语语音记录及其相关情感注释的语料库。


<details>
  <summary>Details</summary>
Motivation: 缺乏丹麦语等较小（口语）语言的功能性数据集。现有的唯一丹麦语情感语音数据库是1997年发布的丹麦情感语音（DES）。

Method: 使用自监督语音模型（SSLM）嵌入和openSMILE特征提取器，为EmoTale和参考数据集开发了SER模型。

Result: SSLM嵌入优于手工制作的特征。

Conclusion: EmoTale语料库的性能与DES语料库相当，最佳模型在使用留一讲话者交叉验证时，在EmoTale语料库上实现了64.1%的非加权平均召回率（UAR）。

Abstract: While multiple emotional speech corpora exist for commonly spoken languages,
there is a lack of functional datasets for smaller (spoken) languages, such as
Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is
the only other database of Danish emotional speech. We present EmoTale; a
corpus comprising Danish and English speech recordings with their associated
enacted emotion annotations. We demonstrate the validity of the dataset by
investigating and presenting its predictive power using speech emotion
recognition (SER) models. We develop SER models for EmoTale and the reference
datasets using self-supervised speech model (SSLM) embeddings and the openSMILE
feature extractor. We find the embeddings superior to the hand-crafted
features. The best model achieves an unweighted average recall (UAR) of 64.1%
on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable
to the performance on DES.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [30] [A comparative study of some wavelet and sampling operators on various features of an image](https://arxiv.org/abs/2508.14043)
*Digvijay Singh,Rahul Shukla,Karunesh Kumar Singh*

Main category: cs.CV

TL;DR: 本研究分析了正抽样 Kantorovich 算子 (SK 算子) 的收敛性质，并使用各种算子进行了局部和全局逼近性质的综合分析。


<details>
  <summary>Details</summary>
Motivation: 本研究包括对一些正抽样 Kantorovich 算子（SK 算子）及其收敛性质的研究。

Method: 使用抽样 Kantorovich (SK)、高斯、双边和基于阈值小波的算子

Result: 通过表格形式展示了理想条件下特定样本水平的算子的性质。通过 SI、SSI 和 SMPI 讨论了 2D Shepp-Logan Phantom 的感兴趣区域 (ROI)，该切片取自 3D 图像，这证明了逼近基本定理 (FTA) 的合理性。

Conclusion: 不同的算子在研究图像的不同特征时具有各自的意义，因为图像的性质是不均匀的（非理想条件）。因此，在某种程度上，一些算子效果很好，而另一些算子对于图像的某些特定特征效果不佳。

Abstract: This research includes the study of some positive sampling Kantorovich
operators (SK operators) and their convergence properties. A comprehensive
analysis of both local and global approximation properties is presented using
sampling Kantorovich (SK), Gaussian, Bilateral and the thresholding
wavelet-based operators in the framework of SK-operators. Explicitly, we start
the article by introducing the basic terminology and state the fundamental
theorem of approximation (FTA) by imposing the various required conditions
corresponding to the various defined operators. We measure the error and study
the other mathematical parameters such as the mean square error (MSE), the
speckle index (SI), the speckle suppression index (SSI), the speckle mean
preservation index (SMPI), and the equivalent number of looks (ENL) at various
levels of resolution parameters. The nature of these operators are demonstrated
via an example under ideal conditions in tabulated form at a certain level of
samples. Eventually, another numerical example is illustrated to discuss the
region of interest (ROI) via SI, SSI and SMPI of 2D Shepp-Logan Phantom taken
slice from the 3D image, which gives the justification of the fundamental
theorem of approximation (FTA). At the end of the derivation and illustrations
we observe that the various operators have their own significance while
studying the various features of the image because of the uneven nature of an
image (non-ideal condition). Therefore, to some extent, some operators work
well and some do not for some specific features of the image.

</details>


### [31] [Federated Action Recognition for Smart Worker Assistance Using FastPose](https://arxiv.org/abs/2508.14113)
*Vinit Hegiste,Vidit Goyal,Tatjana Legler,Martin Ruskowski*

Main category: cs.CV

TL;DR: 本文提出了一种用于姿势识别的联邦学习框架，该框架在保护隐私的同时，提高了跨用户泛化能力


<details>
  <summary>Details</summary>
Motivation: 在智能制造环境中，准确和实时地识别工人动作对于生产力、安全性和人机协作至关重要。虽然基于骨骼的人体活动识别 (HAR) 提供了对光照、视点和背景变化的鲁棒性，但大多数现有方法依赖于集中式数据集，这在隐私敏感的工业场景中是不切实际的。

Method: 使用修改后的 FastPose 模型捕获并处理来自五个参与者的八个与工业相关的上半身手势的自定义骨骼数据集，提出了用于基于姿势的 HAR 的联邦学习 (FL) 框架。

Result: 在全局测试集上，FL Transformer 比集中式训练提高了 +12.4 个百分点，而 FedEnsemble 提供了 +16.3 个百分点的增益。在看不见的外部客户端上，FL 和 FedEnsemble 比集中式准确率分别高出 +52.6 和 +58.3 个百分点。

Conclusion: 联邦学习 (FL) 不仅保护隐私，而且显着增强了跨用户泛化能力，使其成为异构工业环境中可扩展、具有隐私意识的 HAR 的实用解决方案。

Abstract: In smart manufacturing environments, accurate and real-time recognition of
worker actions is essential for productivity, safety, and human-machine
collaboration. While skeleton-based human activity recognition (HAR) offers
robustness to lighting, viewpoint, and background variations, most existing
approaches rely on centralized datasets, which are impractical in
privacy-sensitive industrial scenarios. This paper presents a federated
learning (FL) framework for pose-based HAR using a custom skeletal dataset of
eight industrially relevant upper-body gestures, captured from five
participants and processed using a modified FastPose model. Two temporal
backbones, an LSTM and a Transformer encoder, are trained and evaluated under
four paradigms: centralized, local (per-client), FL with weighted federated
averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the
global test set, the FL Transformer improves over centralized training by +12.4
percentage points, with FedEnsemble delivering a +16.3 percentage points gain.
On an unseen external client, FL and FedEnsemble exceed centralized accuracy by
+52.6 and +58.3 percentage points, respectively. These results demonstrate that
FL not only preserves privacy but also substantially enhances cross-user
generalization, establishing it as a practical solution for scalable,
privacy-aware HAR in heterogeneous industrial settings.

</details>


### [32] [LENS: Learning to Segment Anything with Unified Reinforced Reasoning](https://arxiv.org/abs/2508.14153)
*Lianghui Zhu,Bin Ouyang,Yuxuan Zhang,Tianheng Cheng,Rui Hu,Haocheng Shen,Longjin Ran,Xiaoxin Chen,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: LENS, a scalable reinforcement-learning framework, jointly optimizes the reasoning process and segmentation in an end-to-end manner. It achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%.


<details>
  <summary>Details</summary>
Motivation: Existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains.

Method: introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct

Result: achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%.

Conclusion: RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models.

Abstract: Text-prompted image segmentation enables fine-grained visual understanding
and is critical for applications such as human-computer interaction and
robotics. However, existing supervised fine-tuning methods typically ignore
explicit chain-of-thought (CoT) reasoning at test time, which limits their
ability to generalize to unseen prompts and domains. To address this issue, we
introduce LENS, a scalable reinforcement-learning framework that jointly
optimizes the reasoning process and segmentation in an end-to-end manner. We
propose unified reinforcement-learning rewards that span sentence-, box-, and
segment-level cues, encouraging the model to generate informative CoT
rationales while refining mask quality. Using a publicly available
3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS
achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg
benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to
5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust
prior for text-prompted segmentation and offers a practical path toward more
generalizable Segment Anything models. Code is available at
https://github.com/hustvl/LENS.

</details>


### [33] [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160)
*Ronghao Dang,Yuqian Yuan,Yunxuan Mao,Kehan Li,Jiangpin Liu,Zhikai Wang,Xin Li,Fan Wang,Deli Zhao*

Main category: cs.CV

TL;DR: RynnEC 是一种用于具身认知的视频多模态大型语言模型，它引入了区域编码器和掩码解码器，并在对象属性理解等方面实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 我们介绍了 RynnEC，这是一种专为具身认知设计的视频多模态大型语言模型。

Method: RynnEC 建立在通用视觉语言基础模型之上，并结合了区域编码器和掩码解码器，从而实现灵活的区域级视频交互。为了缓解带注释的 3D 数据集的稀缺性，我们提出了一种基于以自我为中心的视频的流水线，用于生成具身认知数据。

Result: RynnEC 在对象属性理解、对象分割和空间推理方面实现了最先进的性能。

Conclusion: RynnEC 通过提供对物理世界的细粒度感知并实现更精确的交互，推进了具身智能体通用认知核心的开发，并促进了跨各种具身任务的泛化。

Abstract: We introduce RynnEC, a video multimodal large language model designed for
embodied cognition. Built upon a general-purpose vision-language foundation
model, RynnEC incorporates a region encoder and a mask decoder, enabling
flexible region-level video interaction. Despite its compact architecture,
RynnEC achieves state-of-the-art performance in object property understanding,
object segmentation, and spatial reasoning. Conceptually, it offers a
region-centric video paradigm for the brain of embodied agents, providing
fine-grained perception of the physical world and enabling more precise
interactions. To mitigate the scarcity of annotated 3D datasets, we propose an
egocentric video based pipeline for generating embodied cognition data.
Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for
evaluating embodied cognitive capabilities. We anticipate that RynnEC will
advance the development of general-purpose cognitive cores for embodied agents
and facilitate generalization across diverse embodied tasks. The code, model
checkpoints, and benchmark are available at:
https://github.com/alibaba-damo-academy/RynnEC

</details>


### [34] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: This paper introduces a deep equilibrium canonicalizer (DEC) to improve the local scale equivariance of a model. DEC improves model performance and local scale consistency.


<details>
  <summary>Details</summary>
Motivation: Scale variation is a fundamental challenge in computer vision. Objects of the same class can have different sizes, and their perceived size is further affected by the distance from the camera. These variations are local to the objects, i.e., different object sizes may change differently within the same image.

Method: deep equilibrium canonicalizer (DEC)

Result: DEC improves both model performance and local scale consistency across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT.

Conclusion: DEC improves both model performance and local scale consistency across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT.

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [35] [CLIPSym: Delving into Symmetry Detection with CLIP](https://arxiv.org/abs/2508.14197)
*Tinghan Yang,Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.CV

TL;DR: CLIPSym uses CLIP and a rotation-equivariant decoder with Semantic-Aware Prompt Grouping (SAPG) to improve symmetry detection, outperforming existing methods on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions.

Method: CLIPSym leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and $G$-Convolution to detect rotation and reflection symmetries. Developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG).

Result: Benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique are verified.

Conclusion: CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS).

Abstract: Symmetry is one of the most fundamental geometric cues in computer vision,
and detecting it has been an ongoing challenge. With the recent advances in
vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP
model can aid symmetry detection by leveraging the additional symmetry cues
found in the natural image descriptions. We propose CLIPSym, which leverages
CLIP's image and language encoders and a rotation-equivariant decoder based on
a hybrid of Transformer and $G$-Convolution to detect rotation and reflection
symmetries. To fully utilize CLIP's language encoder, we have developed a novel
prompting technique called Semantic-Aware Prompt Grouping (SAPG), which
aggregates a diverse set of frequent object-based prompts to better integrate
the semantic cues for symmetry detection. Empirically, we show that CLIPSym
outperforms the current state-of-the-art on three standard symmetry detection
datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations
verifying the benefits of CLIP's pre-training, the proposed equivariant
decoder, and the SAPG technique. The code is available at
https://github.com/timyoung2333/CLIPSym.

</details>


### [36] [A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment](https://arxiv.org/abs/2508.14203)
*Ghazal Alinezhad Noghre,Armin Danesh Pazho,Hamed Tabkhi*

Main category: cs.CV

TL;DR: This survey comprehensively organizes video anomaly detection research, highlighting progress, limitations, and open challenges across different scenarios and learning methods.


<details>
  <summary>Details</summary>
Motivation: Video Anomaly Detection (VAD) is a pivotal task, and recent deep learning advances have driven progress. However, the field remains fragmented across domains and learning paradigms.

Method: The survey systematically organizes the VAD literature across various supervision levels and adaptive learning methods (online, active, continual learning). It examines the state of VAD across human-centric, vehicle-centric, and environment-centric scenarios.

Result: Identified fundamental contributions and limitations of current VAD methodologies across different application categories.

Conclusion: This survey consolidates insights from VAD subfields to provide a structured foundation for advancing theoretical understanding and real-world applicability of VAD systems. It aims to support researchers and draw attention to open challenges.

Abstract: Video Anomaly Detection (VAD) has emerged as a pivotal task in computer
vision, with broad relevance across multiple fields. Recent advances in deep
learning have driven significant progress in this area, yet the field remains
fragmented across domains and learning paradigms. This survey offers a
comprehensive perspective on VAD, systematically organizing the literature
across various supervision levels, as well as adaptive learning methods such as
online, active, and continual learning. We examine the state of VAD across
three major application categories: human-centric, vehicle-centric, and
environment-centric scenarios, each with distinct challenges and design
considerations. In doing so, we identify fundamental contributions and
limitations of current methodologies. By consolidating insights from subfields,
we aim to provide the community with a structured foundation for advancing both
theoretical understanding and real-world applicability of VAD systems. This
survey aims to support researchers by providing a useful reference, while also
drawing attention to the broader set of open challenges in anomaly detection,
including both fundamental research questions and practical obstacles to
real-world deployment.

</details>


### [37] [Accelerating Image Classification with Graph Convolutional Neural Networks using Voronoi Diagrams](https://arxiv.org/abs/2508.14218)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: 提出了一种新的图像分类方法，该方法结合了GCN和Voronoi图，实现了更高的精度和更快的预处理速度，特别适用于复杂场景。


<details>
  <summary>Details</summary>
Motivation: GCN的集成显著推动了图像分类的最新进展，为处理复杂数据结构提供了一种新的范例。

Method: 利用GCN与Voronoi图相结合进行图像分类，利用它们建模关系数据的卓越能力。使用图像的基于图的表示，其中像素或区域被视为图的顶点，然后以相应Delaunay三角剖分的形式简化。

Result: 在多个基准数据集上，预处理时间和分类精度都显著提高，超过了现有的最新模型，尤其是在涉及复杂场景和细粒度类别的情况下。通过交叉验证验证的实验结果。

Conclusion: 集成了GCN与Voronoi图在图像分类任务中的潜力，为在计算机视觉和非结构化数据领域的基于图的学习范例开辟了新途径。提出了NVGCN，它比常规GCN更快。

Abstract: Recent advances in image classification have been significantly propelled by
the integration of Graph Convolutional Networks (GCNs), offering a novel
paradigm for handling complex data structures. This study introduces an
innovative framework that employs GCNs in conjunction with Voronoi diagrams to
peform image classification, leveraging their exceptional capability to model
relational data. Unlike conventional convolutional neural networks, our
approach utilizes a graph-based representation of images, where pixels or
regions are treated as vertices of a graph, which are then simplified in the
form of the corresponding Delaunay triangulations. Our model yields significant
improvement in pre-processing time and classification accuracy on several
benchmark datasets, surpassing existing state-of-the-art models, especially in
scenarios that involve complex scenes and fine-grained categories. The
experimental results, validated via cross-validation, underscore the potential
of integrating GCNs with Voronoi diagrams in advancing image classification
tasks. This research contributes to the field by introducing a novel approach
to image classification, while opening new avenues for developing graph-based
learning paradigms in other domains of computer vision and non-structured data.
In particular, we have proposed a new version of the GCN in this paper, namely
normalized Voronoi Graph Convolution Network (NVGCN), which is faster than the
regular GCN.

</details>


### [38] [Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models](https://arxiv.org/abs/2508.14264)
*Thanh-Dat Truong,Huu-Thien Tran,Tran Thai Son,Bhiksha Raj,Khoa Luu*

Main category: cs.CV

TL;DR: This paper introduces a new learning mechanism to improve the robust alignment between visual and textual modalities in large multimodal models (LMMs) by solving shuffling problems. The approach achieves state-of-the-art performance on LMM benchmarks.


<details>
  <summary>Details</summary>
Motivation: these models still suffer from some fundamental limitations related to robustness and generalization due to the alignment and correlation between visual and textual features

Method: introducing two new tasks: reconstructing the image order and the text order into the LMM's pre-training and fine-tuning phases. In addition, we propose a new directed-token approach to capture visual and textual knowledge, enabling the capability to reconstruct the correct order of visual inputs. Then, we introduce a new Image-to-Response Guided loss to further improve the visual understanding of the LMM in its responses.

Result: improving the robust alignment between visual and textual modalities by solving shuffling problems. In particular, the proposed approach can improve reasoning capability, visual understanding, and cross-modality alignment

Conclusion: The proposed approach consistently achieves state-of-the-art (SoTA) performance compared with prior LMMs on academic task-oriented and instruction-following LMM benchmarks.

Abstract: Large multimodal models (LMMs) have gained impressive performance due to
their outstanding capability in various understanding tasks. However, these
models still suffer from some fundamental limitations related to robustness and
generalization due to the alignment and correlation between visual and textual
features. In this paper, we introduce a simple but efficient learning mechanism
for improving the robust alignment between visual and textual modalities by
solving shuffling problems. In particular, the proposed approach can improve
reasoning capability, visual understanding, and cross-modality alignment by
introducing two new tasks: reconstructing the image order and the text order
into the LMM's pre-training and fine-tuning phases. In addition, we propose a
new directed-token approach to capture visual and textual knowledge, enabling
the capability to reconstruct the correct order of visual inputs. Then, we
introduce a new Image-to-Response Guided loss to further improve the visual
understanding of the LMM in its responses. The proposed approach consistently
achieves state-of-the-art (SoTA) performance compared with prior LMMs on
academic task-oriented and instruction-following LMM benchmarks.

</details>


### [39] [Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy](https://arxiv.org/abs/2508.14266)
*Rizwan Ahamed,Annahita Amireskandari,Joel Palko,Carol Laxson,Binod Bhattarai,Prashnna Gyawali*

Main category: cs.CV

TL;DR: 研究了数据增强策略如何影响糖尿病视网膜病变(DR)分级的conformal predictors的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在糖尿病视网膜病变(DR)分级等高风险任务中的临床部署需要具有可证明的可靠性。虽然模型实现了高精度，但由于缺乏稳健的不确定性量化，其临床效用受到限制。Conformal prediction (CP)提供了一个无分布框架来生成具有统计覆盖率保证的预测集。然而，数据增强等标准训练实践与这些保证的有效性之间的相互作用尚不清楚。

Method: 使用DDR数据集，评估了在五种增强方案（无增强、标准几何变换、CLAHE、Mixup和CutMix）下训练的两个骨干架构——ResNet-50和Co-Scale Conv-Attentional Transformer (CoaT)。

Result: 结果表明，Mixup和CutMix等样本混合策略不仅提高了预测精度，而且产生了更可靠和有效的不确定性估计。相反，CLAHE等方法会对模型确定性产生负面影响。

Conclusion: Sample-mixing策略（如Mixup和CutMix）不仅提高了预测准确性，还产生了更可靠和有效的不确定性估计。相反，CLAHE等方法会对模型确定性产生负面影响。在构建真正值得信赖的医学影像AI系统时，需要将数据增强策略与下游不确定性量化共同设计。

Abstract: The clinical deployment of deep learning models for high-stakes tasks such as
diabetic retinopathy (DR) grading requires demonstrable reliability. While
models achieve high accuracy, their clinical utility is limited by a lack of
robust uncertainty quantification. Conformal prediction (CP) offers a
distribution-free framework to generate prediction sets with statistical
guarantees of coverage. However, the interaction between standard training
practices like data augmentation and the validity of these guarantees is not
well understood. In this study, we systematically investigate how different
data augmentation strategies affect the performance of conformal predictors for
DR grading. Using the DDR dataset, we evaluate two backbone architectures --
ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under
five augmentation regimes: no augmentation, standard geometric transforms,
CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal
metrics, including empirical coverage, average prediction set size, and correct
efficiency. Our results demonstrate that sample-mixing strategies like Mixup
and CutMix not only improve predictive accuracy but also yield more reliable
and efficient uncertainty estimates. Conversely, methods like CLAHE can
negatively impact model certainty. These findings highlight the need to
co-design augmentation strategies with downstream uncertainty quantification in
mind to build genuinely trustworthy AI systems for medical imaging.

</details>


### [40] [Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning](https://arxiv.org/abs/2508.14276)
*Said Djafar Said,Torkan Gholamalizadeh,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出了一种用于 3D 牙科体积生成的新型条件扩散框架，该框架由牙齿水平的二元属性引导，可以精确控制牙齿的存在和配置。


<details>
  <summary>Details</summary>
Motivation: 尽管牙科 CBCT 扫描对于诊断和治疗计划越来越重要，但在医学图像合成中，生成具有细粒度控制的解剖学上逼真的扫描仍然是一个挑战。

Method: 我们提出了一种新颖的条件扩散框架，用于 3D 牙科体积生成，该框架由牙齿水平的二元属性引导，可以精确控制牙齿的存在和配置。我们的方法集成了基于小波的去噪扩散、FiLM 条件和屏蔽损失函数，以将学习重点放在相关的解剖结构上。

Result: 结果显示出强大的保真度和泛化能力，FID 分数低，强大的修复性能，即使在未见过的扫描中，SSIM 值也高于 0.91。

Conclusion: 该工作通过实现逼真且局部化的牙齿修改，而无需重新扫描，为牙科人工智能工作流程中的手术计划、患者沟通和有针对性的数据增强开辟了机会。

Abstract: Despite the growing importance of dental CBCT scans for diagnosis and
treatment planning, generating anatomically realistic scans with fine-grained
control remains a challenge in medical image synthesis. In this work, we
propose a novel conditional diffusion framework for 3D dental volume
generation, guided by tooth-level binary attributes that allow precise control
over tooth presence and configuration. Our approach integrates wavelet-based
denoising diffusion, FiLM conditioning, and masked loss functions to focus
learning on relevant anatomical structures. We evaluate the model across
diverse tasks, such as tooth addition, removal, and full dentition synthesis,
using both paired and distributional similarity metrics. Results show strong
fidelity and generalization with low FID scores, robust inpainting performance,
and SSIM values above 0.91 even on unseen scans. By enabling realistic,
localized modification of dentition without rescanning, this work opens
opportunities for surgical planning, patient communication, and targeted data
augmentation in dental AI workflows. The codes are available at:
https://github.com/djafar1/tooth-diffusion.

</details>


### [41] [GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting](https://arxiv.org/abs/2508.14278)
*Elena Alegret Regalado,Kunyi Li,Sen Wang,Siyun Liang,Michael Niemeyer,Stefano Gasperini,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: GALA: a novel framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS).


<details>
  <summary>Details</summary>
Motivation: existing methods still struggle to capture fine-grained, language-aware 3D representations from 2D images

Method: a cross-attention module with two learnable codebooks that encode view-independent semantic embeddings

Result: GALA distills a scene-specific 3D instance feature field via self-supervised contrastive learning. This design not only ensures intra-instance feature similarity but also supports seamless 2D and 3D open-vocabulary queries. It reduces memory consumption by avoiding per-Gaussian high-dimensional feature learning.

Conclusion: Extensive experiments on real-world datasets demonstrate GALA's remarkable open-vocabulary performance on both 2D and 3D.

Abstract: 3D scene reconstruction and understanding have gained increasing popularity,
yet existing methods still struggle to capture fine-grained, language-aware 3D
representations from 2D images. In this paper, we present GALA, a novel
framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting
(3DGS). GALA distills a scene-specific 3D instance feature field via
self-supervised contrastive learning. To extend to generalized language feature
fields, we introduce the core contribution of GALA, a cross-attention module
with two learnable codebooks that encode view-independent semantic embeddings.
This design not only ensures intra-instance feature similarity but also
supports seamless 2D and 3D open-vocabulary queries. It reduces memory
consumption by avoiding per-Gaussian high-dimensional feature learning.
Extensive experiments on real-world datasets demonstrate GALA's remarkable
open-vocabulary performance on both 2D and 3D.

</details>


### [42] [Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference](https://arxiv.org/abs/2508.14280)
*Ali Rasekh,Sepehr Kazemi Ranjbar,Simon Gottschalk*

Main category: cs.CV

TL;DR: This paper introduces a new benchmark and a CCI framework for multi-rationale explainable object recognition, achieving state-of-the-art results and improving both accuracy and rationale quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for explainable object recognition using vision-language models suffer from limitations in CLIP's text encoder and provide weak conditioning on explanatory structures. Prior datasets are often restricted to single, noisy rationales that fail to capture the full diversity of discriminative image features.

Method: The paper proposes a contrastive conditional inference (CCI) framework that explicitly models the probabilistic relationships among image embeddings, category labels, and rationales. It overcomes limitations of prompt-based conditioning without requiring training.

Result: The proposed approach achieves state-of-the-art results on the multi-rationale explainable object recognition benchmark, including strong zero-shot performance, and sets a new standard for both classification accuracy and rationale quality.

Conclusion: The paper introduces a new multi-rationale explainable object recognition benchmark and a contrastive conditional inference (CCI) framework that achieves state-of-the-art results, demonstrating strong zero-shot performance and setting a new standard for classification accuracy and rationale quality. The benchmark and framework together provide a more complete evaluation framework for future models.

Abstract: Explainable object recognition using vision-language models such as CLIP
involves predicting accurate category labels supported by rationales that
justify the decision-making process. Existing methods typically rely on
prompt-based conditioning, which suffers from limitations in CLIP's text
encoder and provides weak conditioning on explanatory structures. Additionally,
prior datasets are often restricted to single, and frequently noisy, rationales
that fail to capture the full diversity of discriminative image features. In
this work, we introduce a multi-rationale explainable object recognition
benchmark comprising datasets in which each image is annotated with multiple
ground-truth rationales, along with evaluation metrics designed to offer a more
comprehensive representation of the task. To overcome the limitations of
previous approaches, we propose a contrastive conditional inference (CCI)
framework that explicitly models the probabilistic relationships among image
embeddings, category labels, and rationales. Without requiring any training,
our framework enables more effective conditioning on rationales to predict
accurate object categories. Our approach achieves state-of-the-art results on
the multi-rationale explainable object recognition benchmark, including strong
zero-shot performance, and sets a new standard for both classification accuracy
and rationale quality. Together with the benchmark, this work provides a more
complete framework for evaluating future models in explainable object
recognition. The code will be made available online.

</details>


### [43] [OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA](https://arxiv.org/abs/2508.14286)
*Anushka A. Kore,Frank G. te Nijenhuis,Matthijs van der Sluijs,Wim van Zwam,Charles Majoie,Geert Lycklama à Nijeholt,Danny Ruijters,Frans Vos,Sandra Cornelissen,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: OccluNet, a new deep learning model, automates the detection of vascular occlusions in DSA sequences with high precision and recall.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of vascular occlusions during endovascular thrombectomy is critical in acute ischemic stroke, but interpretation of DSA sequences poses challenges due to anatomical complexity and time constraints.

Method: A spatio-temporal deep learning model that integrates YOLOX with transformer-based temporal attention mechanisms is proposed to automate occlusion detection in DSA sequences.

Result: OccluNet achieved a precision of 89.02% and a recall of 74.87% on DSA images from the MR CLEAN Registry, significantly outperforming the baseline models.

Conclusion: OccluNet, a spatio-temporal deep learning model, can capture temporally consistent features and significantly outperforms baseline models in detecting vascular occlusions.

Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy
(EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital
subtraction angiography (DSA) sequences poses challenges due to anatomical
complexity and time constraints. This work proposes OccluNet, a spatio-temporal
deep learning model that integrates YOLOX, a single-stage object detector, with
transformer-based temporal attention mechanisms to automate occlusion detection
in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on
either individual DSA frames or minimum intensity projections. Two
spatio-temporal variants were explored for OccluNet: pure temporal attention
and divided space-time attention. Evaluation on DSA images from the MR CLEAN
Registry revealed the model's capability to capture temporally consistent
features, achieving precision and recall of 89.02% and 74.87%, respectively.
OccluNet significantly outperformed the baseline models, and both attention
variants attained similar performance. Source code is available at
https://github.com/anushka-kore/OccluNet.git

</details>


### [44] [Pixels to Play: A Foundation Model for 3D Gameplay](https://arxiv.org/abs/2508.14295)
*Yuguang Yue,Chris Green,Samuel Hunt,Irakli Salia,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.CV

TL;DR: P2P0.1是一个基础模型，可以学习玩各种具有可识别的类人行为的3D视频游戏。


<details>
  <summary>Details</summary>
Motivation: 受新兴消费者和开发者用例（AI队友、可控NPC、个性化直播、辅助测试）的推动，我们认为代理必须依赖于玩家可用的相同像素流，并推广到新的游戏，且只需最少的游戏特定工程。

Method: 使用行为克隆进行端到端训练：来自工具化人类游戏玩法的标记演示与未标记的公共视频相结合，我们通过逆动力学模型对这些视频进行动作推算。具有自回归动作输出的仅解码器Transformer处理大型动作空间，同时在单个消费级GPU上保持延迟友好性。

Result: 定性结果表明，P2P0.1在简单的Roblox和经典MS-DOS游戏中表现出色的游戏能力。

Conclusion: P2P0.1在简单Roblox和经典MS-DOS游戏中表现出色的游戏能力，并概述了达到专家级别、文本条件控制所需的扩展和评估步骤。

Abstract: We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play
a wide range of 3D video games with recognizable human-like behavior. Motivated
by emerging consumer and developer use cases - AI teammates, controllable NPCs,
personalized live-streamers, assistive testers - we argue that an agent must
rely on the same pixel stream available to players and generalize to new titles
with minimal game-specific engineering. P2P0.1 is trained end-to-end with
behavior cloning: labeled demonstrations collected from instrumented human
game-play are complemented by unlabeled public videos, to which we impute
actions via an inverse-dynamics model. A decoder-only transformer with
auto-regressive action output handles the large action space while remaining
latency-friendly on a single consumer GPU. We report qualitative results
showing competent play across simple Roblox and classic MS-DOS titles,
ablations on unlabeled data, and outline the scaling and evaluation steps
required to reach expert-level, text-conditioned control.

</details>


### [45] [MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation](https://arxiv.org/abs/2508.14327)
*Guile Wu,David Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出了一种新的多模态多视图视频生成方法，用于自动驾驶，它可以生成高保真度和可控性的视频，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶视频生成方法主要集中于RGB视频生成，缺乏支持多模态视频生成的能力。然而，深度图和语义图等多模态数据对于自动驾驶中整体城市场景理解至关重要。虽然可以使用多个模型来生成不同的模态，但这增加了模型部署的难度，并且没有利用互补线索进行多模态数据生成。

Method: 构建了一个由模态共享组件和模态特定组件组成的统一扩散Transformer模型。然后，我们利用不同的条件输入将可控的场景结构和内容线索编码到统一的扩散模型中，用于多模态多视图视频生成。

Result: 该方法能够生成具有高保真度和可控性的多模态多视图城市场景视频，超越了现有技术水平。

Conclusion: 该方法能够在具有挑战性的真实世界自动驾驶数据集 nuScenes 上生成具有高保真度和可控性的多模态多视图城市场景视频，超越了最先进的方法。

Abstract: Video generation has recently shown superiority in urban scene synthesis for
autonomous driving. Existing video generation approaches to autonomous driving
primarily focus on RGB video generation and lack the ability to support
multi-modal video generation. However, multi-modal data, such as depth maps and
semantic maps, are crucial for holistic urban scene understanding in autonomous
driving. Although it is feasible to use multiple models to generate different
modalities, this increases the difficulty of model deployment and does not
leverage complementary cues for multi-modal data generation. To address this
problem, in this work, we propose a novel multi-modal multi-view video
generation approach to autonomous driving. Specifically, we construct a unified
diffusion transformer model composed of modal-shared components and
modal-specific components. Then, we leverage diverse conditioning inputs to
encode controllable scene structure and content cues into the unified diffusion
model for multi-modal multi-view video generation. In this way, our approach is
capable of generating multi-modal multi-view driving scene videos in a unified
framework. Our experiments on the challenging real-world autonomous driving
dataset, nuScenes, show that our approach can generate multi-modal multi-view
urban scene videos with high fidelity and controllability, surpassing the
state-of-the-art methods.

</details>


### [46] [Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates](https://arxiv.org/abs/2508.14343)
*Dian Ning,Dong Seog Han*

Main category: cs.CV

TL;DR: This paper introduces an inter-class relational loss to improve small object detection by using the spatial relationship between different objects. It also presents a new license plate dataset. The method improves mAP by 10.3% and 1.6% on YOLOv12-T and UAV-DETR, respectively.


<details>
  <summary>Details</summary>
Motivation: IoU-based losses fail to correctly update the gradient of small objects due to an extremely flat gradient, and the learning of small objects' gradients suffers from insufficient gradient updates during the update of multiple objects.

Method: The paper proposes an inter-class relational loss function that leverages the spatial relationship between objects (e.g., car and license plate) to guide small object predictions using larger objects. A loss punishment is added when the predicted bounding box of a small object is not within its related larger object, inversely proportional to the overlapped area.

Result: The paper introduces a new small vehicle multi-license plate dataset (SVMLP) and a novel inter-class relational loss function. The proposed ICR loss penalty improves the standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6% in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively.

Conclusion: The proposed inter-class relational (ICR) loss improves mAP by 10.3% and 1.6% on YOLOv12-T and UAV-DETR, respectively, without hyperparameter tuning, and it can be easily added to existing IoU-based losses.

Abstract: In one-stage multi-object detection tasks, various intersection over union
(IoU)-based solutions aim at smooth and stable convergence near the targets
during training. However, IoU-based losses fail to correctly update the
gradient of small objects due to an extremely flat gradient. During the update
of multiple objects, the learning of small objects' gradients suffers more
because of insufficient gradient updates. Therefore, we propose an inter-class
relational loss to efficiently update the gradient of small objects while not
sacrificing the learning efficiency of other objects based on the simple fact
that an object has a spatial relationship to another object (e.g., a car plate
is attached to a car in a similar position). When the predicted car plate's
bounding box is not within its car, a loss punishment is added to guide the
learning, which is inversely proportional to the overlapped area of the car's
and predicted car plate's bounding box. By leveraging the spatial relationship
at the inter-class level, the loss guides small object predictions using larger
objects and enhances latent information in deeper feature maps. In this paper,
we present twofold contributions using license plate detection as a case study:
(1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse
real-world scenarios with high-quality annotations; and (2) a novel inter-class
relational loss function designed to promote effective detection performance.
We highlight the proposed ICR loss penalty can be easily added to existing
IoU-based losses and enhance the performance. These contributions improve the
standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6%
in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without
any additional hyperparameter tuning. Code and dataset will be available soon.

</details>


### [47] [HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation](https://arxiv.org/abs/2508.14345)
*Gaston Gustavo Rios*

Main category: cs.CV

TL;DR: 提出了一种基于CMLPe的轻量级符号生成模型，以解决SLR中数据不足的问题，并通过合成数据预训练提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏足够的训练数据，手语识别(SLR)模型面临着严重的性能限制。

Method: 基于CMLPe的轻量级符号生成模型，结合合成数据预训练方法。

Result: 合成数据预训练在某些情况下优于传统的增强方法，并且在与它们一起实施时产生互补的好处。在不同的数据集上实现了显著的性能改进。

Conclusion: 通过合成数据预训练持续提高识别准确率，为LSFB和DiSPLaY数据集建立了新的最先进的结果。

Abstract: Sign Language Recognition (SLR) models face significant performance
limitations due to insufficient training data availability. In this article, we
address the challenge of limited data in SLR by introducing a novel and
lightweight sign generation model based on CMLPe. This model, coupled with a
synthetic data pretraining approach, consistently improves recognition
accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY
datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal
that synthetic data pretraining outperforms traditional augmentation methods in
some cases and yields complementary benefits when implemented alongside them.
Our approach democratizes sign generation and synthetic data pretraining for
SLR by providing computationally efficient methods that achieve significant
performance improvements across diverse datasets.

</details>


### [48] [Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model](https://arxiv.org/abs/2508.14349)
*Sean Fletcher,Gabby Scott,Douglas Currie,Xin Zhang,Yuqi Song,Bruce MacLeod*

Main category: cs.CV

TL;DR: 引入了一个新的显微镜图像数据集，该数据集捕获用不同浓度的 Taxol 处理的 C6 神经胶质瘤细胞。为了解决这个问题，我们提出了 ResAttention-KNN。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法需要专门的设备、熟练的人员和大量的样品准备，这使得它们成本高昂、劳动密集且不适合高通量或实时分析。目前还没有公开可用的数据集用于自动形态分析细胞对 Taxol 暴露的反应。

Method: 结合了 ResNet-50 与卷积块注意力模块，并在学习的嵌入空间中使用 k 最近邻分类器。

Result: Taxol 浓度分类的有效解决方案，并为未来研究建立基准。

Conclusion: 发布了一个新的显微镜图像数据集，用于捕获用不同浓度的 Taxol 处理的 C6 神经胶质瘤细胞，并提出了一个名为 ResAttention-KNN 的基线模型，该模型结合了 ResNet-50 与卷积块注意力模块，并在学习的嵌入空间中使用 k 最近邻分类器。

Abstract: Monitoring the effects of the chemotherapeutic agent Taxol at the cellular
level is critical for both clinical evaluation and biomedical research.
However, existing detection methods require specialized equipment, skilled
personnel, and extensive sample preparation, making them expensive,
labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep
learning approaches have shown great promise in medical and biological image
analysis, enabling automated, high-throughput assessment of cellular
morphology. Yet, no publicly available dataset currently exists for automated
morphological analysis of cellular responses to Taxol exposure. To address this
gap, we introduce a new microscopy image dataset capturing C6 glioma cells
treated with varying concentrations of Taxol. To provide an effective solution
for Taxol concentration classification and establish a benchmark for future
studies on this dataset, we propose a baseline model named ResAttention-KNN,
which combines a ResNet-50 with Convolutional Block Attention Modules and uses
a k-Nearest Neighbors classifier in the learned embedding space. This model
integrates attention-based refinement and non-parametric classification to
enhance robustness and interpretability. Both the dataset and implementation
are publicly released to support reproducibility and facilitate future research
in vision-based biomedical analysis.

</details>


### [49] [Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation](https://arxiv.org/abs/2508.14358)
*Zhujun Li,Shuo Zhang,Ioannis Stamos*

Main category: cs.CV

TL;DR: HRC-Pose 是一种用于类别级对象姿态估计的新型仅深度框架，它利用对比学习来学习点云表示，从而优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的类别级对象姿态估计方法仅依赖于 6D 姿态作为监督信号，而没有明确地捕捉姿态的内在连续性，从而导致预测的不一致和对未见姿态的泛化能力降低。

Method: HRC-Pose，一个用于类别级对象姿态估计的新型仅深度框架，它利用对比学习来学习保留 6D 姿态连续性的点云表示。HRC-Pose 将对象姿态解耦为旋转和平移分量，这些分量被单独编码并在整个网络中使用。引入了一种基于 6D 姿态感知分层排序方案的多任务、多类别场景的对比学习策略，该方案通过考虑旋转和平移差异以及类别信息来对比来自多个类别的点云。进一步设计了姿态估计模块，该模块分别处理学习到的旋转感知和翻译感知嵌入。

Result: HRC-Pose 在 REAL275 和 CAMERA25 基准测试中优于现有的仅深度最先进方法，并实时运行。

Conclusion: HRC-Pose 成功学习了连续特征空间，并在 REAL275 和 CAMERA25 基准测试中优于现有的仅深度最先进方法，并实时运行，展示了其有效性和在现实世界中的潜力。

Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size
of objects within given categories. Existing approaches for this task rely
solely on 6D poses as supervisory signals without explicitly capturing the
intrinsic continuity of poses, leading to inconsistencies in predictions and
reduced generalization to unseen poses. To address this limitation, we propose
HRC-Pose, a novel depth-only framework for category-level object pose
estimation, which leverages contrastive learning to learn point cloud
representations that preserve the continuity of 6D poses. HRC-Pose decouples
object pose into rotation and translation components, which are separately
encoded and leveraged throughout the network. Specifically, we introduce a
contrastive learning strategy for multi-task, multi-category scenarios based on
our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds
from multiple categories by considering rotational and translational
differences as well as categorical information. We further design pose
estimation modules that separately process the learned rotation-aware and
translation-aware embeddings. Our experiments demonstrate that HRC-Pose
successfully learns continuous feature spaces. Results on REAL275 and CAMERA25
benchmarks show that our method consistently outperforms existing depth-only
state-of-the-art methods and runs in real-time, demonstrating its effectiveness
and potential for real-world applications. Our code is at
https://github.com/zhujunli1993/HRC-Pose.

</details>


### [50] [Taming Transformer for Emotion-Controllable Talking Face Generation](https://arxiv.org/abs/2508.14359)
*Ziqi Zhang,Cheng Deng*

Main category: cs.CV

TL;DR: This paper introduces a new method for emotion-controllable talking face generation using pre-training, emotion-anchor representation, and an autoregressive transformer, achieving superior results on the MEAD dataset.


<details>
  <summary>Details</summary>
Motivation: Current methods for emotion-controllable talking face generation struggle to effectively model the multimodal relationship related to specific emotions and leverage this relationship to synthesize identity preserving emotional videos.

Method: A novel method is proposed that employs two pre-training strategies to disentangle audio and quantize videos, introduces an emotion-anchor (EA) representation, and uses an autoregressive transformer to model the global distribution of visual tokens.

Result: Extensive experiments on the MEAD dataset demonstrate the superiorities of the proposed method both qualitatively and quantitatively.

Conclusion: The proposed method demonstrates superior performance on the MEAD dataset for emotion-controllable talking face generation, both qualitatively and quantitatively.

Abstract: Talking face generation is a novel and challenging generation task, aiming at
synthesizing a vivid speaking-face video given a specific audio. To fulfill
emotion-controllable talking face generation, current methods need to overcome
two challenges: One is how to effectively model the multimodal relationship
related to the specific emotion, and the other is how to leverage this
relationship to synthesize identity preserving emotional videos. In this paper,
we propose a novel method to tackle the emotion-controllable talking face
generation task discretely. Specifically, we employ two pre-training strategies
to disentangle audio into independent components and quantize videos into
combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA)
representation that integrates the emotional information into visual tokens.
Finally, we introduce an autoregressive transformer to model the global
distribution of the visual tokens under the given conditions and further
predict the index sequence for synthesizing the manipulated videos. We conduct
experiments on the MEAD dataset that controls the emotion of videos conditioned
on multiple emotional audios. Extensive experiments demonstrate the
superiorities of our method both qualitatively and quantitatively.

</details>


### [51] [FastTracker: Real-Time and Accurate Visual Tracking](https://arxiv.org/abs/2508.14370)
*Hamidreza Hashempoor,Yu Dong Hwang*

Main category: cs.CV

TL;DR: A generalized multi-object tracking framework is proposed, which can handle multiple object types, with a particular emphasis on vehicle tracking in complex traffic scenes. The approach achieves strong performance on both the newly introduced dataset and several public benchmarks.


<details>
  <summary>Details</summary>
Motivation: Conventional multi-object tracking (MOT) systems are predominantly designed for pedestrian tracking and often exhibit limited generalization to other object categories. This paper presents a generalized tracking framework capable of handling multiple object types, with a particular emphasis on vehicle tracking in complex traffic scenes.

Method: an occlusion-aware re-identification mechanism and a road-structure-aware tracklet refinement strategy

Result: achieves strong performance on conventional benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets

Conclusion: The proposed approach achieves robust performance on both the newly introduced dataset and several public benchmarks, highlighting its effectiveness in general-purpose object tracking. While our framework is designed for generalized multi-class tracking, it also achieves strong performance on conventional benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets.

Abstract: Conventional multi-object tracking (MOT) systems are predominantly designed
for pedestrian tracking and often exhibit limited generalization to other
object categories. This paper presents a generalized tracking framework capable
of handling multiple object types, with a particular emphasis on vehicle
tracking in complex traffic scenes. The proposed method incorporates two key
components: (1) an occlusion-aware re-identification mechanism that enhances
identity preservation for heavily occluded objects, and (2) a
road-structure-aware tracklet refinement strategy that utilizes semantic scene
priors such as lane directions, crosswalks, and road boundaries to improve
trajectory continuity and accuracy. In addition, we introduce a new benchmark
dataset comprising diverse vehicle classes with frame-level tracking
annotations, specifically curated to support evaluation of vehicle-focused
tracking methods. Extensive experimental results demonstrate that the proposed
approach achieves robust performance on both the newly introduced dataset and
several public benchmarks, highlighting its effectiveness in general-purpose
object tracking. While our framework is designed for generalized multi-class
tracking, it also achieves strong performance on conventional benchmarks, with
HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark
are available: github.com/Hamidreza-Hashempoor/FastTracker,
huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.

</details>


### [52] [TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network](https://arxiv.org/abs/2508.14373)
*Runshi Zhang,Bimeng Jie,Yang He,Junchen Wang*

Main category: cs.CV

TL;DR: TCFNet, a Transformer-based network, addresses limitations in simulating face-bone shape transformations for surgical planning, achieving better accuracy and efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional biomechanical simulation methods are limited by computational time, labor-intensive data processing, and low accuracy. Deep learning methods have limitations in processing large-scale points, receptive fields, and complex preprocessing.

Method: Transformer-based coarse-to-fine point movement network (TCFNet) with a Transformer-based network and a local information aggregation network (LIA-Net).

Result: TCFNet demonstrates superior performance on gathered datasets.

Conclusion: TCFNet achieves outstanding evaluation metrics and visualization results compared to existing SOTA methods.

Abstract: Computer-aided surgical simulation is a critical component of orthognathic
surgical planning, where accurately simulating face-bone shape transformations
is significant. The traditional biomechanical simulation methods are limited by
their computational time consumption levels, labor-intensive data processing
strategies and low accuracy. Recently, deep learning-based simulation methods
have been proposed to view this problem as a point-to-point transformation
between skeletal and facial point clouds. However, these approaches cannot
process large-scale points, have limited receptive fields that lead to noisy
points, and employ complex preprocessing and postprocessing operations based on
registration. These shortcomings limit the performance and widespread
applicability of such methods. Therefore, we propose a Transformer-based
coarse-to-fine point movement network (TCFNet) to learn unique, complicated
correspondences at the patch and point levels for dense face-bone point cloud
transformations. This end-to-end framework adopts a Transformer-based network
and a local information aggregation network (LIA-Net) in the first and second
stages, respectively, which reinforce each other to generate precise point
movement paths. LIA-Net can effectively compensate for the neighborhood
precision loss of the Transformer-based network by modeling local geometric
structures (edges, orientations and relative position features). The previous
global features are employed to guide the local displacement using a gated
recurrent unit. Inspired by deformable medical image registration, we propose
an auxiliary loss that can utilize expert knowledge for reconstructing critical
organs.Compared with the existing state-of-the-art (SOTA) methods on gathered
datasets, TCFNet achieves outstanding evaluation metrics and visualization
results. The code is available at https://github.com/Runshi-Zhang/TCFNet.

</details>


### [53] [QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation](https://arxiv.org/abs/2508.14374)
*Wenyong Zhou,Boyu Li,Jiachen Ren,Taiqiang Wu,Zhilin Ai,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: introduce QuadINR, a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve superior performance with dramatic reductions in hardware consumption


<details>
  <summary>Details</summary>
Motivation: mitigate spectral bias by employing complex AFs, which often incur significant hardware overhead

Method: a hardware-efficient INR that utilizes piecewise quadratic AFs

Result: quadratic functions encompass rich harmonic content in their Fourier series, delivering enhanced expressivity for high-frequency signals, as verified through Neural Tangent Kernel (NTK) analysis

Conclusion: QuadINR achieves up to 2.06dB PSNR improvement over prior work, with an area of only 1914$\\mu$m$^2$ and a dynamic power of 6.14mW, reducing resource and power consumption by up to 97\% and improving latency by up to 93\% vs existing baselines.

Abstract: Implicit Neural Representations (INRs) encode discrete signals continuously
while addressing spectral bias through activation functions (AFs). Previous
approaches mitigate this bias by employing complex AFs, which often incur
significant hardware overhead. To tackle this challenge, we introduce QuadINR,
a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve
superior performance with dramatic reductions in hardware consumption. The
quadratic functions encompass rich harmonic content in their Fourier series,
delivering enhanced expressivity for high-frequency signals, as verified
through Neural Tangent Kernel (NTK) analysis. We develop a unified $N$-stage
pipeline framework that facilitates efficient hardware implementation of
various AFs in INRs. We demonstrate FPGA implementations on the VCU128 platform
and an ASIC implementation in a 28nm process. Experiments across images and
videos show that QuadINR achieves up to 2.06dB PSNR improvement over prior
work, with an area of only 1914$\mu$m$^2$ and a dynamic power of 6.14mW,
reducing resource and power consumption by up to 97\% and improving latency by
up to 93\% vs existing baselines.

</details>


### [54] [Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning](https://arxiv.org/abs/2508.14393)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Juming Xiong,Chongyu Qu,Mengmeng Yin,Yu Wang,Shilin Zhao,Haichun Yang,Daguang Xu,Yucheng Tang,Yuankai Huo*

Main category: cs.CV

TL;DR: Img2ST-Net：一种用于从组织学图像生成高分辨率空间转录组数据的新框架，它通过并行化和超像素表示提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高分辨率 ST 数据（如 Visium HD）上变得低效且不稳定，并且高分辨率 ST 固有的极端稀疏性和低表达水平使预测和评估变得复杂。

Method: 提出 Img2ST-Net，一种用于高效并行高分辨率 ST 预测的新型组织学到 ST 生成框架。通过将 HD ST 数据建模为超像素表示，将任务从图像到组学推断重新 формулировка 为具有数百或数千个输出通道的超内容图像生成问题。此外，还引入了 SSIM-ST，一种基于结构相似性的评估指标，专为高分辨率 ST 分析而定制。

Result: Img2ST-Net 提高了计算效率，更好地保留了空间组学数据固有的空间组织。SSIM-ST 增强了稀疏表达模式下的鲁棒性。该框架具有可扩展性和生物学连贯性。

Conclusion: Img2ST-Net 为高效、准确的大规模 ST 推断提供了一个原则性解决方案，为稳健且具有分辨率感知能力的下一代 ST 建模奠定了基础。

Abstract: Recent advances in multi-modal AI have demonstrated promising potential for
generating the currently expensive spatial transcriptomics (ST) data directly
from routine histology images, offering a means to reduce the high cost and
time-intensive nature of ST data acquisition. However, the increasing
resolution of ST, particularly with platforms such as Visium HD achieving 8um
or finer, introduces significant computational and modeling challenges.
Conventional spot-by-spot sequential regression frameworks become inefficient
and unstable at this scale, while the inherent extreme sparsity and low
expression levels of high-resolution ST further complicate both prediction and
evaluation. To address these limitations, we propose Img2ST-Net, a novel
histology-to-ST generation framework for efficient and parallel high-resolution
ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net
employs a fully convolutional architecture to generate dense, HD gene
expression maps in a parallelized manner. By modeling HD ST data as super-pixel
representations, the task is reformulated from image-to-omics inference into a
super-content image generation problem with hundreds or thousands of output
channels. This design not only improves computational efficiency but also
better preserves the spatial organization intrinsic to spatial omics data. To
enhance robustness under sparse expression patterns, we further introduce
SSIM-ST, a structural-similarity-based evaluation metric tailored for
high-resolution ST analysis. We present a scalable, biologically coherent
framework for high-resolution ST prediction. Img2ST-Net offers a principled
solution for efficient and accurate ST inference at scale. Our contributions
lay the groundwork for next-generation ST modeling that is robust and
resolution-aware. The source code has been made publicly available at
https://github.com/hrlblab/Img2ST-Net.

</details>


### [55] [CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities](https://arxiv.org/abs/2508.14405)
*Yue Gong,Shanyuan Liu,Liuzhuozheng Li,Jian Zhu,Bo Cheng,Liebucha Wu,Xiaoyu Wu,Yuhang Ma,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: CTA-Flux: An adaptation method fits the Chinese text inputs to Flux, a text-to-image generative model, which improves the generation quality and cultural authenticity.


<details>
  <summary>Details</summary>
Motivation: Existing approaches inadequately address culturally specific semantics, compromising image authenticity and quality.

Method: CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to control the Flux backbone directly, significantly reducing the number of parameters while enhancing the model's understanding of Chinese semantics.

Result: This integration significantly improves the generation quality and cultural authenticity without extensive retraining of the entire model, thus maintaining compatibility with existing text-to-image plugins.

Conclusion: CTA-flux supports Chinese and English prompts and achieves superior image generation quality, visual realism, and faithful depiction of Chinese semantics.

Abstract: We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method
fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative
model initially trained on the English corpus. Despite the notable image
generation ability conditioned on English text inputs, Flux performs poorly
when processing non-English prompts, particularly due to linguistic and
cultural biases inherent in predominantly English-centric training datasets.
Existing approaches, such as translating non-English prompts into English or
finetuning models for bilingual mappings, inadequately address culturally
specific semantics, compromising image authenticity and quality. To address
this issue, we introduce a novel method to bridge Chinese semantic
understanding with compatibility in English-centric TTI model communities.
Existing approaches relying on ControlNet-like architectures typically require
a massive parameter scale and lack direct control over Chinese semantics. In
comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to
control the Flux backbone directly, significantly reducing the number of
parameters while enhancing the model's understanding of Chinese semantics. This
integration significantly improves the generation quality and cultural
authenticity without extensive retraining of the entire model, thus maintaining
compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and
ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese
and English prompts and achieves superior image generation quality, visual
realism, and faithful depiction of Chinese semantics.

</details>


### [56] [MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing](https://arxiv.org/abs/2508.14423)
*Jeahun Sung,Changhyun Roh,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: MoCHA-former is proposed to remove moiré patterns in camera-captured screens, addressing limitations of existing methods with decoupled moiré adaptive demoiréing and spatio-temporal adaptive demoiréing.


<details>
  <summary>Details</summary>
Motivation: Frequency aliasing between the camera's color filter array (CFA) and the display's sub-pixels induces moir\'e patterns that severely degrade captured photos and videos. Existing demoir\'eing models suffer from limitations like spatially varying artifact strength, large-scale structures, channel-dependent statistics, and rapid temporal fluctuations.

Method: Moir\'e Conditioned Hybrid Adaptive Transformer (MoCHA-former), which comprises two key components: Decoupled Moir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing (STAD).

Result: MoCHA-former consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

Conclusion: MoCHA-former consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

Abstract: Recent advances in portable imaging have made camera-based screen capture
ubiquitous. Unfortunately, frequency aliasing between the camera's color filter
array (CFA) and the display's sub-pixels induces moir\'e patterns that severely
degrade captured photos and videos. Although various demoir\'eing models have
been proposed to remove such moir\'e patterns, these approaches still suffer
from several limitations: (i) spatially varying artifact strength within a
frame, (ii) large-scale and globally spreading structures, (iii)
channel-dependent statistics and (iv) rapid temporal fluctuations across
frames. We address these issues with the Moir\'e Conditioned Hybrid Adaptive
Transformer (MoCHA-former), which comprises two key components: Decoupled
Moir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing
(STAD). DMAD separates moir\'e and content via a Moir\'e Decoupling Block (MDB)
and a Detail Decoupling Block (DDB), then produces moir\'e-adaptive features
using a Moir\'e Conditioning Block (MCB) for targeted restoration. STAD
introduces a Spatial Fusion Block (SFB) with window attention to capture
large-scale structures, and a Feature Channel Attention (FCA) to model channel
dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs
implicit frame alignment without any explicit alignment module. We analyze
moir\'e characteristics through qualitative and quantitative studies, and
evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former
consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

</details>


### [57] [HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation](https://arxiv.org/abs/2508.14431)
*Bing Han,Yuhua Huang,Pan Gao*

Main category: cs.CV

TL;DR: HyperDiff通过集成扩散模型与HyperGCN来解决单目3D人体姿势估计中的深度模糊和遮挡问题，并在Human3.6M和MPI-INF-3DHP数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D人体姿势估计(HPE)在2D到3D的提升过程中经常遇到诸如深度模糊和遮挡之类的挑战。此外，传统方法在利用骨架结构信息时可能会忽略多尺度骨架特征，这可能会对姿势估计的准确性产生负面影响。

Method: 一种新颖的3D姿势估计方法，HyperDiff，它集成了扩散模型与HyperGCN。

Result: HyperDiff在Human3.6M和MPI-INF-3DHP数据集上实现了最先进的性能。

Conclusion: HyperDiff在Human3.6M和MPI-INF-3DHP数据集上实现了最先进的性能，并且可以灵活地适应不同的计算资源，以平衡性能和效率。

Abstract: Monocular 3D human pose estimation (HPE) often encounters challenges such as
depth ambiguity and occlusion during the 2D-to-3D lifting process.
Additionally, traditional methods may overlook multi-scale skeleton features
when utilizing skeleton structure information, which can negatively impact the
accuracy of pose estimation. To address these challenges, this paper introduces
a novel 3D pose estimation method, HyperDiff, which integrates diffusion models
with HyperGCN. The diffusion model effectively captures data uncertainty,
alleviating depth ambiguity and occlusion. Meanwhile, HyperGCN, serving as a
denoiser, employs multi-granularity structures to accurately model high-order
correlations between joints. This improves the model's denoising capability
especially for complex poses. Experimental results demonstrate that HyperDiff
achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP
datasets and can flexibly adapt to varying computational resources to balance
performance and efficiency.

</details>


### [58] [FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation](https://arxiv.org/abs/2508.14437)
*Gabriel Tjio,Jie Zhang,Xulei Yang,Yun Xing,Nhat Chung,Xiaofeng Cao,Ivor W. Tsang,Chee Keong Kwoh,Qing Guo*

Main category: cs.CV

TL;DR: FOCUS是一种频率调节方法，用于扩散驱动的输入自适应，可在测试时提升模型在语义分割和单目深度估计方面的性能，并减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 模型适应方法在适应领域变化时，难以平衡保留知识和适应领域变化之间的权衡，因为适应领域变化可能导致对任务相关知识的遗忘。

Method: 基于扩散的输入自适应框架内的频率调节方法。利用学习到的空间自适应频率先验，调节扩散驱动去噪过程中的反向步骤，以保留任务相关的语义信息。使用Y型频率预测网络（Y-FPN）分离高低频信息，并使用FrequencyMix数据增强方法提高鲁棒性。

Result: 在15种损坏类型和三个数据集上，FOCUS在语义分割和单目深度估计方面实现了最先进的平均性能。FOCUS还可以与现有的模型适应方法互补，因为我们可以从FOCUS去噪图像中导出伪标签以进行额外的监督。

Conclusion: FOCUS在语义分割和单目深度估计方面表现出色，且能减轻模型适应中的灾难性遗忘。

Abstract: Test-time adaptation enables models to adapt to evolving domains. However,
balancing the tradeoff between preserving knowledge and adapting to domain
shifts remains challenging for model adaptation methods, since adapting to
domain shifts can induce forgetting of task-relevant knowledge. To address this
problem, we propose FOCUS, a novel frequency-based conditioning approach within
a diffusion-driven input-adaptation framework. Utilising learned, spatially
adaptive frequency priors, our approach conditions the reverse steps during
diffusion-driven denoising to preserve task-relevant semantic information for
dense prediction.
  FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network
(Y-FPN) that disentangles high and low frequency information from noisy images.
This minimizes the computational costs involved in implementing our approach in
a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data
augmentation method that perturbs the images across diverse frequency bands,
which improves the robustness of our approach to diverse corruptions.
  We demonstrate the effectiveness of FOCUS for semantic segmentation and
monocular depth estimation across 15 corruption types and three datasets,
achieving state-of-the-art averaged performance. In addition to improving
standalone performance, FOCUS complements existing model adaptation methods
since we can derive pseudo labels from FOCUS-denoised images for additional
supervision. Even under limited, intermittent supervision with the pseudo
labels derived from the FOCUS denoised images, we show that FOCUS mitigates
catastrophic forgetting for recent model adaptation methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli](https://arxiv.org/abs/2508.14214)
*Mattson Ogg,Chace Ashcraft,Ritwik Bose,Raphael Norman-Tenazas,Michael Wolmetz*

Main category: cs.AI

TL;DR: GPT-4o的情感评估与人类相似，但在唤醒评估方面存在差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)越来越多地融入日常生活，了解这些工具如何评估情感刺激或情境至关重要。模型在这些情况下的行为与人类行为的一致性可以说明LLM在某些角色或互动中的有效性。

Method: 从多个流行的LLM中提取评级，用于单词和图像数据集，这些数据集先前已由人类根据其情感内容进行评级。

Result: GPT-4o的情感评级与人类非常相似，尤其是在幸福感方面。LLM在五类情感框架内比在二维框架内对齐得更好，并且LLM评级比人类评级更同质。

Conclusion: GPT-4o在执行相同的情感评级任务时，在多种模式、刺激和大多数评级尺度上与人类参与者非常相似（在许多情况下，r = 0.9 或更高）。然而，人类和LLM评级者之间的唤醒评级不太一致，而幸福评级的一致性最高。总体而言，LLM在五类（幸福、愤怒、悲伤、恐惧、厌恶）情感框架内比在二维（唤醒和效价）组织内对齐得更好。最后，LLM评级比人类评级更加同质。总的来说，这些结果开始描述LLM代理如何解释情感刺激，并突出生物和人工智能在关键行为领域中的异同。

Abstract: Emotions exert an immense influence over human behavior and cognition in both
commonplace and high-stress tasks. Discussions of whether or how to integrate
large language models (LLMs) into everyday life (e.g., acting as proxies for,
or interacting with, human agents), should be informed by an understanding of
how these tools evaluate emotionally loaded stimuli or situations. A model's
alignment with human behavior in these cases can inform the effectiveness of
LLMs for certain roles or interactions. To help build this understanding, we
elicited ratings from multiple popular LLMs for datasets of words and images
that were previously rated for their emotional content by humans. We found that
when performing the same rating tasks, GPT-4o responded very similarly to human
participants across modalities, stimuli and most rating scales (r = 0.9 or
higher in many cases). However, arousal ratings were less well aligned between
human and LLM raters, while happiness ratings were most highly aligned. Overall
LLMs aligned better within a five-category (happiness, anger, sadness, fear,
disgust) emotion framework than within a two-dimensional (arousal and valence)
organization. Finally, LLM ratings were substantially more homogenous than
human ratings. Together these results begin to describe how LLM agents
interpret emotional stimuli and highlight similarities and differences among
biological and artificial intelligence in key behavioral domains.

</details>


### [60] [Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions](https://arxiv.org/abs/2508.14294)
*Maria Leonor Pacheco,Fabio Somenzi,Dananjay Srinivas,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 提出了一种神经符号方法，结合决策程序和LLM来解释复杂决策序列，并通过解决Hitori谜题来展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 解释复杂序列的决策。

Method: 结合决策程序和大型语言模型(llm)的神经符号方法。

Result: 为Hitori谜题的解决方案生成解释。

Conclusion: 实现了辅助人类解决Hitori谜题的工具，并提供了实验证据证明其有效性。

Abstract: We propose a neurosymbolic approach to the explanation of complex sequences
of decisions that combines the strengths of decision procedures and Large
Language Models (LLMs). We demonstrate this approach by producing explanations
for the solutions of Hitori puzzles. The rules of Hitori include local
constraints that are effectively explained by short resolution proofs. However,
they also include a connectivity constraint that is more suitable for visual
explanations. Hence, Hitori provides an excellent testing ground for a flexible
combination of SAT solvers and LLMs. We have implemented a tool that assists
humans in solving Hitori puzzles, and we present experimental evidence of its
effectiveness.

</details>


### [61] [Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning](https://arxiv.org/abs/2508.14410)
*Beinuo Yang,Qishen Zhou,Junyi Li,Xingchen Su,Simon Hu*

Main category: cs.AI

TL;DR: 本文提出了ORThought框架，通过思维链推理自动化优化建模过程，并在复杂问题上表现出色，同时分析了其优缺点。


<details>
  <summary>Details</summary>
Motivation: 优化建模（OM）对于解决复杂的决策问题至关重要。然而，这个过程仍然耗时且容易出错，严重依赖于领域专家。虽然大型语言模型（LLM）通过其自然语言理解和推理能力，在应对这些挑战方面显示出了希望，但目前的方法面临三个关键限制：高基准标记错误率达到42%，狭窄的评估范围，只考虑最优值，以及由于严重依赖多智能体系统或模型微调而导致的计算效率低下。

Method: 提出了ORThought，一个利用专家级优化建模原则通过思维链推理来自动化OM过程的新框架。

Result: ORThought优于包括多智能体框架在内的现有方法，尤其是在复杂的优化问题上具有显著优势。

Conclusion: ORThought在复杂优化问题上优于现有方法，并分析了其成功因素和失败模式，为未来基于LLM的优化建模研究提供了有价值的见解。

Abstract: Optimization Modeling (OM) is essential for solving complex decision-making
problems. However, the process remains time-consuming and error-prone, heavily
relying on domain experts. While Large Language Models (LLMs) show promise in
addressing these challenges through their natural language understanding and
reasoning capabilities, current approaches face three critical limitations:
high benchmark labeling error rates reaching up to 42\%, narrow evaluation
scope that only considers optimal values, and computational inefficiency due to
heavy reliance on multi-agent systems or model fine-tuning. In this work, we
first enhance existing datasets through systematic error correction and more
comprehensive annotation. Additionally, we introduce LogiOR, a new optimization
modeling benchmark from the logistics domain, containing more complex problems
with standardized annotations. Furthermore, we present ORThought, a novel
framework that leverages expert-level optimization modeling principles through
chain-of-thought reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought outperforms existing
approaches, including multi-agent frameworks, with particularly significant
advantages on complex optimization problems. Finally, we provide a systematic
analysis of our method, identifying critical success factors and failure modes,
providing valuable insights for future research on LLM-based optimization
modeling.

</details>


### [62] [The Agent Behavior: Model, Governance and Challenges in the AI Digital Age](https://arxiv.org/abs/2508.14415)
*Qiang Zhang,Pei Yan,Yijia Xu,Chuanpo Fu,Yong Fang,Yang Liu*

Main category: cs.AI

TL;DR: This paper proposes models to analyze the behavioral differences between humans and agents in networked environments to address challenges in trust, responsibility, ethics, and security. The effectiveness of the model is verified through real-world cases. Finally, the paper discusses future research directions.


<details>
  <summary>Details</summary>
Motivation: Advancements in AI have led to agents in networked environments increasingly mirroring human behavior, thereby blurring the boundary between artificial and human actors and bringing significant challenges in trust, responsibility, ethics, security and etc.

Method: This paper proposes the "Network Behavior Lifecycle" model, the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity (HABD)" model.

Result: The effectiveness of the model is verified through real-world cases such as red team penetration and blue team defense.

Conclusion: The paper discusses future research directions in dynamic cognitive governance architecture, behavioral disparity quantification, and meta-governance protocol stacks, aiming to provide a theoretical foundation and technical roadmap for secure and trustworthy human-agent collaboration.

Abstract: Advancements in AI have led to agents in networked environments increasingly
mirroring human behavior, thereby blurring the boundary between artificial and
human actors in specific contexts. This shift brings about significant
challenges in trust, responsibility, ethics, security and etc. The difficulty
in supervising of agent behaviors may lead to issues such as data contamination
and unclear accountability. To address these challenges, this paper proposes
the "Network Behavior Lifecycle" model, which divides network behavior into 6
stages and systematically analyzes the behavioral differences between humans
and agents at each stage. Based on these insights, the paper further introduces
the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity
(HABD)" model, which examine the fundamental distinctions between human and
agent behaviors across 5 dimensions: decision mechanism, execution efficiency,
intention-behavior consistency, behavioral inertia, and irrational patterns.
The effectiveness of the model is verified through real-world cases such as red
team penetration and blue team defense. Finally, the paper discusses future
research directions in dynamic cognitive governance architecture, behavioral
disparity quantification, and meta-governance protocol stacks, aiming to
provide a theoretical foundation and technical roadmap for secure and
trustworthy human-agent collaboration.

</details>


### [63] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.


<details>
  <summary>Details</summary>
Motivation: Tasks that involve active perception, collaborative reasoning, and perspective taking pose persistent challenges for current LLM-based systems.

Method: structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. A structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision.

Result: L-type examples slightly reduce clarification requests and overall action steps, but they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions.

Conclusion: Structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [64] [LeanGeo: Formalizing Competitional Geometry problems in Lean](https://arxiv.org/abs/2508.14644)
*Chendong Song,Zihan Wang,Frederick Pu,Haiming Wang,Xiaohan Lin,Junqi Liu,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: Introduces LeanGeo, a formal system and benchmark for geometry problem-solving, addressing limitations in existing systems and enabling rigorous proof verification. It also evaluates Large Language Models on the benchmark, revealing the necessity for further progress in automated geometric reasoning.


<details>
  <summary>Details</summary>
Motivation: Most existing geometry solving systems cannot express problems within a unified framework, thus are difficult to integrate with other mathematical fields. Besides, since most geometric proofs rely on intuitive diagrams, verifying geometry problems is particularly challenging.

Method: introduce LeanGeo, a unified formal system for formalizing and solving competition-level geometry problems within the Lean 4 theorem prover. LeanGeo features a comprehensive library of high-level geometric theorems with Lean's foundational logic, enabling rigorous proof verification and seamless integration with Mathlib. We also present LeanGeo-Bench, a formal geometry benchmark in LeanGeo, comprising problems from the International Mathematical Olympiad (IMO) and other advanced sources.

Result: LeanGeo, a unified formal system. LeanGeo features a comprehensive library of high-level geometric theorems with Lean's foundational logic, enabling rigorous proof verification and seamless integration with Mathlib. We also present LeanGeo-Bench, a formal geometry benchmark in LeanGeo

Conclusion: The evaluation demonstrates the capabilities and limitations of state-of-the-art Large Language Models on this benchmark, highlighting the need for further advancements in automated geometric reasoning. We open source the theorem library and the benchmark of LeanGeo

Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most
existing geometry solving systems cannot express problems within a unified
framework, thus are difficult to integrate with other mathematical fields.
Besides, since most geometric proofs rely on intuitive diagrams, verifying
geometry problems is particularly challenging. To address these gaps, we
introduce LeanGeo, a unified formal system for formalizing and solving
competition-level geometry problems within the Lean 4 theorem prover. LeanGeo
features a comprehensive library of high-level geometric theorems with Lean's
foundational logic, enabling rigorous proof verification and seamless
integration with Mathlib. We also present LeanGeo-Bench, a formal geometry
benchmark in LeanGeo, comprising problems from the International Mathematical
Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the
capabilities and limitations of state-of-the-art Large Language Models on this
benchmark, highlighting the need for further advancements in automated
geometric reasoning. We open source the theorem library and the benchmark of
LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.

</details>


### [65] [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](https://arxiv.org/abs/2508.14654)
*Peilin Ji,Xiao Xue,Simeng Wang,Wenhao Yan*

Main category: cs.AI

TL;DR: H-J是一个分层多智能体框架，它集成了知识引导的提示、熵约束生成和反馈驱动的优化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 近年来，极端城市降雨事件的频率不断增加，给应急调度系统带来了严峻的挑战。城市洪水经常导致严重的交通拥堵和服务中断，威胁着公共安全和出行。然而，有效的决策仍然受到三个关键挑战的阻碍：(1)在相互竞争的目标(如交通流量、任务完成和风险缓解)之间进行权衡需要动态的、上下文感知的策略；(2)快速变化的环境条件使静态规则变得不足；(3)LLM生成的策略经常遭受语义不稳定和执行不一致的影响。

Method: 我们介绍了一种分层多智能体框架H-J，该框架集成了知识引导的提示、熵约束生成和反馈驱动的优化。

Result: H-J在交通平稳性、任务成功率和系统鲁棒性方面优于基于规则和强化学习的基线。

Conclusion: H-J在交通平稳性、任务成功率和系统鲁棒性方面优于基于规则和强化学习的基线，展示了基于不确定性感知、知识约束的LLM方法在增强城市洪水响应能力方面的潜力。

Abstract: In recent years, the increasing frequency of extreme urban rainfall events
has posed significant challenges to emergency scheduling systems. Urban
flooding often leads to severe traffic congestion and service disruptions,
threatening public safety and mobility. However, effective decision making
remains hindered by three key challenges: (1) managing trade-offs among
competing goals (e.g., traffic flow, task completion, and risk mitigation)
requires dynamic, context-aware strategies; (2) rapidly evolving environmental
conditions render static rules inadequate; and (3) LLM-generated strategies
frequently suffer from semantic instability and execution inconsistency.
Existing methods fail to align perception, global optimization, and multi-agent
coordination within a unified framework. To tackle these challenges, we
introduce H-J, a hierarchical multi-agent framework that integrates
knowledge-guided prompting, entropy-constrained generation, and feedback-driven
optimization. The framework establishes a closed-loop pipeline spanning from
multi-source perception to strategic execution and continuous refinement. We
evaluate H-J on real-world urban topology and rainfall data under three
representative conditions: extreme rainfall, intermittent bursts, and daily
light rain. Experiments show that H-J outperforms rule-based and
reinforcement-learning baselines in traffic smoothness, task success rate, and
system robustness. These findings highlight the promise of uncertainty-aware,
knowledge-constrained LLM-based approaches for enhancing resilience in urban
flood response.

</details>


### [66] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: Introduces MCP-Universe, a comprehensive benchmark to evaluate LLMs in realistic tasks, finding performance limitations in SOTA models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces.

Method: Introduction of MCP-Universe, a comprehensive benchmark designed to evaluate LLMs in realistic tasks through interaction with real-world MCP servers. Implementation of execution-based evaluators, including format evaluators, static evaluators, and dynamic evaluators.

Result: Even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. The benchmark poses a significant long-context challenge and introduces an unknown-tools challenge. Cursor cannot achieve better performance than standard ReAct frameworks.

Conclusion: Leading LLMs exhibit significant performance limitations in realistic tasks through interaction with real-world MCP servers. Enterprise-level agents cannot achieve better performance than standard ReAct frameworks.

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [67] [Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines](https://arxiv.org/abs/2508.14710)
*Swantje Plambeck,Ali Salamati,Eyke Huellermeier,Goerschwin Fey*

Main category: cs.AI

TL;DR: This paper presents a data-driven approach using PAC learning to determine the safety probability of CPS with a discrete abstraction, validated on an automated lane-keeping system.


<details>
  <summary>Details</summary>
Motivation: Cyber-Physical Systems (CPS) are complex systems that require powerful models for tasks like verification, diagnosis, or debugging. Often, suitable models are not available and manual extraction is difficult. Data-driven approaches then provide a solution.

Method: The approach is based on the Probably Approximately Correct (PAC) learning paradigm and follows an active learning paradigm.

Result: The paper proposes a data-driven approach to determine the safety probability of the system on a finite horizon of n time steps, especially providing an additional confidence on the determined probability.

Conclusion: The paper validates the approach with a case study on an automated lane-keeping system.

Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models
for tasks like verification, diagnosis, or debugging. Often, suitable models
are not available and manual extraction is difficult. Data-driven approaches
then provide a solution to, e.g., diagnosis tasks and verification problems
based on data collected from the system. In this paper, we consider CPS with a
discrete abstraction in the form of a Mealy machine. We propose a data-driven
approach to determine the safety probability of the system on a finite horizon
of n time steps. The approach is based on the Probably Approximately Correct
(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic
and probabilistic reachability analysis of systems, especially providing an
additional confidence on the determined probability. The learning process
follows an active learning paradigm, where new learning data is sampled in a
guided way after an initial learning set is collected. We validate the approach
with a case study on an automated lane-keeping system.

</details>


### [68] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: This paper argues for a thicker definition of AI introspection and shows LLMs can appear to introspect lightly while failing to do so meaningfully under the new definition.


<details>
  <summary>Details</summary>
Motivation: Whether AI models can introspect is an increasingly important practical question. But there is no consensus on how introspection is to be defined.

Method: Experiments where LLMs reason about their internal temperature parameters

Result: They can appear to have lightweight introspection while failing to meaningfully introspect per our proposed definition.

Conclusion: LLMs can appear to have lightweight introspection while failing to meaningfully introspect per our proposed definition.

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [69] [Accelerating K-Core Computation in Temporal Graphs](https://arxiv.org/abs/2508.14147)
*Zhuo Ma,Dong Wen,Hanchen Wang,Wentao Li,Wenjie Zhang,Xuemin Lin*

Main category: cs.DB

TL;DR: This paper proposes a new algorithm for computing temporal k-cores in temporal graphs that is more efficient and scalable than existing methods. The algorithm's runtime is proportional to the size of the output, and the computation of 'core times' is relatively inexpensive.


<details>
  <summary>Details</summary>
Motivation: the problem of enumerating all temporal k-cores given a query time range and a temporal graph, which suffers from poor efficiency and scalability in the state-of-the-art solution

Method: a novel algorithm to compute all temporal  𝑘 -cores based on core times

Result: the cost of computing core times is much lower, which demonstrates the close relevance between our overall running time and the result size.

Conclusion: The algorithmic running time is bounded by the size of all resulting temporal k-cores, which is optimal.

Abstract: We address the problem of enumerating all temporal k-cores given a query time
range and a temporal graph, which suffers from poor efficiency and scalability
in the state-of-the-art solution. Motivated by an existing concept called core
times, we propose a novel algorithm to compute all temporal $k$-cores based on
core times and prove that the algorithmic running time is bounded by the size
of all resulting temporal k-cores, which is optimal in this scenario.
Meanwhile, we show that the cost of computing core times is much lower, which
demonstrates the close relevance between our overall running time and the
result size.

</details>


### [70] [Efficient Size Constraint Community Search over Heterogeneous Information Networks](https://arxiv.org/abs/2508.14356)
*Xinjian Zhang,Lu Chen,Chengfei Liu,Rui Zhou,Bo Ning*

Main category: cs.DB

TL;DR: 本文研究了异构信息网络中具有大小限制的社区搜索问题，提出了一个(k, P)-truss模型来衡量社区凝聚力，并设计了精确和启发式算法来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 在异构信息网络（HIN）中的社区搜索的目标是识别一组密切相关的目标节点，其中包括查询目标节点。在实践中，由于资源有限，通常会施加大小约束，而大多数现有的HIN社区搜索工作都忽略了这一点。在本文中，我们将大小限制的社区搜索问题引入到HIN数据中。

Method: 提出了一个新颖的B&B框架，可以有效地生成大小为s的目标节点集。然后，定制了新颖的边界、分支、总排序和候选减少优化，使框架能够有效地得出最佳结果。还设计了一种利用HIN的结构特性来有效获得高质量初始解的启发式算法，该初始解作为全局下界，以进一步增强上述优化。

Result: 证明了这个问题是NP-hard。

Conclusion: 提出的方法在真实世界的数据集上进行了广泛的实验，证明了其有效性和效率。

Abstract: The goal of community search in heterogeneous information networks (HINs) is
to identify a set of closely related target nodes that includes a query target
node. In practice, a size constraint is often imposed due to limited resources,
which has been overlooked by most existing HIN community search works. In this
paper, we introduce the size-bounded community search problem to HIN data.
Specifically, we propose a refined (k, P)-truss model to measure community
cohesiveness, aiming to identify the most cohesive community of size s that
contains the query node. We prove that this problem is NP-hard. To solve this
problem, we develop a novel B\&B framework that efficiently generates target
node sets of size s. We then tailor novel bounding, branching, total ordering,
and candidate reduction optimisations, which enable the framework to
efficiently lead to an optimum result. We also design a heuristic algorithm
leveraging structural properties of HINs to efficiently obtain a high-quality
initial solution, which serves as a global lower bound to further enhance the
above optimisations. Building upon these, we propose two exact algorithms that
enumerate combinations of edges and nodes, respectively. Extensive experiments
on real-world datasets demonstrate the effectiveness and efficiency of the
proposed methods.

</details>


### [71] [A DBMS-independent approach for capturing provenance polynomials through query rewriting](https://arxiv.org/abs/2508.14608)
*Paulo Pintor,Rogério Costa,José Moreira*

Main category: cs.DB

TL;DR: This paper introduces a DBMS-independent query rewriting approach for provenance polynomials in SQL, which supports aggregations and nested queries, and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring data integrity, traceability, and accountability is important in data-driven ecosystems. Current implementations of provenance polynomials have limitations in handling aggregations and nested queries, and are often tightly coupled to a single DBMS.

Method: A query rewriting-based approach for annotating SQL queries with provenance polynomials.

Result: The proposed solution delivers a comprehensive implementation of the theoretical formalisms and demonstrates improved performance and scalability, outperforming existing methods.

Conclusion: This paper presents a comprehensive and DBMS-independent implementation of provenance polynomials for SQL queries, supporting SPJUA operations and nested queries. The experimental results show improved performance and scalability compared to existing methods.

Abstract: In today's data-driven ecosystems, ensuring data integrity, traceability and
accountability is important. Provenance polynomials constitute a powerful
formalism for tracing the origin and the derivations made to produce database
query results. Despite their theoretical expressiveness, current
implementations have limitations in handling aggregations and nested queries,
and some of them and tightly coupled to a single Database Management System
(DBMS), hindering interoperability and broader applicability.
  This paper presents a query rewriting-based approach for annotating
Structured Query Language (SQL) queries with provenance polynomials. The
proposed methods are DBMS-independent and support
Select-Projection-Join-Union-Aggregation (SPJUA) operations and nested queries,
through recursive propagation of provenance annotations. This constitutes the
first full implementation of semiring-based theory for provenance polynomials
extended with semimodule structures. It also presents an experimental
evaluation to assess the validity of the proposed methods and compare the
performance against state-of-the-art systems using benchmark data and queries.
The results indicate that our solution delivers a comprehensive implementation
of the theoretical formalisms proposed in the literature, and demonstrates
improved performance and scalability, outperforming existing methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [72] [FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering](https://arxiv.org/abs/2508.14052)
*Chanyeol Choi,Jihoon Kwon,Alejandro Lopez-Lira,Chaewoon Kim,Minjae Kim,Juneha Hwang,Jaeseon Ha,Hojun Choi,Suyeol Yun,Yongjin Kim,Yongjae Lee*

Main category: cs.IR

TL;DR: Introduces FinAgentBench, a new benchmark for evaluating LLM-based retrieval with multi-step reasoning in the financial domain. It includes a dataset of 3,429 examples and shows that fine-tuning improves performance.


<details>
  <summary>Details</summary>
Motivation: Traditional IR methods often fall short in retrieval accuracy in the financial domain, and there is no benchmark to evaluate multi-step reasoning capabilities of LLMs in this domain.

Method: The paper introduces FinAgentBench, a large-scale benchmark with 3,429 expert-annotated examples on S&P-100 listed firms, and assesses LLM agents' ability to identify relevant document types and pinpoint key passages. The evaluation framework separates these two reasoning steps.

Result: The paper evaluates state-of-the-art models and demonstrates that targeted fine-tuning can significantly improve agentic retrieval performance.

Conclusion: The paper introduces FinAgentBench, a benchmark for evaluating retrieval with multi-step reasoning in finance, and demonstrates how targeted fine-tuning can improve performance. The dataset will be released publicly.

Abstract: Accurate information retrieval (IR) is critical in the financial domain,
where investors must identify relevant information from large collections of
documents. Traditional IR methods-whether sparse or dense-often fall short in
retrieval accuracy, as it requires not only capturing semantic similarity but
also performing fine-grained reasoning over document structure and
domain-specific knowledge. Recent advances in large language models (LLMs) have
opened up new opportunities for retrieval with multi-step reasoning, where the
model ranks passages through iterative reasoning about which information is
most relevant to a given query. However, there exists no benchmark to evaluate
such capabilities in the financial domain. To address this gap, we introduce
FinAgentBench, the first large-scale benchmark for evaluating retrieval with
multi-step reasoning in finance -- a setting we term agentic retrieval. The
benchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms
and assesses whether LLM agents can (1) identify the most relevant document
type among candidates, and (2) pinpoint the key passage within the selected
document. Our evaluation framework explicitly separates these two reasoning
steps to address context limitations. This design enables to provide a
quantitative basis for understanding retrieval-centric LLM behavior in finance.
We evaluate a suite of state-of-the-art models and further demonstrated how
targeted fine-tuning can significantly improve agentic retrieval performance.
Our benchmark provides a foundation for studying retrieval-centric LLM behavior
in complex, domain-specific tasks for finance. We will release the dataset
publicly upon acceptance of the paper and plan to expand and share dataset for
the full S&P 500 and beyond.

</details>


### [73] [Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks](https://arxiv.org/abs/2508.14058)
*Jingmao Zhang,Zhiting Zhao,Yunqi Lin,Jianghong Ma,Tianjun Wei,Haijun Zhang,Xiaofeng Zhang*

Main category: cs.IR

TL;DR: DP2Rec是一种新的推荐模型，它利用游戏时长和多模态信息来提高推荐的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 视频游戏行业的爆炸式增长对推荐系统提出了迫切需求，现有模型未能充分利用游戏时长这一独特的行为信号，并忽略了多模态信息增强多样性的潜力。

Method: 提出了一种新颖的双阶段游戏时长引导推荐模型DP2Rec，旨在联合优化准确性和多样性。该模型包括一个游戏时长引导的兴趣强度探索模块和一个游戏时长引导的多模态随机游走模块。

Result: 在真实游戏数据集上的大量实验表明，DP2Rec在推荐准确性和多样性方面优于现有方法。

Conclusion: DP2Rec在推荐准确性和多样性方面优于现有方法。

Abstract: The explosive growth of the video game industry has created an urgent need
for recommendation systems that can scale with expanding catalogs and maintain
user engagement. While prior work has explored accuracy and diversity in
recommendations, existing models underutilize playtime, a rich behavioral
signal unique to gaming platforms, and overlook the potential of multimodal
information to enhance diversity. In this paper, we propose DP2Rec, a novel
Dual-Phase Playtime-guided Recommendation model designed to jointly optimize
accuracy and diversity. First, we introduce a playtime-guided interest
intensity exploration module that separates strong and weak preferences via
dual-beta modeling, enabling fine-grained user profiling and more accurate
recommendations. Second, we present a playtime-guided multimodal random walks
module that simulates player exploration using transitions guided by both
playtime-derived interest similarity and multimodal semantic similarity. This
mechanism preserves core preferences while promoting cross-category discovery
through latent semantic associations and adaptive category balancing. Extensive
experiments on a real-world game dataset show that DP2Rec outperforms existing
methods in both recommendation accuracy and diversity.

</details>


### [74] [Graph Neural Network for Product Recommendation on the Amazon Co-purchase Graph](https://arxiv.org/abs/2508.14059)
*Mengyang Cao,Frank F. Yang,Yi Jin,Yijun Yan*

Main category: cs.IR

TL;DR: This study assessed the abilities of four GNN architectures on the Amazon Product Co-purchase Network under link prediction settings and demonstrated each model's performance characteristics for deploying GNN in real-world recommendation scenarios.


<details>
  <summary>Details</summary>
Motivation: Identifying relevant information among massive volumes of data is a challenge for modern recommendation systems. Graph Neural Networks (GNNs) have demonstrated significant potential by utilizing structural and semantic relationships through graph-based learning.

Method: This study assessed the abilities of four GNN architectures, LightGCN, GraphSAGE, GAT, and PinSAGE, on the Amazon Product Co-purchase Network under link prediction settings.

Result: We examined practical trade-offs between architectures, model performance, scalability, training complexity and generalization.

Conclusion: This study demonstrated each model's performance characteristics for deploying GNN in real-world recommendation scenarios.

Abstract: Identifying relevant information among massive volumes of data is a challenge
for modern recommendation systems. Graph Neural Networks (GNNs) have
demonstrated significant potential by utilizing structural and semantic
relationships through graph-based learning. This study assessed the abilities
of four GNN architectures, LightGCN, GraphSAGE, GAT, and PinSAGE, on the Amazon
Product Co-purchase Network under link prediction settings. We examined
practical trade-offs between architectures, model performance, scalability,
training complexity and generalization. The outcomes demonstrated each model's
performance characteristics for deploying GNN in real-world recommendation
scenarios.

</details>


### [75] [GPT-2 as a Compression Preprocessor: Improving Gzip for Structured Text Domains](https://arxiv.org/abs/2508.14061)
*Anurag Kumar Ojha*

Main category: cs.IR

TL;DR: a GPT-based preprocessor for such domain-specific files to improve the compression


<details>
  <summary>Details</summary>
Motivation: Data with such volume and complexity needs to be compressed before storing and transmitting efficiently. one of the limitations of gzip is that domain-specific formats like JSON, XML, HTML, and log files, while structured, may have semantic repetition but not syntactic repetition, which gzip finds difficult to compress.

Method: a GPT-based preprocessor

Result: We used different types of both real-world and synthetically generated data, such as logs and HTML files, for the experiment of the proposed model.

Conclusion: We found promising results and an improvement of the Defence logs by 0.34 per cent and HTML files by 5.8 per cent.

Abstract: In the modern era, large volumes of data are being produced continuously,
especially in domain-specific fields such as medical records and clinical
files, defence logs and HTML-based web traffic. Data with such volume and
complexity needs to be compressed before storing and transmitting efficiently.
Data compression has gained significant attention from modern researchers,
resulting in the development of fast and efficient compression algorithms such
as Gzip. However, since gzip works on the principle of repetition of binary
patterns, one of the limitations of gzip is that domain-specific formats like
JSON, XML, HTML, and log files, while structured, may have semantic repetition
but not syntactic repetition, which gzip finds difficult to compress. In this
article, we propose a GPT-based preprocessor for such domain-specific files. We
propose a pipeline made up of GPT-2 taking domain-specific files as input,
which pattern-based compressors like gzip find difficult to work on. The
preprocessor results are output in a file that is designed for compressors like
gzip. After preprocessing, the gzip works on the other end of the pipeline and
compresses the data as usual. We used different types of both real-world and
synthetically generated data, such as logs and HTML files, for the experiment
of the proposed model. We found promising results and an improvement of the
Defence logs by 0.34 per cent and HTML files by 5.8 per cent.

</details>


### [76] [A Multi-Agent Approach to Neurological Clinical Reasoning](https://arxiv.org/abs/2508.14063)
*Moran Sorka,Alon Gorenshtein,Dvir Aran,Shahar Shelly*

Main category: cs.IR

TL;DR: LLM 在神经推理方面需要系统评估。多智能体系统通过模仿专门的认知过程，显著提高了复杂医学推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在医学领域显示出前景，但它们处理专业神经推理的能力需要系统评估。

Method: 使用来自以色列神经病学委员会认证考试的 305 个问题开发了一个综合基准，并沿着三个复杂性维度进行分类：事实知识深度、临床概念整合和推理复杂性。我们评估了十个 LLM，使用了基础模型、检索增强生成 (RAG) 和一种新颖的多代理系统。

Result: OpenAI-o1 实现了最高的基线性能（90.9% 的准确率），而专门的医学模型表现不佳（Meditron-70B 为 52.9%）。 RAG 提供了适度的收益，但在复杂的推理问题上的有效性有限。 相比之下，我们的多代理框架将神经推理分解为专门的认知功能，包括问题分析、知识检索、答案合成和验证，取得了显着改进，特别是对于中端模型。

Conclusion: 使用多代理方法显著增强了复杂医学推理，为具有挑战性的临床环境中的 AI 辅助提供了有希望的方向。

Abstract: Large language models (LLMs) have shown promise in medical domains, but their
ability to handle specialized neurological reasoning requires systematic
evaluation. We developed a comprehensive benchmark using 305 questions from
Israeli Board Certification Exams in Neurology, classified along three
complexity dimensions: factual knowledge depth, clinical concept integration,
and reasoning complexity. We evaluated ten LLMs using base models,
retrieval-augmented generation (RAG), and a novel multi-agent system. Results
showed significant performance variation. OpenAI-o1 achieved the highest base
performance (90.9% accuracy), while specialized medical models performed poorly
(52.9% for Meditron-70B). RAG provided modest benefits but limited
effectiveness on complex reasoning questions. In contrast, our multi-agent
framework, decomposing neurological reasoning into specialized cognitive
functions including question analysis, knowledge retrieval, answer synthesis,
and validation, achieved dramatic improvements, especially for mid-range
models. The LLaMA 3.3-70B-based agentic system reached 89.2% accuracy versus
69.5% for its base model, with substantial gains on level 3 complexity
questions. The multi-agent approach transformed inconsistent subspecialty
performance into uniform excellence, addressing neurological reasoning
challenges that persisted with RAG enhancement. We validated our approach using
an independent dataset of 155 neurological cases from MedQA. Results confirm
that structured multi-agent approaches designed to emulate specialized
cognitive processes significantly enhance complex medical reasoning, offering
promising directions for AI assistance in challenging clinical contexts.

</details>


### [77] [An automatic patent literature retrieval system based on LLM-RAG](https://arxiv.org/abs/2508.14064)
*Yao Ding,Yuqing Wu,Ziyang Ding*

Main category: cs.IR

TL;DR: 本文提出了一种基于LLM-RAG的专利检索框架，提高了专利检索的准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统关键词和基于规则的检索方法难以解决复杂的查询意图或捕捉跨技术领域的语义关联，导致检索结果不完整和相关性低。

Method: 集成大型语言模型（LLM）与检索增强生成（RAG）技术，构建自动化专利检索框架。

Result: gpt35turbo0125RAG配置在语义匹配准确率上达到80.5%，召回率达到92.1%，比基线LLM方法提高了28个百分点，并且在跨领域分类和语义聚类任务中表现出强大的泛化能力。

Conclusion: LLM-RAG集成方法在智能专利检索中有效，为下一代人工智能驱动的知识产权分析平台奠定了基础。

Abstract: With the acceleration of technological innovation efficient retrieval and
classification of patent literature have become essential for intellectual
property management and enterprise RD Traditional keyword and rulebased
retrieval methods often fail to address complex query intents or capture
semantic associations across technical domains resulting in incomplete and
lowrelevance results This study presents an automated patent retrieval
framework integrating Large Language Models LLMs with RetrievalAugmented
Generation RAG technology The system comprises three components: 1) a
preprocessing module for patent data standardization, 2) a highefficiency
vector retrieval engine leveraging LLMgenerated embeddings, and 3) a
RAGenhanced query module that combines external document retrieval with
contextaware response generation Evaluations were conducted on the Google
Patents dataset 20062024 containing millions of global patent records with
metadata such as filing date domain and status The proposed gpt35turbo0125RAG
configuration achieved 805 semantic matching accuracy and 92.1% recall
surpassing baseline LLM methods by 28 percentage points The framework also
demonstrated strong generalization in crossdomain classification and semantic
clustering tasks These results validate the effectiveness of LLMRAG integration
for intelligent patent retrieval providing a foundation for nextgeneration
AIdriven intellectual property analysis platforms

</details>


### [78] [Personalized Contest Recommendation in Fantasy Sports](https://arxiv.org/abs/2508.14065)
*Madiraju Srilakshmi,Kartavya Kothari,Kamlesh Marathe,Vedavyas Chigurupati,Hitesh Kapoor*

Main category: cs.IR

TL;DR: 该论文提出了一个基于Wide and Deep Interaction Ranker (WiDIR)的可扩展的梦幻体育竞赛推荐系统，并在实际生产环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在日常梦幻体育运动中，玩家通过组建运动员团队来相互竞争，这些团队根据真实体育比赛中的实际情况获得梦幻积分。由于玩家偏好也存在很大差异，因此竞赛个性化是将玩家与竞赛相匹配的重要工具。

Method: 该论文的核心是一个Wide and Deep Interaction Ranker (WiDIR)。

Result: 在线实验表明，该系统在召回率和其他关键业务指标方面比其他候选模型有显著提高。

Conclusion: 该论文提出了一个可扩展的竞赛推荐系统，并在一个大型梦幻体育平台上进行了生产化部署，在线实验表明，该系统在召回率和其他关键业务指标方面比其他候选模型有显著提高。

Abstract: In daily fantasy sports, players enter into "contests" where they compete
against each other by building teams of athletes that score fantasy points
based on what actually occurs in a real-life sports match. For any given sports
match, there are a multitude of contests available to players, with substantial
variation across 3 main dimensions: entry fee, number of spots, and the prize
pool distribution. As player preferences are also quite heterogeneous, contest
personalization is an important tool to match players with contests. This paper
presents a scalable contest recommendation system, powered by a Wide and Deep
Interaction Ranker (WiDIR) at its core. We productionized this system at our
company, one of the large fantasy sports platforms with millions of daily
contests and millions of players, where online experiments show a marked
improvement over other candidate models in terms of recall and other critical
business metrics.

</details>


### [79] [Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation](https://arxiv.org/abs/2508.14066)
*Lorenz Brehme,Benedikt Dornauer,Thomas Ströhle,Maximilian Ehrhart,Ruth Breu*

Main category: cs.IR

TL;DR: This paper explores the current state of RAG adoption in real-world settings by interviewing 13 industry practitioners. It finds that RAG applications are mostly limited to domain-specific QA tasks and system evaluation is predominantly conducted by humans.


<details>
  <summary>Details</summary>
Motivation: significant lack of research on its practical application in industrial contexts

Method: semi-structured interview study with 13 industry practitioners

Result: an overview of industry use cases, a consolidated list of system requirements, key challenges and lessons learned from practical experiences, and an analysis of current industry evaluation methods

Conclusion: Current RAG applications are mostly limited to domain-specific QA tasks, with systems still in prototype stages; industry requirements focus primarily on data protection, security, and quality, while issues such as ethics, bias, and scalability receive less attention; data preprocessing remains a key challenge, and system evaluation is predominantly conducted by humans rather than automated methods.

Abstract: Retrieval-Augmented Generation (RAG) is a well-established and rapidly
evolving field within AI that enhances the outputs of large language models by
integrating relevant information retrieved from external knowledge sources.
While industry adoption of RAG is now beginning, there is a significant lack of
research on its practical application in industrial contexts. To address this
gap, we conducted a semistructured interview study with 13 industry
practitioners to explore the current state of RAG adoption in real-world
settings. Our study investigates how companies apply RAG in practice, providing
(1) an overview of industry use cases, (2) a consolidated list of system
requirements, (3) key challenges and lessons learned from practical
experiences, and (4) an analysis of current industry evaluation methods. Our
main findings show that current RAG applications are mostly limited to
domain-specific QA tasks, with systems still in prototype stages; industry
requirements focus primarily on data protection, security, and quality, while
issues such as ethics, bias, and scalability receive less attention; data
preprocessing remains a key challenge, and system evaluation is predominantly
conducted by humans rather than automated methods.

</details>


### [80] [RewardRank: Optimizing True Learning-to-Rank Utility](https://arxiv.org/abs/2508.14180)
*Gaurav Bhatt,Kiran Koshy Thekumparampil,Tanmay Gangwani,Tesi Xiao,Leonid Sigal*

Main category: cs.IR

TL;DR: RewardRank uses counterfactual reward learning to model user behavior and optimize ranking, outperforming baselines on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional ranking systems rely on proxy loss functions that fail to capture complex user behavioral biases, leading to misalignment with actual user utility.

Method: The method involves training a deep utility model to estimate user engagement for item permutations and optimizing a ranking policy to maximize predicted utility via differentiable soft permutation operators.

Result: RewardRank consistently outperforms strong baselines on Baidu-ULTR and Amazon KDD Cup datasets.

Conclusion: This paper introduces RewardRank, a data-driven framework that models user behavior through counterfactual reward learning to optimize ranking policies. Experiments show it outperforms strong baselines on large-scale benchmarks.

Abstract: Traditional ranking systems rely on proxy loss functions that assume
simplistic user behavior, such as users preferring a rank list where items are
sorted by hand-crafted relevance. However, real-world user interactions are
influenced by complex behavioral biases, including position bias, brand
affinity, decoy effects, and similarity aversion, which these objectives fail
to capture. As a result, models trained on such losses often misalign with
actual user utility, such as the probability of any click or purchase across
the ranked list. In this work, we propose a data-driven framework for modeling
user behavior through counterfactual reward learning. Our method, RewardRank,
first trains a deep utility model to estimate user engagement for entire item
permutations using logged data. Then, a ranking policy is optimized to maximize
predicted utility via differentiable soft permutation operators, enabling
end-to-end training over the space of factual and counterfactual rankings. To
address the challenge of evaluation without ground-truth for unseen
permutations, we introduce two automated protocols: (i) $\textit{KD-Eval}$,
using a position-aware oracle for counterfactual reward estimation, and (ii)
$\textit{LLM-Eval}$, which simulates user preferences via large language
models. Experiments on large-scale benchmarks, including Baidu-ULTR and the
Amazon KDD Cup datasets, demonstrate that our approach consistently outperforms
strong baselines, highlighting the effectiveness of modeling user behavior
dynamics for utility-optimized ranking. Our code is available at:
https://github.com/GauravBh1010tt/RewardRank

</details>


### [81] [You Only Evaluate Once: A Tree-based Rerank Method at Meituan](https://arxiv.org/abs/2508.14420)
*Shuli Wang,Yinqiu Huang,Changhao Li,Yuan Zhou,Yonggang Liu,Yongqiang Zhang,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.IR

TL;DR: 这篇论文提出了一种名为YOLOR的单阶段重排序方法，解决了传统两阶段方法的不一致问题，并在美团外卖平台上成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有的重排序方法通常采用两阶段搜索范式，存在GSU容易遗漏ESU中的高价值列表的不一致问题。

Method: 该方法包括一个树状上下文提取模块（TCEM）和一个上下文缓存模块（CCM）。

Result: YOLOR在公共和工业数据集上验证了其性能，已成功部署在美团外卖平台上。

Conclusion: 这篇论文提出了一种名为YOLOR的单阶段重排序方法，并在公共和工业数据集上验证了其性能，已成功部署在美团外卖平台上。

Abstract: Reranking plays a crucial role in modern recommender systems by capturing the
mutual influences within the list. Due to the inherent challenges of
combinatorial search spaces, most methods adopt a two-stage search paradigm: a
simple General Search Unit (GSU) efficiently reduces the candidate space, and
an Exact Search Unit (ESU) effectively selects the optimal sequence. These
methods essentially involve making trade-offs between effectiveness and
efficiency, while suffering from a severe \textbf{inconsistency problem}, that
is, the GSU often misses high-value lists from ESU. To address this problem, we
propose YOLOR, a one-stage reranking method that removes the GSU while
retaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context
Extraction Module (TCEM) that hierarchically aggregates multi-scale contextual
features to achieve "list-level effectiveness", and (2) a Context Cache Module
(CCM) that enables efficient feature reuse across candidate permutations to
achieve "permutation-level efficiency". Extensive experiments across public and
industry datasets validate YOLOR's performance, and we have successfully
deployed YOLOR on the Meituan food delivery platform.

</details>


### [82] [Diverse Negative Sampling for Implicit Collaborative Filtering](https://arxiv.org/abs/2508.14468)
*Yueqing Xuan,Kacper Sokol,Mark Sanderson,Jeffrey Chan*

Main category: cs.IR

TL;DR: 提出DivNS，一种考虑负样本多样性的负采样方法，可以提高推荐系统的泛化能力和推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有的负采样策略通常从密集区域过度采样负样本，导致同质负数据和有限的模型表达能力。

Method: 提出了一种名为多样性负采样 (DivNS) 的新方法，该方法在负采样过程中显式地考虑了负训练数据的多样性。DivNS 首先找到具有较大偏好分数的难负样本，并构建用户特定的缓存来存储未使用但信息量很大的负样本。然后，其多样性增强采样器从缓存中选择不同的负样本子集，同时确保与用户的难负样本不同。最后，合成负样本生成器将选择的不同负样本与难负样本结合起来，以形成更有效的训练数据。

Result: 在四个公共数据集上的大量实验表明，DivNS 在提高推荐质量的同时保持了计算效率。

Conclusion: DivNS通过生成信息量大且多样化的负样本，提高了推荐系统的泛化能力和推荐质量，并在计算效率方面表现良好。

Abstract: Implicit collaborative filtering recommenders are usually trained to learn
user positive preferences. Negative sampling, which selects informative
negative items to form negative training data, plays a crucial role in this
process. Since items are often clustered in the latent space, existing negative
sampling strategies normally oversample negative items from the dense regions.
This leads to homogeneous negative data and limited model expressiveness. In
this paper, we propose Diverse Negative Sampling (DivNS), a novel approach that
explicitly accounts for diversity in negative training data during the negative
sampling process. DivNS first finds hard negative items with large preference
scores and constructs user-specific caches that store unused but highly
informative negative samples. Then, its diversity-augmented sampler selects a
diverse subset of negative items from the cache while ensuring dissimilarity
from the user's hard negatives. Finally, a synthetic negatives generator
combines the selected diverse negatives with hard negatives to form more
effective training data. The resulting synthetic negatives are both informative
and diverse, enabling recommenders to learn a broader item space and improve
their generalisability. Extensive experiments on four public datasets
demonstrate the effectiveness of DivNS in improving recommendation quality
while maintaining computational efficiency.

</details>


### [83] [Distribution-Guided Auto-Encoder for User Multimodal Interest Cross Fusion](https://arxiv.org/abs/2508.14485)
*Moyu Zhang,Yongxiang Tang,Yujun Jin,Jinxin Hu,Yu Zhang*

Main category: cs.IR

TL;DR: This paper proposes DMAE to address the problem of item ID sparsity and neglecting the contextual influence of user behavior sequences in existing multimodal recommendation methods.


<details>
  <summary>Details</summary>
Motivation: Traditional ID-based methods often encounter data sparsity problems stemming from the sparse nature of ID features.Existing multimodal recommendation methods typically employ early fusion approaches, which focus primarily on combining text and image features, while neglecting the contextual influence of user behavior sequences.

Method: Distribution-Guided Multimodal-Interest Auto-Encoder (DMAE), which achieves the cross fusion of user multimodal interest at the behavioral level

Result: DMAE achieves the cross fusion of user multimodal interest at the behavioral level

Conclusion: extensive experiments demonstrate the superiority of DMAE

Abstract: Traditional recommendation methods rely on correlating the embedding vectors
of item IDs to capture implicit collaborative filtering signals to model the
user's interest in the target item. Consequently, traditional ID-based methods
often encounter data sparsity problems stemming from the sparse nature of ID
features. To alleviate the problem of item ID sparsity, recommendation models
incorporate multimodal item information to enhance recommendation accuracy.
However, existing multimodal recommendation methods typically employ early
fusion approaches, which focus primarily on combining text and image features,
while neglecting the contextual influence of user behavior sequences. This
oversight prevents dynamic adaptation of multimodal interest representations
based on behavioral patterns, consequently restricting the model's capacity to
effectively capture user multimodal interests. Therefore, this paper proposes
the Distribution-Guided Multimodal-Interest Auto-Encoder (DMAE), which achieves
the cross fusion of user multimodal interest at the behavioral
level.Ultimately, extensive experiments demonstrate the superiority of DMAE.

</details>


### [84] [Global-Distribution Aware Scenario-Specific Variational Representation Learning Framework](https://arxiv.org/abs/2508.14493)
*Moyu Zhang,Yujun Jin,Jinxin Hu,Yu Zhang*

Main category: cs.IR

TL;DR: 提出了 GSVR 框架，以学习鲁棒的场景特定表征，从而提升多场景推荐效果。


<details>
  <summary>Details</summary>
Motivation: 当前的推荐方法通常使用统一的框架为不同的场景提供个性化推荐，但是它们通常采用共享的底层表征，这在一定程度上阻碍了模型捕捉场景独特性的能力。用户和物品在不同的场景中应该表现出特定的特征，因此需要学习场景特定的表征来区分场景。然而，用户和物品在不同场景中的交互变化会导致数据稀疏问题，阻碍了场景特定表征的获取。

Method: 提出了一个全局分布感知场景特定变分表征学习框架 (GSVR)，该框架可以应用于现有的多场景方法。

Result: 大量的实验结果证实了 GSVR 在帮助现有的多场景推荐方法学习更鲁棒的表征方面的有效性。

Conclusion: GSVR 通过学习更鲁棒的表征，能够有效帮助现有的多场景推荐方法。

Abstract: With the emergence of e-commerce, the recommendations provided by commercial
platforms must adapt to diverse scenarios to accommodate users' varying
shopping preferences. Current methods typically use a unified framework to
offer personalized recommendations for different scenarios. However, they often
employ shared bottom representations, which partially hinders the model's
capacity to capture scenario uniqueness. Ideally, users and items should
exhibit specific characteristics in different scenarios, prompting the need to
learn scenario-specific representations to differentiate scenarios. Yet,
variations in user and item interactions across scenarios lead to data sparsity
issues, impeding the acquisition of scenario-specific representations. To learn
robust scenario-specific representations, we introduce a Global-Distribution
Aware Scenario-Specific Variational Representation Learning Framework (GSVR)
that can be directly applied to existing multi-scenario methods. Specifically,
considering the uncertainty stemming from limited samples, our approach employs
a probabilistic model to generate scenario-specific distributions for each user
and item in each scenario, estimated through variational inference (VI).
Additionally, we introduce the global knowledge-aware multinomial distributions
as prior knowledge to regulate the learning of the posterior user and item
distributions, ensuring similarities among distributions for users with akin
interests and items with similar side information. This mitigates the risk of
users or items with fewer records being overwhelmed in sparse scenarios.
Extensive experimental results affirm the efficacy of GSVR in assisting
existing multi-scenario recommendation methods in learning more robust
representations.

</details>


### [85] [DGenCTR: Towards a Universal Generative Paradigm for Click-Through Rate Prediction via Discrete Diffusion](https://arxiv.org/abs/2508.14500)
*Moyu Zhang,Yun Chen,Yujun Jin,Jinxin Hu,Yu Zhang*

Main category: cs.IR

TL;DR: 提出了一种用于 CTR 预测的两阶段离散扩散生成框架 (DGenCTR)，该框架通过生成模型理解数据分布的能力，缓解传统判别模型在标签稀缺空间中的约束, 并且经过大量的实验验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: CTR 模型严重依赖目标项目和用户之间的大量交叉特征来估计点击该项目的概率，而丢弃这些交叉特征会显着损害模型性能。因此，为了利用生成模型理解数据分布的能力，从而缓解传统判别模型在标签稀缺空间中的约束。

Method: 我们提出了一个专为 CTR 任务设计的新型样本级生成范例：一个两阶段离散扩散生成 CTR 训练框架 (DGenCTR)。

Result: 离线实验和在线 A/B 测试

Conclusion: 该框架的有效性已通过广泛的离线实验和在线 A/B 测试得到最终验证。

Abstract: Recent advances in generative models have inspired the field of recommender
systems to explore generative approaches, but most existing research focuses on
sequence generation, a paradigm ill-suited for click-through rate (CTR)
prediction. CTR models critically depend on a large number of cross-features
between the target item and the user to estimate the probability of clicking on
the item, and discarding these cross-features will significantly impair model
performance. Therefore, to harness the ability of generative models to
understand data distributions and thereby alleviate the constraints of
traditional discriminative models in label-scarce space, diverging from the
item-generation paradigm of sequence generation methods, we propose a novel
sample-level generation paradigm specifically designed for the CTR task: a
two-stage Discrete Diffusion-Based Generative CTR training framework (DGenCTR).
This two-stage framework comprises a diffusion-based generative pre-training
stage and a CTR-targeted supervised fine-tuning stage for CTR. Finally,
extensive offline experiments and online A/B testing conclusively validate the
effectiveness of our framework.

</details>


### [86] [MISS: Multi-Modal Tree Indexing and Searching with Lifelong Sequential Behavior for Retrieval Recommendation](https://arxiv.org/abs/2508.14515)
*Chengcheng Guo,Junda She,Kuo Cai,Shiyao Wang,Qigen Hu,Qiang Luo,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: introduce multi-modal information and lifelong sequence model within the advanced tree-based retrieval model


<details>
  <summary>Details</summary>
Motivation: Existing retrieval methods mostly relay on interaction information, potentially disregarding valuable multi-modal information. In retrieval stage, it is difficult to utilize lifelong behavior because of a large corpus of candidate items.

Method: We propose collaborative general search unit (Co-GSU) and multi-modal general search unit (MM-GSU) for multi-perspective interests searching; multi-modal index tree, which is built using the multi-modal embedding to precisely represent item similarity

Result: the pioneering exploration of leveraging multi-modal information and lifelong sequence model within the advanced tree-based retrieval model

Conclusion: We propose Multi-modal Indexing and Searching with lifelong Sequence (MISS), which contains a multi-modal index tree and a multi-modal lifelong sequence modeling module.

Abstract: Large-scale industrial recommendation systems typically employ a two-stage
paradigm of retrieval and ranking to handle huge amounts of information. Recent
research focuses on improving the performance of retrieval model. A promising
way is to introduce extensive information about users and items. On one hand,
lifelong sequential behavior is valuable. Existing lifelong behavior modeling
methods in ranking stage focus on the interaction of lifelong behavior and
candidate items from retrieval stage. In retrieval stage, it is difficult to
utilize lifelong behavior because of a large corpus of candidate items. On the
other hand, existing retrieval methods mostly relay on interaction information,
potentially disregarding valuable multi-modal information. To solve these
problems, we represent the pioneering exploration of leveraging multi-modal
information and lifelong sequence model within the advanced tree-based
retrieval model. We propose Multi-modal Indexing and Searching with lifelong
Sequence (MISS), which contains a multi-modal index tree and a multi-modal
lifelong sequence modeling module. Specifically, for better index structure, we
propose multi-modal index tree, which is built using the multi-modal embedding
to precisely represent item similarity. To precisely capture diverse user
interests in user lifelong sequence, we propose collaborative general search
unit (Co-GSU) and multi-modal general search unit (MM-GSU) for
multi-perspective interests searching.

</details>


### [87] [OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service](https://arxiv.org/abs/2508.14646)
*Zhipeng Wei,Kuo Cai,Junda She,Jie Chen,Minghao Chen,Yang Zeng,Qiang Luo,Wencong Zeng,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: OneLoc is proposed to tackle the challenges of video recommendation in Kuaishou's local life service by utilizing geographic information and reinforcement learning. It achieves significant improvements in GMV and order numbers.


<details>
  <summary>Details</summary>
Motivation: Recommending videos in Kuaishou App's local life service is challenging because it requires considering user interest and real-time location. Existing end-to-end generative recommendation models don't address the specific challenges of local life service, such as fully utilizing geographic information and balancing multiple objectives.

Method: The paper proposes OneLoc, which leverages geo-aware semantic ID, geo-aware self-attention, and neighbor-aware prompt. Reinforcement learning with geographic and GMV rewards is used to balance multiple objectives.

Result: OneLoc has been deployed in Kuaishou App, serving 400 million daily active users and achieving 21.016% and 17.891% improvements in GMV and order numbers, respectively.

Conclusion: OneLoc achieves outstanding offline and online performance, leading to significant improvements in GMV and order numbers in Kuaishou App's local life service.

Abstract: Local life service is a vital scenario in Kuaishou App, where video
recommendation is intrinsically linked with store's location information. Thus,
recommendation in our scenario is challenging because we should take into
account user's interest and real-time location at the same time. In the face of
such complex scenarios, end-to-end generative recommendation has emerged as a
new paradigm, such as OneRec in the short video scenario, OneSug in the search
scenario, and EGA in the advertising scenario. However, in local life service,
an end-to-end generative recommendation model has not yet been developed as
there are some key challenges to be solved. The first challenge is how to make
full use of geographic information. The second challenge is how to balance
multiple objectives, including user interests, the distance between user and
stores, and some other business objectives. To address the challenges, we
propose OneLoc. Specifically, we leverage geographic information from different
perspectives: (1) geo-aware semantic ID incorporates both video and geographic
information for tokenization, (2) geo-aware self-attention in the encoder
leverages both video location similarity and user's real-time location, and (3)
neighbor-aware prompt captures rich context information surrounding users for
generation. To balance multiple objectives, we use reinforcement learning and
propose two reward functions, i.e., geographic reward and GMV reward. With the
above design, OneLoc achieves outstanding offline and online performance. In
fact, OneLoc has been deployed in local life service of Kuaishou App. It serves
400 million active users daily, achieving 21.016% and 17.891% improvements in
terms of gross merchandise value (GMV) and orders numbers.

</details>


### [88] [Benefiting from Negative yet Informative Feedback by Contrasting Opposing Sequential Patterns](https://arxiv.org/abs/2508.14786)
*Veronika Ivanova,Evgeny Frolov,Alexey Vasilev*

Main category: cs.IR

TL;DR: 提出了一种利用正负反馈训练序列推荐模型的方法，通过对比学习提高推荐质量。


<details>
  <summary>Details</summary>
Motivation: 传统的序列学习模型通常只关注和预测正向交互，忽略了减少推荐中带有负面反馈的物品可以提高用户对服务的满意度。此外，负面反馈可以为更准确地识别真实用户兴趣提供有用的信号。

Method: 训练两个分别处理正向和负向交互序列的 Transformer 编码器。

Result: 与最先进的序列推荐方法相比，该方法在增加真阳性指标的同时，减少了错误推广的负面项目数量，证明了其有效性。

Conclusion: 该方法通过使用包含正负交叉熵以及精心设计的对比项的复合损失函数，将两种类型的反馈纳入序列推荐器的训练目标中，从而在增加真阳性指标的同时，减少了错误推广的负面项目数量。

Abstract: We consider the task of learning from both positive and negative feedback in
a sequential recommendation scenario, as both types of feedback are often
present in user interactions. Meanwhile, conventional sequential learning
models usually focus on considering and predicting positive interactions,
ignoring that reducing items with negative feedback in recommendations improves
user satisfaction with the service. Moreover, the negative feedback can
potentially provide a useful signal for more accurate identification of true
user interests. In this work, we propose to train two transformer encoders on
separate positive and negative interaction sequences. We incorporate both types
of feedback into the training objective of the sequential recommender using a
composite loss function that includes positive and negative cross-entropy as
well as a cleverly crafted contrastive term, that helps better modeling
opposing patterns. We demonstrate the effectiveness of this approach in terms
of increasing true-positive metrics compared to state-of-the-art sequential
recommendation methods while reducing the number of wrongly promoted negative
items.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [89] [Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students](https://arxiv.org/abs/2508.14057)
*Pablo G. Almeida,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Pedro Silva,Eduardo Luz*

Main category: cs.LG

TL;DR: GNNs can improve student dropout prediction if tabular data is transformed into graphs effectively, but the right graph generation and GNN architecture are key.


<details>
  <summary>Details</summary>
Motivation: Predicting students at risk of dropout allows for timely interventions, and GNNs offer a potential advantage by capturing complex relationships inherent in student data.

Method: Transform tabular student data into graph structures using clustering techniques and compare the performance of GNNs against established tabular models.

Result: A specific GNN configuration, GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior performance, notably improving the macro F1-score and accuracy over the strongest tabular baseline (XGBoost).

Conclusion: GNNs have the potential to improve student dropout prediction, but the graph generation strategy and GNN architecture selection are critical.

Abstract: Student dropout is a significant challenge in educational systems worldwide,
leading to substantial social and economic costs. Predicting students at risk
of dropout allows for timely interventions. While traditional Machine Learning
(ML) models operating on tabular data have shown promise, Graph Neural Networks
(GNNs) offer a potential advantage by capturing complex relationships inherent
in student data if structured as graphs. This paper investigates whether
transforming tabular student data into graph structures, primarily using
clustering techniques, enhances dropout prediction accuracy. We compare the
performance of GNNs (a custom Graph Convolutional Network (GCN) and GraphSAGE)
on these generated graphs against established tabular models (Random Forest
(RF), XGBoost, and TabNet) using a real-world student dataset. Our experiments
explore various graph construction strategies based on different clustering
algorithms (K-Means, HDBSCAN) and dimensionality reduction techniques
(Principal Component Analysis (PCA), Uniform Manifold Approximation and
Projection (UMAP)). Our findings demonstrate that a specific GNN configuration,
GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior
performance, notably improving the macro F1-score by approximately 7 percentage
points and accuracy by nearly 2 percentage points over the strongest tabular
baseline (XGBoost). However, other GNN configurations and graph construction
methods did not consistently surpass tabular models, emphasizing the critical
role of the graph generation strategy and GNN architecture selection. This
highlights both the potential of GNNs and the challenges in optimally
transforming tabular data for graph-based learning in this domain.

</details>


### [90] [Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation](https://arxiv.org/abs/2508.14069)
*Chinmoy Biswas,Nafis Faisal,Vivek Chowdhury,Abrar Al-Shadid Abir,Sabir Mahmud,Mithon Rahman,Shaikh Anowarul Fattah,Hafiz Imtiaz*

Main category: cs.LG

TL;DR: 本研究表明，对于具有稀疏性的负荷预测问题，高斯插值是一种合适的选择，并且LSTM模型表现最好。


<details>
  <summary>Details</summary>
Motivation: 实际数据集中缺失或零值导致的数据稀疏性是一个主要挑战。插值方法通常适用于严格平稳(SSS)数据。

Method: 使用高斯插值增强数据，并在数据集上训练多个机器学习和深度学习模型，比较它们的性能。

Result: 大约62%的稀疏数据集（电厂每小时负荷数据）可以用于负荷预测，假设数据是广义平稳(WSS)，如果使用高斯插值增强数据。

Conclusion: LSTM模型在各种经典模型和神经网络模型中表现最佳，适用于处理负荷预测问题。

Abstract: Sparsity, defined as the presence of missing or zero values in a dataset,
often poses a major challenge while operating on real-life datasets. Sparsity
in features or target data of the training dataset can be handled using various
interpolation methods, such as linear or polynomial interpolation, spline,
moving average, or can be simply imputed. Interpolation methods usually perform
well with Strict Sense Stationary (SSS) data. In this study, we show that an
approximately 62\% sparse dataset with hourly load data of a power plant can be
utilized for load forecasting assuming the data is Wide Sense Stationary (WSS),
if augmented with Gaussian interpolation. More specifically, we perform
statistical analysis on the data, and train multiple machine learning and deep
learning models on the dataset. By comparing the performance of these models,
we empirically demonstrate that Gaussian interpolation is a suitable option for
dealing with load forecasting problems. Additionally, we demonstrate that Long
Short-term Memory (LSTM)-based neural network model offers the best performance
among a diverse set of classical and neural network-based models.

</details>


### [91] [Edge-Selector Model Applied for Local Search Neighborhood for Solving Vehicle Routing Problems](https://arxiv.org/abs/2508.14071)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux,Daniele Vigo*

Main category: cs.LG

TL;DR: A hybrid ML and metaheuristic approach for VRPs using edge selection to guide local search, achieving performance improvements on large-scale problems.


<details>
  <summary>Details</summary>
Motivation: To solve Vehicle Routing Problems (VRPs) by guiding the search process within metaheuristic baselines.

Method: A hybrid Machine Learning and metaheuristic mechanism with an edge solution selector model, including a tabular binary classifier and a Graph Neural Network (GNN).

Result: Performance improvements across different baseline metaheuristics, various problem sizes and variants, including CVRP and CVRPTW on benchmark datasets up to 30,000 customer nodes.

Conclusion: The proposed hybrid method demonstrates scalability and generalizability, achieving performance improvements across different baseline metaheuristics, various problem sizes and variants.

Abstract: This research proposes a hybrid Machine Learning and metaheuristic mechanism
that is designed to solve Vehicle Routing Problems (VRPs). The main of our
method is an edge solution selector model, which classifies solution edges to
identify prohibited moves during the local search, hence guiding the search
process within metaheuristic baselines. Two learning-based mechanisms are used
to develop the edge selector: a simple tabular binary classifier and a Graph
Neural Network (GNN). The tabular classifier employs Gradient Boosting Trees
and Feedforward Neural Network as the baseline algorithms. Adjustments to the
decision threshold are also applied to handle the class imbalance in the
problem instance. An alternative mechanism employs the GNN to utilize graph
structure for direct solution edge prediction, with the objective of guiding
local search by predicting prohibited moves. These hybrid mechanisms are then
applied in state-fo-the-art metaheuristic baselines. Our method demonstrates
both scalability and generalizability, achieving performance improvements
across different baseline metaheuristics, various problem sizes and variants,
including the Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time
Windows (CVRPTW). Experimental evaluations on benchmark datasets up to 30,000
customer nodes, supported by pair-wise statistical analysis, verify the
observed improvements.

</details>


### [92] [Multi-Objective Bayesian Optimization with Independent Tanimoto Kernel Gaussian Processes for Diverse Pareto Front Exploration](https://arxiv.org/abs/2508.14072)
*Anabel Yong*

Main category: cs.LG

TL;DR: GP-MOBO是一种新的多目标贝叶斯优化算法，它优于传统方法，能够更有效地进行分子优化，计算开销最小。


<details>
  <summary>Details</summary>
Motivation: 在分子优化中，传统方法无法充分利用指纹维度，计算资源需求大。

Method: 一种新的多目标贝叶斯优化算法，集成了快速最小包，用于精确高斯过程（GPs）。

Result: GP-MOBO优于传统方法，能够充分利用指纹维度，识别更高质量和有效的SMILES，并更广泛地探索化学空间。在DockSTRING数据集上的实验结果表明，GP-MOBO在20次贝叶斯优化迭代中产生了更高的几何平均值。

Conclusion: GP-MOBO在分子优化中表现出色，能够找到更高质量的SMILES，更广泛地探索化学空间，并在DockSTRING数据集上取得了更高的几何平均值。

Abstract: We present GP-MOBO, a novel multi-objective Bayesian Optimization algorithm
that advances the state-of-the-art in molecular optimization. Our approach
integrates a fast minimal package for Exact Gaussian Processes (GPs) capable of
efficiently handling the full dimensionality of sparse molecular fingerprints
without the need for extensive computational resources. GP-MOBO consistently
outperforms traditional methods like GP-BO by fully leveraging fingerprint
dimensionality, leading to the identification of higher-quality and valid
SMILES. Moreover, our model achieves a broader exploration of the chemical
search space, as demonstrated by its superior proximity to the Pareto front in
all tested scenarios. Empirical results from the DockSTRING dataset reveal that
GP-MOBO yields higher geometric mean values across 20 Bayesian optimization
iterations, underscoring its effectiveness and efficiency in addressing complex
multi-objective optimization challenges with minimal computational overhead.

</details>


### [93] [MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets](https://arxiv.org/abs/2508.14073)
*Qian Zhanga,Ruilin Zhang,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 本文提出了一种半监督学习框架MCLPD，用于提高跨数据集帕金森病检测的性能。该框架通过多视图对比预训练和轻量级监督微调，在减少对标注数据依赖的同时，显著提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 脑电图已被证实是一种检测帕金森病（尤其是早期帕金森病）的有效技术。然而，脑电图数据注释的高成本通常导致数据集大小有限以及数据集之间存在相当大的差异，包括采集协议和受试者人口统计学方面的差异，这显着阻碍了模型在跨数据集检测场景中的鲁棒性和泛化性。

Method: 该论文提出了一种名为MCLPD的半监督学习框架，该框架将多视图对比预训练与轻量级监督微调相结合，以提高跨数据集PD检测性能。在预训练期间，MCLPD在未标记的UNM数据集上使用自监督学习。为了构建对比对，它在时域和频域中应用双重增强，从而丰富了数据并自然地融合了时频信息。在微调阶段，仅使用来自另外两个数据集（UI和UC）的一小部分标记数据进行监督优化。

Result: 实验结果表明，仅使用1%的标记数据，MCLPD在UI上实现了0.91的F1分数，在UC上实现了0.81的F1分数，当使用5%的标记数据时，分别进一步提高到0.97和0.87。

Conclusion: MCLPD显著提高了跨数据集的泛化能力，同时减少了对标注数据的依赖，证明了该框架的有效性。

Abstract: Electroencephalography has been validated as an effective technique for
detecting Parkinson's disease,particularly in its early stages.However,the high
cost of EEG data annotation often results in limited dataset size and
considerable discrepancies across datasets,including differences in acquisition
protocols and subject demographics,significantly hinder the robustness and
generalizability of models in cross-dataset detection scenarios.To address such
challenges,this paper proposes a semi-supervised learning framework named
MCLPD,which integrates multi-view contrastive pre-training with lightweight
supervised fine-tuning to enhance cross-dataset PD detection performance.During
pre-training,MCLPD uses self-supervised learning on the unlabeled UNM
dataset.To build contrastive pairs,it applies dual augmentations in both time
and frequency domains,which enrich the data and naturally fuse time-frequency
information.In the fine-tuning phase,only a small proportion of labeled data
from another two datasets (UI and UC)is used for supervised
optimization.Experimental results show that MCLPD achieves F1 scores of 0.91 on
UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97
and 0.87,respectively,when 5%of labeled data is used.Compared to existing
methods,MCLPD substantially improves cross-dataset generalization while
reducing the dependency on labeled data,demonstrating the effectiveness of the
proposed framework.

</details>


### [94] [GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease](https://arxiv.org/abs/2508.14074)
*Qian Zhang,Ruilin Zhang,Biaokai Zhu,Xun Han,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: This paper introduces GEPD, a GAN-enhanced model for Parkinson's disease detection using EEG data, which improves generalizability across different datasets.


<details>
  <summary>Details</summary>
Motivation: Current Parkinson's disease detection methods face challenges in cross-dataset scenarios due to variability in detection methods and small dataset sizes.

Method: The paper proposes a GAN-enhanced generalizable model (GEPD) for EEG-based cross-dataset classification of Parkinson's disease. It includes a generative network for fusion EEG data and a classification network with multiple convolutional neural networks.

Result: The GEPD model performs comparably to state-of-the-art models in cross-dataset settings.

Conclusion: The proposed GEPD model achieves an accuracy of 84.3% and an F1-score of 84.0% in cross-dataset settings, demonstrating its generalizability.

Abstract: Electroencephalography has been established as an effective method for
detecting Parkinson's disease, typically diagnosed early.Current Parkinson's
disease detection methods have shown significant success within individual
datasets, however, the variability in detection methods across different EEG
datasets and the small size of each dataset pose challenges for training a
generalizable model for cross-dataset scenarios. To address these issues, this
paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for
EEG-based cross-dataset classification of Parkinson's disease.First, we design
a generative network that creates fusion EEG data by controlling the
distribution similarity between generated data and real data.In addition, an
EEG signal quality assessment model is designed to ensure the quality of
generated data great.Second, we design a classification network that utilizes a
combination of multiple convolutional neural networks to effectively capture
the time-frequency characteristics of EEG signals, while maintaining a
generalizable structure and ensuring easy convergence.This work is dedicated to
utilizing intelligent methods to study pathological manifestations, aiming to
facilitate the diagnosis and monitoring of neurological diseases.The evaluation
results demonstrate that our model performs comparably to state-of-the-art
models in cross-dataset settings, achieving an accuracy of 84.3% and an
F1-score of 84.0%, showcasing the generalizability of the proposed model.

</details>


### [95] [Explainable Graph Spectral Clustering For Text Embeddings](https://arxiv.org/abs/2508.14075)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Piotr Borkowski,Dariusz Czerski,Eryk Laskowski*

Main category: cs.LG

TL;DR: generalize the explainability of Graph Spectral Clustering results for textual documents by considering GloVe embedding


<details>
  <summary>Details</summary>
Motivation: explainability of Graph Spectral Clustering results for textual documents

Method: considering GloVe embedding

Result:  considering other embeddings of documents, in particular, based on the GloVe embedding idea

Conclusion: generalize the explainability of Graph Spectral Clustering results for textual documents to other embeddings of documents

Abstract: In a previous paper, we proposed an introduction to the explainability of
Graph Spectral Clustering results for textual documents, given that document
similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of
documents, in particular, based on the GloVe embedding idea.

</details>


### [96] [PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning](https://arxiv.org/abs/2508.14076)
*Mengdi Li,Guanqiao Chen,Xufeng Zhao,Haochen Wen,Shu Yang,Di Wang*

Main category: cs.LG

TL;DR: PersRM-R1 is a reasoning-based reward modeling framework that identifies and represents personal factors from limited personal exemplars, outperforming existing models in accuracy and generalizability for personalized LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing reward models struggle to capture nuanced, user-specific preferences, especially with limited data and across diverse domains.

Method: Combines synthetic data generation with a two-stage training pipeline (supervised fine-tuning followed by reinforcement fine-tuning).

Result: PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability.

Conclusion: PersRM-R1 outperforms existing models and matches larger models in accuracy and generalizability, enabling more effective personalized LLMs.

Abstract: Reward models (RMs), which are central to existing post-training methods, aim
to align LLM outputs with human values by providing feedback signals during
fine-tuning. However, existing RMs struggle to capture nuanced, user-specific
preferences, especially under limited data and across diverse domains. Thus, we
introduce PersRM-R1, the first reasoning-based reward modeling framework
specifically designed to identify and represent personal factors from only one
or a few personal exemplars. To address challenges including limited data
availability and the requirement for robust generalization, our approach
combines synthetic data generation with a two-stage training pipeline
consisting of supervised fine-tuning followed by reinforcement fine-tuning.
Experimental results demonstrate that PersRM-R1 outperforms existing models of
similar size and matches the performance of much larger models in both accuracy
and generalizability, paving the way for more effective personalized LLMs.

</details>


### [97] [Label Smoothing is a Pragmatic Information Bottleneck](https://arxiv.org/abs/2508.14077)
*Sota Kudo*

Main category: cs.LG

TL;DR: label smoothing can be interpreted as a practical approach to the information bottleneck, enabling simple implementation.


<details>
  <summary>Details</summary>
Motivation: revisits label smoothing via a form of information bottleneck.

Method: label smoothing

Result: the model output obtained through label smoothing explores the optimal solution of the information bottleneck.

Conclusion: label smoothing exhibits the property of being insensitive to factors that do not contain information about the target, or to factors that provide no additional information about it when conditioned on another variable.

Abstract: This study revisits label smoothing via a form of information bottleneck.
Under the assumption of sufficient model flexibility and no conflicting labels
for the same input, we theoretically and experimentally demonstrate that the
model output obtained through label smoothing explores the optimal solution of
the information bottleneck. Based on this, label smoothing can be interpreted
as a practical approach to the information bottleneck, enabling simple
implementation. As an information bottleneck method, we experimentally show
that label smoothing also exhibits the property of being insensitive to factors
that do not contain information about the target, or to factors that provide no
additional information about it when conditioned on another variable.

</details>


### [98] [Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction](https://arxiv.org/abs/2508.14078)
*Mohamed Hassan Abdalla Idris,Jakub Marek Cebula,Jebraeel Gholinezhad,Shamsul Masum,Hongjie Ma*

Main category: cs.LG

TL;DR: This research introduces a new ML framework using PI-driven feature selection and Inductive Conformal Prediction (ICP) to improve the robustness and reliability of hydrocarbon production forecasts. LSTM model showed superior performance.


<details>
  <summary>Details</summary>
Motivation: Enhance the robustness of out-of-sample hydrocarbon production forecasting, specifically addressing multivariate time series analysis.

Method: The methodology integrates Productivity Index (PI)-driven feature selection with Inductive Conformal Prediction (ICP) and uses LSTM, BiLSTM, GRU, and XGBoost for forecasting.

Result: LSTM achieved the lowest MAE on test (19.468) and genuine out-of-sample forecast data (29.638) for well PF14, with validation on Norne well E1H. PI-based feature selection reduced input dimensionality, and ICP provided valid prediction intervals.

Conclusion: The study demonstrates the potential of combining domain-specific knowledge with ML for reliable hydrocarbon production forecasts, with LSTM outperforming other models.

Abstract: This research introduces a new ML framework designed to enhance the
robustness of out-of-sample hydrocarbon production forecasting, specifically
addressing multivariate time series analysis. The proposed methodology
integrates Productivity Index (PI)-driven feature selection, a concept derived
from reservoir engineering, with Inductive Conformal Prediction (ICP) for
rigorous uncertainty quantification. Utilizing historical data from the Volve
(wells PF14, PF12) and Norne (well E1H) oil fields, this study investigates the
efficacy of various predictive algorithms-namely Long Short-Term Memory (LSTM),
Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and eXtreme Gradient
Boosting (XGBoost) - in forecasting historical oil production rates (OPR_H).
All the models achieved "out-of-sample" production forecasts for an upcoming
future timeframe. Model performance was comprehensively evaluated using
traditional error metrics (e.g., MAE) supplemented by Forecast Bias and
Prediction Direction Accuracy (PDA) to assess bias and trend-capturing
capabilities. The PI-based feature selection effectively reduced input
dimensionality compared to conventional numerical simulation workflows. The
uncertainty quantification was addressed using the ICP framework, a
distribution-free approach that guarantees valid prediction intervals (e.g.,
95% coverage) without reliance on distributional assumptions, offering a
distinct advantage over traditional confidence intervals, particularly for
complex, non-normal data. Results demonstrated the superior performance of the
LSTM model, achieving the lowest MAE on test (19.468) and genuine out-of-sample
forecast data (29.638) for well PF14, with subsequent validation on Norne well
E1H. These findings highlight the significant potential of combining
domain-specific knowledge with advanced ML techniques to improve the
reliability of hydrocarbon production forecasts.

</details>


### [99] [A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy](https://arxiv.org/abs/2508.14079)
*Maxime Heuillet,Rishika Bhagwatkar,Jonas Ngnawé,Yann Pequignot,Alexandre Larouche,Christian Gagné,Irina Rish,Ola Ahmad,Audrey Durand*

Main category: cs.LG

TL;DR: 对鲁棒微调进行了最广泛和最全面的基准测试，发现卷积神经网络通常表现最好。


<details>
  <summary>Details</summary>
Motivation: 图像领域的深度学习模型容易受到微小输入扰动的影响。了解这些设计选择如何影响泛化仍然是一个具有重要实践意义的开放性问题。

Method: 进行了包含 6 个数据集、40 个预训练架构、2 个专用损失和 3 个适应协议的实证研究，产生了 1,440 个训练配置和 7,200 个鲁棒性测量。

Result: 注意力机制架构和鲁棒的预训练表征越来越受欢迎，但我们发现以监督方式在大数据集上预训练的卷积神经网络通常表现最好。

Conclusion: 卷积神经网络在大数据集上以监督方式进行预训练通常表现最好。分析结果既证实又挑战了先前的设计假设，突出了有希望的研究方向，并提供了实践指导。

Abstract: Deep learning models operating in the image domain are vulnerable to small
input perturbations. For years, robustness to such perturbations was pursued by
training models from scratch (i.e., with random initializations) using
specialized loss objectives. Recently, robust fine-tuning has emerged as a more
efficient alternative: instead of training from scratch, pretrained models are
adapted to maximize predictive performance and robustness. To conduct robust
fine-tuning, practitioners design an optimization strategy that includes the
model update protocol (e.g., full or partial) and the specialized loss
objective. Additional design choices include the architecture type and size,
and the pretrained representation. These design choices affect robust
generalization, which is the model's ability to maintain performance when
exposed to new and unseen perturbations at test time. Understanding how these
design choices influence generalization remains an open question with
significant practical implications. In response, we present an empirical study
spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3
adaptation protocols, yielding 1,440 training configurations and 7,200
robustness measurements across five perturbation types. To our knowledge, this
is the most diverse and comprehensive benchmark of robust fine-tuning to date.
While attention-based architectures and robust pretrained representations are
increasingly popular, we find that convolutional neural networks pretrained in
a supervised manner on large datasets often perform best. Our analysis both
confirms and challenges prior design assumptions, highlighting promising
research directions and offering practical guidance.

</details>


### [100] [KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge](https://arxiv.org/abs/2508.14080)
*Guanghao Jin,Jingpei Wu,Tianpei Guo,Yiyi Niu,Weidong Zhou,Guoyang Liu*

Main category: cs.LG

TL;DR: 提出了一个新的基准数据集KnowDR-REC，用于评估多模态大语言模型在知识驱动的视觉定位任务中的推理能力。实验表明现有模型仍有挑战，并存在文本理解和视觉定位脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的REC基准要么仅依赖于图像内的线索，要么缺乏足够细粒度的实例注释，不足以评估多模态大语言模型(MLLM)的推理能力。

Method: 提出了一个新的基准数据集KnowDR-REC，具有三个关键特征：构建于真实世界知识，包含精心构建的负样本，引入了三个新的评估指标。

Result: 在KnowDR-REC上评估了16个最先进的多模态模型，实验结果表明现有的MLLM在知识驱动的视觉定位任务中仍然面临挑战。观察到MLLM中文本理解和视觉定位之间的脱节。

Conclusion: 现有的多模态大语言模型在知识驱动的视觉定位任务中仍然面临挑战，并且在文本理解和视觉定位之间存在脱节，模型容易受到记忆中的捷径关联的影响。

Abstract: Referring Expression Comprehension (REC) is a popular multimodal task that
aims to accurately detect target objects within a single image based on a given
textual expression. However, due to the limitations of earlier models,
traditional REC benchmarks either rely solely on intra-image cues or lack
sufficiently fine-grained instance annotations, making them inadequate for
evaluating the reasoning capabilities of Multi-modal Large Language Models
(MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC,
characterized by three key features: Firstly, it is built upon real-world
knowledge, requiring fine-grained multimodal reasoning across text and image.
Secondly, the dataset includes elaborately constructed negative samples via
fine-grained expression editing, designed to evaluate a model's robustness and
anti-hallucination ability. Lastly, we introduce three novel evaluation metrics
to systematically explore the model's internal reasoning process. We evaluate
16 state-of-the-art multimodal models on KnowDR-REC, with experimental results
showing that existing MLLMs still struggle with knowledge-driven visual
grounding tasks. Furthermore, we observe a decoupling between textual
understanding and visual grounding in MLLMs, where many models are
significantly influenced by memorized shortcut correlations, which severely
affect their behavior on our benchmark and hinder genuine multimodal reasoning.
We anticipate that the proposed benchmark will inspire future research towards
developing more robust, interpretable, and knowledge-intensive visual grounding
frameworks, driving the development of more reliable and robust multimodal
systems for complex real-world scenarios.

</details>


### [101] [Toward Lifelong Learning in Equilibrium Propagation: Sleep-like and Awake Rehearsal for Enhanced Stability](https://arxiv.org/abs/2508.14081)
*Yoshimasa Kubo,Jean Erik Delanois,Maxim Bazhenov*

Main category: cs.LG

TL;DR: This paper introduces a sleep-like replay consolidation (SRC) algorithm for Equilibrium Propagation (EP)-trained RNNs to combat catastrophic forgetting in continuous learning. SRC significantly improves the RNN's ability to retain old knowledge while learning new tasks, achieving comparable or superior performance to Backpropagation Through Time (BPTT)-trained models on various datasets.


<details>
  <summary>Details</summary>
Motivation: Recurrent neural networks (RNNs) trained using Equilibrium Propagation (EP) face a critical challenge in continuous learning: catastrophic forgetting, where previously acquired knowledge is overwritten when new tasks are learned. This limitation contrasts with the human brain's ability to retain and integrate both old and new knowledge.

Method: We propose a sleep-like replay consolidation (SRC) algorithm for EP-trained RNNs.

Result: In class-incremental learning with SRC implemented after each new task training, the EP-trained multilayer RNN model (MRNN-EP) performed significantly better compared to feedforward networks incorporating several well-established regularization techniques. The MRNN-EP performed on par with MRNN trained using Backpropagation Through Time (BPTT) when both were equipped with SRC on MNIST data and surpassed BPTT-based models on the Fashion MNIST, Kuzushiji-MNIST, CIFAR10, and ImageNet datasets.

Conclusion: Sleep-like replay consolidation (SRC) significantly improves RNN's resilience to catastrophic forgetting in continuous learning scenarios, and combining SRC with rehearsal further boosted the network's ability to retain long-term knowledge while continuing to learn new tasks.

Abstract: Recurrent neural networks (RNNs) trained using Equilibrium Propagation (EP),
a biologically plausible training algorithm, have demonstrated strong
performance in various tasks such as image classification and reinforcement
learning. However, these networks face a critical challenge in continuous
learning: catastrophic forgetting, where previously acquired knowledge is
overwritten when new tasks are learned. This limitation contrasts with the
human brain's ability to retain and integrate both old and new knowledge, aided
by processes like memory consolidation during sleep through the replay of
learned information. To address this challenge in RNNs, here we propose a
sleep-like replay consolidation (SRC) algorithm for EP-trained RNNs. We found
that SRC significantly improves RNN's resilience to catastrophic forgetting in
continuous learning scenarios. In class-incremental learning with SRC
implemented after each new task training, the EP-trained multilayer RNN model
(MRNN-EP) performed significantly better compared to feedforward networks
incorporating several well-established regularization techniques. The MRNN-EP
performed on par with MRNN trained using Backpropagation Through Time (BPTT)
when both were equipped with SRC on MNIST data and surpassed BPTT-based models
on the Fashion MNIST, Kuzushiji-MNIST, CIFAR10, and ImageNet datasets.
Combining SRC with rehearsal, also known as "awake replay", further boosted the
network's ability to retain long-term knowledge while continuing to learn new
tasks. Our study reveals the applicability of sleep-like replay techniques to
RNNs and highlights the potential for integrating human-like learning behaviors
into artificial neural networks (ANNs).

</details>


### [102] [Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation](https://arxiv.org/abs/2508.14082)
*Ye Su,Hezhe Qiao,Wei Huang,Lin Chen*

Main category: cs.LG

TL;DR: 提出了一种新的半监督回归框架DRILL，通过离散分布估计和解耦分布对齐来提高泛化能力，实验结果表明DRILL优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督回归方法依赖于伪标签的质量，直接回归无法学习标签分布，容易导致过拟合。

Method: 提出了一种端到端的解耦表示蒸馏框架(DRILL)，将回归任务转换为离散分布估计(DDE)任务，并采用解耦分布对齐(DDA)来对齐教师和学生在桶分布上的目标桶和非目标桶。

Result: DRILL在多个领域的数据集上进行了大量实验，证明了其具有很强的泛化能力，优于现有方法。

Conclusion: DRILL在多个领域的数据集上表现出强大的泛化能力，优于现有方法。

Abstract: Semi-supervised regression (SSR), which aims to predict continuous scores of
samples while reducing reliance on a large amount of labeled data, has recently
received considerable attention across various applications, including computer
vision, natural language processing, and audio and medical analysis. Existing
semi-supervised methods typically apply consistency regularization on the
general regression task by generating pseudo-labels. However, these methods
heavily rely on the quality of pseudo-labels, and direct regression fails to
learn the label distribution and can easily lead to overfitting. To address
these challenges, we introduce an end-to-end Decoupled Representation
distillation framework (DRILL) which is specially designed for the
semi-supervised regression task where we transform the general regression task
into a Discrete Distribution Estimation (DDE) task over multiple buckets to
better capture the underlying label distribution and mitigate the risk of
overfitting associated with direct regression. Then we employ the Decoupled
Distribution Alignment (DDA) to align the target bucket and non-target bucket
between teacher and student on the distribution of buckets, encouraging the
student to learn more robust and generalized knowledge from the teacher.
Extensive experiments conducted on datasets from diverse domains demonstrate
that the proposed DRILL has strong generalization and outperforms the competing
methods.

</details>


### [103] [GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values](https://arxiv.org/abs/2508.14083)
*Songyu Ke,Chenyu Wu,Yuxuan Liang,Xiuwen Yi,Yanping Sun,Junbo Zhang,Yu Zheng*

Main category: cs.LG

TL;DR: This paper introduces a Contrastive Self-learning framework for Spatio-Temporal data to address the problem of inferring accurate crowd flow from low-quality data. The model is pre-trained on noisy data and fine-tuned with accurate crowd flow data, and it outperforms models trained from scratch.


<details>
  <summary>Details</summary>
Motivation: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) The scarcity and rarity of labeled data, 2) The intricate spatio-temporal dependencies among POIs, and 3) The myriad correlations between precise crowd flow and GPS reports.

Method: The crowd flow inference problem is recast as a self-supervised attributed graph representation learning task and introduce a novel Contrastive Self-learning framework for Spatio-Temporal data. The approach initiates with the construction of a spatial adjacency graph founded on the POIs and their respective distances. Then contrastive learning is used to exploit large volumes of unlabeled spatio-temporal data and a swapped prediction approach to anticipate the representation of the target subgraph from similar instances. Following the pre-training phase, the model is fine-tuned with accurate crowd flow data.

Result: The Contrastive Self-learning framework for Spatio-Temporal data pre-trained on extensive noisy data consistently outperforms models trained from scratch on two real-world datasets.

Conclusion: The experiments on two real-world datasets, demonstrate that the Contrastive Self-learning framework for Spatio-Temporal data pre-trained on extensive noisy data consistently outperforms models trained from scratch.

Abstract: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
  To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.

</details>


### [104] [Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure](https://arxiv.org/abs/2508.14085)
*Hanseul Kang,Shervin Karimkashi,Ville Vuorinen*

Main category: cs.LG

TL;DR: Developed a scalable, parameter-aware sparse regression framework for discovering interpretable partial differential equations and subgrid-scale closures from multi-parameter simulation data. Discovered a Smagorinsky-type closure structure from data without prior theoretical assumptions.


<details>
  <summary>Details</summary>
Motivation: discovering interpretable partial differential equations and subgrid-scale closures from multi-parameter simulation data. Building on SINDy (Sparse Identification of Nonlinear Dynamics), our approach addresses key limitations through four innovations: symbolic parameterisation enabling physical parameters to vary within unified regression; Dimensional Similarity Filter enforcing unit-consistency whilst reducing candidate libraries; memory-efficient Gram-matrix accumulation enabling batch processing; and ensemble consensus with coefficient stability analysis for robust model identification.

Method: a scalable, parameter-aware sparse regression framework

Result: Validation on canonical one-dimensional benchmarks demonstrates reliable recovery of governing equations across parameter ranges. Applied to filtered Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} = 0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$, corresponding to a Smagorinsky constant of approximately 0.4004. This represents autonomous discovery of Smagorinsky-type closure structure from data without prior theoretical assumptions.  The discovered model achieves $R^2 = 0.886$ across filter scales and demonstrates improved prediction accuracy compared to classical closures.

Conclusion: The framework's ability to identify physically meaningful SGS forms and calibrate coefficients offers a complementary approach to existing turbulence modelling methods, contributing to the growing field of data-driven closure discovery.

Abstract: We present a scalable, parameter-aware sparse regression framework for
discovering interpretable partial differential equations and subgrid-scale
closures from multi-parameter simulation data. Building on SINDy (Sparse
Identification of Nonlinear Dynamics), our approach addresses key limitations
through four innovations: symbolic parameterisation enabling physical
parameters to vary within unified regression; Dimensional Similarity Filter
enforcing unit-consistency whilst reducing candidate libraries;
memory-efficient Gram-matrix accumulation enabling batch processing; and
ensemble consensus with coefficient stability analysis for robust model
identification.
  Validation on canonical one-dimensional benchmarks demonstrates reliable
recovery of governing equations across parameter ranges. Applied to filtered
Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} =
0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$,
corresponding to a Smagorinsky constant of approximately 0.4004. This
represents autonomous discovery of Smagorinsky-type closure structure from data
without prior theoretical assumptions.
  The discovered model achieves $R^2 = 0.886$ across filter scales and
demonstrates improved prediction accuracy compared to classical closures. The
framework's ability to identify physically meaningful SGS forms and calibrate
coefficients offers a complementary approach to existing turbulence modelling
methods, contributing to the growing field of data-driven closure discovery.

</details>


### [105] [EEGDM: EEG Representation Learning via Generative Diffusion Model](https://arxiv.org/abs/2508.14086)
*Jia Hong Puah,Sim Kuan Goh,Ziwei Zhang,Zixuan Ye,Chow Khuen Chan,Kheng Seang Lim,Si Lei Fong,Kok Sin Woon*

Main category: cs.LG

TL;DR: 提出了一种基于生成扩散模型（EEGDM）的脑电图表示学习框架，该框架在性能优于现有方法的同时，更加轻量化。


<details>
  <summary>Details</summary>
Motivation: 由于注释有限和信号高变异性，从原始脑电信号中学习有意义的表示仍然具有挑战性。最近，脑电图基础模型（FMs）通过采用transformer架构和来自大型语言模型的自监督预训练方法（例如，掩码预测）来学习来自各种脑电图数据的表示，然后在特定的脑电图任务上进行微调，显示出有希望的潜力。然而，这些大型模型在训练和推理过程中通常会产生高计算成本，并且随着模型尺寸的增加，性能提升很小。

Method: 开发了用于扩散预训练的结构化状态空间模型（SSMDP），以更好地捕获脑电信号的时间动态，并使用去噪扩散概率模型训练该架构。然后，通过提出的潜在融合transformer（LFT）将生成的潜在脑电图表示用于下游分类任务。

Result: 经验结果表明，该方法优于现有方法，同时轻量化约19倍。

Conclusion: EEGDM为当前脑电图基础模型提供了一个有前景的替代方案。

Abstract: While electroencephalogram (EEG) has been a crucial tool for monitoring the
brain and diagnosing neurological disorders (e.g., epilepsy), learning
meaningful representations from raw EEG signals remains challenging due to
limited annotations and high signal variability. Recently, EEG foundation
models (FMs) have shown promising potential by adopting transformer
architectures and self-supervised pre-training methods from large language
models (e.g., masked prediction) to learn representations from diverse EEG
data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large
models often incurred high computational costs during both training and
inference, with only marginal performance improvements as model size increases.
In this work, we proposed EEG representation learning framework building upon
Generative Diffusion Model (EEGDM). Specifically, we developed structured
state-space model for diffusion pretraining (SSMDP) to better capture the
temporal dynamics of EEG signals and trained the architecture using a Denoising
Diffusion Probabilistic Model. The resulting latent EEG representations were
then used for downstream classification tasks via our proposed latent fusion
transformer (LFT). To evaluate our method, we used the multi-event Temple
University EEG Event Corpus and compared EEGDM with current state-of-the-art
approaches, including EEG FMs. Empirical results showed that our method
outperformed existing methods while being approximately 19x more lightweight.
These findings suggested that EEGDM offered a promising alternative to current
FMs. Our code is available at: https://github.com/jhpuah/EEGDM.

</details>


### [106] [FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics](https://arxiv.org/abs/2508.14087)
*David Park,Shuhang Li,Yi Huang,Xihaier Luo,Haiwang Yu,Yeonju Go,Christopher Pinkenburg,Yuewei Lin,Shinjae Yoo,Joseph Osborn,Jin Huang,Yihui Ren*

Main category: cs.LG

TL;DR: This paper introduces a particle physics FM that scales, generalizes, and outperforms baselines on diverse tasks. It uses a new dataset and self-supervised training method.


<details>
  <summary>Details</summary>
Motivation: applying this capability to experimental particle physics is challenging due to the sparse, spatially distributed nature of detector data, which differs dramatically from natural language. This work addresses if an FM for particle physics can scale and generalize across diverse tasks.

Method: a novel self-supervised training method for detector data and demonstrate its neural scalability with models that feature up to 188 million parameters. With frozen weights and task-specific adapters

Result: introduce a new dataset with more than 11 million particle collision events and a suite of downstream tasks and labeled data for evaluation.

Conclusion: This particle physics FM consistently outperforms baseline models across all downstream tasks and exhibits robust data-efficient adaptation. The representations extracted by the FM are task-agnostic but can be specialized via a single linear mapping for different downstream tasks.

Abstract: Large language models have revolutionized artificial intelligence by enabling
large, generalizable models trained through self-supervision. This paradigm has
inspired the development of scientific foundation models (FMs). However,
applying this capability to experimental particle physics is challenging due to
the sparse, spatially distributed nature of detector data, which differs
dramatically from natural language. This work addresses if an FM for particle
physics can scale and generalize across diverse tasks. We introduce a new
dataset with more than 11 million particle collision events and a suite of
downstream tasks and labeled data for evaluation. We propose a novel
self-supervised training method for detector data and demonstrate its neural
scalability with models that feature up to 188 million parameters. With frozen
weights and task-specific adapters, this FM consistently outperforms baseline
models across all downstream tasks. The performance also exhibits robust
data-efficient adaptation. Further analysis reveals that the representations
extracted by the FM are task-agnostic but can be specialized via a single
linear mapping for different downstream tasks.

</details>


### [107] [CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection](https://arxiv.org/abs/2508.14088)
*Haomin Wen,Shurui Cao,Leman Akoglu*

Main category: cs.LG

TL;DR: CoBAD 是一种用于检测人类移动集体异常的新模型，它使用双阶段注意力机制建模个体和群体交互，并在大规模数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法主要关注个体移动模式，而集体异常检测旨在识别跨个体集体移动行为的异常，这是一个尚未充分探索的挑战。与个体异常不同，集体异常需要建模个体之间的时空依赖性，从而增加了额外的复杂性。

Method: CoBAD 采用双阶段注意力机制来建模个体移动模式和多个人之间的交互。通过掩码事件和链接重建任务，CoBAD 在大规模集体行为数据上进行预训练。

Result: CoBAD 能够检测两种类型的集体异常：意外的共现异常和缺席异常。

Conclusion: CoBAD 在大规模移动数据集上显著优于现有的异常检测基线，AUCROC 提高了 13%-18%，AUCPR 提高了 19%-70%。

Abstract: Detecting anomalies in human mobility is essential for applications such as
public safety and urban planning. While traditional anomaly detection methods
primarily focus on individual movement patterns (e.g., a child should stay at
home at night), collective anomaly detection aims to identify irregularities in
collective mobility behaviors across individuals (e.g., a child is at home
alone while the parents are elsewhere) and remains an underexplored challenge.
Unlike individual anomalies, collective anomalies require modeling
spatiotemporal dependencies between individuals, introducing additional
complexity. To address this gap, we propose CoBAD, a novel model designed to
capture Collective Behaviors for human mobility Anomaly Detection. We first
formulate the problem as unsupervised learning over Collective Event Sequences
(CES) with a co-occurrence event graph, where CES represents the event
sequences of related individuals. CoBAD then employs a two-stage attention
mechanism to model both the individual mobility patterns and the interactions
across multiple individuals. Pre-trained on large-scale collective behavior
data through masked event and link reconstruction tasks, CoBAD is able to
detect two types of collective anomalies: unexpected co-occurrence anomalies
and absence anomalies, the latter of which has been largely overlooked in prior
work. Extensive experiments on large-scale mobility datasets demonstrate that
CoBAD significantly outperforms existing anomaly detection baselines, achieving
an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is
available at https://github.com/wenhaomin/CoBAD.

</details>


### [108] [Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions](https://arxiv.org/abs/2508.14091)
*Matthew Morris,David J. Tena Cucala,Bernardo Cuenca Grau*

Main category: cs.LG

TL;DR: The paper proposes monotonic GNNs and scoring functions for link prediction, enabling the extraction of sound rules to explain predictions and demonstrating good performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs with provable correspondence guarantees. However, these works address only a form of link prediction based on a restricted, low-expressivity graph encoding/decoding method. In this paper, we consider a more general and popular approach for link prediction where a scoring function is used to decode the GNN output into fact predictions.

Method: adapt GNNs and scoring functions to be monotonic, use the monotonicity to extract sound rules for explaining predictions, and leverage existing results about the kind of rules that scoring functions can capture. Also, define procedures for obtaining equivalent Datalog programs for certain classes of monotonic GNNs with scoring functions.

Result: monotonic GNNs and scoring functions perform well in practice and yield many sound rules.

Conclusion: Monotonic GNNs and scoring functions perform well in practice and yield many sound rules on link prediction benchmarks.

Abstract: Graph neural networks (GNNs) are often used for the task of link prediction:
predicting missing binary facts in knowledge graphs (KGs). To address the lack
of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs
with provable correspondence guarantees. The extracted rules can be used to
explain the GNN's predictions; furthermore, they can help characterise the
expressive power of various GNN models. However, these works address only a
form of link prediction based on a restricted, low-expressivity graph
encoding/decoding method. In this paper, we consider a more general and popular
approach for link prediction where a scoring function is used to decode the GNN
output into fact predictions. We show how GNNs and scoring functions can be
adapted to be monotonic, use the monotonicity to extract sound rules for
explaining predictions, and leverage existing results about the kind of rules
that scoring functions can capture. We also define procedures for obtaining
equivalent Datalog programs for certain classes of monotonic GNNs with scoring
functions. Our experiments show that, on link prediction benchmarks, monotonic
GNNs and scoring functions perform well in practice and yield many sound rules.

</details>


### [109] [Physics-Informed Reward Machines](https://arxiv.org/abs/2508.14093)
*Daniel Ajeleye,Ashutosh Trivedi,Majid Zamani*

Main category: cs.LG

TL;DR: 我们提出了物理信息奖励机器(pRMs)，这是一种符号机器，旨在表达RL代理的复杂学习目标和奖励结构，并通过反事实经验和奖励塑造来利用pRMs的RL算法，最终提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 奖励机器(RM)提供了一种结构化的方式来指定强化学习(RL)中的非马尔可夫奖励，从而提高表达性和可编程性。更广泛地说，它们将关于环境的已知信息(由奖励机制捕获)与仍然未知且必须通过抽样发现的信息分离开来。这种分离支持诸如反事实经验生成和奖励塑造等技术，这些技术降低了样本复杂性并加速了学习。

Method: 我们介绍了物理信息奖励机器(pRMs)，这是一种符号机器，旨在表达RL代理的复杂学习目标和奖励结构，从而实现更可编程、更具表达性和更高效的学习。我们提出了能够通过反事实经验和奖励塑造来利用pRMs的RL算法。

Result: 这些技术加速了RL的训练阶段的奖励获取。结合pRMs可以显著提高多个控制任务的学习效率。

Conclusion: 通过实验结果表明，这些技术可以加速RL训练阶段的奖励获取。在有限和连续物理环境中的实验证明了pRMs的表达性和有效性，说明结合pRMs可以显著提高多个控制任务的学习效率。

Abstract: Reward machines (RMs) provide a structured way to specify non-Markovian
rewards in reinforcement learning (RL), thereby improving both expressiveness
and programmability. Viewed more broadly, they separate what is known about the
environment, captured by the reward mechanism, from what remains unknown and
must be discovered through sampling. This separation supports techniques such
as counterfactual experience generation and reward shaping, which reduce sample
complexity and speed up learning. We introduce physics-informed reward machines
(pRMs), a symbolic machine designed to express complex learning objectives and
reward structures for RL agents, thereby enabling more programmable,
expressive, and efficient learning. We present RL algorithms capable of
exploiting pRMs via counterfactual experiences and reward shaping. Our
experimental results show that these techniques accelerate reward acquisition
during the training phases of RL. We demonstrate the expressiveness and
effectiveness of pRMs through experiments in both finite and continuous
physical environments, illustrating that incorporating pRMs significantly
improves learning efficiency across several control tasks.

</details>


### [110] [Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets](https://arxiv.org/abs/2508.14094)
*Benjamin Pikus,Pratyush Ranjan Tiwari,Burton Ye*

Main category: cs.LG

TL;DR: 在预算有限的情况下，优先选择较难的例子进行GRPO训练，可以在推理任务上获得显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在预算有限的情况下，为语言模型微调收集高质量的训练样本非常昂贵。因此，我们需要研究在固定预算下，从业者应该优先选择容易、中等、困难还是随机难度的例子。

Method: 使用多样本评估，从同一个未标记的池中选择四种难度不同的子集选择策略，比较不同模型大小和系列的Group Relative Policy Optimization (GRPO) 微调。

Result: 在最难的例子上训练产生的性能增益最大，高达47%，而在简单的例子上训练产生的增益最小。分析表明，这种效果源于较难的例子在GRPO训练期间提供了更多可学习的机会。

Conclusion: 在GRPO训练中使用较难的例子，可以在推理任务上获得显著的性能提升。

Abstract: Collecting high-quality training examples for language model fine-tuning is
expensive, with practical budgets limiting the amount of data that can be
procured. We investigate a critical question for resource-constrained
alignment: under a fixed acquisition budget, should practitioners prioritize
examples that are easy, medium, hard, or of random difficulty? We study Group
Relative Policy Optimization (GRPO) fine-tuning across different model sizes
and families, comparing four subset selection policies chosen from the same
unlabeled pool using base-model difficulty estimates obtained via multi-sample
evaluation. Our experiments reveal that training on the hardest examples yields
the largest performance gains, up to 47%, while training on easy examples yield
the smallest gains. Analysis reveals that this effect arises from harder
examples providing more learnable opportunities during GRPO training. These
findings provide practical guidance for budget-constrained post-training:
prioritizing hard examples yields substantial performance gains on reasoning
tasks when using GRPO.

</details>


### [111] [Implicit Hypergraph Neural Network](https://arxiv.org/abs/2508.14101)
*Akash Choudhuri,Yongjian Zhong,Bijaya Adhikari*

Main category: cs.LG

TL;DR: The paper introduces IHNN, an implicit hypergraph neural network, to improve long-range dependency capture in hypergraph learning. IHNN outperforms existing methods in node classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing hypergraph neural networks lose predictive power when aggregating more information to capture long-range dependency. Prior work has not studied long-range dependency issues on hypergraph neural networks.

Method: The paper introduces Implicit Hypergraph Neural Network (IHNN), a novel framework that jointly learns fixed-point representations for both nodes and hyperedges in an end-to-end manner. It leverages implicit differentiation and introduces a tractable projected gradient descent approach for efficient training.

Result: Extensive experiments on real-world hypergraphs for node classification demonstrate that IHNN outperforms the closest prior works in most settings, establishing a new state-of-the-art in hypergraph learning.

Conclusion: The paper proposes Implicit Hypergraph Neural Network (IHNN) to address the issue of losing predictive power when capturing long-range dependency in hypergraph neural networks. IHNN jointly learns fixed-point representations for nodes and hyperedges and is trained efficiently using a projected gradient descent approach. Experiments show IHNN outperforms existing methods.

Abstract: Hypergraphs offer a generalized framework for capturing high-order
relationships between entities and have been widely applied in various domains,
including healthcare, social networks, and bioinformatics. Hypergraph neural
networks, which rely on message-passing between nodes over hyperedges to learn
latent representations, have emerged as the method of choice for predictive
tasks in many of these domains. These approaches typically perform only a small
number of message-passing rounds to learn the representations, which they then
utilize for predictions. The small number of message-passing rounds comes at a
cost, as the representations only capture local information and forego
long-range high-order dependencies. However, as we demonstrate, blindly
increasing the message-passing rounds to capture long-range dependency also
degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture
long-range dependencies in standard graphs while maintaining performance.
Despite their popularity, prior work has not studied long-range dependency
issues on hypergraph neural networks. Here, we first demonstrate that existing
hypergraph neural networks lose predictive power when aggregating more
information to capture long-range dependency. We then propose Implicit
Hypergraph Neural Network (IHNN), a novel framework that jointly learns
fixed-point representations for both nodes and hyperedges in an end-to-end
manner to alleviate this issue. Leveraging implicit differentiation, we
introduce a tractable projected gradient descent approach to train the model
efficiently. Extensive experiments on real-world hypergraphs for node
classification demonstrate that IHNN outperforms the closest prior works in
most settings, establishing a new state-of-the-art in hypergraph learning.

</details>


### [112] [Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces](https://arxiv.org/abs/2508.14102)
*Thomas Gallien*

Main category: cs.LG

TL;DR: This paper studies how different body shapes affect how well TRPO and PPO work in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Growing interest in scalable and reusable control policies translates into a demand for morphological generalization, the ability of control policies to cope with different kinematic structures. Graph-based policy architectures provide a natural and effective mechanism to encode such structural differences. However, the behavior of trust region methods under varying action space dimensionality remains poorly understood.

Method: Theoretical analysis and empirical evaluation using the Gymnasium Swimmer environment.

Result: Demonstrates how varying action space dimensionality influences the optimization landscape under KL-divergence or policy clipping penalties. Empirical evaluation is carried out using the Gymnasium Swimmer environment.

Conclusion: This paper analyzes the influence of varying action space dimensionality on trust region-based policy optimization methods, particularly TRPO and PPO.

Abstract: Trust region-based optimization methods have become foundational
reinforcement learning algorithms that offer stability and strong empirical
performance in continuous control tasks. Growing interest in scalable and
reusable control policies translate also in a demand for morphological
generalization, the ability of control policies to cope with different
kinematic structures. Graph-based policy architectures provide a natural and
effective mechanism to encode such structural differences. However, while these
architectures accommodate variable morphologies, the behavior of trust region
methods under varying action space dimensionality remains poorly understood. To
this end, we conduct a theoretical analysis of trust region-based policy
optimization methods, focusing on both Trust Region Policy Optimization (TRPO)
and its widely used first-order approximation, Proximal Policy Optimization
(PPO). The goal is to demonstrate how varying action space dimensionality
influence the optimization landscape, particularly under the constraints
imposed by KL-divergence or policy clipping penalties. Complementing the
theoretical insights, an empirical evaluation under morphological variation is
carried out using the Gymnasium Swimmer environment. This benchmark offers a
systematically controlled setting for varying the kinematic structure without
altering the underlying task, making it particularly well-suited to study
morphological generalization.

</details>


### [113] [From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery](https://arxiv.org/abs/2508.14111)
*Jiaqi Wei,Yuejin Yang,Xiang Zhang,Yuhan Chen,Xiang Zhuang,Zhangyang Gao,Dongzhan Zhou,Guangshuai Wang,Zhiqiang Gao,Juntai Cao,Zijie Qiu,Xuming He,Qiang Zhang,Chenyu You,Shuangjia Zheng,Ning Ding,Wanli Ouyang,Nanqing Dong,Yu Cheng,Siqi Sun,Lei Bai,Bowen Zhou*

Main category: cs.LG

TL;DR: This survey reviews autonomous scientific discovery using AI across multiple domains, introduces Agentic Science as a paradigm, and identifies key capabilities, workflows, challenges, and opportunities in AI-driven research.


<details>
  <summary>Details</summary>
Motivation: Artificial intelligence (AI) is reshaping scientific discovery, evolving from specialized computational tools into autonomous research partners. The authors position Agentic Science as a pivotal stage within the broader AI for Science paradigm, where AI systems progress from partial assistance to full scientific agency. Enabled by large language models (LLMs), multimodal systems, and integrated research platforms, agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement -- behaviors once regarded as uniquely human.

Method: This survey provides a domain-oriented review of autonomous scientific discovery across life sciences, chemistry, materials science, and physics. The authors unify three previously fragmented perspectives -- process-oriented, autonomy-oriented, and mechanism-oriented -- through a comprehensive framework that connects foundational capabilities, core processes, and domain-specific realizations. Building on this framework, they (i) trace the evolution of AI for Science, (ii) identify five core capabilities underpinning scientific agency, (iii) model discovery as a dynamic four-stage workflow, (iv) review applications across the above domains, and (v) synthesize key challenges and future opportunities.

Result: Agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement.

Conclusion: This work establishes a domain-oriented synthesis of autonomous scientific discovery and positions Agentic Science as a structured paradigm for advancing AI-driven research.

Abstract: Artificial intelligence (AI) is reshaping scientific discovery, evolving from
specialized computational tools into autonomous research partners. We position
Agentic Science as a pivotal stage within the broader AI for Science paradigm,
where AI systems progress from partial assistance to full scientific agency.
Enabled by large language models (LLMs), multimodal systems, and integrated
research platforms, agentic AI shows capabilities in hypothesis generation,
experimental design, execution, analysis, and iterative refinement -- behaviors
once regarded as uniquely human. This survey provides a domain-oriented review
of autonomous scientific discovery across life sciences, chemistry, materials
science, and physics. We unify three previously fragmented perspectives --
process-oriented, autonomy-oriented, and mechanism-oriented -- through a
comprehensive framework that connects foundational capabilities, core
processes, and domain-specific realizations. Building on this framework, we (i)
trace the evolution of AI for Science, (ii) identify five core capabilities
underpinning scientific agency, (iii) model discovery as a dynamic four-stage
workflow, (iv) review applications across the above domains, and (v) synthesize
key challenges and future opportunities. This work establishes a
domain-oriented synthesis of autonomous scientific discovery and positions
Agentic Science as a structured paradigm for advancing AI-driven research.

</details>


### [114] [A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data and Machine Learning](https://arxiv.org/abs/2508.14125)
*Madyan Bagosher,Tala Mustafa,Mohammad Alsmirat,Amal Al-Ali,Isam Mashhour Al Jawarneh*

Main category: cs.LG

TL;DR: The paper proposes a smart framework that integrates multiple data sources to forecast parking occupancy. Random Forest Regression performed best, but LSTM may improve with more data.


<details>
  <summary>Details</summary>
Motivation: Cities face numerous challenges in managing parking and determining occupancy, especially in university campuses. The limited availability of parking spaces on campuses underscores the necessity of implementing efficient systems to allocate vacant parking spots effectively.

Method: The framework integrates multiple data sources, including street maps, mobility, and meteorological data, through a spatial join operation to capture parking behavior and vehicle movement patterns. Several forecasting models, namely, Linear Regression, Support Vector Regression (SVR), Random Forest Regression (RFR), and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was employed using grid search.

Result: Random Forest Regression achieved the lowest RMSE of 0.142 and highest R2 of 0.582.

Conclusion: Random Forest Regression achieved the lowest RMSE of 0.142 and highest R2 of 0.582. However, given the time-series nature of the task, an LSTM model may perform better with additional data and longer timesteps.

Abstract: As urban populations continue to grow, cities face numerous challenges in
managing parking and determining occupancy. This issue is particularly
pronounced in university campuses, where students need to find vacant parking
spots quickly and conveniently during class timings. The limited availability
of parking spaces on campuses underscores the necessity of implementing
efficient systems to allocate vacant parking spots effectively. We propose a
smart framework that integrates multiple data sources, including street maps,
mobility, and meteorological data, through a spatial join operation to capture
parking behavior and vehicle movement patterns over the span of 3 consecutive
days with an hourly duration between 7AM till 3PM. The system will not require
any sensing tools to be installed in the street or in the parking area to
provide its services since all the data needed will be collected using location
services. The framework will use the expected parking entrance and time to
specify a suitable parking area. Several forecasting models, namely, Linear
Regression, Support Vector Regression (SVR), Random Forest Regression (RFR),
and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was
employed using grid search, and model performance is assessed using Root Mean
Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of
Determination (R2). Random Forest Regression achieved the lowest RMSE of 0.142
and highest R2 of 0.582. However, given the time-series nature of the task, an
LSTM model may perform better with additional data and longer timesteps.

</details>


### [115] [Comparison of derivative-free and gradient-based minimization for multi-objective compositional design of shape memory alloys](https://arxiv.org/abs/2508.14127)
*S. Josyula,Y. Noiman,E. J. Payton,T. Giovannelli*

Main category: cs.LG

TL;DR: 本研究使用机器学习和优化算法来优化 SMA 成分，以实现所需的马氏体开始温度 (Ms) 并最大限度地降低成本。


<details>
  <summary>Details</summary>
Motivation: 设计满足性能目标同时保持经济性和可持续性的形状记忆合金 (SMA) 是一项复杂的挑战。在这项工作中，我们专注于优化 SMA 成分以实现所需的马氏体开始温度 (Ms)，同时最大限度地降低成本。

Method: 我们使用机器学习模型作为代理预测器，并应用数值优化方法来搜索合适的合金组合。我们训练了两种类型的机器学习模型，一种是基于树的集成模型，另一种是神经网络，使用实验表征的合金和物理信息特征的数据集。基于树的模型与无导数优化器 (COBYLA) 一起使用，而提供梯度信息的神经网络与基于梯度的优化器 (TRUST-CONSTR) 配对。

Result: 结果表明，虽然两种模型都以相似的精度预测 Ms，但与神经网络配对的优化器更一致地找到更好的解决方案。COBYLA 通常会收敛到次优结果，特别是当起始猜测远离目标时。TRUST-CONSTR 方法表现出更稳定的行为，并且更擅长达到满足两个目标的合金成分。

Conclusion: 本研究展示了一种通过结合物理信息数据、机器学习模型和优化算法来探索新型 SMA 成分的实用方法。虽然数据集的规模小于基于模拟的努力，但实验数据的使用提高了预测的可靠性。该方法可以扩展到其他必须在有限数据下进行设计权衡的材料。

Abstract: Designing shape memory alloys (SMAs) that meet performance targets while
remaining affordable and sustainable is a complex challenge. In this work, we
focus on optimizing SMA compositions to achieve a desired martensitic start
temperature (Ms) while minimizing cost. To do this, we use machine learning
models as surrogate predictors and apply numerical optimization methods to
search for suitable alloy combinations. We trained two types of machine
learning models, a tree-based ensemble and a neural network, using a dataset of
experimentally characterized alloys and physics-informed features. The
tree-based model was used with a derivative-free optimizer (COBYLA), while the
neural network, which provides gradient information, was paired with a
gradient-based optimizer (TRUST-CONSTR). Our results show that while both
models predict Ms with similar accuracy, the optimizer paired with the neural
network finds better solutions more consistently. COBYLA often converged to
suboptimal results, especially when the starting guess was far from the target.
The TRUST-CONSTR method showed more stable behavior and was better at reaching
alloy compositions that met both objectives. This study demonstrates a
practical approach to exploring new SMA compositions by combining
physics-informed data, machine learning models, and optimization algorithms.
Although the scale of our dataset is smaller than simulation-based efforts, the
use of experimental data improves the reliability of the predictions. The
approach can be extended to other materials where design trade-offs must be
made with limited data.

</details>


### [116] [ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification](https://arxiv.org/abs/2508.14134)
*Xin Wu,Fei Teng,Ji Zhang,Xingwang Li,Yuxuan Liang*

Main category: cs.LG

TL;DR: ERIS is proposed to enable guided and reliable feature disentanglement for time series classification, which improves the accuracy on out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: achieving reliable performance on out-of-distribution (OOD) data remains a core obstacle. This obstacle arises from the way models inherently entangle domain-specific and label-relevant features, resulting in spurious correlations. Current methods are largely unguided, lacking the semantic direction required to isolate truly universal features.

Method: an end-to-end Energy-Regularized Information for Shift-Robustness (ERIS) framework to enable guided and reliable feature disentanglement. ERIS incorporates three key mechanisms: an energy-guided calibration mechanism, a weight-level orthogonality strategy, and an auxiliary adversarial training mechanism.

Result: ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy across four benchmarks.

Conclusion: ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy across four benchmarks.

Abstract: An ideal time series classification (TSC) should be able to capture invariant
representations, but achieving reliable performance on out-of-distribution
(OOD) data remains a core obstacle. This obstacle arises from the way models
inherently entangle domain-specific and label-relevant features, resulting in
spurious correlations. While feature disentanglement aims to solve this,
current methods are largely unguided, lacking the semantic direction required
to isolate truly universal features. To address this, we propose an end-to-end
Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework
to enable guided and reliable feature disentanglement. The core idea is that
effective disentanglement requires not only mathematical constraints but also
semantic guidance to anchor the separation process. ERIS incorporates three key
mechanisms to achieve this goal. Specifically, we first introduce an
energy-guided calibration mechanism, which provides crucial semantic guidance
for the separation, enabling the model to self-calibrate. Additionally, a
weight-level orthogonality strategy enforces structural independence between
domain-specific and label-relevant features, thereby mitigating their
interference. Moreover, an auxiliary adversarial training mechanism enhances
robustness by injecting structured perturbations. Experiments demonstrate that
ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy
across four benchmarks.

</details>


### [117] [Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach](https://arxiv.org/abs/2508.14135)
*Collins O. Ogbodo,Timothy J. Rogers,Mattia Dal Borgo,David J. Wagg*

Main category: cs.LG

TL;DR: This study introduces an agent-based decision support framework for adaptive sensor placement across dynamically changing modal test environments.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to test design are typically static-focusing only on global tests without accounting for evolving test campaign parameters or the impact of such changes on previously established decisions. These rigid methodologies often compromise test accuracy and adaptability.

Method: The problem is formulated using an underspecified partially observable Markov decision process, enabling the training of a generalist reinforcement learning agent through a dual-curriculum learning strategy.

Result: A detailed case study on a steel cantilever structure demonstrates the efficacy of the proposed method.

Conclusion: The agent-based decision support framework can effectively optimise sensor locations across frequency segments.

Abstract: Modal testing plays a critical role in structural analysis by providing
essential insights into dynamic behaviour across a wide range of engineering
industries. In practice, designing an effective modal test campaign involves
complex experimental planning, comprising a series of interdependent decisions
that significantly influence the final test outcome. Traditional approaches to
test design are typically static-focusing only on global tests without
accounting for evolving test campaign parameters or the impact of such changes
on previously established decisions, such as sensor configurations, which have
been found to significantly influence test outcomes. These rigid methodologies
often compromise test accuracy and adaptability. To address these limitations,
this study introduces an agent-based decision support framework for adaptive
sensor placement across dynamically changing modal test environments. The
framework formulates the problem using an underspecified partially observable
Markov decision process, enabling the training of a generalist reinforcement
learning agent through a dual-curriculum learning strategy. A detailed case
study on a steel cantilever structure demonstrates the efficacy of the proposed
method in optimising sensor locations across frequency segments, validating its
robustness and real-world applicability in experimental settings.

</details>
