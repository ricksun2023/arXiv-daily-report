{"id": "2507.00477", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00477", "abs": "https://arxiv.org/abs/2507.00477", "authors": ["Qi Wang", "Yixuan Cao", "Yifan Liu", "Jiangtao Zhao", "Ping Luo"], "title": "Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via Continual Pre-training", "comment": null, "summary": "A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system\nenhances a large language model's knowledge by retrieving relevant documents\nbased on user queries. Discrepancies between user queries and document\nphrasings often necessitate query rewriting. However, in specialized domains,\nthe rewriter model may struggle due to limited domain-specific knowledge. To\nresolve this, we propose the R\\&R (Read the doc before Rewriting) rewriter,\nwhich involves continual pre-training on professional documents, akin to how\nstudents prepare for open-book exams by reviewing textbooks. Additionally, it\ncan be combined with supervised fine-tuning for improved results. Experiments\non multiple datasets demonstrate that R\\&R excels in professional QA across\nmultiple domains, effectively bridging the query-document gap, while\nmaintaining good performance in general scenarios, thus advancing the\napplication of RAG-based QA systems in specialized fields.", "AI": {"tldr": "R&R\u901a\u8fc7\u5728\u4e13\u4e1a\u6587\u6863\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86RAG\u5728\u4e13\u4e1a\u9886\u57df\u7531\u4e8e\u9886\u57df\u77e5\u8bc6\u6709\u9650\u800c\u5bfc\u81f4\u7684\u91cd\u5199\u6a21\u578b\u56f0\u96be\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u95ee\u7b54\uff08QA\uff09\u7cfb\u7edf\u901a\u8fc7\u68c0\u7d22\u57fa\u4e8e\u7528\u6237\u67e5\u8be2\u7684\u76f8\u5173\u6587\u6863\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u3002\u7528\u6237\u67e5\u8be2\u548c\u6587\u6863\u63aa\u8f9e\u4e4b\u95f4\u7684\u5dee\u5f02\u901a\u5e38\u9700\u8981\u91cd\u5199\u67e5\u8be2\u3002\u4f46\u662f\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\uff0c\u7531\u4e8e\u9886\u57df\u77e5\u8bc6\u6709\u9650\uff0c\u91cd\u5199\u6a21\u578b\u53ef\u80fd\u4f1a\u9047\u5230\u56f0\u96be\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86R&R\uff08\u91cd\u5199\u524d\u9605\u8bfb\u6587\u6863\uff09\u91cd\u5199\u5668\uff0c\u5b83\u6d89\u53ca\u5728\u4e13\u4e1a\u6587\u6863\u4e0a\u8fdb\u884c\u6301\u7eed\u7684\u9884\u8bad\u7ec3\uff0c\u7c7b\u4f3c\u4e8e\u5b66\u751f\u901a\u8fc7\u590d\u4e60\u6559\u79d1\u4e66\u6765\u51c6\u5907\u5f00\u5377\u8003\u8bd5\u3002", "result": "R&R\u64c5\u957f\u4e8e\u591a\u4e2a\u9886\u57df\u7684\u4e13\u4e1aQA\uff0c\u6709\u6548\u5730\u5f25\u5408\u4e86\u67e5\u8be2-\u6587\u6863\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u5728\u4e00\u822c\u573a\u666f\u4e2d\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "R&R\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cR&R\u64c5\u957f\u4e8e\u591a\u4e2a\u9886\u57df\u7684\u4e13\u4e1aQA\uff0c\u6709\u6548\u5730\u5f25\u5408\u4e86\u67e5\u8be2-\u6587\u6863\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u5728\u4e00\u822c\u573a\u666f\u4e2d\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u57fa\u4e8eRAG\u7684QA\u7cfb\u7edf\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2507.00479", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00479", "abs": "https://arxiv.org/abs/2507.00479", "authors": ["Sixiao Zhang", "Mingrui Liu", "Cheng Long", "Wei Yuan", "Hongxu Chen", "Xiangyu Zhao", "Hongzhi Yin"], "title": "On Mitigating Data Sparsity in Conversational Recommender Systems", "comment": null, "summary": "Conversational recommender systems (CRSs) capture user preference through\ntextual information in dialogues. However, they suffer from data sparsity on\ntwo fronts: the dialogue space is vast and linguistically diverse, while the\nitem space exhibits long-tail and sparse distributions. Existing methods\nstruggle with (1) generalizing to varied dialogue expressions due to\nunderutilization of rich textual cues, and (2) learning informative item\nrepresentations under severe sparsity. To address these problems, we propose a\nCRS model named DACRS. It consists of three modules, namely Dialogue\nAugmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching.\nIn the Dialogue Augmentation module, we apply a two-stage augmentation pipeline\nto augment the dialogue context to enrich the data and improve\ngeneralizability. In the Knowledge-Guided Entity Modeling, we propose a\nknowledge graph (KG) based entity substitution and an entity similarity\nconstraint to enhance the expressiveness of entity embeddings. In the\nDialogue-Entity Matching module, we fuse the dialogue embedding with the\nmentioned entity embeddings through a dialogue-guided attention aggregation to\nacquire user embeddings that contain both the explicit and implicit user\npreferences. Extensive experiments on two public datasets demonstrate the\nstate-of-the-art performance of DACRS.", "AI": {"tldr": "This paper proposes DACRS, a conversational recommender system model that addresses data sparsity issues in dialogue and item spaces through dialogue augmentation, knowledge-guided entity modeling, and dialogue-entity matching. DACRS achieves state-of-the-art results on two public datasets.", "motivation": "Conversational recommender systems (CRSs) capture user preference through textual information in dialogues. However, they suffer from data sparsity on two fronts: the dialogue space is vast and linguistically diverse, while the item space exhibits long-tail and sparse distributions. Existing methods struggle with (1) generalizing to varied dialogue expressions due to underutilization of rich textual cues, and (2) learning informative item representations under severe sparsity.", "method": "a CRS model named DACRS. It consists of three modules, namely Dialogue Augmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching. In the Dialogue Augmentation module, we apply a two-stage augmentation pipeline to augment the dialogue context to enrich the data and improve generalizability. In the Knowledge-Guided Entity Modeling, we propose a knowledge graph (KG) based entity substitution and an entity similarity constraint to enhance the expressiveness of entity embeddings. In the Dialogue-Entity Matching module, we fuse the dialogue embedding with the mentioned entity embeddings through a dialogue-guided attention aggregation to acquire user embeddings that contain both the explicit and implicit user preferences.", "result": "DACRS achieves state-of-the-art performance on two public datasets.", "conclusion": "Extensive experiments on two public datasets demonstrate the state-of-the-art performance of DACRS."}}
{"id": "2507.00487", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00487", "abs": "https://arxiv.org/abs/2507.00487", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "comment": null, "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.", "AI": {"tldr": "MassTool is a multi-task framework that enhances query representation and tool retrieval accuracy for LLMs by using a two-tower architecture and search-based methods.", "motivation": "Existing approaches often neglect the importance of precise query comprehension in tool retrieval for large language models (LLMs).", "method": "MassTool uses a two-tower architecture with a query-centric graph convolution network (QC-GCN) and search-based user intent modeling (SUIM).", "result": "Extensive experiments demonstrate MassTool's effectiveness in improving retrieval accuracy.", "conclusion": "MassTool improves retrieval accuracy through a multi-task search-based framework."}}
{"id": "2507.00094", "categories": ["cs.DB", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.00094", "abs": "https://arxiv.org/abs/2507.00094", "authors": ["Jacobo Casas-Ramos", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Manuel Mucientes", "Manuel Lama"], "title": "Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)", "comment": "Extended version of the paper of the same title accepted at the 23rd\n  International Conference on Business Process Management (BPM 2025)", "summary": "Despite growing interest in process analysis and mining for data-aware\nspecifications, alignment-based conformance checking for declarative process\nmodels has focused on pure control-flow specifications, or mild data-aware\nextensions limited to numerical data and variable-to-constant comparisons. This\nis not surprising: finding alignments is computationally hard, even more so in\nthe presence of data dependencies. In this paper, we challenge this problem in\nthe case where the reference model is captured using data-aware Declare with\ngeneral data types and data conditions. We show that, unexpectedly, it is\npossible to compute data-aware optimal alignments in this rich setting,\nenjoying at once efficiency and expressiveness. This is achieved by carefully\ncombining the two best-known approaches to deal with control flow and data\ndependencies when computing alignments, namely A* search and SMT solving.\nSpecifically, we introduce a novel algorithmic technique that efficiently\nexplores the search space, generating descendant states through the application\nof repair actions aiming at incrementally resolving constraint violations. We\nprove the correctness of our algorithm and experimentally show its efficiency.\nThe evaluation witnesses that our approach matches or surpasses the performance\nof the state of the art while also supporting significantly more expressive\ndata dependencies, showcasing its potential to support real-world applications.", "AI": {"tldr": "This paper presents a new algorithm for data-aware conformance checking of declarative process models that is both efficient and expressive, using A* search and SMT solving.", "motivation": "Alignment-based conformance checking for declarative process models has focused on pure control-flow specifications, or mild data-aware extensions limited to numerical data and variable-to-constant comparisons. Finding alignments is computationally hard, especially with data dependencies.", "method": "The authors introduce a novel algorithmic technique that efficiently explores the search space, generating descendant states through the application of repair actions aiming at incrementally resolving constraint violations.", "result": "The evaluation witnesses that our approach matches or surpasses the performance of the state of the art while also supporting significantly more expressive data dependencies.", "conclusion": "This paper introduces an algorithm for computing data-aware optimal alignments for data-aware Declare models with general data types and data conditions, combining A* search and SMT solving."}}
{"id": "2507.00521", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00521", "abs": "https://arxiv.org/abs/2507.00521", "authors": ["Mugeng Liu", "Siqi Zhong", "Qi Yang", "Yudong Han", "Xuanzhe Liu", "Yun Ma"], "title": "\\texttt{WebANNS}: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers", "comment": "SIGIR 2025", "summary": "Approximate nearest neighbor search (ANNS) has become vital to modern AI\ninfrastructure, particularly in retrieval-augmented generation (RAG)\napplications. Numerous in-browser ANNS engines have emerged to seamlessly\nintegrate with popular LLM-based web applications, while addressing privacy\nprotection and challenges of heterogeneous device deployments. However, web\nbrowsers present unique challenges for ANNS, including computational\nlimitations, external storage access issues, and memory utilization\nconstraints, which state-of-the-art (SOTA) solutions fail to address\ncomprehensively.\n  We propose \\texttt{WebANNS}, a novel ANNS engine specifically designed for\nweb browsers. \\texttt{WebANNS} leverages WebAssembly to overcome computational\nbottlenecks, designs a lazy loading strategy to optimize data retrieval from\nexternal storage, and applies a heuristic approach to reduce memory usage.\nExperiments show that \\texttt{WebANNS} is fast and memory efficient, achieving\nup to $743.8\\times$ improvement in 99th percentile query latency over the SOTA\nengine, while reducing memory usage by up to 39\\%. Note that \\texttt{WebANNS}\ndecreases query time from 10 seconds to the 10-millisecond range in browsers,\nmaking in-browser ANNS practical with user-acceptable latency.", "AI": {"tldr": "WebANNS\u662f\u4e00\u4e2a\u4e13\u4e3a\u6d4f\u89c8\u5668\u8bbe\u8ba1\u7684\u9ad8\u6548ANNS\u5f15\u64ce\uff0c\u5b83\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6848\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u4f7f\u5f97\u6d4f\u89c8\u5668\u5185ANNS\u66f4\u5b9e\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANNS\uff09\u5f15\u64ce\u672a\u80fd\u5168\u9762\u89e3\u51b3Web\u6d4f\u89c8\u5668\u5728\u8ba1\u7b97\u9650\u5236\u3001\u5916\u90e8\u5b58\u50a8\u8bbf\u95ee\u95ee\u9898\u548c\u5185\u5b58\u5229\u7528\u7ea6\u675f\u65b9\u9762\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\uff0c\u800cANNS\u5bf9\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "WebANNS\uff1a\u4e00\u79cd\u4e13\u4e3aWeb\u6d4f\u89c8\u5668\u8bbe\u8ba1\u7684\u65b0\u578bANNS\u5f15\u64ce\uff0c\u5b83\u5229\u7528WebAssembly\u3001\u5ef6\u8fdf\u52a0\u8f7d\u7b56\u7565\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "WebANNS\u5728\u7b2c99\u767e\u5206\u4f4d\u7684\u67e5\u8be2\u5ef6\u8fdf\u65b9\u9762\u6bd4SOTA\u5f15\u64ce\u63d0\u9ad8\u4e86\u9ad8\u8fbe743.8\u500d\uff0c\u540c\u65f6\u5c06\u5185\u5b58\u4f7f\u7528\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe39\uff05\u3002WebANNS\u5c06\u67e5\u8be2\u65f6\u95f4\u4ece10\u79d2\u51cf\u5c11\u523010\u6beb\u79d2\u8303\u56f4\uff0c\u4f7f\u5f97\u6d4f\u89c8\u5668\u4e2d\u7684ANNS\u5728\u7528\u6237\u53ef\u63a5\u53d7\u7684\u5ef6\u8fdf\u4e0b\u53d8\u5f97\u5b9e\u7528\u3002", "conclusion": "WebANNS\u901a\u8fc7\u5229\u7528WebAssembly\u514b\u670d\u8ba1\u7b97\u74f6\u9888\uff0c\u8bbe\u8ba1\u5ef6\u8fdf\u52a0\u8f7d\u7b56\u7565\u4ee5\u4f18\u5316\u4ece\u5916\u90e8\u5b58\u50a8\u7684\u6570\u636e\u68c0\u7d22\uff0c\u5e76\u5e94\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u4ece\u800c\u5728\u6d4f\u89c8\u5668\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u3002"}}
{"id": "2507.00188", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00188", "abs": "https://arxiv.org/abs/2507.00188", "authors": ["Qihan Zhang", "Shaolin Xie", "Ibrahim Sabek"], "title": "LIMAO: A Framework for Lifelong Modular Learned Query Optimization", "comment": "To appear at VLDB 2025 (https://vldb.org/2025/)", "summary": "Query optimizers are crucial for the performance of database systems.\nRecently, many learned query optimizers (LQOs) have demonstrated significant\nperformance improvements over traditional optimizers. However, most of them\noperate under a limited assumption: a static query environment. This limitation\nprevents them from effectively handling complex, dynamic query environments in\nreal-world scenarios. Extensive retraining can lead to the well-known\ncatastrophic forgetting problem, which reduces the LQO generalizability over\ntime. In this paper, we address this limitation and introduce LIMAO (Lifelong\nModular Learned Query Optimizer), a framework for lifelong learning of plan\ncost prediction that can be seamlessly integrated into existing LQOs. LIMAO\nleverages a modular lifelong learning technique, an attention-based neural\nnetwork composition architecture, and an efficient training paradigm designed\nto retain prior knowledge while continuously adapting to new environments. We\nimplement LIMAO in two LQOs, showing that our approach is agnostic to\nunderlying engines. Experimental results show that LIMAO significantly enhances\nthe performance of LQOs, achieving up to a 40% improvement in query execution\ntime and reducing the variance of execution time by up to 60% under dynamic\nworkloads. By leveraging a precise and self-consistent design, LIMAO\neffectively mitigates catastrophic forgetting, ensuring stable and reliable\nplan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup\non selected benchmarks, highlighting its practical advantages in real-world\nquery optimization.", "AI": {"tldr": "LIMAO\u662f\u4e00\u79cd\u7528\u4e8e\u8ba1\u5212\u6210\u672c\u9884\u6d4b\u7684\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684LQO\u4e2d\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u786e\u4fdd\u957f\u671f\u7a33\u5b9a\u7684\u8ba1\u5212\u8d28\u91cf\u3002", "motivation": "\u5927\u591a\u6570\u5b66\u4e60\u578b\u67e5\u8be2\u4f18\u5316\u5668(LQO)\u5728\u9759\u6001\u67e5\u8be2\u73af\u5883\u4e0b\u8fd0\u884c\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5b9e\u9645\u573a\u666f\u4e2d\u590d\u6742\u7684\u52a8\u6001\u67e5\u8be2\u73af\u5883\u3002\u5e7f\u6cdb\u7684\u518d\u57f9\u8bad\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4ece\u800c\u964d\u4f4eLQO\u968f\u65f6\u95f4\u7684\u63a8\u79fb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "LIMAO\u5229\u7528\u6a21\u5757\u5316\u7ec8\u8eab\u5b66\u4e60\u6280\u672f\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u7f51\u7edc\u7ec4\u5408\u67b6\u6784\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u65e8\u5728\u4fdd\u7559\u5148\u524d\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u4e0d\u65ad\u9002\u5e94\u65b0\u7684\u73af\u5883\u3002", "result": "LIMAO\u663e\u8457\u63d0\u9ad8\u4e86LQO\u7684\u6027\u80fd\uff0c\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u63d0\u9ad8\u4e8640%\uff0c\u6267\u884c\u65f6\u95f4\u65b9\u5dee\u964d\u4f4e\u4e8660%\u3002", "conclusion": "LIMAO\u901a\u8fc7\u5229\u7528\u7cbe\u786e\u548c\u81ea\u6d3d\u7684\u8bbe\u8ba1\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u786e\u4fdd\u4e86\u8ba1\u5212\u8d28\u91cf\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002\u4e0ePostgres\u76f8\u6bd4\uff0cLIMAO\u5728\u9009\u5b9a\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe4\u500d\u7684\u52a0\u901f\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u5b9e\u9645\u67e5\u8be2\u4f18\u5316\u4e2d\u7684\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2507.00152", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00152", "abs": "https://arxiv.org/abs/2507.00152", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian M\u00f6ller"], "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "This paper investigates the effectiveness of text-based and multimodal LLMs on table understanding tasks through a cross-domain and cross-modality evaluation. The paper finds that while LLMs maintain robustness across table modalities, they face significant challenges when processing scientific tables.", "motivation": "Although LLMs demonstrate strong performance in downstream tasks, their efficiency in processing tabular data remains underexplored.", "method": "Compare the performance of text-based and multimodal LLMs on tables from scientific vs. non-scientific contexts and examine their robustness on tables represented as images vs. text. Conduct an interpretability analysis to measure context usage and input relevance. Introduce the TableEval benchmark, comprising 3017 tables from scholarly publications, Wikipedia, and financial reports, where each table is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.", "result": "LLMs maintain robustness across table modalities, but face significant challenges when processing scientific tables.", "conclusion": "LLMs maintain robustness across table modalities, but face challenges when processing scientific tables."}}
{"id": "2507.00002", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00002", "abs": "https://arxiv.org/abs/2507.00002", "authors": ["Christopher James Augeri"], "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b58\u50a8\u6846\u67b6HDRAM\uff0c\u901a\u8fc7\u7ed3\u5408\u7ecf\u5178\u3001\u5168\u606f\u548c\u91cf\u5b50\u542f\u53d1\u5f0f\u539f\u7406\uff0c\u63d0\u9ad8\u4e86LLM\u7684\u5173\u8054\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u660e\u663e\u7684\u7cbe\u5ea6\u635f\u5931\uff0c\u8fd9\u91cc\u5c06\u5176\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4fe1\u606f\u4f20\u64ad\u3002\u8fd9\u79cd\u91cd\u6784\u5c06\u95ee\u9898\u4ece\u8ba1\u7b97\u7cbe\u5ea6\u8f6c\u79fb\u5230\u4fe1\u606f\u8bba\u901a\u4fe1\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86HDRAM (\u5168\u606f\u5b9a\u4e49\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668)\uff0c\u4e00\u4e2a\u5c06Transformer\u6f5c\u5728\u7a7a\u95f4\u89c6\u4e3a\u6269\u9891\u4fe1\u9053\u7684\u7b26\u53f7\u5b58\u50a8\u6846\u67b6\u3002HDRAM\u5efa\u7acb\u5728\u8d85tokens\u4e4b\u4e0a\uff0c\u8fd9\u662f\u4e00\u79cd\u96c6\u6210\u4e86\u7ecf\u5178\u7ea0\u9519\u7801(ECC)\u3001\u5168\u606f\u8ba1\u7b97\u548c\u91cf\u5b50\u542f\u53d1\u5f0f\u641c\u7d22\u7684\u7ed3\u6784\u5316\u7b26\u53f7\u4ee3\u7801\uff0c\u901a\u8fc7\u6709\u539f\u5219\u7684\u89e3\u6269\u6765\u6062\u590d\u5206\u5e03\u5f0f\u4fe1\u606f\u3002", "result": "HDRAM\u901a\u8fc7\u76f8\u5e72\u76f8\u4f4d\u5b58\u50a8\u5730\u5740\u5b9e\u73b0\u4e86\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9ad8\u6548\u7684\u952e-\u503c\u64cd\u4f5c\u548cGrover\u5f0f\u641c\u7d22\uff0c\u5728\u6ca1\u6709\u67b6\u6784\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5173\u8054\u68c0\u7d22\u80fd\u529b\u3002", "conclusion": "HDRAM\u901a\u8fc7\u7ed3\u5408ECC\u8bed\u6cd5\u3001\u538b\u7f29\u611f\u77e5\u548cKrylov\u5b50\u7a7a\u95f4\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5173\u8054\u68c0\u7d22\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u7ecf\u5178-\u5168\u606f-\u91cf\u5b50\u542f\u53d1\u5f0f(CHQ)\u539f\u7406\u5982\u4f55\u52a0\u5f3aTransformer\u67b6\u6784\u3002"}}
{"id": "2507.00008", "categories": ["cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00008", "abs": "https://arxiv.org/abs/2507.00008", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "comment": "8 pages, 6 figures", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "AI": {"tldr": "DiMo-GUI\u662f\u4e00\u79cd\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89c6\u89c9\u57fa\u7840\u548c\u6a21\u6001\u611f\u77e5\u4f18\u5316\uff0c\u6539\u8fdb\u4e86GUI\u57fa\u7840\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u5143\u7d20\u7684\u591a\u6837\u6027\u3001\u7a7a\u95f4\u6742\u4e71\u548c\u8bed\u8a00\u7684\u6a21\u7cca\u6027\uff0c\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762 (GUI) \u4e2d\u5bf9\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8fdb\u884c\u57fa\u7840\u5316\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "DiMo-GUI\uff0c\u4e00\u79cd\u7528\u4e8eGUI\u57fa\u7840\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e86\u4e24\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a\u52a8\u6001\u89c6\u89c9\u57fa\u7840\u548c\u6a21\u6001\u611f\u77e5\u4f18\u5316\u3002\u5c06\u8f93\u5165\u5206\u6210\u6587\u672c\u5143\u7d20\u548c\u56fe\u6807\u5143\u7d20\uff0c\u5141\u8bb8\u6a21\u578b\u4f7f\u7528\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u72ec\u7acb\u5730\u5bf9\u6bcf\u4e2a\u6a21\u6001\u8fdb\u884c\u63a8\u7406\u3002\u751f\u6210\u5019\u9009\u7126\u70b9\u533a\u57df\uff0c\u8fd9\u4e9b\u533a\u57df\u4ee5\u6a21\u578b\u7684\u521d\u59cb\u9884\u6d4b\u4e3a\u4e2d\u5fc3\uff0c\u5e76\u9010\u6b65\u653e\u5927\u5230\u5b50\u533a\u57df\u4ee5\u4f18\u5316\u57fa\u7840\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u7ed3\u5408\u6a21\u6001\u5206\u79bb\u548c\u533a\u57df\u805a\u7126\u63a8\u7406\uff0c\u5728\u6807\u51c6GUI\u57fa\u7840\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u63a8\u7406\u7ba1\u9053\u7684\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "DiMo-GUI\u5728\u6807\u51c6GUI\u57fa\u7840\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u5e76\u8bc1\u660e\u4e86\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u63a8\u7406\u7ba1\u9053\u7684\u4e00\u81f4\u6539\u8fdb\uff0c\u7a81\u51fa\u4e86\u5c06\u6a21\u6001\u5206\u79bb\u4e0e\u533a\u57df\u805a\u7126\u63a8\u7406\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.00033", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00033", "abs": "https://arxiv.org/abs/2507.00033", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e27\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5148\u9009\u62e9\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u5e27\u6765\u63d0\u9ad8\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5728\u77ed\u89c6\u9891\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u901a\u5e38\u96be\u4ee5\u8fdb\u884c\u957f\u89c6\u9891\u4e2d\u7684\u8fdc\u7a0b\u63a8\u7406\u3002\u4e3a\u4e86\u6269\u5c55\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u9002\u5e94\u66f4\u957f\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u901a\u5e38\u91c7\u7528\u5e27\u5b50\u91c7\u6837\uff08\u4ee5\u89c4\u5219\u7684\u65f6\u95f4\u95f4\u9694\u9009\u62e9\u5e27\uff09\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5e76\u975e\u6700\u4f73\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u5173\u952e\u5e27\u7684\u4e22\u5931\u6216\u5305\u542b\u6765\u81ea\u591a\u4e2a\u76f8\u4f3c\u5e27\u7684\u5197\u4f59\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5373\u201c\u65f6\u523b\u91c7\u6837\u201d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u6700\u76f8\u5173\u7684\u5e27\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u91c7\u7528\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u65f6\u523b\u68c0\u7d22\u6a21\u578b\u6765\u4f18\u5148\u9009\u62e9\u5e27\u3002", "result": "\u901a\u8fc7\u5728\u56db\u4e2a\u957f\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u4e0e\u95ee\u9898\u6700\u76f8\u5173\u7684\u5e27\uff0c\u589e\u5f3a\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u95ee\u7b54\u65b9\u9762\u7684\u6027\u80fd\u3002\u5728\u56db\u4e2a\u957f\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.00535", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00535", "abs": "https://arxiv.org/abs/2507.00535", "authors": ["Dietmar Jannach", "Amra Deli\u0107", "Francesco Ricci", "Markus Zanker"], "title": "Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support", "comment": "Submitted for publication", "summary": "More than twenty-five years ago, first ideas were developed on how to design\na system that can provide recommendations to groups of users instead of\nindividual users. Since then, a rich variety of algorithmic proposals were\npublished, e.g., on how to acquire individual preferences, how to aggregate\nthem, and how to generate recommendations for groups of users. However, despite\nthe rich literature on the topic, barely any examples of real-world group\nrecommender systems can be found. This lets us question common assumptions in\nacademic research, in particular regarding communication processes in a group\nand how recommendation-supported decisions are made. In this essay, we argue\nthat these common assumptions and corresponding system designs often may not\nmatch the needs or expectations of users. We thus call for a reorientation in\nthis research area, leveraging the capabilities of modern Generative AI\nassistants like ChatGPT. Specifically, as one promising future direction, we\nenvision group recommender systems to be systems where human group members\ninteract in a chat and an AI-based group recommendation agent assists the\ndecision-making process in an agentic way. Ultimately, this shall lead to a\nmore natural group decision-making environment and finally to wider adoption of\ngroup recommendation systems in practice.", "AI": {"tldr": "Group recommender systems need a revamp! Let's use AI assistants in chat interfaces to make them more user-friendly and actually useful.", "motivation": "The lack of real-world group recommender systems despite extensive research suggests that common assumptions in academic research may not align with user needs and expectations.", "method": "The paper proposes integrating AI-based group recommendation agents into chat-based interactions to assist the decision-making process.", "result": "The paper envisions a future where group recommender systems involve human group members interacting in a chat, with an AI agent assisting in decision-making, leading to a more natural environment and wider adoption.", "conclusion": "This paper advocates for a reorientation in group recommender systems research, leveraging Generative AI assistants to create more natural group decision-making environments and promote wider adoption."}}
{"id": "2507.00343", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00343", "abs": "https://arxiv.org/abs/2507.00343", "authors": ["Vishal Chakraborty", "Youri Kaminsky", "Sharad Mehrotra", "Felix Naumann", "Faisal Nawab", "Primal Pappachan", "Mohammad Sadoghi", "Nalini Venkatasubramanian"], "title": "Meaningful Data Erasure in the Presence of Dependencies", "comment": "VLDB 2025 Preprint", "summary": "Data regulations like GDPR require systems to support data erasure but leave\nthe definition of \"erasure\" open to interpretation. This ambiguity makes\ncompliance challenging, especially in databases where data dependencies can\nlead to erased data being inferred from remaining data. We formally define a\nprecise notion of data erasure that ensures any inference about deleted data,\nthrough dependencies, remains bounded to what could have been inferred before\nits insertion. We design erasure mechanisms that enforce this guarantee at\nminimal cost. Additionally, we explore strategies to balance cost and\nthroughput, batch multiple erasures, and proactively compute data retention\ntimes when possible. We demonstrate the practicality and scalability of our\nalgorithms using both real and synthetic datasets.", "AI": {"tldr": "This paper defines a precise notion of data erasure for GDPR compliance in databases, designs efficient erasure mechanisms, and demonstrates their practicality and scalability.", "motivation": "Data regulations like GDPR require systems to support data erasure but leave the definition of 'erasure' open to interpretation. This ambiguity makes compliance challenging, especially in databases where data dependencies can lead to erased data being inferred from remaining data.", "method": "The paper designs erasure mechanisms that enforce the defined guarantee at minimal cost. It also explores strategies to balance cost and throughput, batch multiple erasures, and proactively compute data retention times.", "result": "The paper formally defines a precise notion of data erasure that ensures any inference about deleted data, through dependencies, remains bounded to what could have been inferred before its insertion.", "conclusion": "The paper demonstrates practical and scalable algorithms for data erasure using real and synthetic datasets."}}
{"id": "2507.00163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00163", "abs": "https://arxiv.org/abs/2507.00163", "authors": ["Ari Holtzman", "Chenhao Tan"], "title": "Prompting as Scientific Inquiry", "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "AI": {"tldr": "Prompting should be treated as behavioral science and a key component in the science of LLMs, rather than alchemy.", "motivation": "Prompting is the primary method by which we study and control large language models, and it unlocked major capabilities of LLMs. However, prompting is rarely treated as science.", "method": "Treating LLMs as a new kind of complex and opaque organism and probing the model in its native interface: language.", "result": "Prompting is behavioral science.", "conclusion": "Prompting is a key component in the science of LLMs."}}
{"id": "2507.00003", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.00003", "abs": "https://arxiv.org/abs/2507.00003", "authors": ["Eyhab Al-Masri"], "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "comment": null, "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "AI": {"tldr": "NeutroSENSE, a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT environments, achieved 97% accuracy.", "motivation": "interpretable intrusion detection in IoT environments", "method": "Integrating Random Forest, XGBoost, and Logistic Regression with neutrosophic logic", "result": "NeutroSENSE achieved 97% accuracy, while demonstrating that misclassified samples exhibit significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24).", "conclusion": "Neutrosophic logic enhances both accuracy and explainability, providing a practical foundation for trust-aware AI in edge and fog-based IoT security systems."}}
{"id": "2507.00041", "categories": ["cs.AI", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00041", "abs": "https://arxiv.org/abs/2507.00041", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "AI": {"tldr": "TalentMine\u662f\u4e00\u79cd\u65b0\u578bLLM\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u8bed\u4e49\u4e30\u5bcc\u8868\u683c\u8868\u793a\u6765\u6539\u8fdb\u4eba\u624d\u7ba1\u7406\u7cfb\u7edf\u4e2d\u8868\u683c\u4fe1\u606f\u7684\u68c0\u7d22\uff0c\u5e76\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4eba\u624d\u7ba1\u7406\u7cfb\u7edf\u4e2d\u590d\u6742\u7684\u8868\u683c\u4fe1\u606f\u65f6\u9762\u4e34\u68c0\u7d22\u6311\u6218\uff0c\u73b0\u6709\u7684\u8868\u683c\u63d0\u53d6\u65b9\u6cd5\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u68c0\u7d22\u589e\u5f3a\u7684\u804a\u5929\u5e94\u7528\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTalentMine\u7684LLM\u589e\u5f3a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e13\u95e8\u7684\u591a\u6a21\u6001\u63a8\u7406\u5c06\u63d0\u53d6\u7684\u8868\u683c\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u4ece\u800c\u4fdd\u7559\u8868\u683c\u6570\u636e\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u7ef4\u5ea6\u3002", "result": "TalentMine\u5728\u5458\u5de5\u798f\u5229\u6587\u6863\u96c6\u5408\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86100%\u7684\u51c6\u786e\u7387\uff0c\u800c\u6807\u51c6AWS Textract\u63d0\u53d6\u4e3a0%\uff0cAWS Textract Visual Q&A\u4e3a40%\u3002", "conclusion": "TalentMine\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86100%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eAWS Textract\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4eba\u624d\u7ba1\u7406\u5e94\u7528\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.00042", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00042", "abs": "https://arxiv.org/abs/2507.00042", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "comment": "ICANN 2025", "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "AI": {"tldr": "ER-EMU: An edge model update algorithm based on adaptive experience replay, to address the limitations of catastrophic forgetting.", "motivation": "Continually adapting edge models in cloud-edge collaborative object detection for traffic monitoring suffers from catastrophic forgetting, where models lose previously learned knowledge when adapting to new data distributions. Existing approaches like experience replay and visual prompts offer some mitigation, but struggle to effectively prioritize and leverage historical data for optimal knowledge retention and adaptation. Specifically, simply storing and replaying all historical data can be inefficient, while treating all historical experiences as equally important overlooks their varying relevance to the current domain.", "method": "an edge model update algorithm based on adaptive experience replay, utilizing a limited-size experience buffer managed using a First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target domains, prioritizing the selection of historical data that is most dissimilar to the current target domain. The experience buffer is also updated using a simple random sampling strategy", "result": "Experiments on the Bellevue traffic video dataset, involving repeated day/night cycles, demonstrate that ER-EMU consistently improves the performance of several state-of-the-art cloud-edge collaborative object detection frameworks.", "conclusion": "ER-EMU consistently improves the performance of several state-of-the-art cloud-edge collaborative object detection frameworks."}}
{"id": "2507.00543", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00543", "abs": "https://arxiv.org/abs/2507.00543", "authors": ["Leila Tavakoli", "Hamed Zamani"], "title": "Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications", "comment": "9 pages,5 figures", "summary": "Despite growing interest in using large language models (LLMs) to automate\nannotation, their effectiveness in complex, nuanced, and multi-dimensional\nlabelling tasks remains relatively underexplored. This study focuses on\nannotation for the search clarification task, leveraging a high-quality,\nmulti-dimensional dataset that includes five distinct fine-grained annotation\nsubtasks. Although LLMs have shown impressive capabilities in general settings,\nour study reveals that even state-of-the-art models struggle to replicate\nhuman-level performance in subjective or fine-grained evaluation tasks. Through\na systematic assessment, we demonstrate that LLM predictions are often\ninconsistent, poorly calibrated, and highly sensitive to prompt variations. To\naddress these limitations, we propose a simple yet effective human-in-the-loop\n(HITL) workflow that uses confidence thresholds and inter-model disagreement to\nselectively involve human review. Our findings show that this lightweight\nintervention significantly improves annotation reliability while reducing human\neffort by up to 45%, offering a relatively scalable and cost-effective yet\naccurate path forward for deploying LLMs in real-world evaluation settings.", "AI": {"tldr": "LLMs\u5728\u7ec6\u7c92\u5ea6\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u4eba\u673a\u534f\u4f5c\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u5e76\u51cf\u5c11\u4eba\u5de5\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u4eec\u8d8a\u6765\u8d8a\u6709\u5174\u8da3\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u6765\u81ea\u52a8\u5316\u6ce8\u91ca\uff0c\u4f46\u5b83\u4eec\u5728\u590d\u6742\u3001\u7ec6\u81f4\u548c\u591a\u7ef4\u6807\u8bb0\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30", "result": "LLM\u9884\u6d4b\u901a\u5e38\u4e0d\u4e00\u81f4\uff0c\u6821\u51c6\u4e0d\u826f\uff0c\u5e76\u4e14\u5bf9\u63d0\u793a\u53d8\u5316\u9ad8\u5ea6\u654f\u611f\u3002\u8f7b\u91cf\u7ea7\u5e72\u9884\u663e\u8457\u63d0\u9ad8\u4e86\u6ce8\u91ca\u53ef\u9760\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u9ad8\u8fbe45%\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "LLMs\u5728\u4e3b\u89c2\u6216\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4efb\u52a1\u4e2d\u96be\u4ee5\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u901a\u8fc7\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u9608\u503c\u548c\u6a21\u578b\u95f4\u5dee\u5f02\u7684HITL\u5de5\u4f5c\u6d41\u7a0b\uff0c\u53ef\u4ee5\u5728\u663e\u8457\u63d0\u9ad8\u6ce8\u91ca\u53ef\u9760\u6027\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u9ad8\u8fbe45%\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2507.00379", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00379", "abs": "https://arxiv.org/abs/2507.00379", "authors": ["Zikai Wang", "Qianxi Zhang", "Baotong Lu", "Qi Chen", "Cheng Tan"], "title": "Towards Robustness: A Critique of Current Vector Database Assessments", "comment": null, "summary": "Vector databases are critical infrastructure in AI systems, and average\nrecall is the dominant metric for their evaluation. Both users and researchers\nrely on it to choose and optimize their systems. We show that relying on\naverage recall is problematic. It hides variability across queries, allowing\nsystems with strong mean performance to underperform significantly on hard\nqueries. These tail cases confuse users and can lead to failure in downstream\napplications such as RAG. We argue that robustness consistently achieving\nacceptable recall across queries is crucial to vector database evaluation. We\npropose Robustness-$\\delta$@K, a new metric that captures the fraction of\nqueries with recall above a threshold $\\delta$. This metric offers a deeper\nview of recall distribution, helps vector index selection regarding application\nneeds, and guides the optimization of tail performance. We integrate\nRobustness-$\\delta$@K into existing benchmarks and evaluate mainstream vector\nindexes, revealing significant robustness differences. More robust vector\nindexes yield better application performance, even with the same average\nrecall. We also identify design factors that influence robustness, providing\nguidance for improving real-world performance.", "AI": {"tldr": "\u5e73\u5747\u53ec\u56de\u7387\u4f5c\u4e3a\u5411\u91cf\u6570\u636e\u5e93\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u63a9\u76d6\u4e86\u67e5\u8be2\u4e4b\u95f4\u7684\u5dee\u5f02\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Robustness-$\\\\delta$@K \u6307\u6807\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8be5\u6307\u6807\u80fd\u66f4\u597d\u53cd\u5e94\u5411\u91cf\u6570\u636e\u5e93\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5e73\u5747\u53ec\u56de\u7387\u63a9\u76d6\u4e86\u67e5\u8be2\u4e4b\u95f4\u7684\u53ef\u53d8\u6027\uff0c\u5bfc\u81f4\u5177\u6709\u5f3a\u5927\u5e73\u5747\u6027\u80fd\u7684\u7cfb\u7edf\u5728\u56f0\u96be\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\u660e\u663e\u4e0d\u4f73\u3002\u8fd9\u4e9b\u5c3e\u90e8\u7528\u4f8b\u4f1a\u6df7\u6dc6\u7528\u6237\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u4e0b\u6e38\u5e94\u7528\u7a0b\u5e8f\uff08\u5982 RAG\uff09\u5931\u8d25\u3002", "method": "\u63d0\u51fa Robustness-$\\\\delta$@K\uff0c\u4e00\u79cd\u65b0\u7684\u6307\u6807\uff0c\u7528\u4e8e\u6355\u83b7\u53ec\u56de\u7387\u9ad8\u4e8e\u9608\u503c $\\\\delta$ \u7684\u67e5\u8be2\u6bd4\u4f8b\u3002", "result": "\u5c06 Robustness-$\\\\delta$@K \u96c6\u6210\u5230\u73b0\u6709\u57fa\u51c6\u4e2d\uff0c\u5e76\u8bc4\u4f30\u4e3b\u6d41\u5411\u91cf\u7d22\u5f15\uff0c\u63ed\u793a\u4e86\u663e\u7740\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u3002", "conclusion": "\u9c81\u68d2\u6027\u66f4\u5f3a\u7684\u5411\u91cf\u7d22\u5f15\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u5e94\u7528\u6027\u80fd\uff0c\u5373\u4f7f\u5e73\u5747\u53ec\u56de\u7387\u76f8\u540c\u3002\u8bbe\u8ba1\u56e0\u7d20\u4f1a\u5f71\u54cd\u9c81\u68d2\u6027\uff0c\u4e3a\u63d0\u9ad8\u5b9e\u9645\u6027\u80fd\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.00210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00210", "abs": "https://arxiv.org/abs/2507.00210", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han L\u00f9", "Massimo Caccia", "V\u00e9ronique Eglin", "Alexandre Aussem", "J\u00e9r\u00e9my Espinas", "Alexandre Lacoste"], "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "AI": {"tldr": "\u63d0\u51faLineRetriever\uff0c\u4e00\u79cd\u7528\u4e8eweb\u5bfc\u822a\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u6765\u8bc6\u522b\u548c\u68c0\u7d22\u4e0e\u672a\u6765\u5bfc\u822a\u6b65\u9aa4\u6700\u76f8\u5173\u7684\u89c2\u5bdf\u7ebf\uff0c\u4ece\u800c\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u5185\u4fdd\u6301\u4e00\u81f4\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u662f\u7f51\u9875\u7684\u5e7f\u6cdb\u4e0a\u4e0b\u6587\uff0c\u901a\u5e38\u8868\u793a\u4e3aDOM\u6216\u53ef\u8bbf\u95ee\u6027\u6811\uff08AxTree\uff09\u7ed3\u6784\uff0c\u7ecf\u5e38\u8d85\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u7684\u9650\u5236\u3002\u50cf\u81ea\u4e0b\u800c\u4e0a\u7684\u622a\u65ad\u6216\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u4e4b\u7c7b\u7684\u65b9\u6cd5\u4f1a\u4e22\u5931\u6709\u5173\u9875\u9762\u72b6\u6001\u548c\u64cd\u4f5c\u5386\u53f2\u8bb0\u5f55\u7684\u5173\u952e\u4fe1\u606f\u3002\u8fd9\u5bf9\u4e8eWeb\u4ee3\u7406\u4e2d\u7684\u81ea\u9002\u5e94\u8ba1\u5212\u5c24\u5176\u6210\u95ee\u9898\uff0c\u5728Web\u4ee3\u7406\u4e2d\uff0c\u7406\u89e3\u5f53\u524d\u72b6\u6001\u5bf9\u4e8e\u786e\u5b9a\u5c06\u6765\u7684\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLineRetriever\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u6765\u8bc6\u522b\u548c\u68c0\u7d22\u4e0e\u672a\u6765\u5bfc\u822a\u6b65\u9aa4\u6700\u76f8\u5173\u7684\u89c2\u5bdf\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLineRetriever\u53ef\u4ee5\u51cf\u5c11web agent\u6bcf\u4e00\u6b65\u7684\u89c2\u5bdf\u8303\u56f4\uff0c\u540c\u65f6\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u5185\u4fdd\u6301\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "LineRetriever\u53ef\u4ee5\u51cf\u5c11web agent\u6bcf\u4e00\u6b65\u7684\u89c2\u5bdf\u8303\u56f4\uff0c\u540c\u65f6\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u5185\u4fdd\u6301\u4e00\u81f4\u7684\u6027\u80fd\u3002"}}
{"id": "2507.00004", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.00004", "abs": "https://arxiv.org/abs/2507.00004", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "comment": null, "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u5b9a\u5411\u968f\u673a\u6280\u80fd\u641c\u7d22\uff08DS3\uff09\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5bf9\u8bad\u7ec3-\u63a8\u7406\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u8fdb\u884c\u663e\u5f0f\u8868\u5f81\uff0c\u52a0\u6df1\u7406\u8bba\u7406\u89e3\u5e76\u652f\u6301\u7b97\u6cd5\u8bbe\u8ba1\u548c\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u671f\u95f4\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u3001\u80fd\u6e90\u548c\u8d22\u52a1\u8d44\u6e90\u3002\u63a8\u7406\u6210\u672c\u73b0\u5728\u4ee3\u8868\u4e86\u603b\u4f53\u8d44\u6e90\u8d1f\u62c5\u7684\u4e00\u4e2a\u91cd\u8981\u4e14\u4e0d\u65ad\u589e\u957f\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u5b9a\u5411\u968f\u673a\u6280\u80fd\u641c\u7d22\uff08DS3\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5b83\u5c06\u63a8\u7406\u8868\u793a\u4e3a\u5728\u5b66\u4e60\u7684\u6280\u80fd\u56fe\u4e0a\u7684\u968f\u673a\u904d\u5386\u3002", "result": "\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u6062\u590d\u4e86\u7ecf\u9a8c\u89c2\u5bdf\u5230\u7684\u6a21\u5f0f\uff0c\u5305\u62ec\uff1a\u7cbe\u5ea6\u968f\u5bf9\u6570\u8ba1\u7b97\u7ebf\u6027\u7f29\u653e\uff1b\u9996\u9009\u63a8\u7406\u7b56\u7565\u968f\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u80fd\u529b\u7684\u53d8\u5316\u800c\u53d8\u5316\uff1b\u5373\u4f7f\u5728\u53c2\u6570\u7f29\u653e\u4e0b\u6027\u80fd\u8d8b\u4e8e\u7a33\u5b9a\u65f6\uff0c\u63a8\u7406\u4e5f\u4f1a\u5f15\u53d1\u6d8c\u73b0\u884c\u4e3a\uff1b\u4ee5\u53ca\u5728\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u5185\u6355\u83b7\u7684 Best-of-N (BoN) \u548c\u591a\u6570\u6295\u7968\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u52a0\u6df1\u4e86\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u652f\u6301\u6709\u539f\u5219\u7684\u7b97\u6cd5\u8bbe\u8ba1\u548c\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2507.00048", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00048", "abs": "https://arxiv.org/abs/2507.00048", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "comment": "10 pages, 5 figures", "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "AI": {"tldr": "This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management.... Inspired by the concept of ``frugal twin\", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color.", "motivation": "The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively.", "method": "a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management", "result": "New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments.", "conclusion": "The tools introduced are generally applicable and can easily be extended to other optimization problems."}}
{"id": "2507.00043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00043", "abs": "https://arxiv.org/abs/2507.00043", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "comment": null, "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "AI": {"tldr": "MR-CLIP\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5229\u7528DICOM\u5143\u6570\u636e\u5b66\u4e60\u5bf9\u6bd4\u5ea6\u611f\u77e5\u7684MR\u56fe\u50cf\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4e34\u5e8a\u5e94\u7528\u4e2d\u5143\u6570\u636e\u4e0d\u5b8c\u6574\u548c\u7f3a\u4e4f\u6807\u7b7e\u7684\u95ee\u9898\u3002", "motivation": "\u4e34\u5e8a\u7cfb\u7edf\u4e2d\u5bf9\u78c1\u5171\u632f\u6210\u50cf\u626b\u63cf\u7684\u51c6\u786e\u89e3\u91ca\u4f9d\u8d56\u4e8e\u5bf9\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u7684\u7cbe\u786e\u7406\u89e3\u3002\u7136\u800c\uff0cDICOM\u5143\u6570\u636e\u4e2d\u7684\u91c7\u96c6\u53c2\u6570\u901a\u5e38\u4e0d\u5b8c\u6574\u3001\u5608\u6742\u6216\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u53ef\u9760\u548c\u6807\u51c6\u5316\u7684\u5143\u6570\u636e\u4f1a\u4f7f\u56fe\u50cf\u89e3\u91ca\u3001\u68c0\u7d22\u548c\u96c6\u6210\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7b49\u4efb\u52a1\u590d\u6742\u5316\u3002", "method": "\u63d0\u51fa\u4e86MR-CLIP\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5c06MR\u56fe\u50cf\u4e0e\u5176DICOM\u5143\u6570\u636e\u5bf9\u9f50\uff0c\u4ee5\u5b66\u4e60\u5bf9\u6bd4\u5ea6\u611f\u77e5\u8868\u793a\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u624b\u52a8\u6807\u7b7e\u3002", "result": "MR-CLIP\u6355\u83b7\u4e86\u8de8\u91c7\u96c6\u548c\u626b\u63cf\u5185\u7684\u5bf9\u6bd4\u5ea6\u53d8\u5316\uff0c\u5b9e\u73b0\u4e86\u4e0e\u89e3\u5256\u7ed3\u6784\u65e0\u5173\u7684\u8868\u793a\u3002", "conclusion": "MR-CLIP\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u5bf9\u6bd4\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.00715", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00715", "abs": "https://arxiv.org/abs/2507.00715", "authors": ["Chaoqun Yang", "Xinyu Lin", "Wenjie Wang", "Yongqi Li", "Teng Sun", "Xianjing Han", "Tat-Seng Chua"], "title": "EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens", "comment": "Accepted by KDD 2025", "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.", "AI": {"tldr": "EARN\u662f\u4e00\u79cd\u7528\u4e8e\u52a0\u901fLLMRec\u63a8\u7406\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u65e9\u671f\u5c42\u7684\u4fe1\u606f\u5230\u5bc4\u5b58\u5668\u4ee4\u724c\u5e76\u5728\u540e\u7eed\u5c42\u4e2d\u5173\u6ce8\u8fd9\u4e9b\u4ee4\u724c\u6765\u5b9e\u73b0\u3002", "motivation": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u63a8\u8350\uff08LLMRec\uff09\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u7531\u4e8e\u5927\u91cf\u7684\u8ba1\u7b97\u5f00\u9500\u548cKV\u7f13\u5b58\u7684\u5185\u5b58\u538b\u529b\uff0c\u5b83\u5b58\u5728\u9ad8\u63a8\u7406\u5ef6\u8fdf\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684KV\u7f13\u5b58\u51cf\u5c11\u65b9\u6cd5\u9762\u4e34\u7740\u5173\u952e\u7684\u9650\u5236\uff1a\u8003\u8651\u5230\u63a8\u8350\u4efb\u52a1\u7684\u77ed\u89e3\u7801\u6b65\u9aa4\uff0c\u7f13\u5b58\u538b\u7f29\u63d0\u4f9b\u7684\u52a0\u901f\u6548\u679c\u5fae\u4e4e\u5176\u5fae\uff0c\u800c\u63d0\u793a\u538b\u7f29\u5219\u6709\u4e22\u5f03\u91cd\u8981\u4ea4\u4e92\u5386\u53f2\u7684\u98ce\u9669\u3002", "method": "EARN\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u5229\u7528\u65e9\u671f\u5c42\u5c06\u4fe1\u606f\u538b\u7f29\u5230\u4f4d\u4e8e\u8f93\u5165\u5e8f\u5217\u8fb9\u754c\u7684\u5bc4\u5b58\u5668\u4ee4\u724c\u4e2d\uff0c\u7136\u540e\u5728\u540e\u7eed\u5c42\u4e2d\u4ec5\u5173\u6ce8\u8fd9\u4e9b\u4ee4\u724c\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u3001\u4e24\u79cdLLMRec\u65b9\u6cd5\u548c\u4e24\u79cdLLM\u67b6\u6784\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEARN\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u4e0e\u901a\u7528\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.79\u500d\u7684\u52a0\u901f\u548c80.8%\u7684KV\u7f13\u5b58\u51cf\u5c11\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "EARN\u901a\u8fc7\u5229\u7528\u65e9\u671f\u5c42\u5c06\u4fe1\u606f\u538b\u7f29\u5230\u4f4d\u4e8e\u8f93\u5165\u5e8f\u5217\u8fb9\u754c\u7684\u5bc4\u5b58\u5668\u4ee4\u724c\u4e2d\uff0c\u7136\u540e\u5728\u540e\u7eed\u5c42\u4e2d\u4ec5\u5173\u6ce8\u8fd9\u4e9b\u4ee4\u724c\uff0c\u4ece\u800c\u5728LLMRec\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u6548\u679c\u7684\u5e73\u8861\uff0c\u4e3a\u5de5\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u9645\u7684\u90e8\u7f72\u4f18\u52bf\u3002"}}
{"id": "2507.00427", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00427", "abs": "https://arxiv.org/abs/2507.00427", "authors": ["Hao Wu", "Changzheng Wei", "Yanhao Wang", "Li Lin", "Yilong Leng", "Shiyu He", "Minghao Zhao", "Hanghang Wu", "Ying Yan", "Aoying Zhou"], "title": "Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition", "comment": null, "summary": "This paper investigates the feasibility of achieving zero-knowledge\nverifiability for graph databases, enabling database owners to\ncryptographically prove the query execution correctness without disclosing the\nunderlying data. Although similar capabilities have been explored for\nrelational databases, their implementation for graph databases presents unique\nchallenges. This is mainly attributed to the relatively large complexity of\nqueries in graph databases. When translating graph queries into arithmetic\ncircuits, the circuit scale can be too large to be practically evaluated. To\naddress this issue, we propose to break down graph queries into more\nfine-grained, primitive operators, enabling a step-by-step evaluation through\nsmaller-scale circuits. Accordingly, the verification with ZKP circuits of\ncomplex graph queries can be decomposed into a series of composable\ncryptographic primitives, each designed to verify a fundamental structural\nproperty such as path ordering or edge directionality. Especially, having\nnoticed that the graph expansion (i.e., traversing from nodes to their\nneighbors along edges) operation serves as the backbone of graph query\nevaluation, we design the expansion centric operator decomposition. In addition\nto constructing circuits for the expansion primitives, we also design\nspecialized ZKP circuits for the various attributes that augment this\ntraversal. The circuits are meticulously designed to take advantage of PLONKish\narithmetization. By integrating these optimized circuits, we implement ZKGraph,\na system that provides verifiable query processing while preserving data\nprivacy. Performance evaluation indicates that ZKGraph significantly\noutperforms naive in circuit implementations of graph operators, achieving\nsubstantial improvements in both runtime and memory consumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa ZKGraph\uff0c\u4e00\u4e2a\u4e3a\u56fe\u6570\u636e\u5e93\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u67e5\u8be2\u5904\u7406\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u56fe\u67e5\u8be2\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u7b97\u5b50\u5e76\u4f18\u5316 ZKP \u7535\u8def\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u56fe\u6570\u636e\u5e93\u5b9e\u73b0\u96f6\u77e5\u8bc6\u53ef\u9a8c\u8bc1\u6027\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u56fe\u6570\u636e\u5e93\u4e2d\u67e5\u8be2\u7684\u590d\u6742\u6027\u8f83\u9ad8\uff0c\u76f4\u63a5\u5c06\u56fe\u67e5\u8be2\u8f6c\u6362\u4e3a\u7b97\u672f\u7535\u8def\u4f1a\u5bfc\u81f4\u7535\u8def\u89c4\u6a21\u8fc7\u5927\uff0c\u96be\u4ee5\u5b9e\u9645\u8bc4\u4f30\u3002", "method": "\u5c06\u56fe\u67e5\u8be2\u5206\u89e3\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684\u3001\u539f\u59cb\u7684\u7b97\u5b50\uff0c\u4ece\u800c\u80fd\u591f\u901a\u8fc7\u66f4\u5c0f\u89c4\u6a21\u7684\u7535\u8def\u8fdb\u884c\u9010\u6b65\u8bc4\u4f30\u3002\u8bbe\u8ba1\u4e86\u4ee5\u6269\u5c55\u4e3a\u4e2d\u5fc3\u7684\u7b97\u5b50\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u4e3a\u6269\u5c55\u539f\u8bed\u548c\u5404\u79cd\u5c5e\u6027\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684 ZKP \u7535\u8def\uff0c\u8fd9\u4e9b\u7535\u8def\u5145\u5206\u5229\u7528\u4e86 PLONKish \u7b97\u672f\u5316\u3002", "result": "ZKGraph \u5728\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4f18\u4e8e\u56fe\u7b97\u5b50\u7684\u539f\u751f\u7535\u8def\u5185\u5b9e\u73b0\u3002", "conclusion": "ZKGraph\u7cfb\u7edf\u901a\u8fc7\u96c6\u6210\u4f18\u5316\u7684\u7535\u8def\uff0c\u5b9e\u73b0\u4e86\u53ef\u9a8c\u8bc1\u7684\u67e5\u8be2\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u62a4\u4e86\u6570\u636e\u9690\u79c1\u3002\u6027\u80fd\u8bc4\u4f30\u8868\u660e\uff0cZKGraph \u663e\u8457\u4f18\u4e8e\u56fe\u7b97\u5b50\u7684\u539f\u751f\u7535\u8def\u5185\u5b9e\u73b0\uff0c\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.00214", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00214", "abs": "https://arxiv.org/abs/2507.00214", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u63a8\u7406\u6765\u589e\u5f3a\u6587\u672c\u5206\u7c7b\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u8bad\u7ec3\u4ee5\u8f93\u51fa\u63a8\u7406\u548c\u60c5\u611f\u7684\u751f\u6210\u6a21\u578b\uff0c\u5728\u60c5\u611f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u63d0\u9ad8\u3002", "motivation": "\u6807\u51c6\u5206\u7c7b\u6a21\u578b\u901a\u5e38\u5c06\u8f93\u5165\u76f4\u63a5\u6620\u5c04\u5230\u6807\u7b7e\uff0c\u800c\u6ca1\u6709\u660e\u786e\u7684\u63a8\u7406\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9650\u5236\u5b83\u4eec\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528Llama-3.2-1B-Instruct\u6a21\u578b\u5728\u901a\u7528\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u751f\u6210\u6587\u672c\u63a8\u7406\uff08R\uff09\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u6a21\u578b\u79bb\u7ebf\u521b\u5efa\u589e\u5f3a\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4e0b\u6e38\u751f\u6210\u6a21\u578b\u3002", "result": "\u751f\u6210\u6a21\u578b\u5728\u8f93\u51fa\u63a8\u7406\u548c\u60c5\u611f\u65b9\u9762\uff0c\u4e0e\u4ec5\u8f93\u51fa\u60c5\u611f\u7684\u57fa\u7ebf\u751f\u6210\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u51c6\u786e\u7387\u4e0a\u663e\u7740\u63d0\u9ad8\u4e868.7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u4f7f\u7528LLM\u751f\u6210\u7684\u63a8\u7406\u53ef\u4ee5\u521b\u5efa\u66f4\u4e30\u5bcc\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ece\u800c\u63d0\u9ad8\u5404\u79cd\u4e0b\u6e38NLP\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u660e\u786e\u7684\u89e3\u91ca\u3002"}}
{"id": "2507.00011", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00011", "abs": "https://arxiv.org/abs/2507.00011", "authors": ["Nathan Vaartjes", "Vincent Francois-Lavet"], "title": "Novel RL approach for efficient Elevator Group Control Systems", "comment": "15 pages, 12 figures", "summary": "Efficient elevator traffic management in large buildings is critical for\nminimizing passenger travel times and energy consumption. Because heuristic- or\npattern-detection-based controllers struggle with the stochastic and\ncombinatorial nature of dispatching, we model the six-elevator, fifteen-floor\nsystem at Vrije Universiteit Amsterdam as a Markov Decision Process and train\nan end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).\nKey innovations include a novel action space encoding to handle the\ncombinatorial complexity of elevator dispatching, the introduction of\ninfra-steps to model continuous passenger arrivals, and a tailored reward\nsignal to improve learning efficiency. In addition, we explore various ways to\nadapt the discounting factor to the infra-step formulation. We investigate RL\narchitectures based on Dueling Double Deep Q-learning, showing that the\nproposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a\nhighly stochastic environment, and thereby outperforms a traditional rule-based\nalgorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7535\u68af\u7fa4\u63a7\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\u3002", "motivation": "\u5927\u578b\u5efa\u7b51\u4e2d\u9ad8\u6548\u7684\u7535\u68af\u4ea4\u901a\u7ba1\u7406\u5bf9\u4e8e\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e58\u5ba2\u7684\u65c5\u884c\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u57fa\u4e8e\u542f\u53d1\u5f0f\u6216\u6a21\u5f0f\u68c0\u6d4b\u7684\u63a7\u5236\u5668\u96be\u4ee5\u5904\u7406\u8c03\u5ea6\u7684\u968f\u673a\u6027\u548c\u7ec4\u5408\u6027\u3002", "method": "\u5c06\u963f\u59c6\u65af\u7279\u4e39\u81ea\u7531\u5927\u5b66\u7684\u516d\u90e8\u7535\u68af\u3001\u5341\u4e94\u5c42\u697c\u7cfb\u7edf\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60(RL)\u7535\u68af\u7fa4\u63a7\u7cfb\u7edf(EGCS)\u3002", "result": "\u8be5\u7814\u7a76\u7684\u5173\u952e\u521b\u65b0\u5305\u62ec\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u4f5c\u7a7a\u95f4\u7f16\u7801\uff0c\u7528\u4e8e\u5904\u7406\u7535\u68af\u8c03\u5ea6\u7684\u7ec4\u5408\u590d\u6742\u6027\uff0c\u5f15\u5165\u4e86infra-step\u6765\u6a21\u62df\u8fde\u7eed\u7684\u4e58\u5ba2\u5230\u8fbe\uff0c\u4ee5\u53ca\u5b9a\u5236\u7684\u5956\u52b1\u4fe1\u53f7\u6765\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86\u5404\u79cd\u65b9\u6cd5\u6765\u8c03\u6574\u6298\u6263\u56e0\u5b50\u4ee5\u9002\u5e94infra-step\u516c\u5f0f\u3002\u6211\u4eec\u7814\u7a76\u4e86\u57fa\u4e8eDueling Double Deep Q-learning\u7684RL\u67b6\u6784\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7535\u68af\u7fa4\u63a7\u7cfb\u7edf(EGCS)\u80fd\u591f\u9002\u5e94\u6ce2\u52a8\u7684\u4ea4\u901a\u6a21\u5f0f\uff0c\u4ece\u9ad8\u5ea6\u968f\u673a\u7684\u73af\u5883\u4e2d\u5b66\u4e60\uff0c\u5e76\u56e0\u6b64\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\u3002"}}
{"id": "2507.00050", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.00050", "abs": "https://arxiv.org/abs/2507.00050", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "comment": null, "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "AI": {"tldr": "This paper introduces SEZ-HARN, a novel IMU-based ZS-HAR model that recognizes unseen activities and provides explanations via skeleton videos, achieving competitive accuracy and improved transparency compared to existing black-box models.", "motivation": "lack of comprehensive IMU-based HAR datasets that cover a wide range of activities and the lack of transparency in existing HAR models. Zero-shot HAR (ZS-HAR) overcomes the data limitations, but current models struggle to explain their decisions, making them less transparent", "method": "a novel IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN). It can recognize activities not encountered during training and provide skeleton videos to explain its decision-making process.", "result": "The experiment results demonstrate that SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy.", "conclusion": "SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3% of the best-performing black-box model on PAMAP2 while maintaining comparable performance on the other three datasets."}}
{"id": "2507.00044", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00044", "abs": "https://arxiv.org/abs/2507.00044", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "comment": "14 pages, 5 figures", "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "AI": {"tldr": "This paper proposes and compares three artifact detection approaches for WSIs, and the foundation model-based approach achieved the best performance.", "motivation": "WSI is vulnerable to artifacts introduced during slide preparation and scanning, which can compromise downstream image analysis.", "method": "Three artifact detection approaches were proposed and compared: a foundation model-based approach (FMA) using a fine-tuned Unified Neural Image (UNI) architecture, a deep learning approach (DLA) built on a ResNet50 backbone, and a knowledge-based approach (KBA) leveraging handcrafted features from texture, color, and frequency-based metrics.", "result": "The FMA achieved the highest patch-wise AUROC of 0.995, outperforming the ResNet50-based method (AUROC: 0.977) and the KBA (AUROC: 0.940).", "conclusion": "The foundation model-based approach (FMA) achieved the highest patch-wise AUROC of 0.995, outperforming the ResNet50-based method and the knowledge-based approach. A quality report scorecard was developed to quantify high-quality patches and visualize artifact distributions."}}
{"id": "2507.00938", "categories": ["cs.IR", "cs.AI", "cs.DB", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.00938", "abs": "https://arxiv.org/abs/2507.00938", "authors": ["Zihao Sun", "Meng Fang", "Ling Chen"], "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks", "comment": "10 pages, 9 figures, 4 tables", "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86 WebArXiv\uff0c\u8fd9\u662f\u4e00\u4e2a\u9759\u6001\u4e14\u65f6\u4e0d\u53d8\u7684\u57fa\u51c6\uff0c\u5305\u542b 275 \u4e2a\u57fa\u4e8e web \u7684\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u57fa\u4e8e arXiv \u5e73\u53f0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u52a8\u6001\u53cd\u5c04\u673a\u5236\uff0c\u5141\u8bb8 agent \u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u5730\u68c0\u7d22\u76f8\u5173\u7684\u8fc7\u53bb\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u4f7f\u5f97\u8bc4\u4f30 web agent \u5177\u6709\u6311\u6218\u6027\uff0c\u8fd9\u4e9b\u57fa\u51c6\u901a\u5e38\u4f9d\u8d56\u4e8e\u52a8\u6001\u5185\u5bb9\u6216\u8fc7\u4e8e\u7b80\u5316\u7684\u6a21\u62df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u52a8\u6001\u53cd\u5c04\u673a\u5236\uff0c\u5141\u8bb8 agent \u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u5730\u68c0\u7d22\u76f8\u5173\u7684\u8fc7\u53bb\u6b65\u9aa4\u3002", "result": "WebArXiv \u786e\u4fdd\u4e86\u53ef\u91cd\u590d\u548c\u53ef\u9760\u7684\u8bc4\u4f30\uff0c\u56e0\u4e3a\u5b83\u5c06\u4efb\u52a1\u951a\u5b9a\u5728\u5177\u6709\u786e\u5b9a\u6027 ground truth \u548c\u6807\u51c6\u5316\u52a8\u4f5c\u8f68\u8ff9\u7684\u56fa\u5b9a web \u5feb\u7167\u4e2d\u3002\u786e\u5b9a\u4e86\u4e00\u79cd\u5e38\u89c1\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5373 Rigid History Reflection\uff0c\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0cagent \u8fc7\u5ea6\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u4ea4\u4e92\u5386\u53f2\u3002", "conclusion": "\u901a\u8fc7\u5728 WebArXiv \u4e0a\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u4e0d\u540c Web agent \u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u53cd\u5c04\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.00489", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00489", "abs": "https://arxiv.org/abs/2507.00489", "authors": ["Pengyu Chen", "Zizheng Guo", "Jianwei Yang", "Dongjing Miao"], "title": "Towards Efficient Random-Order Enumeration for Join Queries", "comment": null, "summary": "In many data analysis pipelines, a basic and time-consuming process is to\nproduce join results and feed them into downstream tasks. Numerous enumeration\nalgorithms have been developed for this purpose. To be a statistically\nmeaningful representation of the whole join result, the result tuples are\nrequired to be enumerated in uniformly random order. However, existing studies\nlack an efficient random-order enumeration algorithm with a worst-case runtime\nguarantee for (cyclic) join queries. In this paper, we study the problem of\nenumerating the results of a join query in random order. We develop an\nefficient random-order enumeration algorithm for join queries with no large\nhidden constants in its complexity, achieving expected\n$O(\\frac{\\mathrm{AGM}(Q)}{|Res(Q)|}\\log^2|Q|)$ delay,\n$O(\\mathrm{AGM}(Q)\\log|Q|)$ total running time after $O(|Q|\\log|Q|)$-time index\nconstruction, where $|Q|$ is the size of input, $\\mathrm{AGM}(Q)$ is the AGM\nbound, and $|Res(Q)|$ is the size of the join result. We prove that our\nalgorithm is near-optimal in the worst case, under the combinatorial $k$-clique\nhypothesis. Our algorithm requires no query-specific preprocessing and can be\nflexibly adapted to many common database indexes with only minor modifications.\nWe also devise two non-trivial techniques to speed up the enumeration, and\nprovide an experimental study on our enumeration algorithm along with the\nspeed-up techniques. The experimental results show that our algorithm, enhanced\nwith the proposed techniques, significantly outperforms existing\nstate-of-the-art methods.", "AI": {"tldr": "This paper introduces a near-optimal random-order enumeration algorithm for join queries that is faster than existing methods.", "motivation": "Existing studies lack an efficient random-order enumeration algorithm with a worst-case runtime guarantee for (cyclic) join queries. The goal is to produce join results in uniformly random order for statistically meaningful representation.", "method": "The authors develop an efficient random-order enumeration algorithm for join queries with specific time and delay complexities. They also devise two non-trivial techniques to speed up the enumeration.", "result": "The proposed algorithm achieves specific time and delay complexities, is near-optimal, requires no query-specific preprocessing, and outperforms existing state-of-the-art methods in experiments.", "conclusion": "The paper presents an efficient random-order enumeration algorithm for join queries, which is near-optimal and outperforms existing methods."}}
{"id": "2507.00216", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00216", "abs": "https://arxiv.org/abs/2507.00216", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "title": "Towards Style Alignment in Cross-Cultural Translation", "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "AI": {"tldr": "LLM\u7ffb\u8bd1\u5728\u98ce\u683c\u4e0a\u5b58\u5728\u504f\u5dee\uff0cRASTA\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u98ce\u683c\u6982\u5ff5\u6765\u6539\u8fdbLLM\u7684\u7ffb\u8bd1\u98ce\u683c\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4f20\u8fbe\u6587\u5316\u4ea4\u6d41\u89c4\u8303\u3002", "motivation": "\u6587\u5316\u5dee\u5f02\u7ecf\u5e38\u5bfc\u81f4\u8bf4\u8bdd\u8005\u7684\u9884\u671f\u98ce\u683c\u4e0e\u542c\u8005\u7684\u7406\u89e3\u98ce\u683c\u4e0d\u4e00\u81f4\uff0c\u4f8b\u5982\uff0c\u793c\u8c8c\u7ecf\u5e38\u5728\u7ffb\u8bd1\u4e2d\u4e22\u5931\u3002", "method": "RASTA\uff08\u68c0\u7d22\u589e\u5f3a\u98ce\u683c\u5bf9\u9f50\uff09", "result": "RASTA\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u7f13\u89e3LLM\u5728\u7ffb\u8bd1\u98ce\u683c\u4e0a\u7684\u504f\u5dee\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u4f20\u8fbe\u6587\u5316\u4ea4\u6d41\u89c4\u8303\u3002", "conclusion": "LLMs\u5728\u7ffb\u8bd1\u98ce\u683c\u4e0a\u5b58\u5728\u504f\u5dee\uff0c\u503e\u5411\u4e8e\u4e2d\u7acb\uff0c\u5e76\u4e14\u5728\u975e\u897f\u65b9\u8bed\u8a00\u4e2d\u8868\u73b0\u66f4\u5dee\u3002\u4f7f\u7528RASTA\uff08\u68c0\u7d22\u589e\u5f3a\u98ce\u683c\u5bf9\u9f50\uff09\u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5b66\u4e60\u5230\u7684\u98ce\u683c\u6982\u5ff5\u6765\u9f13\u52b1LLM\u7ffb\u8bd1\u9002\u5f53\u5730\u4f20\u8fbe\u6587\u5316\u4ea4\u6d41\u89c4\u8303\u5e76\u5bf9\u9f50\u98ce\u683c\u3002"}}
{"id": "2507.00012", "categories": ["cs.LG", "cs.AI", "E.4"], "pdf": "https://arxiv.org/pdf/2507.00012", "abs": "https://arxiv.org/abs/2507.00012", "authors": ["Linfeng Ye", "Shayan Mohajer Hamidi", "En-hui Yang"], "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information", "comment": "27 pages, 6 figures, Transactions on Machine Learning Research", "summary": "A deep neural network (DNN) is said to be undistillable if, when used as a\nblack-box input-output teacher, it cannot be distilled through knowledge\ndistillation (KD). In this case, the distilled student (referred to as the\nknockoff student) does not outperform a student trained independently with\nlabel smoothing (LS student) in terms of prediction accuracy. To protect\nintellectual property of DNNs, it is desirable to build undistillable DNNs. To\nthis end, it is first observed that an undistillable DNN may have the trait\nthat each cluster of its output probability distributions in response to all\nsample instances with the same label should be highly concentrated to the\nextent that each cluster corresponding to each label should ideally collapse\ninto one probability distribution. Based on this observation and by measuring\nthe concentration of each cluster in terms of conditional mutual information\n(CMI), a new training method called CMI minimized (CMIM) method is proposed,\nwhich trains a DNN by jointly minimizing the conventional cross entropy (CE)\nloss and the CMI values of all temperature scaled clusters across the entire\ntemperature spectrum. The resulting CMIM model is shown, by extensive\nexperiments, to be undistillable by all tested KD methods existing in the\nliterature. That is, the knockoff students distilled by these KD methods from\nthe CMIM model underperform the respective LS students. In addition, the CMIM\nmodel is also shown to performs better than the model trained with the CE loss\nalone in terms of their own prediction accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5CMIM\uff0c\u53ef\u4ee5\u6784\u5efa\u65e0\u6cd5\u63d0\u70bc\u7684DNN\uff0c\u5e76\u4e14CMIM\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u4e5f\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528CE\u635f\u5931\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u4fdd\u62a4DNN\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u6784\u5efa\u65e0\u6cd5\u63d0\u70bc\u7684DNN\u662f\u53ef\u53d6\u7684\u3002\u4e3a\u6b64\uff0c\u9996\u5148\u89c2\u5bdf\u5230\u65e0\u6cd5\u63d0\u70bc\u7684DNN\u53ef\u80fd\u5177\u6709\u8fd9\u6837\u7684\u7279\u6027\uff1a\u5176\u54cd\u5e94\u4e8e\u5177\u6709\u76f8\u540c\u6807\u7b7e\u7684\u6240\u6709\u6837\u672c\u5b9e\u4f8b\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03\u7684\u6bcf\u4e2a\u96c6\u7fa4\u5e94\u8be5\u9ad8\u5ea6\u96c6\u4e2d\uff0c\u4ee5\u81f3\u4e8e\u5bf9\u5e94\u4e8e\u6bcf\u4e2a\u6807\u7b7e\u7684\u6bcf\u4e2a\u96c6\u7fa4\u5e94\u8be5\u7406\u60f3\u5730\u5d29\u6e83\u4e3a\u4e00\u4e2a\u6982\u7387\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u79f0\u4e3aCMI\u6700\u5c0f\u5316\uff08CMIM\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5171\u540c\u6700\u5c0f\u5316\u4f20\u7edf\u4ea4\u53c9\u71b5\uff08CE\uff09\u635f\u5931\u548c\u6574\u4e2a\u6e29\u5ea6\u8303\u56f4\u5185\u6240\u6709\u6e29\u5ea6\u7f29\u653e\u96c6\u7fa4\u7684CMI\u503c\u6765\u8bad\u7ec3DNN\u3002", "result": "CMIM\u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u6240\u6709\u6d4b\u8bd5\u7684KD\u65b9\u6cd5\u8fdb\u884c\u63d0\u70bc\uff0c\u5e76\u4e14CMIM\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528CE\u635f\u5931\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCMIM\u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u6587\u732e\u4e2d\u5df2\u6709\u7684\u6240\u6709KD\u65b9\u6cd5\u8fdb\u884c\u63d0\u70bc\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u7531\u8fd9\u4e9bKD\u65b9\u6cd5\u4eceCMIM\u6a21\u578b\u4e2d\u63d0\u70bc\u51fa\u7684\u4eff\u5192\u5b66\u751f\u4e0d\u5982\u5404\u81ea\u7684LS\u5b66\u751f\u3002\u6b64\u5916\uff0cCMIM\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u4e5f\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528CE\u635f\u5931\u8bad\u7ec3\u7684\u6a21\u578b\u3002"}}
{"id": "2507.00054", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00054", "abs": "https://arxiv.org/abs/2507.00054", "authors": ["Shreyansh Padarha"], "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "comment": "17 Pages, 7 figures", "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "AI": {"tldr": "AdvDistill\u901a\u8fc7\u5956\u52b1\u673a\u5236\u6539\u8fdb\u6570\u636e\u96c6\u84b8\u998f\uff0c\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u80fd\u591f\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u8f6c\u79fb\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4f46\u5b66\u751f\u6a21\u578b\u5e38\u5e38\u53ea\u662f\u590d\u5236\u6559\u5e08\u6a21\u578b\u7684\u5206\u5e03\u5185\u54cd\u5e94\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faAdvDistill\uff0c\u4e00\u4e2a\u5956\u52b1\u5f15\u5bfc\u7684\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\u3002\u5229\u7528\u6765\u81ea\u6559\u5e08\u6a21\u578b\u7684\u591a\u4e2a\u751f\u6210\uff08\u54cd\u5e94\uff09\uff0c\u5e76\u57fa\u4e8e\u89c4\u5219\u9a8c\u8bc1\u5668\u5206\u914d\u5956\u52b1\uff0c\u8fd9\u4e9b\u5956\u52b1\u4f5c\u4e3a\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u65f6\u7684\u6743\u91cd\u3002", "result": "AdvDistill\u5728\u6570\u5b66\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u6570\u636e\u96c6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5956\u52b1\u673a\u5236\u7684\u6709\u6548\u6027\u548c\u76ca\u5904\u3002", "conclusion": "AdvDistill\u901a\u8fc7\u5728\u6570\u636e\u96c6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u6a21\u578b\u5728\u6570\u5b66\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.00045", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00045", "abs": "https://arxiv.org/abs/2507.00045", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "comment": null, "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "AI": {"tldr": "The paper introduces a new challenging task, CaughtCheating, to evaluate the detective perception and reasoning capabilities of MLLMs, finding that their performance drops significantly in this task.", "motivation": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives?", "method": "We investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task.", "result": "find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating.", "conclusion": "CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities."}}
{"id": "2507.00839", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.00839", "abs": "https://arxiv.org/abs/2507.00839", "authors": ["Chiyu Hao", "Jixian Su", "Shixuan Sun", "Hao Zhang", "Sen Gao", "Jianwen Zhao", "Chenyi Zhang", "Jieru Zhao", "Chen Chen", "Minyi Guo"], "title": "RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries", "comment": "17 pages, 18 figures", "summary": "Dynamic graph storage systems are essential for real-time applications such\nas social networks and recommendation, where graph data continuously evolves.\nHowever, they face significant challenges in efficiently handling concurrent\nread and write operations. We find that existing methods suffer from write\nqueries interfering with read efficiency, substantial time and space overhead\ndue to per-edge versioning, and an inability to balance performance, such as\nslow searches under concurrent workloads. To address these issues, we propose\nRapidStore, a holistic approach for efficient in-memory dynamic graph storage\ndesigned for read-intensive workloads. Our key idea is to exploit the\ncharacteristics of graph queries through a decoupled system design that\nseparates the management of read and write queries and decouples version data\nfrom graph data. Particularly, we design an efficient dynamic graph store to\ncooperate with the graph concurrency control mechanism. Experimental results\ndemonstrate that RapidStore enables fast and scalable concurrent graph queries,\neffectively balancing the performance of inserts, searches, and scans, and\nsignificantly improving efficiency in dynamic graph storage systems.", "AI": {"tldr": "RapidStore \u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u5185\u5b58\u52a8\u6001\u56fe\u5b58\u50a8\u7684\u6574\u4f53\u65b9\u6cd5\uff0c\u4e13\u4e3a\u8bfb\u53d6\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u800c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u79bb\u8bfb\u5199\u64cd\u4f5c\u5e76\u89e3\u8026\u7248\u672c\u63a7\u5236\u6765\u4f18\u5316\u5e76\u53d1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u52a8\u6001\u56fe\u5b58\u50a8\u7cfb\u7edf\u5728\u6709\u6548\u5904\u7406\u5e76\u53d1\u8bfb\u5199\u64cd\u4f5c\u65f6\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u5199\u67e5\u8be2\u4f1a\u5e72\u6270\u8bfb\u53d6\u6548\u7387\uff0c\u5e76\u4e14\u7531\u4e8e\u6bcf\u4e2a\u8fb9\u7684\u7248\u672c\u63a7\u5236\u800c\u4ea7\u751f\u5927\u91cf\u65f6\u95f4\u548c\u7a7a\u95f4\u5f00\u9500\uff0c\u5e76\u4e14\u65e0\u6cd5\u5e73\u8861\u6027\u80fd\uff0c\u4f8b\u5982\u5e76\u53d1\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u641c\u7d22\u901f\u5ea6\u8f83\u6162\u3002", "method": "RapidStore \u91c7\u7528\u4e86\u4e00\u79cd\u6574\u4f53\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u8bfb\u5199\u67e5\u8be2\u7684\u7ba1\u7406\u548c\u5c06\u7248\u672c\u6570\u636e\u4e0e\u56fe\u6570\u636e\u89e3\u8026\u7684\u89e3\u8026\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5229\u7528\u4e86\u56fe\u67e5\u8be2\u7684\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u52a8\u6001\u56fe\u5b58\u50a8\uff0c\u4ee5\u914d\u5408\u56fe\u5e76\u53d1\u63a7\u5236\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRapidStore \u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u5e76\u53d1\u56fe\u67e5\u8be2\uff0c\u6709\u6548\u5e73\u8861\u63d2\u5165\u3001\u641c\u7d22\u548c\u626b\u63cf\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u52a8\u6001\u56fe\u5b58\u50a8\u7cfb\u7edf\u7684\u6548\u7387\u3002", "conclusion": "RapidStore \u901a\u8fc7\u5206\u79bb\u8bfb\u5199\u67e5\u8be2\u7ba1\u7406\u548c\u89e3\u8026\u7248\u672c\u6570\u636e\u4e0e\u56fe\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u5e76\u53d1\u56fe\u67e5\u8be2\uff0c\u6709\u6548\u5e73\u8861\u4e86\u63d2\u5165\u3001\u641c\u7d22\u548c\u626b\u63cf\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u6001\u56fe\u5b58\u50a8\u7cfb\u7edf\u7684\u6548\u7387\u3002"}}
{"id": "2507.00239", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00239", "abs": "https://arxiv.org/abs/2507.00239", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "AI": {"tldr": "\u8d8a\u72f1\u6a21\u578b\u62d2\u7edd\u7684\u4fe1\u606f\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u89e3\u7801\uff0cinstruction-tuning\u53ea\u662f\u6291\u5236\u4e86\u6709\u5bb3\u4fe1\u606f\u7684\u76f4\u63a5\u8868\u8fbe", "motivation": "\u7814\u7a76\u901a\u8fc7\u8d8a\u72f1\u63d0\u793a\u8bbf\u95ee\u7684\u4fe1\u606f\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53ef\u89e3\u7801", "method": "\u4f7f\u7528\u5728\u7ebf\u6027\u63a2\u9488\u89e3\u7801LM\u9690\u85cf\u72b6\u6001\u8bbf\u95ee\u7684\u4fe1\u606f", "result": "\u5927\u91cf\u6700\u521d\u62d2\u7edd\u7684\u4fe1\u606f\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u89e3\u7801\uff1b\u63a2\u9488\u9884\u6d4b\u503c\u4e0eLM\u751f\u6210\u7684\u6210\u5bf9\u6bd4\u8f83\u76f8\u5173", "conclusion": "instruction-tuning\u5e76\u4e0d\u80fd\u5b8c\u5168\u6d88\u9664\u6216\u91cd\u65b0\u5b9a\u4f4d\u8868\u5f81\u7a7a\u95f4\u4e2d\u7684\u6709\u5bb3\u4fe1\u606f\uff0c\u800c\u53ea\u662f\u6291\u5236\u4e86\u5176\u76f4\u63a5\u8868\u8fbe\uff0c\u4f7f\u5176\u5728\u7ebf\u6027\u53ef\u8bbf\u95ee\u4e14\u95f4\u63a5\u5f71\u54cd\u4e0b\u6e38\u884c\u4e3a\u3002"}}
{"id": "2507.00013", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.00013", "abs": "https://arxiv.org/abs/2507.00013", "authors": ["Hyunwoo Seo", "Chiehyeon Lim"], "title": "ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting", "comment": "Accepted by KDD 2025 research track", "summary": "Forecasting complex time series is an important yet challenging problem that\ninvolves various industrial applications. Recently, masked time-series modeling\nhas been proposed to effectively model temporal dependencies for forecasting by\nreconstructing masked segments from unmasked ones. However, since the semantic\ninformation in time series is involved in intricate temporal variations\ngenerated by multiple time series components, simply masking a raw time series\nignores the inherent semantic structure, which may cause MTM to learn spurious\ntemporal patterns present in the raw data. To capture distinct temporal\nsemantics, we show that masked modeling techniques should address entangled\npatterns through a decomposition approach. Specifically, we propose ST-MTM, a\nmasked time-series modeling framework with seasonal-trend decomposition, which\nincludes a novel masking method for the seasonal-trend components that\nincorporates different temporal variations from each component. ST-MTM uses a\nperiod masking strategy for seasonal components to produce multiple masked\nseasonal series based on inherent multi-periodicity and a sub-series masking\nstrategy for trend components to mask temporal regions that share similar\nvariations. The proposed masking method presents an effective pre-training task\nfor learning intricate temporal variations and dependencies. Additionally,\nST-MTM introduces a contrastive learning task to support masked modeling by\nenhancing contextual consistency among multiple masked seasonal\nrepresentations. Experimental results show that our proposed ST-MTM achieves\nconsistently superior forecasting performance compared to existing masked\nmodeling, contrastive learning, and supervised forecasting methods.", "AI": {"tldr": "ST-MTM is proposed to address the problem of forecasting complex time series. It uses seasonal-trend decomposition and a novel masking method to capture distinct temporal semantics. It achieves better performance than existing methods.", "motivation": "Simply masking a raw time series ignores the inherent semantic structure, which may cause MTM to learn spurious temporal patterns present in the raw data. To capture distinct temporal semantics, we show that masked modeling techniques should address entangled patterns through a decomposition approach.", "method": "ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition, which includes a novel masking method for the seasonal-trend components that incorporates different temporal variations from each component. ST-MTM uses a period masking strategy for seasonal components and a sub-series masking strategy for trend components. Additionally, ST-MTM introduces a contrastive learning task to support masked modeling by enhancing contextual consistency among multiple masked seasonal representations.", "result": "achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.", "conclusion": "ST-MTM achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods."}}
{"id": "2507.00079", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00079", "abs": "https://arxiv.org/abs/2507.00079", "authors": ["Ethan Smyth", "Alessandro Suglia"], "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "AI": {"tldr": "VoyagerVision, a multi-modal model, uses visual input to build structures in Minecraft, outperforming Voyager in open-ended task completion.", "motivation": "The paper explores how visual inputs enhance a model's ability to interpret spatial environments, increasing the number of tasks it can perform and extending its open-ended potential in AGI.", "method": "VoyagerVision, a multi-modal model building on Voyager, uses screenshots for visual feedback to construct structures in Minecraft.", "result": "VoyagerVision created an average of 2.75 unique structures within 50 iterations, and achieved 50% success in building unit tests in flat worlds.", "conclusion": "VoyagerVision, a multi-modal model using screenshots as visual feedback, can create structures in Minecraft, averaging 2.75 unique structures in 50 iterations, outperforming Voyager. It succeeds in 50% of building unit tests in flat worlds, with failures mainly in complex structures."}}
{"id": "2507.00046", "categories": ["cs.CV", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.00046", "abs": "https://arxiv.org/abs/2507.00046", "authors": ["Akshansh Mishra", "Eyob Mesele Sefene", "Shivraman Thapliyal"], "title": "Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process", "comment": "7 pages, 4 figures", "summary": "This work proposes an evolutionary computing-based image segmentation\napproach for analyzing soundness in Additive Friction Stir Deposition (AFSD)\nprocesses. Particle Swarm Optimization (PSO) was employed to determine optimal\nsegmentation thresholds for detecting defects and features in multilayer AFSD\nbuilds. The methodology integrates gradient magnitude analysis with distance\ntransforms to create novel attention-weighted visualizations that highlight\ncritical interface regions. Five AFSD samples processed under different\nconditions were analyzed using multiple visualization techniques i.e.\nself-attention maps, and multi-channel visualization. These complementary\napproaches reveal subtle material transition zones and potential defect regions\nwhich were not readily observable through conventional imaging. The PSO\nalgorithm automatically identified optimal threshold values (ranging from\n156-173) for each sample, enabling precise segmentation of material interfaces.\nThe multi-channel visualization technique effectively combines boundary\ninformation (red channel), spatial relationships (green channel), and material\ndensity data (blue channel) into cohesive representations that quantify\ninterface quality. The results demonstrate that attention-based analysis\nsuccessfully identifies regions of incomplete bonding and inhomogeneities in\nAFSD joints, providing quantitative metrics for process optimization and\nquality assessment of additively manufactured components.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u589e\u6750\u6469\u64e6\u6405\u62cc\u6c89\u79ef (AFSD) \u8fc7\u7a0b\u4e2d\u7684\u5b8c\u6574\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u589e\u6750\u6469\u64e6\u6405\u62cc\u6c89\u79ef (AFSD) \u8fc7\u7a0b\u4e2d\u7684\u5b8c\u6574\u6027\u3002", "method": "\u91c7\u7528\u7c92\u5b50\u7fa4\u4f18\u5316 (PSO) \u7b97\u6cd5\u6765\u786e\u5b9a\u6700\u4f73\u5206\u5272\u9608\u503c\uff0c\u4ee5\u68c0\u6d4b\u591a\u5c42 AFSD \u6784\u5efa\u4e2d\u7684\u7f3a\u9677\u548c\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5c06\u68af\u5ea6\u5e45\u5ea6\u5206\u6790\u4e0e\u8ddd\u79bb\u53d8\u6362\u76f8\u7ed3\u5408\uff0c\u4ee5\u521b\u5efa\u7a81\u51fa\u5173\u952e\u754c\u9762\u533a\u57df\u7684\u65b0\u578b\u6ce8\u610f\u529b\u52a0\u6743\u53ef\u89c6\u5316\u3002", "result": "PSO \u7b97\u6cd5\u81ea\u52a8\u8bc6\u522b\u4e86\u6bcf\u4e2a\u6837\u672c\u7684\u6700\u4f73\u9608\u503c\uff08\u8303\u56f4\u4e3a 156-173\uff09\uff0c\u4ece\u800c\u80fd\u591f\u7cbe\u786e\u5206\u5272\u6750\u6599\u754c\u9762\u3002\u591a\u901a\u9053\u53ef\u89c6\u5316\u6280\u672f\u6709\u6548\u5730\u5c06\u8fb9\u754c\u4fe1\u606f\uff08\u7ea2\u8272\u901a\u9053\uff09\u3001\u7a7a\u95f4\u5173\u7cfb\uff08\u7eff\u8272\u901a\u9053\uff09\u548c\u6750\u6599\u5bc6\u5ea6\u6570\u636e\uff08\u84dd\u8272\u901a\u9053\uff09\u7ec4\u5408\u6210\u91cf\u5316\u754c\u9762\u8d28\u91cf\u7684\u8fde\u8d2f\u8868\u793a\u3002", "conclusion": "\u6ce8\u610f\u529b\u5206\u6790\u6210\u529f\u8bc6\u522b\u4e86 AFSD \u63a5\u5934\u4e2d\u4e0d\u5b8c\u5168\u7ed3\u5408\u548c\u4e0d\u5747\u5300\u7684\u533a\u57df\uff0c\u4e3a\u589e\u6750\u5236\u9020\u90e8\u4ef6\u7684\u5de5\u827a\u4f18\u5316\u548c\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9a\u91cf\u6307\u6807\u3002"}}
{"id": "2507.00330", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00330", "abs": "https://arxiv.org/abs/2507.00330", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "AI": {"tldr": "COLDSELECT \u662f\u4e00\u79cd\u8054\u5408 verbalizer \u548c\u5b9e\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u53ef\u5bf9\u6570\u636e\u591a\u6837\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5bf9\u6a21\u677f\u3001verbalizer \u548c\u5c11\u6837\u672c\u5b9e\u4f8b\u9009\u62e9\u654f\u611f\uff0c\u5c24\u5176\u662f\u5728\u6ca1\u6709\u6807\u8bb0\u6570\u636e\u7684\u51b7\u542f\u52a8\u8bbe\u7f6e\u4e2d\u3002\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u5b9e\u4f8b\u548c verbalizer \u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "COLDSELECT\uff0c\u4e00\u79cd\u8054\u5408 verbalizer \u548c\u5b9e\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u53ef\u5bf9\u6570\u636e\u591a\u6837\u6027\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCOLDSELECT \u5728\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u548c\u589e\u5f3a\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e verbalizer \u548c\u5c11\u6837\u672c\u5b9e\u4f8b\u9009\u62e9\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "COLDSELECT\u5728\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u548c\u589e\u5f3a\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2507.00244", "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "pdf": "https://arxiv.org/pdf/2507.00244", "abs": "https://arxiv.org/abs/2507.00244", "authors": ["Isabella Senturia", "Matilde Marcolli"], "title": "The Algebraic Structure of Morphosyntax", "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "AI": {"tldr": "We present a mathematical model of the morphology-syntax interface.", "motivation": "Within the context of the mathematical formulation of Merge and the Strong Minimalist Thesis,", "method": "We present a mathematical model of the morphology-syntax interface. In this setting, morphology has compositional properties responsible for word formation, organized into a magma of morphological trees. However, unlike syntax, we do not have movement within morphology. A coproduct decomposition exists, but it requires extending the set of morphological trees beyond those which are generated solely by the magma, to a larger set of possible morphological inputs to syntactic trees. These participate in the formation of morphosyntactic trees as an algebra over an operad, and a correspondence between algebras over an operad.", "result": "The process of structure formation for morphosyntactic trees can then be described in terms of this operadic correspondence that pairs syntactic and morphological data and the morphology coproduct.", "conclusion": "We reinterpret in this setting certain operations of Distributed Morphology as transformation that allow for flexibility in moving the boundary between syntax and morphology within the morphosyntactic objects."}}
{"id": "2507.00014", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00014", "abs": "https://arxiv.org/abs/2507.00014", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering.", "AI": {"tldr": "Introduces SWE-Bench-CL, a continual learning benchmark for evaluating AI agents in software engineering, featuring chronologically ordered GitHub issues, an evaluation framework, and specialized metrics.", "motivation": "Existing LLMs perform well on static code-generation benchmarks, but real-world software development is a continuous stream of evolving issues, fixes, and feature requests. The paper aims to address the gap in evaluating an agent's ability to accumulate experience, transfer knowledge, and resist catastrophic forgetting in such dynamic environments.", "method": "The paper introduces SWE-Bench-CL, which organizes GitHub issues into chronologically ordered sequences. It uses a LangGraph-based evaluation framework with a FAISS-backed semantic memory module. The paper also introduces specialized continual learning metrics, including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and composite continual learning scores.", "result": "The paper provides a preliminary analysis of inter-task structural similarity and contextual sensitivity. It compares memory-enabled and memory-disabled agents across diverse Python repositories, and shares the code and data publicly.", "conclusion": "The paper introduces SWE-Bench-CL, a continual learning benchmark for evaluating the ability of AI agents to adapt and learn in software engineering scenarios. The benchmark is built on chronologically ordered GitHub issues and includes an evaluation framework, specialized continual learning metrics, and a rigorous experimental protocol. The results of comparing memory-enabled and memory-disabled agents are shared to provide a reproducible platform."}}
{"id": "2507.00092", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00092", "abs": "https://arxiv.org/abs/2507.00092", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "comment": "19 pages, 2 figures, 9 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "AI": {"tldr": "SAGE-nano \u91c7\u7528\u9006\u5411\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u89e3\u91ca\u6027\u548c\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u900f\u660e AI \u7cfb\u7edf\u505a\u51fa\u4e86\u8d21\u732e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u4ecd\u7136\u6709\u4e9b\u9ed1\u76d2\u3002", "method": "SAGE-nano \u91c7\u7528\u4e86\u4e00\u79cd\u5143\u8ba4\u77e5\u7ed3\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u8fc7\u7a0b\u53cd\u601d\uff0c\u4ee5\u8bc6\u522b\u4e3b\u8981\u7684\u51b3\u7b56\u70b9\u5e76\u751f\u6210\u63a8\u7406\u9009\u62e9\u7684\u89e3\u91ca\u3002", "result": "SAGE-nano \u5728 AQUA-RAT \u4e0a\u7684\u63a8\u7406\u51c6\u786e\u7387\u4e3a 74.6%\uff0c\u89e3\u91ca\u8d28\u91cf\u7684\u4eba\u7c7b\u504f\u597d\u5f97\u5206\u4e3a 92.1%\uff0c\u5e76\u4e14\u6027\u80fd\u51e0\u4e4e\u4e0e Claude-3.5 Sonnet \u6216 GPT-4o \u7b49\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "SAGE-nano \u901a\u8fc7\u9006\u5411\u63a8\u7406\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e76\u4e14\u9006\u5411\u63a8\u7406\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u900f\u660e AI \u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.00049", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00049", "abs": "https://arxiv.org/abs/2507.00049", "authors": ["Feiyang Kang", "Nadine Chang", "Maying Shen", "Marc T. Law", "Rafid Mahmood", "Ruoxi Jia", "Jose M. Alvarez"], "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training", "comment": "Preprint", "summary": "The computational burden and inherent redundancy of large-scale datasets\nchallenge the training of contemporary machine learning models. Data pruning\noffers a solution by selecting smaller, informative subsets, yet existing\nmethods struggle: density-based approaches can be task-agnostic, while\nmodel-based techniques may introduce redundancy or prove computationally\nprohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid\nframework that synergistically integrates density-based pruning with\nmodel-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions\ndata and applies an initial density-based pruning. It then employs a proxy\nmodel to evaluate the impact of this initial pruning within each cluster by\ncomparing losses on kept versus pruned samples. This task-aware signal\nadaptively adjusts cluster-specific pruning thresholds, enabling more\naggressive pruning in redundant clusters while preserving critical data in\ninformative ones. Extensive experiments on large-scale object detection\nbenchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster\nR-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms\nprominent baselines, substantially reduces performance degradation (e.g., over\n54% versus random sampling on Waymo), and achieves near-original model\nperformance while pruning 20% of data, highlighting its efficacy in enhancing\ndata efficiency for large-scale model training. Code is open-sourced.", "AI": {"tldr": "AdaDeDup is a hybrid data pruning framework that combines density-based pruning with model-informed feedback to improve data efficiency for large-scale model training.", "motivation": "The computational burden and inherent redundancy of large-scale datasets challenge the training of contemporary machine learning models. Data pruning offers a solution, yet existing methods struggle: density-based approaches can be task-agnostic, while model-based techniques may introduce redundancy or prove computationally prohibitive.", "method": "Adaptive De-Duplication (AdaDeDup), a novel hybrid framework that synergistically integrates density-based pruning with model-informed feedback in a cluster-adaptive manner.", "result": "It significantly outperforms prominent baselines, substantially reduces performance degradation (e.g., over 54% versus random sampling on Waymo), and achieves near-original model performance while pruning 20% of data, highlighting its efficacy in enhancing data efficiency for large-scale model training.", "conclusion": "AdaDeDup significantly outperforms prominent baselines, substantially reduces performance degradation, and achieves near-original model performance while pruning 20% of data."}}
{"id": "2507.00518", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.00518", "abs": "https://arxiv.org/abs/2507.00518", "authors": ["Walid Bendada", "Guillaume Salha-Galvan", "Romain Hennequin", "Th\u00e9o Bontempelli", "Thomas Bouab\u00e7a", "Tristan Cazenave"], "title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling", "comment": "42nd International Conference on Machine Learning (ICML 2025)", "summary": "This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable\nmethod for exploring large action sets in reinforcement learning problems where\nhyperspherical embedding vectors represent these actions. vMF-exp involves\ninitially sampling a state embedding representation using a von Mises-Fisher\ndistribution, then exploring this representation's nearest neighbors, which\nscales to virtually unlimited numbers of candidate actions. We show that, under\ntheoretical assumptions, vMF-exp asymptotically maintains the same probability\nof exploring each action as Boltzmann Exploration (B-exp), a popular\nalternative that, nonetheless, suffers from scalability issues as it requires\ncomputing softmax values for each action. Consequently, vMF-exp serves as a\nscalable alternative to B-exp for exploring large action sets with\nhyperspherical embeddings. Experiments on simulated data, real-world public\ndata, and the successful large-scale deployment of vMF-exp on the recommender\nsystem of a global music streaming service empirically validate the key\nproperties of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u63a2\u7d22\u65b9\u6cd5vMF-exp\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u5927\u578b\u52a8\u4f5c\u96c6\u7684\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4e2d\u5927\u578b\u52a8\u4f5c\u96c6\u7684\u95ee\u9898\uff0c\u4f20\u7edfBoltzmann\u63a2\u7d22\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3avon Mises-Fisher exploration (vMF-exp) \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7 von Mises-Fisher \u5206\u5e03\u91c7\u6837\u72b6\u6001\u5d4c\u5165\u8868\u793a\uff0c\u7136\u540e\u63a2\u7d22\u8be5\u8868\u793a\u7684\u6700\u8fd1\u90bb\u3002", "result": "vMF-exp\u5728\u7406\u8bba\u4e0a\u4e0eBoltzmann\u63a2\u7d22\u5177\u6709\u76f8\u540c\u7684\u52a8\u4f5c\u63a2\u7d22\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u6570\u636e\u3001\u771f\u5b9e\u4e16\u754c\u516c\u5171\u6570\u636e\u4ee5\u53ca\u5168\u7403\u97f3\u4e50\u6d41\u5a92\u4f53\u670d\u52a1\u7684\u63a8\u8350\u7cfb\u7edf\u4e0a\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "vMF-exp\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u63a2\u7d22\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5177\u6709\u8d85\u7403\u9762\u5d4c\u5165\u5411\u91cf\u7684\u5927\u578b\u52a8\u4f5c\u96c6\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u6210\u529f\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5173\u952e\u7279\u6027\u3002"}}
{"id": "2507.00246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00246", "abs": "https://arxiv.org/abs/2507.00246", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "AI": {"tldr": "Non-English languages can be more token-efficient for reasoning while preserving accuracy, suggesting genuine shifts in reasoning behavior.", "motivation": "Most research focuses solely on English, even though many models are pretrained on multilingual data. This work investigates: Is English the most token-efficient language for reasoning?", "method": "Evaluate three open-source RLMs: DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven typologically diverse languages.", "result": "Reasoning in non-English languages reduces token usage, and also preserves accuracy. These gains persist even after translating the reasoning traces into English, suggesting genuine shifts in reasoning behavior rather than surface-level linguistic effects.", "conclusion": "Reasoning in non-English languages reduces token usage and preserves accuracy. These gains persist even after translating the reasoning traces into English, suggesting genuine shifts in reasoning behavior rather than surface-level linguistic effects. The extent of improvement depends on the models multilingual strength."}}
{"id": "2507.00015", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.00015", "abs": "https://arxiv.org/abs/2507.00015", "authors": ["Lu Zhang", "Sangarapillai Lambotharan", "Gan Zheng", "Guisheng Liao", "Xuekang Liu", "Fabio Roli", "Carsten Maple"], "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications", "comment": null, "summary": "The remarkable success of transformers across various fields such as natural\nlanguage processing and computer vision has paved the way for their\napplications in automatic modulation classification, a critical component in\nthe communication systems of Internet of Things (IoT) devices. However, it has\nbeen observed that transformer-based classification of radio signals is\nsusceptible to subtle yet sophisticated adversarial attacks. To address this\nissue, we have developed a defensive strategy for transformer-based modulation\nclassification systems to counter such adversarial attacks. In this paper, we\npropose a novel vision transformer (ViT) architecture by introducing a new\nconcept known as adversarial indicator (AdvI) token to detect adversarial\nattacks. To the best of our knowledge, this is the first work to propose an\nAdvI token in ViT to defend against adversarial attacks. Integrating an\nadversarial training method with a detection mechanism using AdvI token, we\ncombine a training time defense and running time defense in a unified neural\nnetwork model, which reduces architectural complexity of the system compared to\ndetecting adversarial perturbations using separate models. We investigate into\nthe operational principles of our method by examining the attention mechanism.\nWe show the proposed AdvI token acts as a crucial element within the ViT,\ninfluencing attention weights and thereby highlighting regions or features in\nthe input data that are potentially suspicious or anomalous. Through\nexperimental results, we demonstrate that our approach surpasses several\ncompetitive methods in handling white-box attack scenarios, including those\nutilizing the fast gradient method, projected gradient descent attacks and\nbasic iterative method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9Transformer (ViT)\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u79f0\u4e3a\u5bf9\u6297\u6307\u6807(AdvI)\u4ee4\u724c\u7684\u65b0\u6982\u5ff5\u6765\u68c0\u6d4b\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u4ee5\u89e3\u51b3\u57fa\u4e8etransformer\u7684\u65e0\u7ebf\u7535\u4fe1\u53f7\u5206\u7c7b\u5bb9\u6613\u53d7\u5230\u7ec6\u5fae\u4f46\u590d\u6742\u7684\u5bf9\u6297\u6027\u653b\u51fb\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8etransformer\u7684\u65e0\u7ebf\u7535\u4fe1\u53f7\u5206\u7c7b\u5bb9\u6613\u53d7\u5230\u7ec6\u5fae\u4f46\u590d\u6742\u7684\u5bf9\u6297\u6027\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9Transformer (ViT)\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u79f0\u4e3a\u5bf9\u6297\u6307\u6807(AdvI)\u4ee4\u724c\u7684\u65b0\u6982\u5ff5\u6765\u68c0\u6d4b\u5bf9\u6297\u6027\u653b\u51fb\u3002", "result": "\u63d0\u51fa\u7684AdvI\u4ee4\u724c\u4f5c\u4e3aViT\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u5143\u7d20\uff0c\u5f71\u54cd\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u4ece\u800c\u7a81\u51fa\u8f93\u5165\u6570\u636e\u4e2d\u6f5c\u5728\u7684\u53ef\u7591\u6216\u5f02\u5e38\u533a\u57df\u6216\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u767d\u76d2\u653b\u51fb\u573a\u666f\uff08\u5305\u62ec\u5feb\u901f\u68af\u5ea6\u6cd5\u3001\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u653b\u51fb\u548c\u57fa\u672c\u8fed\u4ee3\u6cd5\uff09\u65b9\u9762\u4f18\u4e8e\u51e0\u79cd\u7ade\u4e89\u65b9\u6cd5\u3002"}}
{"id": "2507.00180", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00180", "abs": "https://arxiv.org/abs/2507.00180", "authors": ["Vidhi Rathore"], "title": "BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis", "comment": null, "summary": "Modernizing legacy software systems is a critical but challenging task, often\nhampered by a lack of documentation and understanding of the original system's\nintricate decision logic. Traditional approaches like behavioral cloning merely\nreplicate input-output behavior without capturing the underlying intent. This\npaper proposes a novel pipeline to automatically extract interpretable decision\nlogic from legacy systems treated as black boxes. The approach uses a\nReinforcement Learning (RL) agent to explore the input space and identify\ncritical decision boundaries by rewarding actions that cause meaningful changes\nin the system's output. These counterfactual state transitions, where the\noutput changes, are collected and clustered using K-Means. Decision trees are\nthen trained on these clusters to extract human-readable rules that approximate\nthe system's decision logic near the identified boundaries. I demonstrated the\npipeline's effectiveness on three dummy legacy systems with varying complexity,\nincluding threshold-based, combined-conditional, and non-linear range logic.\nResults show that the RL agent successfully focuses exploration on relevant\nboundary regions, and the extracted rules accurately reflect the core logic of\nthe underlying dummy systems, providing a promising foundation for generating\nspecifications and test cases during legacy migration.", "AI": {"tldr": "This paper proposes a novel pipeline to automatically extract interpretable decision logic from legacy systems treated as black boxes.", "motivation": "Modernizing legacy software systems is a critical but challenging task, often hampered by a lack of documentation and understanding of the original system's intricate decision logic. Traditional approaches like behavioral cloning merely replicate input-output behavior without capturing the underlying intent.", "method": "The approach uses a Reinforcement Learning (RL) agent to explore the input space and identify critical decision boundaries by rewarding actions that cause meaningful changes in the system's output. These counterfactual state transitions, where the output changes, are collected and clustered using K-Means. Decision trees are then trained on these clusters to extract human-readable rules that approximate the system's decision logic near the identified boundaries.", "result": "I demonstrated the pipeline's effectiveness on three dummy legacy systems with varying complexity, including threshold-based, combined-conditional, and non-linear range logic.", "conclusion": "The RL agent successfully focuses exploration on relevant boundary regions, and the extracted rules accurately reflect the core logic of the underlying dummy systems, providing a promising foundation for generating specifications and test cases during legacy migration."}}
{"id": "2507.00052", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00052", "abs": "https://arxiv.org/abs/2507.00052", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "title": "VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) hold great promise for streamlining\nlabour-intensive medical imaging workflows, yet systematic security evaluations\nin clinical settings remain scarce. We introduce VSF--Med, an end-to-end\nvulnerability-scoring framework for medical VLMs that unites three novel\ncomponents: (i) a rich library of sophisticated text-prompt attack templates\ntargeting emerging threat vectors; (ii) imperceptible visual perturbations\ncalibrated by structural similarity (SSIM) thresholds to preserve clinical\nrealism; and (iii) an eight-dimensional rubric evaluated by two independent\njudge LLMs, whose raw scores are consolidated via z-score normalization to\nyield a 0--32 composite risk metric. Built entirely on publicly available\ndatasets and accompanied by open-source code, VSF--Med synthesizes over 30,000\nadversarial variants from 5,000 radiology images and enables reproducible\nbenchmarking of any medical VLM with a single command. Our consolidated\nanalysis reports mean z-score shifts of $0.90\\sigma$ for\npersistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness,\nand $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs.\nNotably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase\nof $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases\nof $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection\nattacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86VSF-Med\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u533b\u5b66VLM\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u53d1\u73b0Llama-3.2-11B-Vision-Instruct\u548cGPT-4o\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u5de5\u4f5c\u6d41\u7a0b\u52b3\u52a8\u5bc6\u96c6\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7b80\u5316\u8fd9\u4e9b\u6d41\u7a0b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e34\u5e8a\u73af\u5883\u4e2d\u5bf9VLM\u7684\u7cfb\u7edf\u5b89\u5168\u8bc4\u4f30\u4ecd\u7136\u5f88\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVSF-Med\u7684\u7aef\u5230\u7aef\u6f0f\u6d1e\u8bc4\u5206\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5b89\u5168\u6027\u3002\u8be5\u6846\u67b6\u5305\u62ec\uff1a(i)\u4e00\u4e2a\u5305\u542b\u590d\u6742\u6587\u672c\u63d0\u793a\u653b\u51fb\u6a21\u677f\u7684\u4e30\u5bcc\u5e93\uff0c\u9488\u5bf9\u65b0\u5174\u5a01\u80c1\u5411\u91cf\uff1b(ii)\u901a\u8fc7\u7ed3\u6784\u76f8\u4f3c\u6027\uff08SSIM\uff09\u9608\u503c\u6821\u51c6\u7684\u3001\u4e0d\u53ef\u5bdf\u89c9\u7684\u89c6\u89c9\u6270\u52a8\uff0c\u4ee5\u4fdd\u6301\u4e34\u5e8a\u771f\u5b9e\u611f\uff1b(iii)\u4e00\u4e2a\u516b\u7ef4\u8bc4\u4f30\u6807\u51c6\uff0c\u7531\u4e24\u4e2a\u72ec\u7acb\u7684LLM\u8bc4\u4f30\u8005\u8bc4\u4f30\uff0c\u5176\u539f\u59cb\u5206\u6570\u901a\u8fc7z-score\u6807\u51c6\u5316\u8fdb\u884c\u6574\u5408\uff0c\u4ee5\u4ea7\u751f0-32\u7684\u7efc\u5408\u98ce\u9669\u6307\u6807\u3002", "result": "\u7efc\u5408\u5206\u6790\u62a5\u544a\u663e\u793a\uff0c\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684VLM\uff0c\u653b\u51fb\u6548\u679c\u6301\u4e45\u6027\u7684\u5e73\u5747z-score\u53d8\u5316\u4e3a0.90\u03c3\uff0c\u63d0\u793a\u6ce8\u5165\u6709\u6548\u6027\u7684\u5e73\u5747z-score\u53d8\u5316\u4e3a0.74\u03c3\uff0c\u5b89\u5168\u7ed5\u8fc7\u6210\u529f\u7387\u7684\u5e73\u5747z-score\u53d8\u5316\u4e3a0.63\u03c3\u3002", "conclusion": "VSF-Med\u53d1\u73b0\uff0cLlama-3.2-11B-Vision-Instruct\u5728\u653b\u51fb\u6548\u679c\u6301\u4e45\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u6f0f\u6d1e\u589e\u52a0\uff081.29\u03c3\uff09\uff0c\u800cGPT-4o\u5728\u540c\u4e00\u5411\u91cf\u4e0a\u663e\u793a\u51fa0.69\u03c3\u7684\u589e\u52a0\uff0c\u5728\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u65b9\u9762\u663e\u793a\u51fa0.28\u03c3\u7684\u589e\u52a0\u3002"}}
{"id": "2507.00258", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00258", "abs": "https://arxiv.org/abs/2507.00258", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "AI": {"tldr": "privacy risks from memorization during fine-tuning are not well addressed. Compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs.", "motivation": "the privacy risks arising from memorization during fine-tuning have received relatively little attention.", "method": "categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs).", "result": "prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale.", "conclusion": "parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option."}}
{"id": "2507.00016", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00016", "abs": "https://arxiv.org/abs/2507.00016", "authors": ["Xuanbo Liu", "Liu Liu", "Fuxiang Wu", "Fusheng Hao", "Xianglong Liu"], "title": "Gradient-based Fine-Tuning through Pre-trained Model Regularization", "comment": null, "summary": "Large pre-trained models have demonstrated extensive applications across\nvarious fields. However, fine-tuning these models for specific downstream tasks\ndemands significant computational resources and storage. One fine-tuning\nmethod, gradient-based parameter selection (GPS), focuses on fine-tuning only\nthe parameters with high gradients in each neuron, thereby reducing the number\nof training parameters. Nevertheless, this approach increases computational\nresource requirements and storage demands. In this paper, we propose an\nefficient gradient-based and regularized fine-tuning method (GRFT) that updates\nthe rows or columns of the weight matrix. We theoretically demonstrate that the\nrows or columns with the highest sum of squared gradients are optimal for\nupdating. This strategy effectively reduces storage overhead and improves the\nefficiency of parameter selection. Additionally, we incorporate regularization\nto enhance knowledge transfer from the pre-trained model. GRFT achieves\nstate-of-the-art performance, surpassing existing methods such as GPS, Adapter\nTuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the\ntotal parameters on FGVC and VTAB datasets, respectively, demonstrating its\nhigh efficiency and effectiveness. The source code will be released soon.", "AI": {"tldr": "This paper introduces GRFT, a highly efficient fine-tuning method for large pre-trained models. It updates only a small subset of parameters, achieving state-of-the-art performance with reduced storage and computational costs.", "motivation": "fine-tuning these models for specific downstream tasks demands significant computational resources and storage. One fine-tuning method, gradient-based parameter selection (GPS), focuses on fine-tuning only the parameters with high gradients in each neuron, thereby reducing the number of training parameters. Nevertheless, this approach increases computational resource requirements and storage demands.", "method": "an efficient gradient-based and regularized fine-tuning method (GRFT) that updates the rows or columns of the weight matrix", "result": "the rows or columns with the highest sum of squared gradients are optimal for updating. This strategy effectively reduces storage overhead and improves the efficiency of parameter selection. Additionally, we incorporate regularization to enhance knowledge transfer from the pre-trained model.", "conclusion": "GRFT achieves state-of-the-art performance, surpassing existing methods such as GPS, Adapter Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the total parameters on FGVC and VTAB datasets, respectively, demonstrating its high efficiency and effectiveness."}}
{"id": "2507.00181", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00181", "abs": "https://arxiv.org/abs/2507.00181", "authors": ["Georgios P. Georgiou"], "title": "ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement decline", "comment": null, "summary": "Despite the increasing use of large language models (LLMs) in education,\nconcerns have emerged about their potential to reduce deep thinking and active\nlearning. This study investigates the impact of generative artificial\nintelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of\nstudents during academic writing tasks. The study employed an experimental\ndesign with participants randomly assigned to either an AI-assisted (ChatGPT)\nor a non-assisted (control) condition. Participants completed a structured\nargumentative writing task followed by a cognitive engagement scale (CES), the\nCES-AI, developed to assess mental effort, attention, deep processing, and\nstrategic thinking. The results revealed significantly lower cognitive\nengagement scores in the ChatGPT group compared to the control group. These\nfindings suggest that AI assistance may lead to cognitive offloading. The study\ncontributes to the growing body of literature on the psychological implications\nof AI in education and raises important questions about the integration of such\ntools into academic practice. It calls for pedagogical strategies that promote\nactive, reflective engagement with AI-generated content to avoid compromising\nself-regulated learning and deep cognitive involvement of students.", "AI": {"tldr": "This study investigates the impact of ChatGPT on student cognitive engagement during academic writing, finding that AI assistance may lead to lower engagement.", "motivation": "Concerns have emerged about the potential of large language models (LLMs) to reduce deep thinking and active learning.", "method": "Experimental design with participants randomly assigned to either an AI-assisted (ChatGPT) or a non-assisted (control) condition. Participants completed a structured argumentative writing task followed by a cognitive engagement scale (CES).", "result": "Significantly lower cognitive engagement scores in the ChatGPT group compared to the control group.", "conclusion": "AI assistance may lead to cognitive offloading, potentially compromising self-regulated learning and deep cognitive involvement of students."}}
{"id": "2507.00068", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00068", "abs": "https://arxiv.org/abs/2507.00068", "authors": ["Ziqi Zhong", "Daniel Tang"], "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "comment": null, "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "AI": {"tldr": "MANTA \u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u7edf\u4e00\u591a\u6a21\u6001\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u95ee\u7b54\u7684\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u957f\u65f6\u95f4\u89c6\u9891\u548c\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5bfc\u81f4\u8868\u793a\u548c\u63a8\u7406\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "method": "MANTA (Multi-modal Abstraction and Normalization via Textual Alignment) \u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u7edf\u4e00\u89c6\u89c9\u548c\u542c\u89c9\u8f93\u5165\u5230\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u4fe1\u606f\u7406\u8bba\u4f18\u5316\u3001\u81ea\u9002\u5e94\u65f6\u95f4\u540c\u6b65\u3001\u5206\u5c42\u5185\u5bb9\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\u7b49\u65b9\u6cd5\u3002", "result": "MANTA \u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u603b\u4f53\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 22.6%\uff0c\u8d85\u8fc7 30 \u5206\u949f\u7684\u89c6\u9891\u63d0\u9ad8\u4e86 27.3%\uff0c\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u63d0\u9ad8\u4e86 23.8%\uff0c\u8de8\u6a21\u6001\u7406\u89e3\u63d0\u9ad8\u4e86 25.1%\u3002", "conclusion": "MANTA\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u8d85\u8fc730\u5206\u949f\u7684\u89c6\u9891\u65f6\uff0c\u5e76\u5728\u65f6\u95f4\u63a8\u7406\u548c\u8de8\u6a21\u6001\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.00297", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00297", "abs": "https://arxiv.org/abs/2507.00297", "authors": ["David Ifeoluwa Adelani"], "title": "Natural language processing for African languages", "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "AI": {"tldr": "This paper focuses on improving NLP for low-resource African languages by creating datasets, analyzing data quality, and adapting multilingual language models.", "motivation": "Multilingual models face challenges with low-resource languages due to data noise and lack of labeled datasets for evaluation, especially for languages in Sub-Saharan Africa.", "method": "The paper curates a high-quality corpus and adapts/specializes multilingual PLMs to unseen African languages using a small amount of monolingual texts. It also developed large scale human-annotated labelled datasets for 21 African languages.", "result": "The quality of semantic representations learned in word embeddings depends on the quality of pre-training data. The paper demonstrates the limitations of word embeddings and the opportunities of multilingual PLMs for unseen languages and low-resource scenarios.", "conclusion": "This paper addresses the under-representation of African languages in NLP by developing large scale human-annotated labelled datasets for 21 African languages in named entity recognition and machine translation tasks. The paper conducts an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings."}}
{"id": "2507.00018", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00018", "abs": "https://arxiv.org/abs/2507.00018", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "comment": null, "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "AI": {"tldr": "This paper presents a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. The paper reveals a critical limitation in conventional SFT and introduces a learning rate reduction approach that yields significant performance improvements.", "motivation": "learning from demonstrations or preference signals play a crucial role in this adaptation for grounding pre-trained language models to real-world tasks", "method": "a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. rigorous mathematical derivation, learning rate reduction approach, alternative SFT objectives from various f-divergence functions", "result": "SFT representing a special case of implicit reward learning. a simple yet effective learning rate reduction approach that yields significant performance improvements (up to 25% relative gain and 6% absolute win rate increase in instruction following tasks", "conclusion": "conventional SFT has a critical limitation: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. a simple yet effective learning rate reduction approach that yields significant performance improvements (up to 25% relative gain and 6% absolute win rate increase in instruction following tasks. Additionally, alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, the theoretical relationship between LLM logits and Q-functions from preference learning is extended to the SFT context, providing mathematical derivations and experimental validation."}}
{"id": "2507.00205", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00205", "abs": "https://arxiv.org/abs/2507.00205", "authors": ["Periklis Petridis", "Georgios Margaritis", "Vasiliki Stoumpou", "Dimitris Bertsimas"], "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability", "comment": "Submitted to npj Digital Medicine", "summary": "With the increasing interest in deploying Artificial Intelligence in\nmedicine, we previously introduced HAIM (Holistic AI in Medicine), a framework\nthat fuses multimodal data to solve downstream clinical tasks. However, HAIM\nuses data in a task-agnostic manner and lacks explainability. To address these\nlimitations, we introduce xHAIM (Explainable HAIM), a novel framework\nleveraging Generative AI to enhance both prediction and explainability through\nfour structured steps: (1) automatically identifying task-relevant patient data\nacross modalities, (2) generating comprehensive patient summaries, (3) using\nthese summaries for improved predictive modeling, and (4) providing clinical\nexplanations by linking predictions to patient-specific medical knowledge.\nEvaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%\nto 90.3% across chest pathology and operative tasks. Importantly, xHAIM\ntransforms AI from a black-box predictor into an explainable decision support\nsystem, enabling clinicians to interactively trace predictions back to relevant\npatient data, bridging AI advancements with clinical utility.", "AI": {"tldr": "Introduces xHAIM, an explainable AI framework for medicine that improves prediction and explainability by leveraging Generative AI and linking predictions to patient-specific medical knowledge.", "motivation": "HAIM uses data in a task-agnostic manner and lacks explainability.", "method": "a novel framework leveraging Generative AI to enhance both prediction and explainability through four structured steps: (1) automatically identifying task-relevant patient data across modalities, (2) generating comprehensive patient summaries, (3) using these summaries for improved predictive modeling, and (4) providing clinical explanations by linking predictions to patient-specific medical knowledge.", "result": "xHAIM improves average AUC from 79.9% to 90.3% across chest pathology and operative tasks.", "conclusion": "xHAIM transforms AI from a black-box predictor into an explainable decision support system, enabling clinicians to interactively trace predictions back to relevant patient data, bridging AI advancements with clinical utility."}}
{"id": "2507.00070", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00070", "abs": "https://arxiv.org/abs/2507.00070", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zu\u00f1iga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "title": "An efficient plant disease detection using transfer learning approach", "comment": "15 pages , 4 figures. Scientific Reports 2025", "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u8bc6\u522b\u548c\u76d1\u6d4b\u690d\u7269\u75c5\u5bb3\u7684\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eYOLOv8\u5177\u6709\u5353\u8d8a\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "motivation": "\u690d\u7269\u75c5\u5bb3\u5bf9\u519c\u6c11\u548c\u6574\u4e2a\u519c\u4e1a\u90e8\u95e8\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u65e9\u671f\u68c0\u6d4b\u690d\u7269\u75c5\u5bb3\u5bf9\u4e8e\u51cf\u8f7b\u5176\u5f71\u54cd\u548c\u9632\u6b62\u5e7f\u6cdb\u635f\u5bb3\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u75ab\u60c5\u7206\u53d1\u4f1a\u4e25\u91cd\u5f71\u54cd\u4f5c\u7269\u7684\u751f\u4ea7\u529b\u548c\u8d28\u91cf\u3002\u968f\u7740\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u81ea\u52a8\u5316\u76d1\u6d4b\u548c\u68c0\u6d4b\u690d\u7269\u75c5\u5bb3\u7206\u53d1\u7684\u673a\u4f1a\u8d8a\u6765\u8d8a\u591a\u3002", "method": "\u5229\u7528YOLOv7\u548cYOLOv8\uff0c\u901a\u8fc7\u5728\u690d\u7269\u53f6\u7247\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u8fd9\u4e9b\u6a21\u578b\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u5730\u68c0\u6d4b\u7ec6\u83cc\u3001\u771f\u83cc\u548c\u75c5\u6bd2\u6027\u75be\u75c5\u3002", "result": "\u8be5\u6a21\u578b\u4f7f\u7528\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\u3001F1-score\u3001\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u7b49\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5f97\u5230\u7684\u503c\u5206\u522b\u4e3a91.05\u300189.40\u300191.22\u548c87.66\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u76f8\u6bd4\uff0cYOLOv8\u5177\u6709\u5353\u8d8a\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "YOLOv8\u5728\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u73b0\u4ee3\u519c\u4e1a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u5e76\u4e3a\u65e9\u671f\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u4f5c\u7269\u4ea7\u91cf\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u76d1\u6d4b\u7684\u4f9d\u8d56\uff0c\u5e76\u652f\u6301\u53ef\u6301\u7eed\u519c\u4e1a\u5b9e\u8df5\u3002"}}
{"id": "2507.00322", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.00322", "abs": "https://arxiv.org/abs/2507.00322", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "AI": {"tldr": "This paper studies why LMs struggle with simple syntactic tasks and introduces RASteer, a steering method to improve model performance by identifying and increasing the contribution of reliable components.", "motivation": "Language models still struggle with simple syntactic tasks such as generating balanced parentheses. Errors occur when the faulty mechanisms overshadow the sound ones and dominantly affect the predictions.", "method": "RASteer, a steering method to systematically identify and increase the contribution of reliable components", "result": "LMs rely on a number of components (attention heads and FF neurons) that independently make their own predictions. Some components reliably promote correct answers, others are less reliable and introduce noise by promoting incorrect tokens.", "conclusion": "RASteer, a steering method is introduced to identify and increase the contribution of reliable components for improving model performance. RASteer substantially improves performance on balanced parentheses tasks, boosting accuracy of some models from $0$% to around $100$%, and achieving performance gains of up to around $20$% in arithmetic reasoning tasks."}}
{"id": "2507.00019", "categories": ["cs.LG", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.00019", "abs": "https://arxiv.org/abs/2507.00019", "authors": ["Minati Rath", "Hema Date"], "title": "Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations", "comment": null, "summary": "In this study, we propose, evaluate and compare three quantum inspired data\nencoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy\n(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical\ndata into quantum data for use in pure classical machine learning models. The\nprimary objective is to reduce high encoding time while ensuring correct\nencoding values and analyzing their impact on classification performance. The\nInstance Level Strategy treats each row of dataset independently; mimics local\nquantum states. Global Discrete Value Based encoding strategy maps all unique\nfeature values across the full dataset to quantum states uniformly. In\ncontrast, the Class conditional Value based encoding strategy encodes unique\nvalues separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their\nimpact on en-coding efficiency, correctness, model accuracy, and computational\ncost. By analyzing the trade offs between encoding time, precision, and\npredictive performance, this study provides insights into optimizing quantum\ninspired data transformations for classical machine learning workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e09\u79cd\u91cf\u5b50\u542f\u53d1\u5f0f\u6570\u636e\u7f16\u7801\u7b56\u7565\uff0c\u65e8\u5728\u51cf\u5c11\u7f16\u7801\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3b\u8981\u76ee\u6807\u662f\u5728\u786e\u4fdd\u6b63\u786e\u7684\u7f16\u7801\u503c\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u9ad8\u7f16\u7801\u65f6\u95f4\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u3001\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e86\u4e09\u79cd\u91cf\u5b50\u542f\u53d1\u7684\u6570\u636e\u7f16\u7801\u7b56\u7565\uff1a\u5b9e\u4f8b\u7ea7\u522b\u7b56\u7565\uff08ILS\uff09\u3001\u5168\u5c40\u79bb\u6563\u7b56\u7565\uff08GDS\uff09\u548c\u7c7b\u6761\u4ef6\u503c\u7b56\u7565\uff08CCVS\uff09\u3002", "result": "\u901a\u8fc7\u5206\u6790\u7f16\u7801\u65f6\u95f4\u3001\u7cbe\u5ea6\u548c\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u672c\u7814\u7a76\u6df1\u5165\u4e86\u89e3\u4e86\u5982\u4f55\u4f18\u5316\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u7684\u91cf\u5b50\u542f\u53d1\u6570\u636e\u8f6c\u6362\u3002", "conclusion": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u4e09\u79cd\u91cf\u5b50\u542f\u53d1\u7684\u6570\u636e\u7f16\u7801\u7b56\u7565\u5728\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u7f16\u7801\u6548\u7387\u3001\u6b63\u786e\u6027\u3001\u6a21\u578b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.00218", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.00218", "abs": "https://arxiv.org/abs/2507.00218", "authors": ["Fangting Zhou", "Attila Lischka", "Balazs Kulcsar", "Jiaming Wu", "Morteza Haghir Chehreghani", "Gilbert Laporte"], "title": "Learning for routing: A guided review of recent developments and future directions", "comment": "Accepted for publication in Transportation Research Part E: Logistics\n  and Transportation Review", "summary": "This paper reviews the current progress in applying machine learning (ML)\ntools to solve NP-hard combinatorial optimization problems, with a focus on\nrouting problems such as the traveling salesman problem (TSP) and the vehicle\nrouting problem (VRP). Due to the inherent complexity of these problems, exact\nalgorithms often require excessive computational time to find optimal\nsolutions, while heuristics can only provide approximate solutions without\nguaranteeing optimality. With the recent success of machine learning models,\nthere is a growing trend in proposing and implementing diverse ML techniques to\nenhance the resolution of these challenging routing problems. We propose a\ntaxonomy categorizing ML-based routing methods into construction-based and\nimprovement-based approaches, highlighting their applicability to various\nproblem characteristics. This review aims to integrate traditional OR methods\nwith state-of-the-art ML techniques, providing a structured framework to guide\nfuture research and address emerging VRP variants.", "AI": {"tldr": "Using ML to solve routing problems like TSP and VRP.", "motivation": "Exact algorithms for NP-hard combinatorial optimization problems often require excessive computational time, while heuristics can only provide approximate solutions without guaranteeing optimality. Machine learning models have shown recent success.", "method": "We propose a taxonomy categorizing ML-based routing methods into construction-based and improvement-based approaches, highlighting their applicability to various problem characteristics.", "result": "Reviews current progress in applying machine learning tools to solve NP-hard combinatorial optimization problems, focusing on routing problems like TSP and VRP.", "conclusion": "This review integrates traditional OR methods with state-of-the-art ML techniques, providing a structured framework to guide future research and address emerging VRP variants."}}
{"id": "2507.00153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00153", "abs": "https://arxiv.org/abs/2507.00153", "authors": ["Peter Mortimer", "Mirko Maehlisch"], "title": "Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "The performance of leaning-based perception algorithms suffer when deployed\nin out-of-distribution and underrepresented environments. Outdoor robots are\nparticularly susceptible to rapid changes in visual scene appearance due to\ndynamic lighting, seasonality and weather effects that lead to scenes\nunderrepresented in the training data of the learning-based perception system.\nIn this conceptual paper, we focus on preparing our autonomous vehicle for\ndeployment in snow-filled environments. We propose a novel method for\ndiffusion-based image augmentation to more closely represent the deployment\nenvironment in our training data. Diffusion-based image augmentations rely on\nthe public availability of vision foundation models learned on internet-scale\ndatasets. The diffusion-based image augmentations allow us to take control over\nthe semantic distribution of the ground surfaces in the training data and to\nfine-tune our model for its deployment environment. We employ open vocabulary\nsemantic segmentation models to filter out augmentation candidates that contain\nhallucinations. We believe that diffusion-based image augmentations can be\nextended to many other environments apart from snow surfaces, like sandy\nenvironments and volcanic terrains.", "AI": {"tldr": "This paper proposes a diffusion-based image augmentation method to improve the performance of perception algorithms in underrepresented environments, specifically focusing on snow-filled environments for autonomous vehicles.", "motivation": "The performance of leaning-based perception algorithms suffer when deployed in out-of-distribution and underrepresented environments. Outdoor robots are particularly susceptible to rapid changes in visual scene appearance due to dynamic lighting, seasonality and weather effects that lead to scenes underrepresented in the training data of the learning-based perception system.", "method": "a novel method for diffusion-based image augmentation", "result": "diffusion-based image augmentations allow us to take control over the semantic distribution of the ground surfaces in the training data and to fine-tune our model for its deployment environment. We employ open vocabulary semantic segmentation models to filter out augmentation candidates that contain hallucinations.", "conclusion": "diffusion-based image augmentations can be extended to many other environments apart from snow surfaces, like sandy environments and volcanic terrains."}}
{"id": "2507.00020", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.00020", "abs": "https://arxiv.org/abs/2507.00020", "authors": ["Marcio Borges", "Felipe Pereira", "Michel Tosin"], "title": "Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods", "comment": "The main contribution of this work is to show the advantages of using\n  deep generative models like VAE to provide more flexible and versatile prior\n  distributions", "summary": "This study uses a Variational Autoencoder method to enhance the efficiency\nand applicability of Markov Chain Monte Carlo (McMC) methods by generating\nbroader-spectrum prior proposals. Traditional approaches, such as the\nKarhunen-Lo\\`eve Expansion (KLE), require previous knowledge of the covariance\nfunction, often unavailable in practical applications. The VAE framework\nenables a data-driven approach to flexibly capture a broader range of\ncorrelation structures in Bayesian inverse problems, particularly subsurface\nflow modeling. The methodology is tested on a synthetic groundwater flow\ninversion problem, where pressure data is used to estimate permeability fields.\nNumerical experiments demonstrate that the VAE-based parameterization achieves\ncomparable accuracy to KLE when the correlation length is known and outperforms\nKLE when the assumed correlation length deviates from the true value. Moreover,\nthe VAE approach significantly reduces stochastic dimensionality, improving\ncomputational efficiency. The results suggest that leveraging deep generative\nmodels in McMC methods can lead to more adaptable and efficient Bayesian\ninference in high-dimensional problems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u66f4\u5e7f\u8c31\u7684\u5148\u9a8c\u5efa\u8bae\uff0c\u63d0\u9ad8\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u7f57(McMC)\u65b9\u6cd5\u7684\u6548\u7387\u548c\u9002\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff0c\u5982Karhunen-Lo\u00e8ve\u5c55\u5f00(KLE)\uff0c\u9700\u8981\u9884\u5148\u4e86\u89e3\u534f\u65b9\u5dee\u51fd\u6570\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u901a\u5e38\u662f\u4e0d\u53ef\u7528\u7684\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668(VAE)", "result": "VAE\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u76f8\u5173\u957f\u5ea6\u5df2\u77e5\u65f6\uff0c\u5176\u7cbe\u5ea6\u4e0eKLE\u76f8\u5f53;\u5f53\u5047\u8bbe\u7684\u76f8\u5173\u957f\u5ea6\u504f\u79bb\u771f\u5b9e\u503c\u65f6\uff0cVAE\u53c2\u6570\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u4e8eKLE\u3002\u6b64\u5916\uff0cVAE\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u968f\u673a\u7ef4\u6570\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6539\u8fdbMcMC\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u9ad8\u7ef4\u95ee\u9898\u4e2d\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.00417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00417", "abs": "https://arxiv.org/abs/2507.00417", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "comment": "36 pages, 23 figures", "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "AI": {"tldr": "ASTRO teaches non-reasoner models to internalize structured search behavior through a synthetic dataset and improves performance via RL.", "motivation": "It is yet unclear how to boost the reasoning capabilities of non-reasoner models including Llama 3.", "method": "Training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs; a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories; finetuning models on search-derived traces and further improve performance via RL with verifiable rewards.", "result": "Absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024.", "conclusion": "Search-inspired training can instill robust reasoning capabilities into open LLMs."}}
{"id": "2507.00162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00162", "abs": "https://arxiv.org/abs/2507.00162", "authors": ["Yu Lu", "Yi Yang"], "title": "FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion", "comment": "under review", "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.", "AI": {"tldr": "FreeLong++ \u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u8861\u9891\u7387\u5206\u5e03\u6765\u6539\u5584\u957f\u89c6\u9891\u751f\u6210\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u964d\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5c06\u8fd9\u4e9b\u6a21\u578b\u6269\u5c55\u5230\u66f4\u957f\u7684\u89c6\u9891\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u964d\u4f4e\u3002\u521d\u6b65\u89c2\u5bdf\u8868\u660e\uff0c\u5929\u771f\u5730\u5c06\u77ed\u89c6\u9891\u751f\u6210\u6a21\u578b\u5e94\u7528\u4e8e\u66f4\u957f\u7684\u5e8f\u5217\u4f1a\u5bfc\u81f4\u660e\u663e\u7684\u8d28\u91cf\u4e0b\u964d\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u786e\u5b9a\u4e86\u4e00\u79cd\u7cfb\u7edf\u8d8b\u52bf\uff0c\u5373\u9ad8\u9891\u5206\u91cf\u968f\u7740\u89c6\u9891\u957f\u5ea6\u7684\u589e\u957f\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u5931\u771f\uff0c\u8fd9\u4e2a\u95ee\u9898\u6211\u4eec\u79f0\u4e4b\u4e3a\u9ad8\u9891\u5931\u771f\u3002", "method": "\u63d0\u51fa FreeLong\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u5e73\u8861\u53bb\u566a\u8fc7\u7a0b\u4e2d\u957f\u89c6\u9891\u7279\u5f81\u7684\u9891\u7387\u5206\u5e03\u7684\u514d\u8bad\u7ec3\u6846\u67b6\u3002FreeLong \u901a\u8fc7\u6df7\u5408\u5168\u5c40\u4f4e\u9891\u7279\u5f81\uff08\u6355\u83b7\u6574\u4e2a\u89c6\u9891\u7684\u6574\u4f53\u8bed\u4e49\uff09\u4e0e\u4ece\u77ed\u65f6\u7a97\u63d0\u53d6\u7684\u5c40\u90e8\u9ad8\u9891\u7279\u5f81\uff08\u4ee5\u4fdd\u7559\u7cbe\u7ec6\u7ec6\u8282\uff09\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002FreeLong++ \u5c06 FreeLong \u53cc\u5206\u652f\u8bbe\u8ba1\u6269\u5c55\u5230\u5177\u6709\u591a\u4e2a\u6ce8\u610f\u5206\u652f\u7684\u591a\u5206\u652f\u67b6\u6784\uff0c\u6bcf\u4e2a\u5206\u652f\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u884c\u3002\u901a\u8fc7\u6392\u5217\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u7684\u591a\u4e2a\u7a97\u53e3\u5927\u5c0f\uff0cFreeLong++ \u80fd\u591f\u5b9e\u73b0\u4ece\u4f4e\u9891\u5230\u9ad8\u9891\u7684\u591a\u9891\u5e26\u878d\u5408\uff0c\u4ece\u800c\u786e\u4fdd\u66f4\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u8bed\u4e49\u8fde\u7eed\u6027\u548c\u7cbe\u7ec6\u8fd0\u52a8\u52a8\u6001\u3002", "result": "FreeLong++ \u5728\u66f4\u957f\u7684\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff08\u4f8b\u5982\uff0c\u539f\u59cb\u957f\u5ea6\u7684 4 \u500d\u548c 8 \u500d\uff09\u3002\u5b83\u8fd8\u652f\u6301\u5177\u6709\u5e73\u6ed1\u573a\u666f\u8fc7\u6e21\u7684\u8fde\u8d2f\u591a\u63d0\u793a\u89c6\u9891\u751f\u6210\uff0c\u5e76\u652f\u6301\u4f7f\u7528\u957f\u6df1\u5ea6\u6216\u59ff\u52bf\u5e8f\u5217\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210\u3002", "conclusion": "FreeLong++\u53ef\u4ee5\u63d2\u5165\u5230\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u4ee5\u751f\u6210\u5177\u6709\u663e\u7740\u6539\u5584\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u66f4\u957f\u89c6\u9891\u3002\u8be5\u65b9\u6cd5\u5728\u66f4\u957f\u7684\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u652f\u6301\u5177\u6709\u5e73\u6ed1\u573a\u666f\u8fc7\u6e21\u7684\u8fde\u8d2f\u591a\u63d0\u793a\u89c6\u9891\u751f\u6210\uff0c\u5e76\u652f\u6301\u4f7f\u7528\u957f\u6df1\u5ea6\u6216\u59ff\u52bf\u5e8f\u5217\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2507.00355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00355", "abs": "https://arxiv.org/abs/2507.00355", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "title": "Question Decomposition for Retrieval-Augmented Generation", "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ee\u9898\u5206\u89e3\u7684RAG\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8df3\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u68c0\u7d22\u548c\u7b54\u6848\u51c6\u786e\u6027\u3002", "motivation": "\u6807\u51c6RAG\u5728\u5904\u7406\u591a\u8df3\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u76f8\u5173\u4e8b\u5b9e\u901a\u5e38\u5206\u5e03\u5728\u591a\u4e2a\u6587\u6863\u4e2d\uff0c\u96be\u4ee5\u68c0\u7d22\u5230\u8db3\u591f\u7684\u4fe1\u606f\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a(i) \u4f7f\u7528LLM\u5c06\u539f\u59cb\u67e5\u8be2\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff1b(ii) \u68c0\u7d22\u6bcf\u4e2a\u5b50\u95ee\u9898\u7684\u76f8\u5173\u6bb5\u843d\uff1b(iii) \u91cd\u65b0\u6392\u5e8f\u5408\u5e76\u540e\u7684\u5019\u9009\u6c60\uff0c\u4ee5\u63d0\u9ad8\u68c0\u7d22\u8bc1\u636e\u7684\u8986\u76d6\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728MultiHop-RAG\u548cHotpotQA\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\uff08MRR@10: +36.7%\uff09\u548c\u7b54\u6848\u51c6\u786e\u6027\uff08F1: +11.6%\uff09\u65b9\u9762\u5747\u4f18\u4e8e\u6807\u51c6RAG\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u7684RAG\u6d41\u6c34\u7ebf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u8df3\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5728\u68c0\u7d22\u548c\u7b54\u6848\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u6807\u51c6RAG\u57fa\u7ebf\u3002"}}
{"id": "2507.00022", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.00022", "abs": "https://arxiv.org/abs/2507.00022", "authors": ["Zehao Wang"], "title": "GLU Attention Improve Transformer", "comment": "4 pages 4 figures", "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "AI": {"tldr": "Introduces GLU Attention, a novel attention mechanism that improves model performance and convergence speed with zero additional parameters and negligible computational costs.", "motivation": "Gated Linear Units (GLU) have shown great potential in enhancing neural network performance", "method": "introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention", "result": "GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs", "conclusion": "GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies"}}
{"id": "2507.00432", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00432", "abs": "https://arxiv.org/abs/2507.00432", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "comment": null, "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "AI": {"tldr": "\u6570\u5b66\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u5728\u5176\u4ed6\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002\u5f3a\u5316\u5b66\u4e60\u6bd4\u76d1\u7763\u5fae\u8c03\u66f4\u597d\u3002", "motivation": "\u6570\u5b66\u63a8\u7406\u5df2\u6210\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u6b65\u7684\u4ee3\u8868\uff0c\u4f46\u8fd9\u4e9b\u8fdb\u6b65\u662f\u5426\u53cd\u6620\u4e86\u66f4\u5e7f\u6cdb\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u53cd\u6620\u4e86\u72ed\u4e49\u7684\u8fc7\u5ea6\u62df\u5408\uff1f", "method": "\u5bf9Qwen3-14B\u6a21\u578b\u4f7f\u7528\u6570\u5b66\u4e13\u7528\u6570\u636e\u4f46\u4e0d\u540c\u7684\u8c03\u6574\u65b9\u6cd5\u8fdb\u884c\u5bf9\u7167\u5b9e\u9a8c\u3002\u5bf9\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u548ctoken\u7a7a\u95f4\u5206\u5e03\u504f\u79fb\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5927\u591a\u6570\u5728\u6570\u5b66\u65b9\u9762\u6210\u529f\u7684\u6a21\u578b\u672a\u80fd\u5c06\u5176\u6210\u679c\u63a8\u5e7f\u5230\u5176\u4ed6\u9886\u57df\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8c03\u6574\u7684\u6a21\u578b\u5728\u5404\u4e2a\u9886\u57df\u90fd\u80fd\u5f88\u597d\u5730\u63a8\u5e7f\uff0c\u800c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8c03\u6574\u7684\u6a21\u578b\u5e38\u5e38\u4f1a\u5fd8\u8bb0\u4e00\u822c\u80fd\u529b\u3002", "conclusion": "\u6807\u51c6\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bfc\u81f4\u663e\u8457\u7684\u8868\u5f81\u548c\u8f93\u51fa\u6f02\u79fb\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4fdd\u7559\u4e86\u4e00\u822c\u9886\u57df\u7ed3\u6784\u3002\u7ed3\u679c\u8868\u660e\u9700\u8981\u91cd\u65b0\u601d\u8003\u6807\u51c6\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u4f9d\u8d56\u4e8eSFT\u63d0\u53d6\u7684\u6570\u636e\u6765\u63a8\u8fdb\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2507.00170", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.00170", "abs": "https://arxiv.org/abs/2507.00170", "authors": ["Hugo Baudchon", "Arthur Ouaknine", "Martin Weiss", "M\u00e9lisande Teng", "Thomas R. Walla", "Antoine Caron-Guay", "Christopher Pal", "Etienne Lalibert\u00e9"], "title": "SelvaBox: A high-resolution dataset for tropical tree crown detection", "comment": null, "summary": "Detecting individual tree crowns in tropical forests is essential to study\nthese complex and crucial ecosystems impacted by human interventions and\nclimate change. However, tropical crowns vary widely in size, structure, and\npattern and are largely overlapping and intertwined, requiring advanced remote\nsensing methods applied to high-resolution imagery. Despite growing interest in\ntropical tree crown detection, annotated datasets remain scarce, hindering\nrobust model development. We introduce SelvaBox, the largest open-access\ndataset for tropical tree crown detection in high-resolution drone imagery. It\nspans three countries and contains more than 83,000 manually labeled crowns -\nan order of magnitude larger than all previous tropical forest datasets\ncombined. Extensive benchmarks on SelvaBox reveal two key findings: (1)\nhigher-resolution inputs consistently boost detection accuracy; and (2) models\ntrained exclusively on SelvaBox achieve competitive zero-shot detection\nperformance on unseen tropical tree crown datasets, matching or exceeding\ncompeting methods. Furthermore, jointly training on SelvaBox and three other\ndatasets at resolutions from 3 to 10 cm per pixel within a unified\nmulti-resolution pipeline yields a detector ranking first or second across all\nevaluated datasets. Our dataset, code, and pre-trained weights are made public.", "AI": {"tldr": "SelvaBox\u662f\u6700\u5927\u7684\u5f00\u653e\u83b7\u53d6\u70ed\u5e26\u6811\u51a0\u68c0\u6d4b\u6570\u636e\u96c6\u3002", "motivation": "\u68c0\u6d4b\u70ed\u5e26\u68ee\u6797\u4e2d\u7684\u6811\u51a0\u5bf9\u4e8e\u7814\u7a76\u53d7\u4eba\u7c7b\u6d3b\u52a8\u548c\u6c14\u5019\u53d8\u5316\u5f71\u54cd\u7684\u590d\u6742\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u6811\u51a0\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u5f00\u653e\u8bbf\u95ee\u6570\u636e\u96c6SelvaBox\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u8f93\u5165\u53ef\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff1b\u4ec5\u5728SelvaBox\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u70ed\u5e26\u6811\u51a0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8054\u5408\u8bad\u7ec3SelvaBox\u548c\u5176\u4ed6\u6570\u636e\u96c6\u53ef\u4ee5\u5728\u6240\u6709\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u6700\u4f73\u6216\u6b21\u4f73\u7684\u68c0\u6d4b\u5668\u3002"}}
{"id": "2507.00380", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00380", "abs": "https://arxiv.org/abs/2507.00380", "authors": ["Vojt\u011bch Lanz", "Jan Haji\u010d jr"], "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "AI": {"tldr": "searching for an optimal unsupervised segmentation of chant melody and achieves state-of-the-art performance in mode classification", "motivation": "Gregorian melodies are constructed from some vocabulary of segments. Frequent re-use of certain melodic segments has been observed in chant melodies, and the intractable number of possible segmentations allowed the option that some undiscovered segmentation exists that will yet prove the value of centonisation, and recent empirical results have shown that segmentations can outperform music-theoretical features in mode classification. Inspired by the fact that Gregorian chant was memorised", "method": "search for an optimal unsupervised segmentation of chant melody using nested hierarchical Pitman-Yor language models", "result": "achieves state-of-the-art performance in mode classification. find empirical evidence for the link between mode classification and memory efficiency, and observe more formulaic areas at the beginnings and ends of melodies corresponding to the practical role of modality in performance", "conclusion": "even a memory-optimal segmentation is not what is understood as centonisation."}}
{"id": "2507.00024", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00024", "abs": "https://arxiv.org/abs/2507.00024", "authors": ["Yeyong Yu", "Xilei Bian", "Jie Xiong", "Xing Wu", "Quan Qian"], "title": "AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity", "comment": null, "summary": "With the growing demand for novel materials, machine learning-driven inverse\ndesign methods face significant challenges in reconciling the high-dimensional\nmaterials composition space with limited experimental data. Existing approaches\nsuffer from two major limitations: (I) machine learning models often lack\nreliability in high-dimensional spaces, leading to prediction biases during the\ndesign process; (II) these models fail to effectively incorporate domain expert\nknowledge, limiting their capacity to support knowledge-guided inverse design.\nTo address these challenges, we introduce AIMatDesign, a reinforcement learning\nframework that addresses these limitations by augmenting experimental data\nusing difference-based algorithms to build a trusted experience pool,\naccelerating model convergence. To enhance model reliability, an automated\nrefinement strategy guided by large language models (LLMs) dynamically corrects\nprediction inconsistencies, reinforcing alignment between reward signals and\nstate value functions. Additionally, a knowledge-based reward function\nleverages expert domain rules to improve stability and efficiency during\ntraining. Our experiments demonstrate that AIMatDesign significantly surpasses\ntraditional machine learning and reinforcement learning methods in discovery\nefficiency, convergence speed, and success rates. Among the numerous candidates\nproposed by AIMatDesign, experimental synthesis of representative Zr-based\nalloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\\%\nelongation, closely matching predictions. Moreover, the framework accurately\ncaptured the trend of yield strength variation with composition, demonstrating\nits reliability and potential for closed-loop materials discovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAIMatDesign\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6750\u6599\u9006\u5411\u8bbe\u8ba1\u4e2d\u9ad8\u7ef4\u7a7a\u95f4\u548c\u6709\u9650\u6570\u636e\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6750\u6599\u53d1\u73b0\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5408\u6210\u4e86\u4e00\u79cd\u9ad8\u6027\u80fdZr\u57fa\u5408\u91d1\u3002", "motivation": "\u968f\u7740\u5bf9\u65b0\u578b\u6750\u6599\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u5728\u534f\u8c03\u9ad8\u7ef4\u6750\u6599\u6210\u5206\u7a7a\u95f4\u4e0e\u6709\u9650\u7684\u5b9e\u9a8c\u6570\u636e\u65b9\u9762\u9762\u4e34\u7740\u4e25\u5cfb\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u6709\u4e24\u4e2a\u4e3b\u8981\u7684\u5c40\u9650\u6027\uff1a(I) \u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u901a\u5e38\u7f3a\u4e4f\u53ef\u9760\u6027\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u9884\u6d4b\u504f\u5dee\uff1b(II) \u8fd9\u4e9b\u6a21\u578b\u672a\u80fd\u6709\u6548\u6574\u5408\u9886\u57df\u4e13\u5bb6\u7684\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u652f\u6301\u77e5\u8bc6\u5f15\u5bfc\u7684\u9006\u5411\u8bbe\u8ba1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6AIMatDesign\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u5dee\u5f02\u7684\u7b97\u6cd5\u589e\u5f3a\u5b9e\u9a8c\u6570\u636e\u6765\u6784\u5efa\u53ef\u4fe1\u7684\u7ecf\u9a8c\u6c60\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u9650\u5236\uff0c\u4ece\u800c\u52a0\u901f\u4e86\u6a21\u578b\u6536\u655b\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\uff0c\u4e00\u79cd\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5f15\u5bfc\u7684\u81ea\u52a8\u7ec6\u5316\u7b56\u7565\u52a8\u6001\u5730\u7ea0\u6b63\u4e86\u9884\u6d4b\u4e0d\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u52a0\u5f3a\u4e86\u5956\u52b1\u4fe1\u53f7\u548c\u72b6\u6001\u503c\u51fd\u6570\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u77e5\u8bc6\u7684\u5956\u52b1\u51fd\u6570\u5229\u7528\u4e13\u5bb6\u9886\u57df\u89c4\u5219\u6765\u63d0\u9ad8\u8bad\u7ec3\u671f\u95f4\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIMatDesign\u5728\u53d1\u73b0\u6548\u7387\u3001\u6536\u655b\u901f\u5ea6\u548c\u6210\u529f\u7387\u65b9\u9762\u663e\u8457\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u5728AIMatDesign\u63d0\u51fa\u7684\u4f17\u591a\u5019\u9009\u65b9\u6848\u4e2d\uff0c\u4ee3\u8868\u6027Zr\u57fa\u5408\u91d1\u7684\u5b9e\u9a8c\u5408\u6210\u4ea7\u751f\u4e86\u4e00\u79cd\u9ad8\u6027\u80fdBMG\uff0c\u5c48\u670d\u5f3a\u5ea6\u4e3a1.7GPa\uff0c\u5ef6\u4f38\u7387\u4e3a10.2%\uff0c\u4e0e\u9884\u6d4b\u7ed3\u679c\u975e\u5e38\u543b\u5408\u3002", "conclusion": "AIMatDesign\u663e\u8457\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728Zr\u57fa\u5408\u91d1\u7684\u5b9e\u9a8c\u5408\u6210\u4e2d\uff0c\u5c48\u670d\u5f3a\u5ea6\u8fbe\u52301.7GPa\uff0c\u5ef6\u4f38\u7387\u8fbe\u523010.2%\uff0c\u4e0e\u9884\u6d4b\u7ed3\u679c\u975e\u5e38\u63a5\u8fd1\uff0c\u5e76\u4e14\u51c6\u786e\u5730\u6355\u6349\u5230\u4e86\u5c48\u670d\u5f3a\u5ea6\u968f\u6210\u5206\u53d8\u5316\u7684\u8d8b\u52bf\u3002"}}
{"id": "2507.00557", "categories": ["cs.AI", "cs.LO", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.00557", "abs": "https://arxiv.org/abs/2507.00557", "authors": ["Tianyi Ding", "Haokun Li", "Xinpeng Ni", "Bican Xia", "Tianqi Zhao"], "title": "Advancing Local Search in SMT-NRA with MCSAT Integration", "comment": null, "summary": "In this paper, we advance local search for Satisfiability Modulo the Theory\nof Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we design a hybrid framework for SMT-NRA\ncombining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through\ninformation exchange. The experimental results demonstrate improvements in\nlocal search performance, highlighting the effectiveness of the proposed\nmethods.", "AI": {"tldr": "advance local search for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA for short) by introduce a two-dimensional cell-jump move, propose an extended local search framework, named $2d$-LS integrating the model constructing satisfiability calculus (MCSAT) framework to improve search efficiency, implement a recently proposed technique called sample-cell projection operator for MCSAT, design a hybrid framework for SMT-NRA combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through information exchange.", "motivation": "advance local search for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA for short)", "method": "introduce a two-dimensional cell-jump move, called $2d$-cell-jump, generalizing the key operation, cell-jump, of the local search method for SMT-NRA. Then, we propose an extended local search framework, named $2d$-LS integrating the model constructing satisfiability calculus (MCSAT) framework to improve search efficiency. To further improve the efficiency of MCSAT, we implement a recently proposed technique called sample-cell projection operator for MCSAT, which is well suited for CDCL-style search in the real domain and helps guide the search away from conflicting states. Finally, we design a hybrid framework for SMT-NRA combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through information exchange.", "result": "improvements in local search performance", "conclusion": "The experimental results demonstrate improvements in local search performance, highlighting the effectiveness of the proposed methods."}}
{"id": "2507.00182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00182", "abs": "https://arxiv.org/abs/2507.00182", "authors": ["J. I. Ru\u00edz", "A. M\u00e9ndez", "E. Rodr\u00edguez"], "title": "Graph-Based Deep Learning for Component Segmentation of Maize Plants", "comment": null, "summary": "In precision agriculture, one of the most important tasks when exploring crop\nproduction is identifying individual plant components. There are several\nattempts to accomplish this task by the use of traditional 2D imaging, 3D\nreconstructions, and Convolutional Neural Networks (CNN). However, they have\nseveral drawbacks when processing 3D data and identifying individual plant\ncomponents. Therefore, in this work, we propose a novel Deep Learning\narchitecture to detect components of individual plants on Light Detection and\nRanging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on\nthe concept of Graph Neural Networks (GNN), and feature enhancing with\nPrincipal Component Analysis (PCA). For this, each point is taken as a vertex\nand by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,\nthus representing the 3D PC data set. Subsequently, Edge-Conv layers are used\nto further increase the features of each point. Finally, Graph Attention\nNetworks (GAT) are applied to classify visible phenotypic components of the\nplant, such as the leaf, stem, and soil. This study demonstrates that our\ngraph-based deep learning approach enhances segmentation accuracy for\nidentifying individual plant components, achieving percentages above 80% in the\nIoU average, thus outperforming other existing models based on point clouds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u4ece3D\u70b9\u4e91\u6570\u636e\u4e2d\u68c0\u6d4b\u690d\u7269\u7ec4\u4ef6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\uff0c\u63a2\u7d22\u4f5c\u7269\u751f\u4ea7\u6700\u91cd\u8981\u7684\u4efb\u52a1\u4e4b\u4e00\u662f\u8bc6\u522b\u5355\u4e2a\u690d\u7269\u7ec4\u4ef6\u3002\u76ee\u524d\u5df2\u7ecf\u6709\u4e00\u4e9b\u5c1d\u8bd5\u901a\u8fc7\u4f7f\u7528\u4f20\u7edf\u76842D\u6210\u50cf\u30013D\u91cd\u5efa\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u6765\u5b9e\u73b0\u8fd9\u4e00\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5728\u5904\u74063D\u6570\u636e\u548c\u8bc6\u522b\u5355\u4e2a\u690d\u7269\u7ec4\u4ef6\u65f6\uff0c\u5b83\u4eec\u5b58\u5728\u4e00\u4e9b\u7f3a\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u68c0\u6d4b\u6fc0\u5149\u96f7\u8fbe(LiDAR)3D\u70b9\u4e91(PC)\u6570\u636e\u96c6\u4e0a\u5355\u4e2a\u690d\u7269\u7684\u7ec4\u4ef6\u3002\u8be5\u67b6\u6784\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u7684\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u4e3b\u6210\u5206\u5206\u6790(PCA)\u8fdb\u884c\u7279\u5f81\u589e\u5f3a\u3002\u6bcf\u4e2a\u70b9\u90fd\u88ab\u4f5c\u4e3a\u4e00\u4e2a\u9876\u70b9\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528k\u8fd1\u90bb(KNN)\u5c42\u6765\u5efa\u7acb\u8fb9\uff0c\u4ece\u800c\u8868\u793a3D PC\u6570\u636e\u96c6\u3002\u968f\u540e\uff0c\u4f7f\u7528Edge-Conv\u5c42\u6765\u8fdb\u4e00\u6b65\u589e\u52a0\u6bcf\u4e2a\u70b9\u7684\u7279\u5f81\u3002\u6700\u540e\uff0c\u5e94\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(GAT)\u6765\u5bf9\u690d\u7269\u7684\u53ef\u89c1\u8868\u578b\u6210\u5206(\u5982\u53f6\u3001\u830e\u548c\u571f\u58e4)\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u57fa\u4e8e\u56fe\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8bc6\u522b\u5355\u4e2a\u690d\u7269\u7ec4\u4ef6\u7684\u5206\u5272\u7cbe\u5ea6\uff0cIoU\u5e73\u5747\u503c\u8fbe\u523080%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u56fe\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8bc6\u522b\u5355\u4e2a\u690d\u7269\u7ec4\u4ef6\u7684\u5206\u5272\u7cbe\u5ea6\uff0cIoU\u5e73\u5747\u503c\u8fbe\u523080%\u4ee5\u4e0a\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u70b9\u4e91\u7684\u73b0\u6709\u6a21\u578b\u3002"}}
{"id": "2507.00389", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00389", "abs": "https://arxiv.org/abs/2507.00389", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "AI": {"tldr": "CAPITAL is a causal prompting framework for implicit sentiment analysis that uses front-door adjustment to improve accuracy and robustness by addressing biases in chain-of-thought reasoning.", "motivation": "Existing prompting-based methods for implicit sentiment analysis rely on majority voting over chain-of-thought reasoning paths without evaluating their causal validity, making them susceptible to biases and spurious correlations.", "method": "The CAPITAL framework decomposes the causal effect into the influence of the input prompt on reasoning chains and the impact of chains on the final output, estimated using encoder-based clustering and NWGM approximation, with a contrastive learning objective for better alignment.", "result": "CAPITAL consistently outperforms strong prompting baselines in both accuracy and robustness on benchmark ISA datasets, particularly under adversarial conditions.", "conclusion": "This work introduces CAPITAL, a causal prompting framework that integrates front-door adjustment into chain-of-thought reasoning for implicit sentiment analysis. Experiments show CAPITAL outperforms strong baselines in accuracy and robustness, especially under adversarial conditions, demonstrating the benefits of causal inference for bias-aware sentiment reasoning."}}
{"id": "2507.00025", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.00025", "abs": "https://arxiv.org/abs/2507.00025", "authors": ["Tiexin Qin", "Hong Yan", "Haoliang Li"], "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation", "comment": "Accepted by TPAMI 2025", "summary": "Learning the underlying dynamics from data with deep neural networks has\nshown remarkable potential in modeling various complex physical dynamics.\nHowever, current approaches are constrained in their ability to make reliable\npredictions in a specific domain and struggle with generalizing to unseen\nsystems that are governed by the same general dynamics but differ in\nenvironmental characteristics. In this work, we formulate a parameter-efficient\nmethod, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can\nreadily generalize to new dynamics via adaptation in the Fourier space.\nSpecifically, FNSDA identifies the shareable dynamics based on the known\nenvironments using an automatic partition in Fourier modes and learns to adjust\nthe modes specific for each new environment by conditioning on low-dimensional\nlatent systematic parameters for efficient generalization. We evaluate our\napproach on four representative families of dynamic systems, and the results\nshow that FNSDA can achieve superior or competitive generalization performance\ncompared to existing methods with a significantly reduced parameter cost. Our\ncode is available at https://github.com/WonderSeven/FNSDA.", "AI": {"tldr": "FNSDA\uff1a\u4e00\u79cd\u5085\u91cc\u53f6\u795e\u7ecf\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5728\u5085\u91cc\u53f6\u7a7a\u95f4\u4e2d\u8fdb\u884c\u81ea\u9002\u5e94\uff0c\u53ef\u4ee5\u8f7b\u677e\u5730\u63a8\u5e7f\u5230\u65b0\u7684\u52a8\u6001\uff0c\u5e76\u5728\u52a8\u6001\u7cfb\u7edf\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u53c2\u6570\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5b9a\u9886\u57df\u505a\u51fa\u53ef\u9760\u9884\u6d4b\u7684\u80fd\u529b\u6709\u9650\uff0c\u5e76\u4e14\u96be\u4ee5\u63a8\u5e7f\u5230\u53d7\u76f8\u540c\u4e00\u822c\u52a8\u529b\u5b66\u652f\u914d\u4f46\u5728\u73af\u5883\u7279\u5f81\u4e0a\u4e0d\u540c\u7684\u672a\u89c1\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5373\u7528\u4e8e\u52a8\u6001\u81ea\u9002\u5e94\u7684\u5085\u91cc\u53f6\u795e\u7ecf\u6a21\u62df\u5668 (FNSDA)\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u5728\u5085\u91cc\u53f6\u7a7a\u95f4\u4e2d\u8fdb\u884c\u81ea\u9002\u5e94\u6765\u8f7b\u677e\u5730\u63a8\u5e7f\u5230\u65b0\u7684\u52a8\u6001\u3002", "result": "FNSDA\u5728\u56db\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u52a8\u6001\u7cfb\u7edf\u5bb6\u65cf\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cFNSDA\u53ef\u4ee5\u5b9e\u73b0\u5353\u8d8a\u6216\u5177\u6709\u7ade\u4e89\u529b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u4e14\u53c2\u6570\u6210\u672c\u663e\u7740\u964d\u4f4e\u3002", "conclusion": "FNSDA\u5728\u52a8\u6001\u7cfb\u7edf\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u53c2\u6570\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002"}}
{"id": "2507.00726", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00726", "abs": "https://arxiv.org/abs/2507.00726", "authors": ["Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo", "Dongmin Park", "Jongho Park"], "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess", "comment": "27 pages", "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.", "AI": {"tldr": "RL for LLMs in chess strategic reasoning shows promise but plateaus below expert levels due to limitations in the pretrained models' understanding of chess.", "motivation": "Investigate whether LLMs can develop strategic reasoning capabilities through RL in chess, an area that remains largely unexplored.", "method": "Leverage a chess-pretrained action-value network to provide dense reward on the LLM's output move quality, which can be seen as a form of knowledge distillation.", "result": "Distillation-based dense rewards often outperform sparse binary rewards.", "conclusion": "LLMs plateau far below expert levels in chess strategic reasoning, suggesting a deficit in their internal understanding of chess that RL alone cannot fully overcome."}}
{"id": "2507.00224", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00224", "abs": "https://arxiv.org/abs/2507.00224", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "comment": "Accepted to AIED 2025 Late Breaking Results Track", "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts.", "AI": {"tldr": "This paper introduces FiboSB, a new dataset for 6D pose estimation in collaborative settings, evaluates existing methods, and fine-tunes YOLO11-x to improve object detection performance.", "motivation": "existing systems often lack the ability to accurately capture real-world interactions between students and physical objects. This issue could be addressed with automatic 6D pose estimation, i.e., estimation of an object's position and orientation in 3D space from RGB images or videos. For collaborative groups that interact with physical objects, 6D pose estimates allow AI systems to relate objects and entities.", "method": "introduce FiboSB, a novel and challenging 6D pose video dataset featuring groups of three participants solving an interactive task featuring small hand-held cubes and a weight scale. evaluated four state-of-the-art 6D pose estimation methods on FiboSB, exposing the limitations of current algorithms on collaborative group work. address this by fine-tuning YOLO11-x for FiboSB, achieving an overall mAP_50 of 0.898.", "result": "evaluated four state-of-the-art 6D pose estimation methods on FiboSB, exposing the limitations of current algorithms on collaborative group work. An error analysis of these methods reveals that the 6D pose methods' object detection modules fail. We address this by fine-tuning YOLO11-x for FiboSB, achieving an overall mAP_50 of 0.898.", "conclusion": "The dataset, benchmark results, and analysis of YOLO11-x errors presented here lay the groundwork for leveraging the estimation of 6D poses in difficult collaborative contexts."}}
{"id": "2507.00439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00439", "abs": "https://arxiv.org/abs/2507.00439", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u7b80\u5355\u7684\u76d1\u7763\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u4e0e\u4e0d\u540c\u4eba\u7fa4\u7684\u5bf9\u9f50\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\u4ee5\u4f9b\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e0d\u540c\u4eba\u7fa4\u5bf9\u4e3b\u89c2\u95ee\u9898\u7684\u56de\u7b54\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u76f8\u5bf9\u7b80\u5355\u7684\u76d1\u7763", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u76f8\u5bf9\u7b80\u5355\u7684\u76d1\u7763\u53ef\u4ee5\u5927\u5927\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u4e0e\u4e0d\u540c\u4eba\u7fa4\u7684\u5bf9\u9f50\u5ea6\uff0c\u8fd9\u662f\u901a\u8fc7\u8de8\u8d8a\u5404\u79cd\u4e3b\u9898\u7684\u4e09\u4e2a\u6570\u636e\u96c6\u6765\u8861\u91cf\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u591a\u4e2aLLM\u548cprompt\u7b56\u7565\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5f00\u6e90\u5176\u5de5\u4f5c\uff0c\u4e3a\u4e00\u4e2a\u6709\u7528\u7684\u57fa\u51c6\uff0c\u4ee5\u6fc0\u53d1\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2507.00026", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.00026", "abs": "https://arxiv.org/abs/2507.00026", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "AI": {"tldr": "This paper introduces ROSE, a new framework using reinforcement learning to generate diverse and realistic adversarial prompts for evaluating LLM safety, outperforming existing methods.", "motivation": "Existing manual safety benchmarks are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. Current automated methods often suffer from insufficient adversarial topic coverage and weak alignment with real-world contexts.", "method": "a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts.", "result": "ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics.", "conclusion": "ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with improvements in evaluation metrics. ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs."}}
{"id": "2507.00810", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.00810", "abs": "https://arxiv.org/abs/2507.00810", "authors": ["Qing Xu", "Xiaohua Xuan"], "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis", "comment": null, "summary": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6570\u503c\u7b97\u6cd5\u6765\u89e3\u51b3minimax\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u6536\u655b\u6027\u8bc1\u660e\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u4e2a\u9886\u57df", "motivation": "\u89e3\u51b3 minimax \u95ee\u9898", "method": "\u6539\u8fdb\u7684\u6570\u503c\u7b97\u6cd5\uff0c\u57fa\u4e8e\u975e\u5149\u6ed1\u4f18\u5316\uff0c\u4e8c\u6b21\u89c4\u5212\u548c\u8fed\u4ee3\u8fc7\u7a0b", "result": "\u4e3a\u8be5\u7b97\u6cd5\u5728\u68af\u5ea6\u8fde\u7eed\u6027\u548c\u6709\u754c\u6027\u7b49\u6e29\u548c\u5047\u8bbe\u4e0b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6536\u655b\u6027\u8bc1\u660e", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6570\u503c\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u975e\u5149\u6ed1\u4f18\u5316\u3001\u4e8c\u6b21\u89c4\u5212\u548c\u8fed\u4ee3\u8fc7\u7a0b\u7684 minimax \u95ee\u9898\uff0c\u5e76\u4e3a\u8be5\u7b97\u6cd5\u5728\u68af\u5ea6\u8fde\u7eed\u6027\u548c\u6709\u754c\u6027\u7b49\u6e29\u548c\u5047\u8bbe\u4e0b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6536\u655b\u6027\u8bc1\u660e\u3002 \u8be5\u7b97\u6cd5\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9c81\u68d2\u4f18\u5316\u3001\u4e0d\u5e73\u8861\u5b66\u4e60\u7b49\u9886\u57df\u3002"}}
{"id": "2507.00243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00243", "abs": "https://arxiv.org/abs/2507.00243", "authors": ["Chi-Yao Huang", "Zeel Bhatt", "Yezhou Yang"], "title": "VOCAL: Visual Odometry via ContrAstive Learning", "comment": null, "summary": "Breakthroughs in visual odometry (VO) have fundamentally reshaped the\nlandscape of robotics, enabling ultra-precise camera state estimation that is\ncrucial for modern autonomous systems. Despite these advances, many\nlearning-based VO techniques rely on rigid geometric assumptions, which often\nfall short in interpretability and lack a solid theoretical basis within fully\ndata-driven frameworks. To overcome these limitations, we introduce VOCAL\n(Visual Odometry via ContrAstive Learning), a novel framework that reimagines\nVO as a label ranking challenge. By integrating Bayesian inference with a\nrepresentation learning framework, VOCAL organizes visual features to mirror\ncamera states. The ranking mechanism compels similar camera states to converge\ninto consistent and spatially coherent representations within the latent space.\nThis strategic alignment not only bolsters the interpretability of the learned\nfeatures but also ensures compatibility with multimodal data sources. Extensive\nevaluations on the KITTI dataset highlight VOCAL's enhanced interpretability\nand flexibility, pushing VO toward more general and explainable spatial\nintelligence.", "AI": {"tldr": "VOCAL\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u6765\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u8bb8\u591a\u57fa\u4e8e\u5b66\u4e60\u7684VO\u6280\u672f\u4f9d\u8d56\u4e8e\u521a\u6027\u7684\u51e0\u4f55\u5047\u8bbe\uff0c\u8fd9\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e14\u5728\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u5185\u7f3a\u4e4f\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "VOCAL (Visual Odometry via ContrAstive Learning)\uff1a\u901a\u8fc7\u5c06\u8d1d\u53f6\u65af\u63a8\u7406\u4e0e\u8868\u793a\u5b66\u4e60\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u7ec4\u7ec7\u89c6\u89c9\u7279\u5f81\u4ee5\u53cd\u6620\u76f8\u673a\u72b6\u6001\uff0c\u5e76\u4f7f\u7528\u6392\u5e8f\u673a\u5236\u4fc3\u4f7f\u76f8\u4f3c\u7684\u76f8\u673a\u72b6\u6001\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6536\u655b\u6210\u4e00\u81f4\u4e14\u7a7a\u95f4\u8fde\u8d2f\u7684\u8868\u793a\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u7a81\u51fa\u4e86VOCAL\u7684\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "VOCAL\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5c06\u89c6\u89c9\u91cc\u7a0b\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6807\u7b7e\u6392\u5e8f\u95ee\u9898\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u63a8\u52a8VO\u671d\u7740\u66f4\u901a\u7528\u548c\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u667a\u80fd\u53d1\u5c55\u3002"}}
{"id": "2507.00460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00460", "abs": "https://arxiv.org/abs/2507.00460", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5728\u516c\u5171\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u7684\u201c\u4f5c\u5f0a\u201d\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f31\u70b9\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b9e\u9645\u6548\u679c\u4e0d\u4f73\uff0c\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\u3002", "motivation": "\u5f00\u653e\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u57fa\u51c6\uff08\u4f8b\u5982 HELM \u548c BIG-bench\uff09\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u900f\u660e\u7684\u534f\u8bae\uff0c\u6709\u52a9\u4e8e\u8bed\u8a00\u6a21\u578b (LM) \u7684\u516c\u5e73\u6bd4\u8f83\u3001\u53ef\u91cd\u590d\u6027\u548c\u8fed\u4ee3\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u5f00\u653e\u6027\u4e5f\u5e26\u6765\u4e86\u5173\u952e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u5728\u516c\u5171\u6d4b\u8bd5\u96c6\u4e0a\u76f4\u63a5\u5fae\u8c03 BART\u3001T5 \u548c GPT-2 \u7684\u8f83\u5c0f\u53d8\u4f53\u6765\u7cfb\u7edf\u5730\u6784\u5efa\u201c\u4f5c\u5f0a\u201d\u6a21\u578b", "result": "\u6784\u5efa\u7684\u201c\u4f5c\u5f0a\u201d\u6a21\u578b\u5728\u8457\u540d\u7684\u5f00\u653e\u5f0f\u6574\u4f53\u57fa\u51c6 (HELM) \u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u6392\u540d\uff0c\u5c3d\u7ba1\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u4e14\u5b9e\u7528\u6027\u6709\u9650\u3002", "conclusion": "\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u9ad8\u6392\u884c\u699c\u6027\u80fd\u53ef\u80fd\u5e76\u4e0d\u603b\u662f\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u6709\u6548\u6027\uff1b\u79c1\u6709\u6216\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u5fc5\u987b\u8865\u5145\u5f00\u653e\u8bc4\u4f30\u4ee5\u4fdd\u969c\u5b8c\u6574\u6027\uff1b\u5fc5\u987b\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u8bc4\u4f30\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\uff0c\u4ee5\u786e\u4fdd\u7a33\u5065\u548c\u53ef\u4fe1\u7684 LM \u8bc4\u4f30\u3002"}}
{"id": "2507.00028", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00028", "abs": "https://arxiv.org/abs/2507.00028", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "comment": null, "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "AI": {"tldr": "HiT-JEPA \u662f\u4e00\u79cd\u7528\u4e8e\u5b66\u4e60\u8de8\u8bed\u4e49\u62bd\u8c61\u7ea7\u522b\u7684\u591a\u5c3a\u5ea6\u57ce\u5e02\u8f68\u8ff9\u8868\u793a\u7684\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u6355\u83b7\u591a\u6837\u5316\u548c\u4e92\u8865\u4fe1\u606f\u7684\u8f68\u8ff9\u8868\u793a\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5355\u4e2a\u6a21\u578b\u4e2d\u7ed3\u5408\u8f68\u8ff9\u7ec6\u7c92\u5ea6\u7ec6\u8282\u548c\u9ad8\u7ea7\u6458\u8981\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4fdd\u6301\u5c40\u90e8\u7ec6\u5fae\u5dee\u522b\u7684\u540c\u65f6\u5173\u6ce8\u957f\u671f\u4f9d\u8d56\u6027\u7684\u80fd\u529b\u3002", "method": "HiT-JEPA\uff08\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u7684\u8f68\u8ff9\u8bed\u4e49\u5206\u5c42\u4ea4\u4e92\uff09", "result": "\u5728\u7528\u4e8e\u8f68\u8ff9\u76f8\u4f3c\u6027\u8ba1\u7b97\u7684\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e", "conclusion": "HiT-JEPA \u7684\u5206\u5c42\u8bbe\u8ba1\u4ea7\u751f\u4e86\u66f4\u4e30\u5bcc\u3001\u591a\u5c3a\u5ea6\u7684\u8868\u793a\u3002"}}
{"id": "2507.00841", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.00841", "abs": "https://arxiv.org/abs/2507.00841", "authors": ["Siyuan Liang", "Tianmeng Fang", "Zhe Liu", "Aishan Liu", "Yan Xiao", "Jinyuan He", "Ee-Chien Chang", "Xiaochun Cao"], "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents", "comment": "12 pages", "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u79fb\u52a8\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u5e8f\u5217\u4fe1\u606f\u7684\u98ce\u9669\u8bc6\u522b\u673a\u5236\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8f85\u52a9\u8bc4\u4f30\u65b9\u6848\uff0c\u521d\u6b65\u9a8c\u8bc1\u8868\u660e\u53ef\u4ee5\u63d0\u9ad8\u98ce\u9669\u8bc6\u522b\u5e76\u964d\u4f4e\u8d8a\u72f1\u6982\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u8d8a\u72f1\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u8bf1\u5bfc\u667a\u80fd\u4f53\u7ed5\u8fc7\u884c\u4e3a\u7ea6\u675f\uff0c\u89e6\u53d1\u98ce\u9669\u64cd\u4f5c\u3002\u73b0\u6709\u7684\u5b89\u5168\u63aa\u65bd\u5728\u590d\u6742\u4ea4\u4e92\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u9ad8\u6548\u4e00\u81f4\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6574\u5408\u884c\u4e3a\u5e8f\u5217\u4fe1\u606f\uff0c\u6784\u5efa\u98ce\u9669\u5224\u522b\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8f85\u52a9\u8bc4\u4f30\u65b9\u6848\u3002", "result": "\u5728\u51e0\u4e2a\u4ee3\u8868\u6027\u7684\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u521d\u6b65\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u5bf9\u98ce\u9669\u884c\u4e3a\u7684\u8bc6\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u5bf9\u98ce\u9669\u884c\u4e3a\u7684\u8bc6\u522b\uff0c\u5e76\u6709\u52a9\u4e8e\u964d\u4f4e\u667a\u80fd\u4f53\u88ab\u7834\u89e3\u7684\u53ef\u80fd\u6027\u3002\u5e0c\u671b\u8fd9\u9879\u7814\u7a76\u80fd\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\u5efa\u6a21\u548c\u4fdd\u62a4\u63d0\u4f9b\u4e00\u4e9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2507.00248", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00248", "abs": "https://arxiv.org/abs/2507.00248", "authors": ["Nikita Nikitin", "Eugene Fomin"], "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "AI": {"tldr": "This paper presents a novel framework for real-time sign language recognition using lightweight DNNs trained on limited data, achieving 92% accuracy in isolated sign recognition with less than 10ms latency on edge devices.", "motivation": "The motivation is to address key challenges in sign language recognition, including data scarcity, high computational costs, and discrepancies in frame rates between training and inference environments.", "method": "The method involves encoding sign language specific parameters into vectorized inputs, leveraging MediaPipe for landmark extraction, and using a DNN architecture optimized for sub 10MB deployment.", "result": "The model enables accurate classification of 343 signs with less than 10ms latency on edge devices.", "conclusion": "The model achieves 92% accuracy in isolated sign recognition and has been integrated into the 'slait ai' web application, where it demonstrates stable inference."}}
{"id": "2507.00509", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00509", "abs": "https://arxiv.org/abs/2507.00509", "authors": ["To Eun Kim", "Jo\u00e3o Coelho", "Gbemileke Onilude", "Jai Singh"], "title": "TeamCMU at Touch\u00e9: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u57fa\u4e8eRAG\u7684\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7ba1\u7406\u5e7f\u544a\u7684\u6a21\u5757\u5316\u7ba1\u9053\uff0c\u5e76\u5229\u7528\u5bf9\u6297\u6027\u534f\u540c\u8fdb\u5316\u6846\u67b6\u6765\u5f00\u53d1\u66f4\u590d\u6742\u7684\u5e7f\u544a\u611f\u77e5\u751f\u6210\u641c\u7d22\u7cfb\u7edf\u548c\u5f3a\u5927\u7684\u5e7f\u544a\u5206\u7c7b\u5668\u3002", "motivation": "\u968f\u7740\u4f1a\u8bdd\u641c\u7d22\u5f15\u64ce\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u9a71\u52a8\u7684\u57fa\u4e8e\u751f\u6210\u7684\u8303\u4f8b\uff0c\u5c06\u5e7f\u544a\u96c6\u6210\u5230\u751f\u6210\u7684\u54cd\u5e94\u4e2d\u65e2\u5e26\u6765\u4e86\u5546\u4e1a\u673a\u4f1a\uff0c\u4e5f\u7ed9\u7528\u6237\u4f53\u9a8c\u5e26\u6765\u4e86\u6311\u6218\u3002\u4e0e\u5e7f\u544a\u660e\u786e\u5212\u5206\u7684\u4f20\u7edf\u641c\u7d22\u4e0d\u540c\uff0c\u751f\u6210\u7cfb\u7edf\u6a21\u7cca\u4e86\u4fe1\u606f\u5185\u5bb9\u548c\u4fc3\u9500\u6750\u6599\u4e4b\u95f4\u7684\u754c\u9650\uff0c\u5f15\u53d1\u4e86\u5bf9\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u7684\u62c5\u5fe7\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eRAG\u7684\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5e7f\u544a\u7ba1\u7406\u7684\u6a21\u5757\u5316\u7ba1\u9053\uff0c\u5305\u62ec\u7528\u4e8e\u65e0\u7f1d\u5e7f\u544a\u96c6\u6210\u7684\u5e7f\u544a\u91cd\u5199\u5668\u548c\u7528\u4e8e\u68c0\u6d4b\u7684\u9c81\u68d2\u5e7f\u544a\u5206\u7c7b\u5668\u3002\u5229\u7528\u5408\u6210\u6570\u636e\u6765\u8bad\u7ec3\u9ad8\u6027\u80fd\u5206\u7c7b\u5668\uff0c\u7136\u540e\u7528\u4e8e\u6307\u5bfc\u4e24\u79cd\u4e92\u8865\u7684\u5e7f\u544a\u96c6\u6210\u7b56\u7565\uff1a\u5e7f\u544a\u91cd\u5199\u5668\u7684\u76d1\u7763\u5fae\u8c03\u548cbest-of-N\u62bd\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5019\u9009\u8005\u4e2d\u9009\u62e9\u6700\u4e0d\u660e\u663e\u7684\u5e7f\u544a\u96c6\u6210\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7814\u7a76\u7684\u5e7f\u544a\u5206\u7c7b\u5668\u5728\u8425\u9500\u7b56\u7565\u7684\u542f\u53d1\u4e0b\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u589e\u5f3a\u7684\u5408\u6210\u5e7f\u544a\u6570\u636e\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5fae\u8c03\u548cbest-of-N\u62bd\u6837\uff0c\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u5e7f\u544a\u7684\u9690\u853d\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u65e0\u7f1d\u7684\u96c6\u6210\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u534f\u540c\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u66f4\u590d\u6742\u7684\u5e7f\u544a\u611f\u77e5\u751f\u6210\u641c\u7d22\u7cfb\u7edf\u548c\u5f3a\u5927\u7684\u5e7f\u544a\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u5fae\u8c03\u548cbest-of-N\u62bd\u6837\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e7f\u544a\u7684\u9690\u853d\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u65e0\u7f1d\u7684\u96c6\u6210\u3002"}}
{"id": "2507.00029", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00029", "abs": "https://arxiv.org/abs/2507.00029", "authors": ["Wenbing Li", "Zikai Song", "Hang Zhou", "Yunyao Zhang", "Junqing Yu", "Wei Yang"], "title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "comment": null, "summary": "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts\n(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit\nprevailing limitations: they either swap entire attention/feed-forward layers\nfor switch experts or bolt on parallel expert branches, diluting parameter\nefficiency and task fidelity. We propose the LoRA-Mixer, a modular and\nlightweight MoE framework that integrates LoRA experts. Our core innovation\nlies in replacing the projection matrices of the attention module's\ninput/output linear layers with dynamically routed, task-specific LoRA experts.\nThis design ensures seamless compatibility with diverse foundation models,\nincluding transformers and state space models (SSMs), by leveraging their\ninherent linear projection structures. The framework supports two operational\nparadigms: (1) joint optimization of LoRA experts and routing mechanisms via a\nnovel hard-soft routing strategy, or (2) direct deployment of pre-trained,\nfrozen LoRA modules sourced from external repositories. To enable robust router\ntraining with limited data while ensuring stable routing decisions and\nmaximizing expert reuse, we introduce an adaptive Specialization Balance Loss\n(SBL) that jointly optimizes expert balance and task-specific alignment.\nExtensive experiments on seven benchmark datasets, including MedQA, CoLA,\nSST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of\nLoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer\nachieves significant improvements of 7.61%, 4.88%, and 3.08% over the base\nmodels, respectively. Compared with state-of-the-art methods, LoRA-Mixer\nachieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,\nusing only 48% of the parameters, demonstrating its efficiency and strong\nperformance.", "AI": {"tldr": "LoRA-Mixer is a parameter-efficient MoE framework that integrates LoRA experts for adapting LLMs, achieving strong performance on various benchmarks.", "motivation": "Combining LoRA with MoE for adapting LLMs has limitations in parameter efficiency and task fidelity.", "method": "The LoRA-Mixer replaces projection matrices with dynamically routed, task-specific LoRA experts and introduces an adaptive Specialization Balance Loss (SBL) for router training.", "result": "LoRA-Mixer achieves improvements of 7.61%, 4.88%, and 3.08% on GSM8K, HumanEval, and MedQA, respectively, and outperforms state-of-the-art methods with only 48% of the parameters.", "conclusion": "LoRA-Mixer achieves significant improvements over base models and state-of-the-art methods with fewer parameters, demonstrating its efficiency and strong performance."}}
{"id": "2507.00951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00951", "abs": "https://arxiv.org/abs/2507.00951", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Philip Torr", "Seyedali Mirjalili"], "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "comment": null, "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "AI": {"tldr": "This paper discusses the limitations of current AI models and explores pathways towards AGI, emphasizing the importance of memory, reasoning, and modularity.", "motivation": "Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency.", "method": "cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior.", "result": "VLMs are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior.", "conclusion": "True intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior."}}
{"id": "2507.00253", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00253", "abs": "https://arxiv.org/abs/2507.00253", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "comment": null, "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet.", "AI": {"tldr": "GazeTarget360\u662f\u4e00\u4e2a\u65b0\u7684\u7cfb\u7edf\uff0c\u53ef\u4ee5\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1360\u5ea6\u7684\u89c6\u7ebf\u76ee\u6807\uff0c\u5373\u4f7f\u4eba\u4eec\u6ca1\u6709\u770b\u7740\u76f8\u673a\u3002", "motivation": "\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u4eba\u7c7b\u7684\u89c6\u7ebf\u76ee\u6807\u662f\u5b9e\u73b0\u4e0b\u6e38\u4efb\u52a1\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f8b\u5982\uff0c\u73b0\u5b9e\u4e16\u754c\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6ce8\u610f\u529b\u4f30\u8ba1\u548c\u8fd0\u52a8\u9884\u6d4b\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u5df2\u7ecf\u901a\u8fc7\u4ed4\u7ec6\u79fb\u9664\u6846\u67b6\u5916\u6837\u672c\uff0c\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u6846\u67b6\u5185\u76ee\u6807\u5b9a\u4f4d\u95ee\u9898\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u89c6\u89c9\u7684\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f8b\u5982OpenFace\uff0c\u4e0d\u80fd\u6709\u6548\u5730\u5438\u6536\u56fe\u50cf\u4e2d\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u5e76\u4e14\u4e0d\u80fd\u5728\u53d7\u8bd5\u8005\u770b\u5411\u8fdc\u79bb\u76f8\u673a\u7684\u5730\u65b9\u65f6\u9884\u6d4b\u89c6\u7ebf\u76ee\u6807\u3002", "method": "\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u773c\u795e\u4ea4\u6d41\u68c0\u6d4b\u5668\u3001\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u89e3\u7801\u5668\u7684\u6761\u4ef6\u63a8\u7406\u5f15\u64ce\u3002", "result": "\u4ea4\u53c9\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0cGazeTarget360\u7cfb\u7edf\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u4ea7\u751f\u51c6\u786e\u4e14\u53ef\u9760\u7684\u89c6\u7ebf\u76ee\u6807\u9884\u6d4b\u3002", "conclusion": "GazeTarget360\u7cfb\u7edf\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u4ea7\u751f\u51c6\u786e\u4e14\u53ef\u9760\u7684\u89c6\u7ebf\u76ee\u6807\u9884\u6d4b\uff0c\u4f7f\u5176\u6210\u4e3a\u9996\u4e2a\u80fd\u591f\u4ece\u771f\u5b9e\u76f8\u673a\u955c\u5934\u4e2d\u9884\u6d4b\u89c6\u7ebf\u76ee\u6807\u7684\u9ad8\u6548\u4e14\u53ef\u90e8\u7f72\u7684\u7cfb\u7edf\u3002"}}
{"id": "2507.00534", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00534", "abs": "https://arxiv.org/abs/2507.00534", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "AI": {"tldr": "Nirantar \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df ASR \u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5728\u5370\u5ea6\u6536\u96c6\u7684\u589e\u91cf\u6570\u636e\uff0c\u53ef\u4ee5\u8bc4\u4f30\u8bed\u8a00\u589e\u91cf (LIL)\u3001\u9886\u57df\u589e\u91cf (DIL) \u548c\u65b0\u7684\u8bed\u8a00\u589e\u91cf\u9886\u57df\u589e\u91cf\u5b66\u4e60 (LIDIL) \u573a\u666f\u3002", "motivation": "\u4e3a\u4e86\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7684 CL \u6311\u6218\uff0cNirantar \u5229\u7528\u901a\u8fc7\u81ea\u7136\u4e8b\u4ef6\u5728\u5370\u5ea6 22 \u79cd\u8bed\u8a00\u548c 208 \u4e2a\u5730\u533a\u6536\u96c6\u7684\u589e\u91cf\u6570\u636e\u3002", "method": "Nirantar\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df ASR \u4e2d\u7684\u6301\u7eed\u5b66\u4e60 (CL)\u3002", "result": "\u901a\u8fc7\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u80fd\u59cb\u7ec8\u5982\u4e00\u5730\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684 CL \u7b56\u7565\u3002"}}
{"id": "2507.00030", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00030", "abs": "https://arxiv.org/abs/2507.00030", "authors": ["Abhishek Verma", "Nallarasan V", "Balaraman Ravindran"], "title": "Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in complex\nsequential decision-making tasks, such as playing Atari 2600 games and\nmastering board games. A critical yet underexplored aspect of DRL is the\ntemporal scale of action execution. We propose a novel paradigm that integrates\ncontextual bandits with DRL to adaptively select action durations, enhancing\npolicy flexibility and computational efficiency. Our approach augments a Deep\nQ-Network (DQN) with a contextual bandit module that learns to choose optimal\naction repetition rates based on state contexts. Experiments on Atari 2600\ngames demonstrate significant performance improvements over static duration\nbaselines, highlighting the efficacy of adaptive temporal abstractions in DRL.\nThis paradigm offers a scalable solution for real-time applications like gaming\nand robotics, where dynamic action durations are critical.", "AI": {"tldr": "DRL with contextual bandits selects action durations, improving performance and efficiency.", "motivation": "temporal scale of action execution is a critical yet underexplored aspect of DRL", "method": "integrates contextual bandits with DRL to adaptively select action durations", "result": "significant performance improvements over static duration baselines on Atari 2600 games", "conclusion": "adaptive temporal abstractions in DRL improve performance on Atari 2600 games"}}
{"id": "2507.00979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00979", "abs": "https://arxiv.org/abs/2507.00979", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "AI": {"tldr": "This paper presents CIP, a technique using causal influence diagrams to make LLM-powered agents safer, with positive results in code execution and mobile device control.", "motivation": "Ensuring the safe and reliable behavior of autonomous agents powered by large language models (LLMs) is crucial for preventing unintended consequences.", "method": "The method consists of three key steps: (1) initializing a CID based on task specifications, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes.", "result": "Experimental results demonstrate that the proposed method effectively enhances safety in both code execution and mobile device control tasks.", "conclusion": "This paper introduces a novel technique called CIP that uses causal influence diagrams (CIDs) to improve the safety of autonomous agents powered by large language models (LLMs). Experimental results show that CIP enhances safety in code execution and mobile device control tasks."}}
{"id": "2507.00261", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.00261", "abs": "https://arxiv.org/abs/2507.00261", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "comment": null, "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "AI": {"tldr": "VirtualFencer extracts 3D fencing motion and strategy from video to generate realistic fencing behavior, showcasing its capabilities through self-play, fencing against real fencers' motion, and interactive fencing against a professional.", "motivation": "The combination of motion diversity with underlying two-player strategy motivates the application of data-driven modeling to fencing.", "method": "A system capable of extracting 3D fencing motion and strategy from in-the-wild video without supervision, and then using that extracted knowledge to generate realistic fencing behavior.", "result": "The system demonstrates versatile capabilities.", "conclusion": "The system can fence against itself, a real fencer's motion, and interactively against a professional fencer."}}
{"id": "2507.00540", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00540", "abs": "https://arxiv.org/abs/2507.00540", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Capsule \u7f51\u7edc\u7684\u8bed\u4e49\u610f\u56fe\u5efa\u6a21\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u4e2d\u610f\u56fe\u8bc6\u522b\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u610f\u56fe\u8bc6\u522b\u51c6\u786e\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Capsule \u7f51\u7edc\u7684\u8bed\u4e49\u610f\u56fe\u5efa\u6a21\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001F1 \u503c\u548c\u610f\u56fe\u68c0\u6d4b\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u7ed3\u6784\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u8bed\u4e49\u6761\u4ef6\u4e0b\u7684\u610f\u56fe\u8bc6\u522b\u3002"}}
{"id": "2507.00031", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00031", "abs": "https://arxiv.org/abs/2507.00031", "authors": ["Chuan Li", "Jiang You", "Hassine Moungla", "Vincent Gauthier", "Miguel Nunez-del-Prado", "Hugo Alatrista-Salas"], "title": "Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru", "comment": null, "summary": "Accurate modeling of human mobility is critical for understanding epidemic\nspread and deploying timely interventions. In this work, we leverage a\nlarge-scale spatio-temporal dataset collected from Peru's national Digital\nContact Tracing (DCT) application during the COVID-19 pandemic to forecast\nmobility flows across urban regions. A key challenge lies in the spatial\nsparsity of hourly mobility counts across hexagonal grid cells, which limits\nthe predictive power of conventional time series models. To address this, we\npropose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)\ntechnique that augments each cell's features with aggregated signals from its\nimmediate H3 neighbors. We evaluate this strategy on three forecasting\nbackbones: NLinear, PatchTST, and K-U-Net, under various historical input\nlengths. Experimental results show that SPN consistently improves forecasting\nperformance, achieving up to 9.85 percent reduction in test MSE. Our findings\ndemonstrate that spatial smoothing of sparse mobility signals provides a simple\nyet effective path toward robust spatio-temporal forecasting during public\nhealth crises.", "AI": {"tldr": "This paper proposes a spatial neighborhood fusion technique to improve the forecasting of mobility flows across urban regions using a large-scale spatio-temporal dataset collected from Peru's national Digital Contact Tracing (DCT) application during the COVID-19 pandemic. The technique addresses the spatial sparsity of hourly mobility counts and improves forecasting performance.", "motivation": "Accurate modeling of human mobility is critical for understanding epidemic spread and deploying timely interventions. A key challenge lies in the spatial sparsity of hourly mobility counts across hexagonal grid cells, which limits the predictive power of conventional time series models.", "method": "A lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN) technique that augments each cell's features with aggregated signals from its immediate H3 neighbors.", "result": "SPN consistently improves forecasting performance, achieving up to 9.85 percent reduction in test MSE.", "conclusion": "Spatial smoothing of sparse mobility signals provides a simple yet effective path toward robust spatio-temporal forecasting during public health crises."}}
{"id": "2507.00263", "categories": ["cs.CV", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.00263", "abs": "https://arxiv.org/abs/2507.00263", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "comment": null, "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "AI": {"tldr": "This paper introduces a machine learning pipeline for room scene discovery, grouping, and bed type identification in vacation rentals, addressing the problem of unstructured property images and outperforming existing methods.", "motivation": "Travelers face challenges in understanding the spatial layout of vacation rental properties due to the lack of structured categorization of property images, especially when multiple rooms of the same type are present.", "method": "A computationally efficient machine learning pipeline integrating a supervised room-type detection model, a supervised overlap detection model, and a clustering algorithm, along with a Multi-modal Large Language Model (MLLM) for bed type identification.", "result": "The models were evaluated individually and the pipeline was assessed in its entirety, showing strong performance.", "conclusion": "The proposed pipeline demonstrates strong performance, significantly outperforming established approaches like contrastive learning and clustering with pretrained embeddings."}}
{"id": "2507.00547", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00547", "abs": "https://arxiv.org/abs/2507.00547", "authors": ["Malmi Amadoru"], "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u5728\u4e3b\u9898\u5efa\u6a21\u4e2d\u786e\u4fdd\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u5957\u6307\u5357\uff0c\u8fd9\u4e9b\u6307\u5357\u53ef\u4ee5\u5e2e\u52a9\u65b0\u624b\u7814\u7a76\u8005\u548c\u5ba1\u7a3f\u4eba\u3002", "motivation": "\u9ad8\u7ea7\u8ba1\u7b97\u7b97\u6cd5\u7684\u5174\u8d77\u4e3a\u8ba1\u7b97\u5bc6\u96c6\u578b\u7814\u7a76\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u8fd9\u4e9b\u7b97\u6cd5\u7684\u4e0d\u900f\u660e\u6027\u4ee5\u53ca\u5e94\u7528\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u4e25\u8c28\u6027\u5e26\u6765\u4e86\u65b9\u6cd5\u8bba\u4e0a\u7684\u6311\u6218\uff0c\u53ef\u80fd\u4f1a\u524a\u5f31\u5bf9\u7814\u7a76\u7684\u4fe1\u4efb\u3002\u5173\u4e8e\u8fd9\u79cd\u65b0\u7814\u7a76\u7c7b\u578b\u4e2d\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u7684\u8ba8\u8bba\u4ecd\u5728\u51fa\u73b0\u3002", "method": "\u901a\u8fc7\u8bf4\u660e\u7ed3\u6784\u4e3b\u9898\u5efa\u6a21\u7b97\u6cd5\u7684\u5e94\u7528\u5e76\u63d0\u51fa\u4e00\u5957\u6307\u5357", "result": "\u8ba8\u8bba\u4e86\u5982\u4f55\u786e\u4fdd\u4e3b\u9898\u5efa\u6a21\u7814\u7a76\u7684\u4e25\u8c28\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u6307\u5357", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u4e3a\u786e\u4fdd\u4e3b\u9898\u5efa\u6a21\u7814\u7a76\u7684\u4e25\u8c28\u6027\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u6307\u5357\uff0c\u8fd9\u4e9b\u6307\u5357\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u5e94\u7528\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002\u8fd9\u4e9b\u6307\u5357\u5bf9\u65b0\u624b\u7814\u7a76\u8005\u4ee5\u53ca\u5904\u7406\u4e3b\u9898\u5efa\u6a21\u624b\u7a3f\u7684\u7f16\u8f91\u548c\u5ba1\u9605\u8005\u5c24\u5176\u6709\u5e2e\u52a9\u3002\u8be5\u7814\u7a76\u4e3a\u4e3b\u9898\u5efa\u6a21\u6587\u732e\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u52a0\u5165\u4e86\u5173\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u578b\u7406\u8bba\u6784\u5efa\u7814\u7a76\u4e2d\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u7684\u65b0\u5174\u5bf9\u8bdd\u3002"}}
{"id": "2507.00034", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.00034", "abs": "https://arxiv.org/abs/2507.00034", "authors": ["Reece Bourisaw", "Reid McCants", "Jean-Marie Le Corre", "Anna Iskhakova", "Arsen S. Iskhakov"], "title": "Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark", "comment": null, "summary": "Critical heat flux (CHF) marks the onset of boiling crisis in light-water\nreactors, defining safe thermal-hydraulic operating limits. To support Phase II\nof the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power\nprofiles, this work compiles and digitizes a broad CHF dataset covering both\nuniform and non-uniform axial heating conditions. Heating profiles were\nextracted from technical reports, interpolated onto a consistent axial mesh,\nvalidated via energy-balance checks, and encoded in machine-readable formats\nfor benchmark compatibility.\n  Classical CHF correlations exhibit substantial errors under uniform heating\nand degrade markedly when applied to non-uniform profiles, while modern tabular\nmethods offer improved but still imperfect predictions. A neural network\ntrained solely on uniform data performs well in that regime but fails to\ngeneralize to spatially varying scenarios, underscoring the need for models\nthat explicitly incorporate axial power distributions. By providing these\ncurated datasets and baseline modeling results, this study lays the groundwork\nfor advanced transfer-learning strategies, rigorous uncertainty quantification,\nand design-optimization efforts in the next phase of the CHF benchmark.", "AI": {"tldr": "This study compiles a CHF dataset for both uniform and non-uniform heating conditions to support the OECD/NEA AI/ML CHF benchmark. It highlights the limitations of existing models and sets the stage for advanced modeling techniques.", "motivation": "Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. The OECD/NEA AI/ML CHF benchmark introduces spatially varying power profiles.", "method": "This work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats.", "result": "The curated datasets and baseline modeling results lay the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.", "conclusion": "Classical CHF correlations exhibit substantial errors and degrade markedly when applied to non-uniform profiles, while a neural network trained solely on uniform data fails to generalize to spatially varying scenarios."}}
{"id": "2507.00287", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00287", "abs": "https://arxiv.org/abs/2507.00287", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "title": "Self-Supervised Multiview Xray Matching", "comment": "MICCAI 2025", "summary": "Accurate interpretation of multi-view radiographs is crucial for diagnosing\nfractures, muscular injuries, and other anomalies. While significant advances\nhave been made in AI-based analysis of single images, current methods often\nstruggle to establish robust correspondences between different X-ray views, an\nessential capability for precise clinical evaluations. In this work, we present\na novel self-supervised pipeline that eliminates the need for manual annotation\nby automatically generating a many-to-many correspondence matrix between\nsynthetic X-ray views. This is achieved using digitally reconstructed\nradiographs (DRR), which are automatically derived from unannotated CT volumes.\nOur approach incorporates a transformer-based training phase to accurately\npredict correspondences across two or more X-ray views. Furthermore, we\ndemonstrate that learning correspondences among synthetic X-ray views can be\nleveraged as a pretraining strategy to enhance automatic multi-view fracture\ndetection on real data. Extensive evaluations on both synthetic and real X-ray\ndatasets show that incorporating correspondences improves performance in\nmulti-view fracture classification.", "AI": {"tldr": "This paper presents a self-supervised pipeline to generate a many-to-many correspondence matrix between synthetic X-ray views, and demonstrates that learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data.", "motivation": "current methods often struggle to establish robust correspondences between different X-ray views, an essential capability for precise clinical evaluations", "method": "a novel self-supervised pipeline that eliminates the need for manual annotation by automatically generating a many-to-many correspondence matrix between synthetic X-ray views", "result": "learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data", "conclusion": "incorporating correspondences improves performance in multi-view fracture classification"}}
{"id": "2507.00579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00579", "abs": "https://arxiv.org/abs/2507.00579", "authors": ["Miriam Ansch\u00fctz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "AI": {"tldr": "Proposes a multilingual hallucination identifier combining retrieval-based fact verification and a BERT-based system, achieving competitive results across multiple languages.", "motivation": "Hallucinations are one of the major problems of LLMs, hindering their trustworthiness and deployment to wider use cases. However, most of the research on hallucinations focuses on English data, neglecting the multilingual nature of LLMs.", "method": "a two-part pipeline that combines retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned to identify common hallucination patterns", "result": "achieves competitive results across all languages, reaching top-10 results in eight languages, including English. Moreover, it supports multiple languages beyond the fourteen covered by the shared task", "conclusion": "This multilingual hallucination identifier can help to improve LLM outputs and their usefulness in the future."}}
{"id": "2507.00036", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.00036", "abs": "https://arxiv.org/abs/2507.00036", "authors": ["Rohan Putatunda", "Sanjay Purushotham", "Ratnaksha Lele", "Vandana P. Janeja"], "title": "IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting", "comment": "16 pages, 4 figures", "summary": "Drifting icebergs in the polar oceans play a key role in the Earth's climate\nsystem, impacting freshwater fluxes into the ocean and regional ecosystems\nwhile also posing a challenge to polar navigation. However, accurately\nforecasting iceberg trajectories remains a formidable challenge, primarily due\nto the scarcity of spatiotemporal data and the complex, nonlinear nature of\niceberg motion, which is also impacted by environmental variables. The iceberg\nmotion is influenced by multiple dynamic environmental factors, creating a\nhighly variable system that makes trajectory identification complex. These\nlimitations hinder the ability of deep learning models to effectively capture\nthe underlying dynamics and provide reliable predictive outcomes. To address\nthese challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep\nlearning model that combines an analytical formulation of iceberg drift\nphysics, with an augmented residual learning model. The model learns the\npattern of mismatch between the analytical solution and ground-truth\nobservations, which is combined with a rotate-augmented spectral neural network\nthat captures both global and local patterns from the data to forecast future\niceberg drift positions. We compare IDRIFTNET model performance with\nstate-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings\ndemonstrate that IDRIFTNET outperforms other models by achieving a lower Final\nDisplacement Error (FDE) and Average Displacement Error (ADE) across a variety\nof time points. These results highlight IDRIFTNET's effectiveness in capturing\nthe complex, nonlinear drift of icebergs for forecasting iceberg trajectories\nunder limited data and dynamic environmental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7684\u3001\u7269\u7406\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bIDRIFTNET\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u51b0\u5c71\u8f68\u8ff9\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u51b0\u5c71\u8f68\u8ff9\u4ecd\u7136\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u65f6\u7a7a\u6570\u636e\u7684\u7a00\u7f3a\u4ee5\u53ca\u51b0\u5c71\u8fd0\u52a8\u7684\u590d\u6742\u975e\u7ebf\u6027\u6027\u8d28\uff0c\u51b0\u5c71\u8fd0\u52a8\u53d7\u5230\u591a\u79cd\u52a8\u6001\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408IDRIFTNET\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u51b0\u5c71\u6f02\u79fb\u7269\u7406\u7684\u89e3\u6790\u516c\u5f0f\u548c\u4e00\u4e2a\u589e\u5f3a\u7684\u6b8b\u5dee\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u65cb\u8f6c\u589e\u5f3a\u7684\u9891\u8c31\u795e\u7ecf\u7f51\u7edc\uff0c\u4ece\u6570\u636e\u4e2d\u6355\u83b7\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u5f0f\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u7684\u51b0\u5c71\u6f02\u79fb\u4f4d\u7f6e\u3002", "result": "IDRIFTNET\u6a21\u578b\u5728\u4e24\u4e2a\u5357\u6781\u51b0\u5c71A23A\u548cB22A\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5728\u5404\u79cd\u65f6\u95f4\u70b9\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u6700\u7ec8\u4f4d\u79fb\u8bef\u5dee\uff08FDE\uff09\u548c\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\uff08ADE\uff09\u3002", "conclusion": "IDRIFTNET\u6a21\u578b\u5728\u9884\u6d4b\u51b0\u5c71\u8f68\u8ff9\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u51b0\u5c71\u5728\u6709\u9650\u6570\u636e\u548c\u52a8\u6001\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u590d\u6742\u975e\u7ebf\u6027\u6f02\u79fb\u3002"}}
{"id": "2507.00292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00292", "abs": "https://arxiv.org/abs/2507.00292", "authors": ["Ali Mammadov", "Lo\u00efc Le Folgoc", "Guillaume Hocquet", "Pietro Gori"], "title": "Reducing Variability of Multiple Instance Learning Methods for Digital Pathology", "comment": "MICCAI 2025", "summary": "Digital pathology has revolutionized the field by enabling the digitization\nof tissue samples into whole slide images (WSIs). However, the high resolution\nand large size of WSIs present significant challenges when it comes to applying\nDeep Learning models. As a solution, WSIs are often divided into smaller\npatches with a global label (\\textit{i.e., diagnostic}) per slide, instead of a\n(too) costly pixel-wise annotation. By treating each slide as a bag of patches,\nMultiple Instance Learning (MIL) methods have emerged as a suitable solution\nfor WSI classification. A major drawback of MIL methods is their high\nvariability in performance across different runs, which can reach up to 10-15\nAUC points on the test set, making it difficult to compare different MIL\nmethods reliably. This variability mainly comes from three factors: i) weight\ninitialization, ii) batch (shuffling) ordering, iii) and learning rate. To\naddress that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL\nmethods. We first train multiple models for a few epochs and average the most\nstable and promising ones based on validation scores. This approach can be\napplied to any existing MIL model to reduce performance variability. It also\nsimplifies hyperparameter tuning and improves reproducibility while maintaining\ncomputational efficiency. We extensively validate our approach on WSI\nclassification tasks using 2 different datasets, 3 initialization strategies\nand 5 MIL methods, for a total of more than 2000 experiments.", "AI": {"tldr": "Introduces a Multi-Fidelity, Model Fusion strategy for MIL methods to reduce performance variability and improve reproducibility in WSI classification tasks.", "motivation": "High variability in performance across different runs of Multiple Instance Learning (MIL) methods, which can reach up to 10-15 AUC points on the test set, making it difficult to compare different MIL methods reliably. This variability mainly comes from three factors: i) weight initialization, ii) batch (shuffling) ordering, iii) and learning rate.", "method": "The authors introduce a Multi-Fidelity, Model Fusion strategy for MIL methods, where they train multiple models for a few epochs and average the most stable and promising ones based on validation scores.", "result": "The proposed approach reduces performance variability, simplifies hyperparameter tuning and improves reproducibility while maintaining computational efficiency. The approach is validated on WSI classification tasks.", "conclusion": "The authors introduce a Multi-Fidelity, Model Fusion strategy for MIL methods. They train multiple models for a few epochs and average the most stable and promising ones based on validation scores. This approach can be applied to any existing MIL model to reduce performance variability, simplifies hyperparameter tuning and improves reproducibility while maintaining computational efficiency. The approach is validated on WSI classification tasks using 2 different datasets, 3 initialization strategies and 5 MIL methods, for a total of more than 2000 experiments."}}
{"id": "2507.00601", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00601", "abs": "https://arxiv.org/abs/2507.00601", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "AI": {"tldr": "A unified framework combining knowledge transfer and parameter-efficient fine-tuning enhances LLM adaptation in low-resource languages, achieving superior performance and stability.", "motivation": "This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios.", "method": "This paper proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies, introducing knowledge alignment loss and soft prompt tuning.", "result": "The proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X, demonstrating particularly strong advantages under extremely data-scarce conditions.", "conclusion": "The proposed method enhances task-specific adaptability while preserving the general capabilities of large language models, making it well-suited for complex semantic modeling and multilingual processing tasks."}}
{"id": "2507.00037", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1"], "pdf": "https://arxiv.org/pdf/2507.00037", "abs": "https://arxiv.org/abs/2507.00037", "authors": ["Phoomraphee Luenam", "Andreas Spanopoulos", "Amit Sant", "Thomas Hofmann", "Sotiris Anagnostidis", "Sidak Pal Singh"], "title": "Model Fusion via Neuron Interpolation", "comment": "5 figures, 15 tables, 23 pages", "summary": "Model fusion aims to combine the knowledge of multiple models by creating one\nrepresentative model that captures the strengths of all of its parents.\nHowever, this process is non-trivial due to differences in internal\nrepresentations, which can stem from permutation invariance, random\ninitialization, or differently distributed training data. We present a novel,\nneuron-centric family of model fusion algorithms designed to integrate multiple\ntrained neural networks into a single network effectively regardless of\ntraining data distribution. Our algorithms group intermediate neurons of parent\nmodels to create target representations that the fused model approximates with\nits corresponding sub-network. Unlike prior approaches, our approach\nincorporates neuron attribution scores into the fusion process. Furthermore,\nour algorithms can generalize to arbitrary layer types. Experimental results on\nvarious benchmark datasets demonstrate that our algorithms consistently\noutperform previous fusion techniques, particularly in zero-shot and non-IID\nfusion scenarios. The code is available at\nhttps://github.com/AndrewSpano/neuron-interpolation-model-fusion.", "AI": {"tldr": "This paper introduces neuron-centric model fusion algorithms that effectively integrate multiple neural networks into a single network, outperforming previous methods, especially in challenging scenarios.", "motivation": "Model fusion aims to combine the knowledge of multiple models, but differences in internal representations due to permutation invariance, random initialization, or differently distributed training data make this process non-trivial.", "method": "A novel, neuron-centric family of model fusion algorithms is presented to integrate multiple trained neural networks into a single network. The algorithms group intermediate neurons of parent models to create target representations that the fused model approximates with its corresponding sub-network. Neuron attribution scores are incorporated into the fusion process, and the algorithms can generalize to arbitrary layer types.", "result": "The proposed algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID fusion scenarios, on various benchmark datasets.", "conclusion": "The proposed neuron-centric model fusion algorithms outperform previous techniques, especially in zero-shot and non-IID scenarios, as demonstrated on various benchmark datasets."}}
{"id": "2507.00327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00327", "abs": "https://arxiv.org/abs/2507.00327", "authors": ["Chuyan Zhang", "Kefan Wang", "Yun Gu"], "title": "Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes", "comment": "Accepted by ICCV 2025", "summary": "Low-Rank Adaptation (LoRA) has proven effective in reducing computational\ncosts while maintaining performance comparable to fully fine-tuned foundation\nmodels across various tasks. However, its fixed low-rank structure restricts\nits adaptability in scenarios with substantial domain gaps, where higher ranks\nare often required to capture domain-specific complexities. Current adaptive\nLoRA methods attempt to overcome this limitation by dynamically expanding or\nselectively allocating ranks, but these approaches frequently depend on\ncomputationally intensive techniques such as iterative pruning, rank searches,\nor additional regularization. To address these challenges, we introduce Stable\nRank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the\nstable rank of pre-trained weight matrices as a natural prior for layer-wise\nrank allocation. By leveraging the stable rank, which reflects the intrinsic\ndimensionality of the weights, SR-LoRA enables a principled and efficient\nredistribution of ranks across layers, enhancing adaptability without incurring\nadditional search costs. Empirical evaluations on few-shot tasks with\nsignificant domain gaps show that SR-LoRA consistently outperforms recent\nadaptive LoRA variants, achieving a superior trade-off between performance and\nefficiency. Our code is available at\nhttps://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.", "AI": {"tldr": "SR-LoRA uses stable rank to improve LoRA's adaptability in domain gaps without extra search costs, outperforming other adaptive LoRA methods.", "motivation": "Fixed low-rank structure restricts its adaptability in scenarios with substantial domain gaps, where higher ranks are often required to capture domain-specific complexities. Current adaptive LoRA methods attempt to overcome this limitation by dynamically expanding or selectively allocating ranks, but these approaches frequently depend on computationally intensive techniques such as iterative pruning, rank searches, or additional regularization.", "method": "Stable Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the stable rank of pre-trained weight matrices as a natural prior for layer-wise rank allocation.", "result": "principled and efficient redistribution of ranks across layers, enhancing adaptability without incurring additional search costs.", "conclusion": "SR-LoRA consistently outperforms recent adaptive LoRA variants, achieving a superior trade-off between performance and efficiency."}}
{"id": "2507.00606", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00606", "abs": "https://arxiv.org/abs/2507.00606", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "AI": {"tldr": "MoR is a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering, which significantly enhances performance and eliminates the need for task-specific prompts.", "motivation": "LLMs' reliance on manually crafted, task-specific prompts limits adaptability and efficiency.", "method": "We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.", "result": "MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines.", "conclusion": "MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks."}}
{"id": "2507.00038", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00038", "abs": "https://arxiv.org/abs/2507.00038", "authors": ["Fei Chen", "Wenchi Zhou"], "title": "Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information", "comment": null, "summary": "Data reduction plays a vital role in data-centric AI by identifying the most\ninformative instance within large-scale datasets to enhance model training\nefficiency. The core challenge lies in how to select the optimal\ninstances-rather than the entire datasets-to improve data quality and training\nefficiency. In this paper, we propose an effective data reduction strategy\nbased on Pointwise V-information(PVI). First, we quantify instance difficulty\nusing PVI and filter out low-difficulty instances enabling a static approach.\nExperiments demonstrate that removing 10%-30% of the data preserves the\nclassifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we\nuse a progressive learning approach to training the classifiers on instances\nsorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy\ngain over conventional training. Our results suggest that with the effective\ndata reduction strategy, training a classifier on the selected optimal subset\ncould enhance the model performance and boost training efficiency. Moreover, we\nhave transferred the PVI framework, which previously applied only to English\ndatasets, to diverse Chinese NLP tasks and base models, leading to valuable\ninsights for cross-lingual data reduction and faster training. The codes are\nreleased at https://github.com/zhouwenchi/DatasetReductionStrategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePVI\u7684\u6570\u636e\u7f29\u51cf\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u8fc7\u6ee4\u4f4e\u96be\u5ea6\u5b9e\u4f8b\u5e76\u4f7f\u7528\u6e10\u8fdb\u5f0f\u5b66\u4e60\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u82f1\u8bed\u548c\u4e2d\u6587\u6570\u636e\u96c6\u3002", "motivation": "\u5728\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u4e2d\uff0c\u6570\u636e\u7f29\u51cf\u901a\u8fc7\u8bc6\u522b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5b9e\u4f8b\u6765\u63d0\u9ad8\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5982\u4f55\u9009\u62e9\u6700\u4f73\u5b9e\u4f8b\uff08\u800c\u4e0d\u662f\u6574\u4e2a\u6570\u636e\u96c6\uff09\u4ee5\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u57fa\u4e8ePointwise V-information(PVI)\u7684\u6709\u6548\u6570\u636e\u7f29\u51cf\u7b56\u7565", "result": "\u5220\u966410%-30%\u7684\u6570\u636e\u53ef\u4fdd\u6301\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u800c\u51c6\u786e\u7387\u4ec5\u635f\u59310.0001%\u81f30.76%\u3002\u5728\u6309\u5347\u5e8fPVI\u6392\u5e8f\u7684\u5b9e\u4f8b\u4e0a\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u52a0\u901f\u6536\u655b\uff0c\u5e76\u4e14\u6bd4\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u63d0\u9ad8\u4e860.8%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u7684\u6570\u636e\u7f29\u51cf\u7b56\u7565\uff0c\u5728\u9009\u62e9\u7684\u6700\u4f73\u5b50\u96c6\u4e0a\u8bad\u7ec3\u5206\u7c7b\u5668\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u5c06PVI\u6846\u67b6\u4ece\u4ec5\u9002\u7528\u4e8e\u82f1\u8bed\u6570\u636e\u96c6\u8f6c\u79fb\u5230\u5404\u79cd\u4e2d\u6587NLP\u4efb\u52a1\u548c\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u8de8\u8bed\u8a00\u6570\u636e\u7f29\u51cf\u548c\u66f4\u5feb\u7684\u8bad\u7ec3\u5e26\u6765\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.00328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00328", "abs": "https://arxiv.org/abs/2507.00328", "authors": ["Xuan Liu", "Yinhao Ren", "Marc D. Ryser", "Lars J. Grimm", "Joseph Y. Lo"], "title": "MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms", "comment": null, "summary": "Accurate lesion tracking in temporal mammograms is essential for monitoring\nbreast cancer progression and facilitating early diagnosis. However, automated\nlesion correspondence across exams remains a challenges in computer-aided\ndiagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,\na mask-guided lesion tracking framework that automates lesion localization\nacross consecutively exams. Our approach follows a coarse-to-fine strategy\nincorporating three key modules: global search, local search, and score\nrefinement. To support large-scale training and evaluation, we introduce a new\ndataset with curated prior-exam annotations for 730 mass and calcification\ncases from the public EMBED mammogram dataset, yielding over 20000 lesion\npairs, making it the largest known resource for temporal lesion tracking in\nmammograms. Experimental results demonstrate that MammoTracker achieves 0.455\naverage overlap and 0.509 accuracy, surpassing baseline models by 8%,\nhighlighting its potential to enhance CAD-based lesion progression analysis.\nOur dataset will be available at\nhttps://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.", "AI": {"tldr": "MammoTracker, a mask-guided lesion tracking framework, automates lesion localization across consecutively exams and introduces a new large dataset for temporal lesion tracking in mammograms.", "motivation": "Accurate lesion tracking in temporal mammograms is essential for monitoring breast cancer progression and facilitating early diagnosis. However, automated lesion correspondence across exams remains a challenges in computer-aided diagnosis (CAD) systems, limiting their effectiveness.", "method": "a mask-guided lesion tracking framework that automates lesion localization across consecutively exams. Our approach follows a coarse-to-fine strategy incorporating three key modules: global search, local search, and score refinement.", "result": "introduce a new dataset with curated prior-exam annotations for 730 mass and calcification cases from the public EMBED mammogram dataset, yielding over 20000 lesion pairs, making it the largest known resource for temporal lesion tracking in mammograms.", "conclusion": "MammoTracker achieves 0.455 average overlap and 0.509 accuracy, surpassing baseline models by 8%, highlighting its potential to enhance CAD-based lesion progression analysis."}}
{"id": "2507.00665", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00665", "abs": "https://arxiv.org/abs/2507.00665", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "AI": {"tldr": "This paper introduces SAFER, a framework using sparse autoencoders to interpret and improve reward models in RLHF, enhancing safety alignment in LLMs with minimal data modification.", "motivation": "Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque.", "method": "We present sparse Autoencoder For Enhanced Reward model (SAFER), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. Using these feature-level signals, we design targeted data poisoning and denoising strategies.", "result": "We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses.", "conclusion": "SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks."}}
{"id": "2507.00039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00039", "abs": "https://arxiv.org/abs/2507.00039", "authors": ["Lucas Potin", "Rosa Figueiredo", "Vincent Labatut", "Christine Largeron"], "title": "Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing", "comment": null, "summary": "Graph classification aims to categorize graphs based on their structural and\nattribute features, with applications in diverse fields such as social network\nanalysis and bioinformatics. Among the methods proposed to solve this task,\nthose relying on patterns (i.e. subgraphs) provide good explainability, as the\npatterns used for classification can be directly interpreted. To identify\nmeaningful patterns, a standard approach is to use a quality measure, i.e. a\nfunction that evaluates the discriminative power of each pattern. However, the\nliterature provides tens of such measures, making it difficult to select the\nmost appropriate for a given application. Only a handful of surveys try to\nprovide some insight by comparing these measures, and none of them specifically\nfocuses on graphs. This typically results in the systematic use of the most\nwidespread measures, without thorough evaluation. To address this issue, we\npresent a comparative analysis of 38 quality measures from the literature. We\ncharacterize them theoretically, based on four mathematical properties. We\nleverage publicly available datasets to constitute a benchmark, and propose a\nmethod to elaborate a gold standard ranking of the patterns. We exploit these\nresources to perform an empirical comparison of the measures, both in terms of\npattern ranking and classification performance. Moreover, we propose a\nclustering-based preprocessing step, which groups patterns appearing in the\nsame graphs to enhance classification performance. Our experimental results\ndemonstrate the effectiveness of this step, reducing the number of patterns to\nbe processed while achieving comparable performance. Additionally, we show that\nsome popular measures widely used in the literature are not associated with the\nbest results.", "AI": {"tldr": "\u6211\u4eec\u5bf9\u56fe\u5206\u7c7b\u4e2d\u7684 38 \u79cd\u8d28\u91cf\u5ea6\u91cf\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u53d1\u73b0\u4e00\u4e9b\u6d41\u884c\u7684\u5ea6\u91cf\u6807\u51c6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u9884\u5904\u7406\u6b65\u9aa4\u6765\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u7528\u4e8e\u5206\u7c7b\u7684\u6a21\u5f0f\u53ef\u4ee5\u76f4\u63a5\u89e3\u91ca\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u8d56\u4e8e\u6a21\u5f0f\uff08\u5373\u5b50\u56fe\uff09\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u4e86\u8bc6\u522b\u6709\u610f\u4e49\u7684\u6a21\u5f0f\uff0c\u4e00\u79cd\u6807\u51c6\u65b9\u6cd5\u662f\u4f7f\u7528\u8d28\u91cf\u5ea6\u91cf\uff0c\u5373\u8bc4\u4f30\u6bcf\u4e2a\u6a21\u5f0f\u7684\u533a\u5206\u80fd\u529b\u7684\u51fd\u6570\u3002\u7136\u800c\uff0c\u6587\u732e\u63d0\u4f9b\u4e86\u6570\u5341\u79cd\u8fd9\u6837\u7684\u5ea6\u91cf\uff0c\u56e0\u6b64\u5f88\u96be\u4e3a\u7ed9\u5b9a\u7684\u5e94\u7528\u9009\u62e9\u6700\u5408\u9002\u7684\u5ea6\u91cf\u3002\u53ea\u6709\u5c11\u6570\u8c03\u67e5\u8bd5\u56fe\u901a\u8fc7\u6bd4\u8f83\u8fd9\u4e9b\u5ea6\u91cf\u6765\u63d0\u4f9b\u4e00\u4e9b\u89c1\u89e3\uff0c\u800c\u4e14\u6ca1\u6709\u4e00\u4e2a\u8c03\u67e5\u4e13\u95e8\u5173\u6ce8\u56fe\u3002", "method": "\u5bf9\u6765\u81ea\u6587\u732e\u768438\u4e2a\u8d28\u91cf\u6307\u6807\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002\u6211\u4eec\u57fa\u4e8e\u56db\u4e2a\u6570\u5b66\u5c5e\u6027\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u7406\u8bba\u4e0a\u7684\u8868\u5f81\u3002\u6211\u4eec\u5229\u7528\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u6765\u6784\u6210\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u6765\u8be6\u7ec6\u8bf4\u660e\u6a21\u5f0f\u7684\u9ec4\u91d1\u6807\u51c6\u6392\u5e8f\u3002\u6211\u4eec\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u5bf9\u8fd9\u4e9b\u63aa\u65bd\u8fdb\u884c\u7ecf\u9a8c\u6bd4\u8f83\uff0c\u5305\u62ec\u6a21\u5f0f\u6392\u5e8f\u548c\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u805a\u7c7b\u7684\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u8be5\u6b65\u9aa4\u5bf9\u51fa\u73b0\u5728\u540c\u4e00\u56fe\u4e2d\u7684\u6a21\u5f0f\u8fdb\u884c\u5206\u7ec4\uff0c\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8fd9\u4e00\u6b65\u9aa4\u7684\u6709\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u8981\u5904\u7406\u7684\u6a21\u5f0f\u6570\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8868\u660e\uff0c\u6587\u732e\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e00\u4e9b\u6d41\u884c\u63aa\u65bd\u4e0e\u6700\u4f73\u7ed3\u679c\u65e0\u5173\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e00\u4e9b\u5728\u6587\u732e\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u6d41\u884c\u63aa\u65bd\u4e0e\u6700\u4f73\u7ed3\u679c\u65e0\u5173\u3002"}}
{"id": "2507.00334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00334", "abs": "https://arxiv.org/abs/2507.00334", "authors": ["Mengyi Shan", "Zecheng He", "Haoyu Ma", "Felix Juefei-Xu", "Peizhao Zhang", "Tingbo Hou", "Ching-Yao Chuang"], "title": "Populate-A-Scene: Affordance-Aware Human Video Generation", "comment": "Project page: https://shanmy.github.io/Populate-A-Scene", "summary": "Can a video generation model be repurposed as an interactive world simulator?\nWe explore the affordance perception potential of text-to-video models by\nteaching them to predict human-environment interaction. Given a scene image and\na prompt describing human actions, we fine-tune the model to insert a person\ninto the scene, while ensuring coherent behavior, appearance, harmonization,\nand scene affordance. Unlike prior work, we infer human affordance for video\ngeneration (i.e., where to insert a person and how they should behave) from a\nsingle scene image, without explicit conditions like bounding boxes or body\nposes. An in-depth study of cross-attention heatmaps demonstrates that we can\nuncover the inherent affordance perception of a pre-trained video model without\nlabeled affordance datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u62df\u5668\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u4f7f\u5176\u80fd\u591f\u9884\u6d4b\u4eba\u4e0e\u73af\u5883\u7684\u4e92\u52a8\uff0c\u5e76\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u5185\u5728\u7684 affordance \u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684 affordance \u611f\u77e5\u6f5c\u529b\uff0c\u901a\u8fc7\u6559\u5bfc\u5b83\u4eec\u9884\u6d4b\u4eba\u4e0e\u73af\u5883\u7684\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u9884\u6d4b\u4eba\u4e0e\u73af\u5883\u7684\u4e92\u52a8\uff0c\u4ece\u800c\u5c06\u4eba\u63d2\u5165\u573a\u666f\u4e2d\uff0c\u540c\u65f6\u786e\u4fdd\u8fde\u8d2f\u7684\u884c\u4e3a\u3001\u5916\u89c2\u3001\u534f\u8c03\u548c\u573a\u666f affordance\u3002", "result": "\u6df1\u5165\u7814\u7a76 cross-attention \u70ed\u56fe\u8868\u660e\uff0c\u6211\u4eec\u53ef\u4ee5\u63ed\u793a\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u7684\u5185\u5728 affordance \u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6a21\u578b\u5177\u6709\u5185\u5728\u7684 affordance \u611f\u77e5\u80fd\u529b\uff0c\u65e0\u9700\u6807\u6ce8\u7684 affordance \u6570\u636e\u96c6\u3002"}}
{"id": "2507.00700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00700", "abs": "https://arxiv.org/abs/2507.00700", "authors": ["Ahmed Sabir", "Azinovi\u010d Gasper", "Mengsay Loem", "Rajesh Sharma"], "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "comment": null, "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "AI": {"tldr": "VLMs trained on different languages exhibit culturally grounded attentional patterns.", "motivation": "investigate whether Vision-Language Models (VLMs) trained predominantly on different languages, specifically Japanese and English, exhibit similar culturally grounded attentional patterns.", "method": "comparative analysis of image descriptions", "result": "VLMs not only internalize the structural properties of language but also reproduce cultural behaviors embedded in the training data", "conclusion": "VLMs reproduce cultural behaviors embedded in the training data, indicating that cultural cognition may implicitly shape model outputs."}}
{"id": "2507.00055", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.00055", "abs": "https://arxiv.org/abs/2507.00055", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "comment": "Accepted at INTERSPEECH 2025", "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks.", "AI": {"tldr": "LiSER \u5229\u7528\u672a\u6807\u8bb0\u7684\u97f3\u9891-\u89c6\u89c9\u6570\u636e\u8fdb\u884c SER\uff0c\u4f7f\u7528\u57fa\u4e8e\u9ad8\u7ea7\u8bed\u97f3\u548c\u9762\u90e8\u8868\u793a\u6a21\u578b\u7684\u5927\u578b\u6559\u5e08\u6a21\u578b\uff0c\u5c06\u5173\u4e8e\u8bed\u97f3\u60c5\u7eea\u548c\u9762\u90e8\u8868\u60c5\u7684\u77e5\u8bc6\u4ece\u6559\u5e08\u6a21\u578b\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u3002", "motivation": "\u8bed\u97f3\u754c\u9762\u4e2d\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b (SER) \u53ef\u4ee5\u6839\u636e\u7528\u6237\u7684\u60c5\u7eea\u81ea\u5b9a\u4e49\u54cd\u5e94\uff0c\u4ece\u800c\u4f7f\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u53d7\u76ca\u3002\u4f7f\u7528\u591a\u6a21\u6001\u97f3\u9891-\u89c6\u89c9\u7ebf\u7d22\u5f00\u53d1 SER \u7cfb\u7edf\u662f\u6709\u76ca\u7684\u3002\u7136\u800c\uff0c\u6536\u96c6\u5927\u91cf\u7528\u4e8e\u5f00\u53d1\u7684\u6570\u636e\u975e\u5e38\u6602\u8d35\u3002", "method": "\u77e5\u8bc6\u84b8\u998f\u6846\u67b6 LightweightSER (LiSER)", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6 RAVDESS \u548c CREMA-D \u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c", "conclusion": "LiSER \u53ef\u4ee5\u5728 SER \u4efb\u52a1\u4e2d\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u8bb0\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.00339", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.2.6; I.4.6"], "pdf": "https://arxiv.org/pdf/2507.00339", "abs": "https://arxiv.org/abs/2507.00339", "authors": ["Alexander Moore", "Amar Saini", "Kylie Cancilla", "Doug Poland", "Carmen Carrano"], "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video", "comment": "9 pages, 2 figures", "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,", "AI": {"tldr": "Introduces MOVi-MC-AC, a large-scale multi-camera amodal segmentation and content completion dataset with ground-truth amodal content labels.", "motivation": "Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene.", "method": "introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene.  provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content.", "result": "Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content.", "conclusion": "MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content."}}
{"id": "2507.00718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00718", "abs": "https://arxiv.org/abs/2507.00718", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "comment": null, "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "AI": {"tldr": "LLMs can generate financial reports from time series data using prompt engineering and model selection.", "motivation": "Explore the potential of large language models (LLMs) to generate financial reports from time series data.", "method": "A framework encompassing prompt engineering, model selection, and evaluation with an automated highlighting system.", "result": "Experiments utilizing both data from the real stock market indices and synthetic time series demonstrate the capability of LLMs to produce coherent and informative financial reports.", "conclusion": "LLMs can produce coherent and informative financial reports from time series data."}}
{"id": "2507.00061", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.00061", "abs": "https://arxiv.org/abs/2507.00061", "authors": ["Hoang-Dieu Vu", "Duc-Nghia Tran", "Quang-Tu Pham", "Hieu H. Pham", "Nicolas Vuillerme", "Duc-Tan Tran"], "title": "Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data", "comment": null, "summary": "This paper introduces Smooth-Distill, a novel self-distillation framework\ndesigned to simultaneously perform human activity recognition (HAR) and sensor\nplacement detection using wearable sensor data. The proposed approach utilizes\na unified CNN-based architecture, MTL-net, which processes accelerometer data\nand branches into two outputs for each respective task. Unlike conventional\ndistillation methods that require separate teacher and student models, the\nproposed framework utilizes a smoothed, historical version of the model itself\nas the teacher, significantly reducing training computational overhead while\nmaintaining performance benefits. To support this research, we developed a\ncomprehensive accelerometer-based dataset capturing 12 distinct sleep postures\nacross three different wearing positions, complementing two existing public\ndatasets (MHealth and WISDM). Experimental results show that Smooth-Distill\nconsistently outperforms alternative approaches across different evaluation\nscenarios, achieving notable improvements in both human activity recognition\nand device placement detection tasks. This method demonstrates enhanced\nstability in convergence patterns during training and exhibits reduced\noverfitting compared to traditional multitask learning baselines. This\nframework contributes to the practical implementation of knowledge distillation\nin human activity recognition systems, offering an effective solution for\nmultitask learning with accelerometer data that balances accuracy and training\nefficiency. More broadly, it reduces the computational cost of model training,\nwhich is critical for scenarios requiring frequent model updates or training on\nresource-constrained platforms. The code and model are available at\nhttps://github.com/Kuan2vn/smooth\\_distill.", "AI": {"tldr": "Smooth-Distill: a self-distillation framework for HAR and sensor placement detection using wearable sensor data, reducing computational overhead and improving accuracy.", "motivation": "Conventional distillation methods require separate teacher and student models, leading to training computational overhead. Frequent model updates or training on resource-constrained platforms.", "method": "A unified CNN-based architecture, MTL-net, which processes accelerometer data and branches into two outputs for each respective task. A smoothed, historical version of the model itself as the teacher.", "result": "Notable improvements in both human activity recognition and device placement detection tasks. Enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines.", "conclusion": "Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving improvements in both human activity recognition and device placement detection tasks. It reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms."}}
{"id": "2507.00356", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00356", "abs": "https://arxiv.org/abs/2507.00356", "authors": ["Zhiwei Yi", "Xin Cheng", "Jingyu Ma", "Ruifei Zhu", "Junwei Tian", "Yuanxiu Zhou", "Xinge Zhao", "Hongzhe Li"], "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation", "comment": "A Remote Sensing Fundation Model for Very High Resolution Images", "summary": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.", "AI": {"tldr": "CGEarthEye\u662f\u4e00\u4e2a\u4e3a\u5409\u6797\u4e00\u53f7\u536b\u661f\u8bbe\u8ba1\u7684\u9065\u611f\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u5b83\u5728\u591a\u4e2a\u9065\u611f\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u6709\u671b\u4fc3\u8fdb\u5409\u6797\u4e00\u53f7\u6570\u636e\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002", "motivation": "\u4e0e\u4e2d\u7b49\u5206\u8fa8\u7387\u6570\u636e\u7684\u5f00\u653e\u53ef\u8bbf\u95ee\u6027\u548c\u9ad8\u65f6\u7a7a\u8986\u76d6\u7387\u76f8\u6bd4\uff0c\u8d85\u9ad8\u5206\u8fa8\u7387\u5149\u5b66RS\u56fe\u50cf\u7684\u6709\u9650\u83b7\u53d6\u6e20\u9053\u9650\u5236\u4e86\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08RSVFM\uff09\u7684\u8fdb\u5c55\u3002\u4f5c\u4e3a\u4e16\u754c\u4e0a\u6700\u5927\u7684\u4e9a\u7c73\u7ea7\u5546\u4e1aRS\u536b\u661f\u661f\u5ea7\uff0c\u5409\u6797\u4e00\u53f7\u661f\u5ea7\u62e5\u6709\u4e30\u5bcc\u7684\u4e9a\u7c73\u7ea7\u56fe\u50cf\u8d44\u6e90\u3002", "method": "CGEarthEye\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u5409\u6797\u4e00\u53f7\u536b\u661f\u7279\u5f81\u8bbe\u8ba1\u7684RSVFM\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u603b\u8ba121\u4ebf\u53c2\u6570\u3002\u901a\u8fc7\u591a\u5c42\u6b21\u8868\u5f81\u805a\u7c7b\u548c\u91c7\u6837\u7b56\u7565\uff0c\u5f00\u53d1\u4e86JLSSD\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a1500\u4e07\u89c4\u6a21\u7684\u591a\u65f6\u6001\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6570\u636e\u96c6\uff0c\u5177\u6709\u5168\u7403\u8986\u76d6\uff0c\u5e76\u5728\u4e00\u5e74\u5185\u8fdb\u884c\u5b63\u5ea6\u65f6\u95f4\u91c7\u6837\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5b63\u8282\u5bf9\u6bd4\u3001\u57fa\u4e8e\u589e\u5f3a\u7684\u5bf9\u6bd4\u548c\u63a9\u7801\u8865\u4e01\u4ee4\u724c\u5bf9\u6bd4\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u6db5\u76d6\u56db\u4e2a\u5178\u578bRS\u4efb\u52a1\u768410\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cCGEarthEye\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0cCGEarthEye\u5728\u7279\u5f81\u53ef\u89c6\u5316\u3001\u6a21\u578b\u6536\u655b\u3001\u53c2\u6570\u6548\u7387\u548c\u5b9e\u9645\u6620\u5c04\u5e94\u7528\u4e2d\u5177\u6709\u5353\u8d8a\u7684\u7279\u6027\u3002", "conclusion": "CGEarthEye\u7684\u5353\u8d8a\u8868\u5f81\u80fd\u529b\u5c06\u4fc3\u8fdb\u5409\u6797\u4e00\u53f7\u6570\u636e\u5728\u4f20\u7edf\u5730\u7403\u89c2\u6d4b\u5e94\u7528\u4e2d\u66f4\u5e7f\u6cdb\u3001\u66f4\u9ad8\u6548\u7684\u5e94\u7528\u3002"}}
{"id": "2507.00769", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00769", "abs": "https://arxiv.org/abs/2507.00769", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "comment": null, "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "AI": {"tldr": "LitBench\u662f\u4e00\u4e2a\u7528\u4e8e\u521b\u9020\u6027\u5199\u4f5c\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8bad\u7ec3\u540e\u7684\u5956\u52b1\u6a21\u578b\u6bd4\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u597d\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u7684\u5224\u65ad\u66f4\u4e00\u81f4\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u521b\u9020\u6027\u5199\u4f5c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5f00\u653e\u5f0f\u53d9\u8ff0\u7f3a\u4e4f\u57fa\u672c\u4e8b\u5b9e\u3002\u5728\u6ca1\u6709\u9ad8\u6027\u80fd\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6210\u7684\uff08OTS\uff09\u8bed\u8a00\u6a21\u578b\u88ab\u7528\u4f5czero-shot\u8bc4\u5224\u5668\uff0c\u4f46\u5b83\u4eec\u5728\u8fd9\u79cd\u4e0a\u4e0b\u6587\u4e2d\u7684\u53ef\u9760\u6027\u5c1a\u4e0d\u6e05\u695a\u3002\u4e3a\u4e86\u8ffd\u6c42\u5bf9\u521b\u9020\u6027\u5199\u4f5c\u7684\u7a33\u5065\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u4e86LitBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u521b\u9020\u6027\u5199\u4f5c\u9a8c\u8bc1\u7684\u6807\u51c6\u57fa\u51c6\u548c\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81eaReddit\u76842,480\u4e2a\u53bb\u504f\u3001\u4eba\u5de5\u6807\u8bb0\u7684\u6545\u4e8b\u6bd4\u8f83\u7684\u4fdd\u7559\u6d4b\u8bd5\u96c6\u548c\u4e00\u4e2a\u5305\u542b43,827\u5bf9\u4eba\u7c7b\u504f\u597d\u6807\u7b7e\u7684\u8bad\u7ec3\u8bed\u6599\u5e93\u3002\u4f7f\u7528LitBench\uff0c\u6211\u4eec\u5bf9zero-shot LLM\u8bc4\u5224\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bad\u7ec3Bradley Terry\u548c\u751f\u6210\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5728\u7ebf\u4eba\u5de5\u7814\u7a76\uff0c\u4ee5\u9a8c\u8bc1\u65b0\u751f\u6210\u7684LLM\u6545\u4e8b\u7684\u5956\u52b1\u6a21\u578b\u6392\u540d\u3002", "result": "Claude-3.7-Sonnet\u662f\u6700\u5f3a\u5927\u7684\u73b0\u6210\u8bc4\u5224\u5668\uff0c\u4e0e\u4eba\u7c7b\u504f\u597d\u8fbe\u621073%\u7684\u534f\u8bae\uff1b\u5728\u8bad\u7ec3\u6709\u7d20\u7684\u5956\u52b1\u6a21\u578b\u4e2d\uff0cBradley-Terry\u548c\u751f\u6210\u5956\u52b1\u6a21\u578b\u90fd\u8fbe\u5230\u4e8678%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u6240\u6709\u73b0\u6210\u7684\u8bc4\u5224\u5668\u3002\u5728\u7ebf\u4eba\u5de5\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u6211\u4eec\u8bad\u7ec3\u6709\u7d20\u7684\u5956\u52b1\u6a21\u578b\u5728\u65b0\u751f\u6210\u7684LLM\u6545\u4e8b\u4e2d\u59cb\u7ec8\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u8bad\u7ec3\u540e\u7684\u5956\u52b1\u6a21\u578b\u5728\u521b\u9020\u6027\u5199\u4f5c\u8bc4\u4f30\u65b9\u9762\u4f18\u4e8e\u73b0\u6210\u7684LLM\u8bc4\u5224\u5668\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002"}}
{"id": "2507.00073", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.00073", "abs": "https://arxiv.org/abs/2507.00073", "authors": ["Urvi Pawar", "Kunal Telangi"], "title": "Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory", "comment": "Submitted to Journal of Machine Learning Research (JMLR), June 2025.\n  24 pages, 3 figures. Under review", "summary": "We propose Fractional Policy Gradients (FPG), a reinforcement learning\nframework incorporating fractional calculus for long-term temporal modeling in\npolicy optimization. Standard policy gradient approaches face limitations from\nMarkovian assumptions, exhibiting high variance and inefficient sampling. By\nreformulating gradients using Caputo fractional derivatives, FPG establishes\npower-law temporal correlations between state transitions. We develop an\nefficient recursive computation technique for fractional temporal-difference\nerrors with constant time and memory requirements. Theoretical analysis shows\nFPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus\nstandard policy gradients while preserving convergence. Empirical validation\ndemonstrates 35-68% sample efficiency gains and 24-52% variance reduction\nversus state-of-the-art baselines. This framework provides a mathematically\ngrounded approach for leveraging long-range dependencies without computational\noverhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86Fractional Policy Gradients (FPG)\uff0c\u5229\u7528\u5206\u6570\u9636\u5bfc\u6570\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u6548\u7387\u548c\u65b9\u5dee\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u9762\u4e34\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u7684\u9650\u5236\uff0c\u8868\u73b0\u51fa\u9ad8\u65b9\u5dee\u548c\u4f4e\u6548\u91c7\u6837\u3002", "method": "\u4f7f\u7528Caputo fractional derivatives\u91cd\u65b0\u6784\u5efa\u68af\u5ea6\uff0c\u5efa\u7acb\u4e86\u72b6\u6001\u8f6c\u79fb\u4e4b\u95f4\u7684\u5e42\u5f8b\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9012\u5f52\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "FPG\u5b9e\u73b0\u4e86O(t^(-alpha))\u9636\u7684\u6e10\u8fd1\u65b9\u5dee\u964d\u4f4e\uff0c\u6837\u672c\u6548\u7387\u63d0\u9ad8\u4e8635-68%\uff0c\u65b9\u5dee\u964d\u4f4e\u4e8624-52%\u3002", "conclusion": "Fractional Policy Gradients (FPG)\u6846\u67b6\u901a\u8fc7\u5229\u7528\u957f\u671f\u4f9d\u8d56\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\u548c\u65b9\u5dee\u964d\u4f4e\uff0c\u4e14\u6ca1\u6709\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2507.00363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00363", "abs": "https://arxiv.org/abs/2507.00363", "authors": ["Xingjun Wang", "Lianlei Shan"], "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control", "comment": null, "summary": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023},\naddressing challenges in initialization, optimization, and density control.\nGaussian Splatting is an alternative for rendering realistic images while\nsupporting real-time performance, and it has gained popularity due to its\nexplicit 3D Gaussian representation. However, 3DGS heavily depends on accurate\ninitialization and faces difficulties in optimizing unstructured Gaussian\ndistributions into ordered surfaces, with limited adaptive density control\nmechanism proposed so far. Our first key contribution is a geometry-guided\ninitialization to predict Gaussian parameters, ensuring precise placement and\nfaster convergence. We then introduce a surface-aligned optimization strategy\nto refine Gaussian placement, improving geometric accuracy and aligning with\nthe surface normals of the scene. Finally, we present a dynamic adaptive\ndensity control mechanism that adjusts Gaussian density based on regional\ncomplexity, for visual fidelity. These innovations enable our method to achieve\nhigh-fidelity real-time rendering and significant improvements in visual\nquality, even in complex scenes. Our method demonstrates comparable or superior\nresults to state-of-the-art methods, rendering high-fidelity images in real\ntime.", "AI": {"tldr": "Proposes a method to enhance 3D Gaussian Splatting (3DGS) by addressing challenges in initialization, optimization, and density control, achieving high-fidelity real-time rendering.", "motivation": "Addressing challenges in initialization, optimization, and density control of 3D Gaussian Splatting (3DGS). 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far.", "method": "A geometry-guided initialization to predict Gaussian parameters, a surface-aligned optimization strategy to refine Gaussian placement, and a dynamic adaptive density control mechanism.", "result": "Enables high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Demonstrates comparable or superior results to state-of-the-art methods.", "conclusion": "The method achieves high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes, demonstrating comparable or superior results to state-of-the-art methods."}}
{"id": "2507.00782", "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "pdf": "https://arxiv.org/pdf/2507.00782", "abs": "https://arxiv.org/abs/2507.00782", "authors": ["Matthieu Pierre Boyer"], "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "comment": "15 pages, preprint before submission to CSL 2026", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u7684\u529f\u80fd\u7f16\u7a0b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u6211\u4eec\u63d0\u9ad8\u66f4\u4f20\u7edf\u7684\u5916\u5ef6\u98ce\u683c\u7684\u8868\u73b0\u529b\u3002", "motivation": "\u7814\u7a76\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u7684\u529f\u80fd\u7f16\u7a0b\u65b9\u6cd5\uff0c\u5141\u8bb8\u6211\u4eec\u63d0\u9ad8\u66f4\u4f20\u7edf\u7684\u5916\u5ef6\u98ce\u683c\u7684\u8868\u73b0\u529b", "method": "\u5f62\u5f0f\u5316\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8303\u7574\u7684\u7c7b\u578b\u548c\u6548\u679c\u7cfb\u7edf\uff0c\u5e76\u6784\u9020\u4e86\u4e00\u4e2a\u56fe\u89e3\u5fae\u79ef\u5206\u6765\u5efa\u6a21\u89e3\u6790\u548c\u5904\u7406\u6548\u679c", "result": "\u53e5\u5b50\u7684\u5916\u5ef6\u88ab\u6709\u6548\u5730\u8ba1\u7b97\u51fa\u6765", "conclusion": "\u6a21\u578b\u7528\u4e8e\u6709\u6548\u5730\u8ba1\u7b97\u53e5\u5b50\u7684\u5916\u5ef6\u3002"}}
{"id": "2507.00075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00075", "abs": "https://arxiv.org/abs/2507.00075", "authors": ["Yifan Sun", "Yushan Liang", "Zhen Zhang", "Jiaye Teng"], "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap", "comment": "24 pages", "summary": "Self-improvement is among the most prominent techniques within the realm of\nlarge language models (LLM), aiming to enhance the LLM performance without\nrelying on external data. Despite its significance, generally how LLM\nperformances evolve during the self-improvement process remains underexplored.\nIn this paper, we theoretically model the training dynamics of self-improvement\nvia the concept of solver-verifier gap. This is inspired by the conjecture that\nthe performance enhancement of self-improvement stems from the gap between\nLLM's solver capability and verifier capability. Based on the theoretical\nframework, we further introduce how to predict the ultimate power of\nself-improvement using only information from the first few training epochs. We\nempirically validate the effectiveness of the theoretical model on various LLMs\nand datasets. Beyond self-improvement, we extend our analysis to investigate\nhow external data influences these dynamics within the framework. Notably, we\nfind that under limited external data regimes, such external data can be\nutilized at any stage without significantly affecting final performances, which\naccords with the empirical observations.", "AI": {"tldr": "\u672c\u6587\u5bf9 LLM \u81ea\u6211\u5b8c\u5584\u8fc7\u7a0b\u4e2d\u7684\u8bad\u7ec3\u52a8\u6001\u8fdb\u884c\u4e86\u7406\u8bba\u5efa\u6a21\uff0c\u5e76\u63d0\u51fa\u4e86\u9884\u6d4b\u81ea\u6211\u5b8c\u5584\u80fd\u529b\u7684\u65b9\u6848\u3002", "motivation": "\u65e8\u5728\u589e\u5f3a LLM \u6027\u80fd\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u3002\u4e00\u822c\u6765\u8bf4\uff0cLLM \u5728\u81ea\u6211\u5b8c\u5584\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u5982\u4f55\u6f14\u53d8\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u6c42\u89e3\u5668-\u9a8c\u8bc1\u5668\u5dee\u8ddd\u7684\u6982\u5ff5\uff0c\u5bf9\u81ea\u6211\u6539\u8fdb\u7684\u8bad\u7ec3\u52a8\u6001\u8fdb\u884c\u7406\u8bba\u5efa\u6a21\u3002", "result": "\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u5982\u4f55\u4ec5\u4f7f\u7528\u524d\u51e0\u4e2a\u8bad\u7ec3 epoch \u7684\u4fe1\u606f\u6765\u9884\u6d4b\u81ea\u6211\u6539\u8fdb\u7684\u6700\u7ec8\u80fd\u529b\u3002\u6211\u4eec\u4ece\u7ecf\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u6a21\u578b\u5728\u5404\u79cd LLM \u548c\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5728\u6709\u9650\u7684\u5916\u90e8\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u5916\u90e8\u6570\u636e\u53ef\u4ee5\u5728\u4efb\u4f55\u9636\u6bb5\u4f7f\u7528\uff0c\u800c\u4e0d\u4f1a\u663e\u7740\u5f71\u54cd\u6700\u7ec8\u6027\u80fd\uff0c\u8fd9\u4e0e\u7ecf\u9a8c\u89c2\u5bdf\u4e00\u81f4\u3002"}}
{"id": "2507.00365", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00365", "abs": "https://arxiv.org/abs/2507.00365", "authors": ["Wanghui Xiao"], "title": "An Improved U-Net Model for Offline handwriting signature denoising", "comment": null, "summary": "Handwriting signatures, as an important means of identity recognition, are\nwidely used in multiple fields such as financial transactions, commercial\ncontracts and personal affairs due to their legal effect and uniqueness. In\nforensic science appraisals, the analysis of offline handwriting signatures\nrequires the appraiser to provide a certain number of signature samples, which\nare usually derived from various historical contracts or archival materials.\nHowever, the provided handwriting samples are often mixed with a large amount\nof interfering information, which brings severe challenges to handwriting\nidentification work. This study proposes a signature handwriting denoising\nmodel based on the improved U-net structure, aiming to enhance the robustness\nof the signature recognition system. By introducing discrete wavelet transform\nand PCA transform, the model's ability to suppress noise has been enhanced. The\nexperimental results show that this modelis significantly superior to the\ntraditional methods in denoising effect, can effectively improve the clarity\nand readability of the signed images, and provide more reliable technical\nsupport for signature analysis and recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbU-net\u7ed3\u6784\u7684\u7b7e\u540d\u624b\u5199\u4f53\u53bb\u566a\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u7b7e\u540d\u8bc6\u522b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u624b\u5199\u7b7e\u540d\u4f5c\u4e3a\u4e00\u79cd\u91cd\u8981\u7684\u8eab\u4efd\u8bc6\u522b\u624b\u6bb5\uff0c\u56e0\u5176\u6cd5\u5f8b\u6548\u529b\u548c\u72ec\u7279\u6027\u800c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u91d1\u878d\u4ea4\u6613\u3001\u5546\u4e1a\u5408\u540c\u548c\u4e2a\u4eba\u4e8b\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u3002\u5728\u6cd5\u533b\u5b66\u9274\u5b9a\u4e2d\uff0c\u79bb\u7ebf\u624b\u5199\u7b7e\u540d\u7684\u5206\u6790\u9700\u8981\u9274\u5b9a\u4eba\u63d0\u4f9b\u4e00\u5b9a\u6570\u91cf\u7684\u7b7e\u540d\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u901a\u5e38\u6765\u6e90\u4e8e\u5404\u79cd\u5386\u53f2\u5408\u540c\u6216\u6863\u6848\u6750\u6599\u3002\u7136\u800c\uff0c\u6240\u63d0\u4f9b\u7684\u7b14\u8ff9\u6837\u672c\u5f80\u5f80\u6df7\u6742\u7740\u5927\u91cf\u7684\u5e72\u6270\u4fe1\u606f\uff0c\u8fd9\u7ed9\u7b14\u8ff9\u9274\u5b9a\u5de5\u4f5c\u5e26\u6765\u4e86\u4e25\u5cfb\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbU-net\u7ed3\u6784\u7684\u7b7e\u540d\u624b\u5199\u4f53\u53bb\u566a\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548cPCA\u53d8\u6362\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u6291\u5236\u566a\u58f0\u7684\u80fd\u529b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u53bb\u566a\u6548\u679c\u4e0a\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u7b7e\u540d\u56fe\u50cf\u7684\u6e05\u6670\u5ea6\u548c\u53ef\u8bfb\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u53bb\u566a\u6548\u679c\u4e0a\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u7b7e\u540d\u56fe\u50cf\u7684\u6e05\u6670\u5ea6\u548c\u53ef\u8bfb\u6027\uff0c\u5e76\u4e3a\u7b7e\u540d\u5206\u6790\u548c\u8bc6\u522b\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.00783", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.00783", "abs": "https://arxiv.org/abs/2507.00783", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "title": "Generative AI and the future of scientometrics: current topics and future questions", "comment": null, "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "AI": {"tldr": "This paper reviews the use of GenAI in scientometrics, finding promise in language generation tasks but limitations in tasks requiring stable semantics, and discusses the potential impact of GenAI on measuring science.", "motivation": "review the use of GenAI in scientometrics, and to begin a debate on the broader implications for the field. inquire whether, by generating large amounts of scientific language, GenAI might have a fundamental impact on our field by affecting textual characteristics used to measure science, such as authors, words, and references.", "method": "critical engagement with recent experiments using GenAI in scientometrics, including topic labelling, the analysis of citation contexts, predictive applications, scholars' profiling, and research assessment. systematically compare the performance of different GenAI models for specific tasks.", "result": "GenAI shows promise in tasks where language generation dominates, such as labelling, but faces limitations in tasks that require stable semantics, pragmatic reasoning, or structured domain knowledge. However, these results might become quickly outdated.", "conclusion": "GenAI shows promise in tasks where language generation dominates, but faces limitations in tasks that require stable semantics, pragmatic reasoning, or structured domain knowledge. Careful empirical work and theoretical reflection will be essential to remain capable of interpreting the evolving patterns of knowledge production."}}
{"id": "2507.00078", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00078", "abs": "https://arxiv.org/abs/2507.00078", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "title": "The language of time: a language model perspective on time-series foundation models", "comment": null, "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u8868\u5f81\u5b66\u4e60\u673a\u5236\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u91ca\u4e86\u5176\u5353\u8d8a\u6027\u80fd\u7684\u539f\u56e0\uff0c\u5e76\u4e3a\u7406\u89e3\u3001\u8bc4\u4f30\u548c\u63d0\u9ad8\u5176\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8868\u8fbe\u80fd\u529b\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8de8\u9886\u57df\u8fc1\u79fb\u80fd\u529b\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u53cd\u6620\u4e86\u4e0d\u540c\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u4f7f\u5f97\u8de8\u9886\u57df\u8fc1\u79fb\u5728\u76f4\u89c9\u4e0a\u4f3c\u4e4e\u662f\u4e0d\u5408\u7406\u7684\uff0c\u7136\u800c\u8fd9\u4e0e\u6a21\u578b\u7684\u5b9e\u8bc1\u6210\u529f\u76f8\u77db\u76fe\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6096\u8bba\uff0c", "method": "\u4ece\u7406\u8bba\u548c\u5b9e\u9a8c\u7684\u89d2\u5ea6\uff0c\u7814\u7a76\u4e86\u57fa\u4e8epatch\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u8868\u5f81\u5b66\u4e60\u673a\u5236\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217patch\u53ef\u4ee5\u5fe0\u5b9e\u5730\u91cf\u5316\u4e3a\u79bb\u6563\u8bcd\u6c47\u8868\uff0c\u5176\u5173\u952e\u7edf\u8ba1\u7279\u6027\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u7edf\u8ba1\u7279\u6027\u9ad8\u5ea6\u4e00\u81f4\u3002\u8fd9\u79cd\u6cdb\u5316\u4f7f\u5f97\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u80fd\u591f\u7ee7\u627f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u8868\u793a\u548c\u8fc1\u79fb\u80fd\u529b\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u5b83\u4eec\u5728\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3\u3001\u8bc4\u4f30\u548c\u63d0\u9ad8\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u7406\u8bba\u57fa\u77f3\u3002"}}
{"id": "2507.00368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00368", "abs": "https://arxiv.org/abs/2507.00368", "authors": ["Hikaru Shijo", "Yutaka Yoshihama", "Kenichi Yadani", "Norifumi Murata"], "title": "Out-of-Distribution Detection with Adaptive Top-K Logits Integration", "comment": null, "summary": "Neural networks often make overconfident predictions from out-of-distribution\n(OOD) samples. Detection of OOD data is therefore crucial to improve the safety\nof machine learning. The simplest and most powerful method for OOD detection is\nMaxLogit, which uses the model's maximum logit to provide an OOD score. We have\ndiscovered that, in addition to the maximum logit, some other logits are also\nuseful for OOD detection. Based on this finding, we propose a new method called\nATLI (Adaptive Top-k Logits Integration), which adaptively determines effective\ntop-k logits that are specific to each model and combines the maximum logit\nwith the other top-k logits. In this study we evaluate our proposed method\nusing ImageNet-1K benchmark. Extensive experiments showed our proposed method\nto reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit\napproach, and decreased FPR95 by an additional 2.67% compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684OOD\u68c0\u6d4b\u65b9\u6cd5ATLI\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u591a\u4e2alogits\u6765\u63d0\u9ad8OOD\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u7ecf\u5e38\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u6837\u672c\u505a\u51fa\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u68c0\u6d4bOOD\u6570\u636e\u5bf9\u4e8e\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aATLI\uff08\u81ea\u9002\u5e94Top-k Logits\u96c6\u6210\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u6bcf\u4e2a\u6a21\u578b\u7279\u5b9a\u7684\u6709\u6548top-k logits\uff0c\u5e76\u5c06\u6700\u5927logit\u4e0e\u5176\u4ed6top-k logits\u7ed3\u5408\u3002", "result": "\u5728ImageNet-1K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0eMaxLogit\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5047\u9633\u6027\u7387\uff08FPR95\uff09\u964d\u4f4e\u4e866.73%\uff0c\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cFPR95\u964d\u4f4e\u4e862.67%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684OOD\u68c0\u6d4b\u65b9\u6cd5ATLI\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u6709\u6548\u7684top-k logits\u5e76\u5c06\u6700\u5927logit\u4e0e\u5176\u4ed6top-k logits\u7ed3\u5408\uff0c\u5728ImageNet-1K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0eMaxLogit\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5047\u9633\u6027\u7387\uff08FPR95\uff09\u964d\u4f4e\u4e866.73%\uff0c\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cFPR95\u964d\u4f4e\u4e862.67%\u3002"}}
{"id": "2507.00814", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.00814", "abs": "https://arxiv.org/abs/2507.00814", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "title": "Many LLMs Are More Utilitarian Than One", "comment": "9 pages, 8 Figures, 7 tables", "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "AI": {"tldr": "LLM \u7fa4\u4f53\u5728\u9053\u5fb7\u56f0\u5883\u4e2d\u8868\u73b0\u51fa\u529f\u5229\u4e3b\u4e49\u503e\u5411\uff0c\u4f46\u5176\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "motivation": "\u7406\u89e3 LLM \u5728\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u96c6\u4f53\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u4e2d\u7fa4\u4f53\u5ba1\u8bae\u5982\u4f55\u5bfc\u81f4\u529f\u5229\u4e3b\u4e49\u63d0\u5347\u7684\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u516d\u4e2a\u6a21\u578b\u6765\u7814\u7a76\u591a\u667a\u80fd\u4f53 LLM \u7cfb\u7edf\u4e2d\u7684\u9053\u5fb7\u5224\u65ad\uff1a(1) \u5355\u72ec\u63a8\u7406\uff1b(2) \u6210\u5bf9\u6216\u6210\u4e09\u5730\u8fdb\u884c\u591a\u8f6e\u8ba8\u8bba\u3002", "result": "\u5728\u4e2a\u4eba\u9053\u5fb7\u56f0\u5883\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u5728\u7fa4\u4f53\u4e2d\u6bd4\u5355\u72ec\u884c\u52a8\u65f6\u66f4\u5bb9\u6613\u63a5\u53d7\u9053\u5fb7\u8fdd\u89c4\u884c\u4e3a\u3002\u4e00\u4e9b\u6a21\u578b\u8ba4\u53ef\u6700\u5927\u5316\u6574\u4f53\u798f\u7949\u7684\u884c\u4e3a\uff0c\u5373\u4f7f\u8fd9\u4e9b\u884c\u4e3a\u4f7f\u964c\u751f\u4eba\u53d7\u76ca\u8d85\u8fc7\u4e86\u719f\u6089\u7684\u4eba\u3002\u5176\u4ed6\u6a21\u578b\u5219\u66f4\u613f\u610f\u5728\u7fa4\u4f53\u4e2d\u8fdd\u53cd\u9053\u5fb7\u89c4\u8303\u3002\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u4e0d\u540c\uff0cLLM \u7fa4\u4f53\u8868\u73b0\u51fa\u89c4\u8303\u654f\u611f\u6027\u964d\u4f4e\u6216\u516c\u6b63\u6027\u589e\u5f3a\u3002", "conclusion": "LLM \u96c6\u4f53\u5728\u9053\u5fb7\u56f0\u5883\u4e2d\u8868\u73b0\u51fa\u529f\u5229\u4e3b\u4e49\u503e\u5411\uff0c\u4f46\u5176\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u8fd9\u5bf9\u4e8e AI \u5bf9\u9f50\u3001\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u548c\u4eba\u5de5\u9053\u5fb7\u63a8\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.00080", "categories": ["cs.LG", "nlin.AO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.00080", "abs": "https://arxiv.org/abs/2507.00080", "authors": ["Ali Tavasoli", "Heman Shakeri"], "title": "Online Meal Detection Based on CGM Data Dynamics", "comment": null, "summary": "We utilize dynamical modes as features derived from Continuous Glucose\nMonitoring (CGM) data to detect meal events. By leveraging the inherent\nproperties of underlying dynamics, these modes capture key aspects of glucose\nvariability, enabling the identification of patterns and anomalies associated\nwith meal consumption. This approach not only improves the accuracy of meal\ndetection but also enhances the interpretability of the underlying glucose\ndynamics. By focusing on dynamical features, our method provides a robust\nframework for feature extraction, facilitating generalization across diverse\ndatasets and ensuring reliable performance in real-world applications. The\nproposed technique offers significant advantages over traditional approaches,\nimproving detection accuracy,", "AI": {"tldr": "Dynamical modes from CGM data are used to detect meal events, improving accuracy and interpretability compared to traditional methods.", "motivation": "By leveraging the inherent properties of underlying dynamics, these modes capture key aspects of glucose variability, enabling the identification of patterns and anomalies associated with meal consumption.", "method": "We utilize dynamical modes as features derived from Continuous Glucose Monitoring (CGM) data to detect meal events.", "result": "This approach not only improves the accuracy of meal detection but also enhances the interpretability of the underlying glucose dynamics.", "conclusion": "The proposed technique offers significant advantages over traditional approaches, improving detection accuracy."}}
{"id": "2507.00371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00371", "abs": "https://arxiv.org/abs/2507.00371", "authors": ["Xin Yang", "Ruiming Du", "Hanyang Huang", "Jiayang Xie", "Pengyao Xie", "Leisen Fang", "Ziyue Guo", "Nanjun Jiang", "Yu Jiang", "Haiyan Cen"], "title": "PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching", "comment": null, "summary": "Organ segmentation of plant point clouds is a prerequisite for the\nhigh-resolution and accurate extraction of organ-level phenotypic traits.\nAlthough the fast development of deep learning has boosted much research on\nsegmentation of plant point clouds, the existing techniques for organ\nsegmentation still face limitations in resolution, segmentation accuracy, and\ngeneralizability across various plant species. In this study, we proposed a\nnovel approach called plant segmentation neural radiance fields (PlantSegNeRF),\naiming to directly generate high-precision instance point clouds from\nmulti-view RGB image sequences for a wide range of plant species. PlantSegNeRF\nperformed 2D instance segmentation on the multi-view images to generate\ninstance masks for each organ with a corresponding ID. The multi-view instance\nIDs corresponding to the same plant organ were then matched and refined using a\nspecially designed instance matching module. The instance NeRF was developed to\nrender an implicit scene, containing color, density, semantic and instance\ninformation. The implicit scene was ultimately converted into high-precision\nplant instance point clouds based on the volume density. The results proved\nthat in semantic segmentation of point clouds, PlantSegNeRF outperformed the\ncommonly used methods, demonstrating an average improvement of 16.1%, 18.3%,\n17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the\nsecond-best results on structurally complex datasets. More importantly,\nPlantSegNeRF exhibited significant advantages in plant point cloud instance\nsegmentation tasks. Across all plant datasets, it achieved average improvements\nof 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.\nThis study extends the organ-level plant phenotyping and provides a\nhigh-throughput way to supply high-quality 3D data for the development of\nlarge-scale models in plant science.", "AI": {"tldr": "PlantSegNeRF is a new method for high-precision plant organ segmentation from multi-view images, outperforming existing techniques in accuracy and generalizability.", "motivation": "Existing plant point cloud organ segmentation techniques have limitations in resolution, accuracy, and generalizability across plant species.", "method": "A novel approach called PlantSegNeRF is proposed, which generates high-precision instance point clouds from multi-view RGB image sequences. It uses 2D instance segmentation, instance matching, and instance NeRF to render and convert an implicit scene into high-precision plant instance point clouds.", "result": "PlantSegNeRF outperforms existing methods in semantic segmentation, with improvements of 16.1% in precision, 18.3% in recall, 17.8% in F1-score, and 24.2% in IoU. It also shows significant advantages in plant point cloud instance segmentation, improving mPrec by 11.7%, mRec by 38.2%, mCov by 32.2% and mWCov by 25.3%.", "conclusion": "PlantSegNeRF extends organ-level plant phenotyping and offers a high-throughput method for providing high-quality 3D data, beneficial for large-scale plant science models."}}
{"id": "2507.00828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00828", "abs": "https://arxiv.org/abs/2507.00828", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolom\u00e9", "Jordan Boyd-Graber", "Philip Resnik"], "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "comment": "Accepted to ACL 2025 (Main)", "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "AI": {"tldr": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\u548c\u4e00\u4e2a\u76f8\u5e94\u7684\u81ea\u52a8\u8fd1\u4f3c\uff0c\u53d1\u73b0\u6700\u4f73\u7684LLM\u4ee3\u7406\u5728\u7edf\u8ba1\u5b66\u4e0a\u4e0e\u4eba\u5de5\u6ce8\u91ca\u5458\u6ca1\u6709\u533a\u522b\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u53ef\u4ee5\u4f5c\u4e3a\u5408\u7406\u7684\u66ff\u4ee3\u3002", "motivation": "\u4e3b\u9898\u6a21\u578b\u548c\u6587\u6863\u805a\u7c7b\u8bc4\u4f30\u8981\u4e48\u4f7f\u7528\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u7b26\u7684\u81ea\u52a8\u6307\u6807\uff0c\u8981\u4e48\u9700\u8981\u96be\u4ee5\u6269\u5c55\u7684\u4e13\u5bb6\u6807\u7b7e\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\u548c\u4e00\u4e2a\u76f8\u5e94\u7684\u81ea\u52a8\u8fd1\u4f3c\uff0c\u53cd\u6620\u4e86\u4ece\u4e1a\u8005\u5bf9\u6a21\u578b\u7684\u5b9e\u9645\u4f7f\u7528\u3002", "result": "\u6211\u4eec\u6536\u96c6\u4e86\u5927\u91cf\u4eba\u7fa4\u5de5\u4f5c\u8005\u5bf9\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5404\u79cd\u4e3b\u9898\u6a21\u578b\u8f93\u51fa\u7684\u6ce8\u91ca\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u6ce8\u91ca\u6765\u9a8c\u8bc1\u81ea\u52a8\u4ee3\u7406\uff0c", "conclusion": "\u6700\u4f73\u7684LLM\u4ee3\u7406\u5728\u7edf\u8ba1\u5b66\u4e0a\u4e0e\u4eba\u5de5\u6ce8\u91ca\u5458\u6ca1\u6709\u533a\u522b\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u4f5c\u4e3a\u5408\u7406\u7684\u66ff\u4ee3\u3002"}}
{"id": "2507.00082", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00082", "abs": "https://arxiv.org/abs/2507.00082", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "comment": "17 pages, 16 figures, IEEE Internet of Things", "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedHLM\u7684\u901a\u4fe1\u9ad8\u6548HLM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a8\u7406\u4e0e\u8054\u90a6\u5b66\u4e60\uff0c\u4ee5\u51cf\u5c11LLM\u4f20\u8f93\u3002", "motivation": "\u6df7\u5408\u8bed\u8a00\u6a21\u578b\uff08HLM\uff09\u7ed3\u5408\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u4f4e\u5ef6\u8fdf\u6548\u7387\u4e0e\u96c6\u4e2d\u5f0f\u670d\u52a1\u5668\u4e0a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u7cbe\u5ea6\u3002\u4f46\u662f\uff0c\u542b\u7cca\u4e0d\u6e05\u6216\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u9884\u6d4b\u4ecd\u7136\u9700\u8981\u9891\u7e41\u5378\u8f7d\u5230LLM\uff0c\u4ece\u800c\u5bfc\u81f4\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u4ea7\u751f\u5927\u91cf\u7684\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedHLM\u7684\u901a\u4fe1\u9ad8\u6548HLM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a8\u7406\u4e0e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u3002FedHLM\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u534f\u540c\u5b66\u4e60token\u7ea7\u522b\u7684\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0c\u8be5\u9608\u503c\u63a7\u5236\u4f55\u65f6\u9700\u8981LLM\u534f\u52a9\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5229\u7528\u57fa\u4e8e\u5d4c\u5165\u7684token\u8868\u793a\u8fdb\u884c\u5bf9\u7b49\uff08P2P\uff09\u89e3\u6790\uff0c\u4f7f\u5ba2\u6237\u7aef\u80fd\u591f\u91cd\u7528\u8bed\u4e49\u76f8\u4f3c\u5bf9\u7b49\u65b9\u63a8\u65ad\u7684token\uff0c\u800c\u65e0\u9700\u4e0eLLM\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5f15\u5165\u4e86\u5206\u5c42\u6a21\u578b\u805a\u5408\uff1a\u8fb9\u7f18\u670d\u52a1\u5668\u901a\u8fc7\u5ba2\u6237\u7aef\u66f4\u65b0\u6765\u6539\u8fdb\u672c\u5730\u8def\u7531\u7b56\u7565\uff0c\u800c\u8de8\u96c6\u7fa4\u534f\u8c03\u5219\u5bf9\u9f50\u5168\u5c40\u51b3\u7b56\u8fb9\u754c\u3002", "result": "FedHLM\u51cf\u5c11\u4e86\u8d85\u8fc795%\u7684LLM\u4f20\u8f93\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "FedHLM\u5728\u5927\u578b\u65b0\u95fb\u5206\u7c7b\u4efb\u52a1\u4e2d\u51cf\u5c11\u4e86\u8d85\u8fc795%\u7684LLM\u4f20\u8f93\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u975e\u5e38\u9002\u5408\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u8fb9\u7f18AI\u5e94\u7528\u3002"}}
{"id": "2507.00372", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00372", "abs": "https://arxiv.org/abs/2507.00372", "authors": ["Xinge Yang", "Chuong Nguyen", "Wenbin Wang", "Kaizhang Kang", "Wolfgang Heidrich", "Xiaoxing Li"], "title": "Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur", "comment": null, "summary": "Modern cameras with large apertures often suffer from a shallow depth of\nfield, resulting in blurry images of objects outside the focal plane. This\nlimitation is particularly problematic for fixed-focus cameras, such as those\nused in smart glasses, where adding autofocus mechanisms is challenging due to\nform factor and power constraints. Due to unmatched optical aberrations and\ndefocus properties unique to each camera system, deep learning models trained\non existing open-source datasets often face domain gaps and do not perform well\nin real-world settings. In this paper, we propose an efficient and scalable\ndataset synthesis approach that does not rely on fine-tuning with real-world\ndata. Our method simultaneously models depth-dependent defocus and spatially\nvarying optical aberrations, addressing both computational complexity and the\nscarcity of high-quality RGB-D datasets. Experimental results demonstrate that\na network trained on our low resolution synthetic images generalizes\neffectively to high resolution (12MP) real-world images across diverse scenes.", "AI": {"tldr": "This paper proposes a method to train networks using synthetic data that generalizes to real-world images by modeling depth-dependent defocus and spatially varying optical aberrations, which is effective for fixed-focus cameras such as those used in smart glasses.", "motivation": "Modern cameras with large apertures often suffer from a shallow depth of field, resulting in blurry images of objects outside the focal plane. This limitation is particularly problematic for fixed-focus cameras, such as those used in smart glasses, where adding autofocus mechanisms is challenging due to form factor and power constraints. Due to unmatched optical aberrations and defocus properties unique to each camera system, deep learning models trained on existing open-source datasets often face domain gaps and do not perform well in real-world settings.", "method": "an efficient and scalable dataset synthesis approach that does not rely on fine-tuning with real-world data. Our method simultaneously models depth-dependent defocus and spatially varying optical aberrations", "result": "a network trained on our low resolution synthetic images generalizes effectively to high resolution (12MP) real-world images across diverse scenes.", "conclusion": "A network trained on our low resolution synthetic images generalizes effectively to high resolution (12MP) real-world images across diverse scenes."}}
{"id": "2507.00838", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00838", "abs": "https://arxiv.org/abs/2507.00838", "authors": ["Karol Przystalski", "Jan K. Argasi\u0144ski", "Iwona Grabska-Gradzi\u0144ska", "Jeremi K. Ochab"], "title": "Stylometry recognizes human and LLM-generated texts in short samples", "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "AI": {"tldr": "This paper uses stylometry to distinguish between texts generated by LLMs and humans, achieving high accuracy in classification tasks.", "motivation": "addressing issues of model attribution, intellectual property, and ethical AI use", "method": "applying stylometry and tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based stylometric features", "result": "cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1 in binary classification", "conclusion": "it is possible to distinguish machine- from human-generated texts at least for a well-defined text type"}}
{"id": "2507.00083", "categories": ["cs.LG", "cs.AI", "91A80, 91B62, 68T07", "I.2.6; J.7; K.4.1; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.00083", "abs": "https://arxiv.org/abs/2507.00083", "authors": ["Wei Meng"], "title": "Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks", "comment": "This paper proposes the first closed-loop causal modeling framework\n  (IA-STGNN) that links tactical strike variables to strategic delay outcomes\n  via graph neural networks with counterfactual reasoning", "summary": "This study addresses the lack of structured causal modeling between tactical\nstrike behavior and strategic delay in current strategic-level simulations,\nparticularly the structural bottlenecks in capturing intermediate variables\nwithin the \"resilience - nodal suppression - negotiation window\" chain. We\npropose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),\na novel framework that closes the causal loop from tactical input to strategic\ndelay output. The model integrates graph attention mechanisms, counterfactual\nsimulation units, and spatial intervention node reconstruction to enable\ndynamic simulations of strike configurations and synchronization strategies.\nTraining data are generated from a multi-physics simulation platform (GEANT4 +\nCOMSOL) under NIST SP 800-160 standards, ensuring structural traceability and\npolicy-level validation. Experimental results demonstrate that IA-STGNN\nsignificantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),\nachieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5\npercent accuracy, while improving causal path consistency and intervention\nstability. IA-STGNN enables interpretable prediction of strategic delay and\nsupports applications such as nuclear deterrence simulation, diplomatic window\nassessment, and multi-strategy optimization, providing a structured and\ntransparent AI decision-support mechanism for high-level policy modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 IA-STGNN \u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6218\u7565\u6a21\u62df\u4e2d\u6218\u672f\u6253\u51fb\u884c\u4e3a\u548c\u6218\u7565\u5ef6\u8fdf\u4e4b\u95f4\u7f3a\u4e4f\u7ed3\u6784\u5316\u56e0\u679c\u5efa\u6a21\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6218\u7565\u7ea7\u6a21\u62df\u4e2d\u7f3a\u4e4f\u6218\u672f\u6253\u51fb\u884c\u4e3a\u548c\u6218\u7565\u5ef6\u8fdf\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u56e0\u679c\u5efa\u6a21\uff0c\u7279\u522b\u662f\u5728\u6355\u83b7\u201c\u5f39\u6027-\u8282\u70b9\u6291\u5236-\u8c08\u5224\u7a97\u53e3\u201d\u94fe\u4e2d\u7684\u4e2d\u95f4\u53d8\u91cf\u65f6\u5b58\u5728\u7ed3\u6784\u6027\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u5e72\u9884\u611f\u77e5\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc (IA-STGNN)\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u56fe\u6ce8\u610f\u529b\u673a\u5236\u3001\u53cd\u4e8b\u5b9e\u6a21\u62df\u5355\u5143\u548c\u7a7a\u95f4\u5e72\u9884\u8282\u70b9\u91cd\u6784\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6253\u51fb\u914d\u7f6e\u548c\u540c\u6b65\u7b56\u7565\u7684\u52a8\u6001\u6a21\u62df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIA-STGNN \u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b (ST-GNN\u3001GCN-LSTM\u3001XGBoost)\uff0cMAE \u964d\u4f4e\u4e86 12.8%\uff0cTop-5% \u51c6\u786e\u7387\u63d0\u9ad8\u4e86 18.4%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u56e0\u679c\u8def\u5f84\u4e00\u81f4\u6027\u548c\u5e72\u9884\u7a33\u5b9a\u6027\u3002", "conclusion": "IA-STGNN \u5b9e\u73b0\u4e86\u5bf9\u6218\u7565\u5ef6\u8fdf\u7684\u53ef\u89e3\u91ca\u9884\u6d4b\uff0c\u5e76\u652f\u6301\u6838\u5a01\u6151\u6a21\u62df\u3001\u5916\u4ea4\u7a97\u53e3\u8bc4\u4f30\u548c\u591a\u7b56\u7565\u4f18\u5316\u7b49\u5e94\u7528\uff0c\u4e3a\u9ad8\u7ea7\u7b56\u7565\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u548c\u900f\u660e\u7684 AI \u51b3\u7b56\u652f\u6301\u673a\u5236\u3002"}}
{"id": "2507.00373", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00373", "abs": "https://arxiv.org/abs/2507.00373", "authors": ["Ian Jin", "Fanxin Xia", "Feng Ding", "Xinfeng Zhang", "Meiqin Liu", "Yao Zhao", "Weisi Lin", "Lili Meng"], "title": "Customizable ROI-Based Deep Image Compression", "comment": null, "summary": "Region of Interest (ROI)-based image compression optimizes bit allocation by\nprioritizing ROI for higher-quality reconstruction. However, as the users\n(including human clients and downstream machine tasks) become more diverse,\nROI-based image compression needs to be customizable to support various\npreferences. For example, different users may define distinct ROI or require\ndifferent quality trade-offs between ROI and non-ROI. Existing ROI-based image\ncompression schemes predefine the ROI, making it unchangeable, and lack\neffective mechanisms to balance reconstruction quality between ROI and non-ROI.\nThis work proposes a paradigm for customizable ROI-based deep image\ncompression. First, we develop a Text-controlled Mask Acquisition (TMA) module,\nwhich allows users to easily customize their ROI for compression by just\ninputting the corresponding semantic \\emph{text}. It makes the encoder\ncontrolled by text. Second, we design a Customizable Value Assign (CVA)\nmechanism, which masks the non-ROI with a changeable extent decided by users\ninstead of a constant one to manage the reconstruction quality trade-off\nbetween ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)\nmodule, where the latent spatial prior of the mask and the latent\nRate-Distortion Optimization (RDO) prior of the image are extracted and fused\nin the latent space, and further used to optimize the latent representation of\nthe source image. Experimental results demonstrate that our proposed\ncustomizable ROI-based deep image compression paradigm effectively addresses\nthe needs of customization for ROI definition and mask acquisition as well as\nthe reconstruction quality trade-off management between the ROI and non-ROI.", "AI": {"tldr": "Proposes a customizable ROI-based deep image compression paradigm with TMA, CVA, and LMA modules to address the needs of customization for ROI definition and reconstruction quality trade-off.", "motivation": "Existing ROI-based image compression schemes predefine the ROI, making it unchangeable, and lack effective mechanisms to balance reconstruction quality between ROI and non-ROI. This work proposes a paradigm for customizable ROI-based deep image compression to support various preferences.", "method": "develop a Text-controlled Mask Acquisition (TMA) module, design a Customizable Value Assign (CVA) mechanism, present a Latent Mask Attention (LMA) module", "result": "Experimental results demonstrate that our proposed customizable ROI-based deep image compression paradigm effectively addresses the needs of customization for ROI definition and mask acquisition as well as the reconstruction quality trade-off management between the ROI and non-ROI.", "conclusion": "This work proposes a customizable ROI-based deep image compression paradigm that effectively addresses the needs of customization for ROI definition and mask acquisition as well as the reconstruction quality trade-off management between the ROI and non-ROI."}}
{"id": "2507.00875", "categories": ["cs.CL", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.00875", "abs": "https://arxiv.org/abs/2507.00875", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "AI": {"tldr": "TransLaw, a multi-agent framework, translates Hong Kong legal judgments with high accuracy and cost reduction, outperforming GPT-4o in some aspects.", "motivation": "The potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures.", "method": "A novel multi-agent framework called TransLaw, employing three specialized agents: Translator, Annotator, and Proofreader.", "result": "The framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services.", "conclusion": "The TransLaw framework surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, but trails human experts in contextualizing complex terminology and stylistic naturalness."}}
{"id": "2507.00085", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00085", "abs": "https://arxiv.org/abs/2507.00085", "authors": ["Ruiyuan Jiang", "Dongyao Jia", "Eng Gee Lim", "Pengfei Fan", "Yuli Zhang", "Shangbo Wang"], "title": "A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism", "comment": null, "summary": "Accurate traffic prediction is essential for Intelligent Transportation\nSystems (ITS), yet current methods struggle with the inherent complexity and\nnon-linearity of traffic dynamics, making it difficult to integrate spatial and\ntemporal characteristics. Furthermore, existing approaches use static\ntechniques to address non-stationary and anomalous historical data, which\nlimits adaptability and undermines data smoothing. To overcome these\nchallenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative\nframework for network-level traffic speed prediction. GFEN introduces a novel\ntopological spatiotemporal graph fusion technique that meticulously extracts\nand merges spatial and temporal correlations from both data distribution and\nnetwork topology using trainable methods, enabling the modeling of multi-scale\nspatiotemporal features. Additionally, GFEN employs a hybrid methodology\ncombining a k-th order difference-based mathematical framework with an\nattention-based deep learning structure to adaptively smooth historical\nobservations and dynamically mitigate data anomalies and non-stationarity.\nExtensive experiments demonstrate that GFEN surpasses state-of-the-art methods\nby approximately 6.3% in prediction accuracy and exhibits convergence rates\nnearly twice as fast as recent hybrid models, confirming its superior\nperformance and potential to significantly enhance traffic prediction system\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u7edc\u7ea7\u4ea4\u901a\u901f\u5ea6\u9884\u6d4b\u6846\u67b6\uff0c\u540d\u4e3a\u56fe\u878d\u5408\u589e\u5f3a\u7f51\u7edc(GFEN)\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u5f53\u524d\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4ea4\u901a\u52a8\u6001\u7684\u5185\u5728\u590d\u6742\u6027\u548c\u975e\u7ebf\u6027\uff0c\u96be\u4ee5\u6574\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u6280\u672f\u6765\u5904\u7406\u975e\u5e73\u7a33\u548c\u5f02\u5e38\u7684\u5386\u53f2\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u9002\u5e94\u6027\u5e76\u7834\u574f\u4e86\u6570\u636e\u5e73\u6ed1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u878d\u5408\u589e\u5f3a\u7f51\u7edc(GFEN)\uff0cGFEN \u91c7\u7528\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u5c06\u57fa\u4e8ek\u9636\u5dee\u5206\u7684\u6570\u5b66\u6846\u67b6\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u7ed3\u6784\u76f8\u7ed3\u5408\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u5e73\u6ed1\u5386\u53f2\u89c2\u6d4b\u6570\u636e\uff0c\u5e76\u52a8\u6001\u5730\u7f13\u89e3\u6570\u636e\u5f02\u5e38\u548c\u975e\u5e73\u7a33\u6027\u3002", "result": "GFEN\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7ea66.3%\uff0c\u5e76\u4e14\u6536\u655b\u901f\u5ea6\u51e0\u4e4e\u662f\u6700\u8fd1\u6df7\u5408\u6a21\u578b\u7684\u4e24\u500d\u3002", "conclusion": "GFEN\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7ea66.3%\uff0c\u5e76\u4e14\u6536\u655b\u901f\u5ea6\u51e0\u4e4e\u662f\u6700\u8fd1\u6df7\u5408\u6a21\u578b\u7684\u4e24\u500d\uff0c\u8bc1\u660e\u4e86\u5176\u5353\u8d8a\u7684\u6027\u80fd\u548c\u663e\u8457\u63d0\u9ad8\u4ea4\u901a\u9884\u6d4b\u7cfb\u7edf\u6548\u7387\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.00377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00377", "abs": "https://arxiv.org/abs/2507.00377", "authors": ["Jianhao Xie", "Ziang Zhang", "Zhenyu Weng", "Yuesheng Zhu", "Guibo Luo"], "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis", "comment": "11 pages,3 figures", "summary": "Recent advancements in deep learning for medical image segmentation are often\nlimited by the scarcity of high-quality training data.While diffusion models\nprovide a potential solution by generating synthetic images, their\neffectiveness in medical imaging remains constrained due to their reliance on\nlarge-scale medical datasets and the need for higher image quality. To address\nthese challenges, we present MedDiff-FT, a controllable medical image\ngeneration method that fine-tunes a diffusion foundation model to produce\nmedical images with structural dependency and domain specificity in a\ndata-efficient manner. During inference, a dynamic adaptive guiding mask\nenforces spatial constraints to ensure anatomically coherent synthesis, while a\nlightweight stochastic mask generator enhances diversity through hierarchical\nrandomness injection. Additionally, an automated quality assessment protocol\nfilters suboptimal outputs using feature-space metrics, followed by mask\ncorrosion to refine fidelity. Evaluated on five medical segmentation\ndatasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's\nsegmentation performance by an average of 1% in Dice score. The framework\neffectively balances generation quality, diversity, and computational\nefficiency, offering a practical solution for medical data augmentation. The\ncode is available at https://github.com/JianhaoXie1/MedDiff-FT.", "AI": {"tldr": "MedDiff-FT\u662f\u4e00\u79cd\u53ef\u63a7\u7684\u533b\u5b66\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6269\u6563\u6a21\u578b\u5e76\u7ed3\u5408\u52a8\u6001\u5f15\u5bfc\u548c\u8d28\u91cf\u8bc4\u4f30\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u8fdb\u5c55\u53d7\u5230\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002\u867d\u7136\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u50cf\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u5b83\u4eec\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u4ee5\u53ca\u9700\u8981\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u56e0\u6b64\u5b83\u4eec\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u6709\u6548\u6027\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u7684\u533b\u5b66\u56fe\u50cf\u751f\u6210\u65b9\u6cd5MedDiff-FT\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u6269\u6563\u57fa\u7840\u6a21\u578b\u4ee5\u6570\u636e\u6709\u6548\u7684\u65b9\u5f0f\u751f\u6210\u5177\u6709\u7ed3\u6784\u4f9d\u8d56\u6027\u548c\u9886\u57df\u7279\u5f02\u6027\u7684\u533b\u5b66\u56fe\u50cf\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u52a8\u6001\u81ea\u9002\u5e94\u5f15\u5bfcmask\u5f3a\u5236\u7a7a\u95f4\u7ea6\u675f\u4ee5\u786e\u4fdd\u89e3\u5256\u5b66\u4e0a\u8fde\u8d2f\u7684\u5408\u6210\uff0c\u800c\u8f7b\u91cf\u7ea7\u968f\u673amask\u751f\u6210\u5668\u901a\u8fc7\u5206\u5c42\u968f\u673a\u6027\u6ce8\u5165\u6765\u589e\u5f3a\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u534f\u8bae\u4f7f\u7528\u7279\u5f81\u7a7a\u95f4\u5ea6\u91cf\u6765\u8fc7\u6ee4\u6b21\u4f18\u8f93\u51fa\uff0c\u7136\u540e\u8fdb\u884cmask\u8150\u8680\u4ee5\u63d0\u9ad8\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cMedDiff-FT\u7684\u5408\u6210\u56fe\u50cf-mask\u5bf9\u5c06SOTA\u65b9\u6cd5\u7684\u5206\u5272\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e861%\u7684Dice\u5206\u6570\u3002", "conclusion": "MedDiff-FT\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u5408\u6210\u56fe\u50cf\uff0c\u6709\u6548\u5e73\u8861\u4e86\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u533b\u5b66\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u4e94\u4e2a\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0c\u5c06SOTA\u65b9\u6cd5\u7684\u5206\u5272\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e861%\u7684Dice\u5206\u6570\u3002"}}
{"id": "2507.00883", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00883", "abs": "https://arxiv.org/abs/2507.00883", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "comment": null, "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "AI": {"tldr": "Culturally adapted variants of the GSM8K test set reveal a consistent performance gap.", "motivation": "Existing benchmarks like GSM8K are predominantly rooted in Western norms, including names, currencies, and everyday scenarios.", "method": "We create culturally adapted variants of the GSM8K test set for five regions Africa, India, China, Korea, and Japan using prompt-based transformations followed by manual verification. We evaluate six large language models (LLMs), ranging from 8B to 72B parameters, across five prompting strategies", "result": "reveal a consistent performance gap", "conclusion": "LLMs perform best on the original US-centric dataset and comparatively worse on culturally adapted versions. Models with reasoning capabilities are more resilient to these shifts."}}
{"id": "2507.00087", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00087", "abs": "https://arxiv.org/abs/2507.00087", "authors": ["Jiale Zhao", "Pengzhi Mao", "Kaifei Wang", "Yiming Li", "Yaping Peng", "Ranfei Chen", "Shuqi Lu", "Xiaohong Ji", "Jiaxiang Ding", "Xin Zhang", "Yucheng Liao", "Weinan E", "Weijie Zhang", "Han Wen", "Hao Chi"], "title": "pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation", "comment": null, "summary": "Deep learning has advanced mass spectrometry data interpretation, yet most\nmodels remain feature extractors rather than unified scoring frameworks. We\npresent pUniFind, the first large-scale multimodal pre-trained model in\nproteomics that integrates end-to-end peptide-spectrum scoring with open,\nzero-shot de novo sequencing. Trained on over 100 million open search-derived\nspectra, pUniFind aligns spectral and peptide modalities via cross modality\nprediction and outperforms traditional engines across diverse datasets,\nparticularly achieving a 42.6 percent increase in the number of identified\npeptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind\nidentifies 60 percent more PSMs than existing de novo methods despite a\n300-fold larger search space. A deep learning based quality control module\nfurther recovers 38.5 percent additional peptides including 1,891 mapped to the\ngenome but absent from reference proteomes while preserving full fragment ion\ncoverage. These results establish a unified, scalable deep learning framework\nfor proteomic analysis, offering improved sensitivity, modification coverage,\nand interpretability.", "AI": {"tldr": "pUniFind, a large-scale multimodal pre-trained model in proteomics, integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing, outperforming traditional methods and improving sensitivity, modification coverage, and interpretability.", "motivation": "Deep learning has advanced mass spectrometry data interpretation, yet most models remain feature extractors rather than unified scoring frameworks.", "method": "a large-scale multimodal pre-trained model that integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing", "result": "pUniFind outperforms traditional engines across diverse datasets, particularly achieving a 42.6 percent increase in the number of identified peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind identifies 60 percent more PSMs than existing de novo methods despite a 300-fold larger search space. A deep learning based quality control module further recovers 38.5 percent additional peptides including 1,891 mapped to the genome but absent from reference proteomes while preserving full fragment ion coverage.", "conclusion": "pUniFind is a unified, scalable deep learning framework for proteomic analysis, offering improved sensitivity, modification coverage, and interpretability."}}
{"id": "2507.00392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00392", "abs": "https://arxiv.org/abs/2507.00392", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space", "comment": null, "summary": "Feature matching plays a fundamental role in many computer vision tasks, yet\nexisting methods heavily rely on scarce and clean multi-view image collections,\nwhich constrains their generalization to diverse and challenging scenarios.\nMoreover, conventional feature encoders are typically trained on single-view 2D\nimages, limiting their capacity to capture 3D-aware correspondences. In this\npaper, we propose a novel two-stage framework that lifts 2D images to 3D space,\nnamed as \\textbf{Lift to Match (L2M)}, taking full advantage of large-scale and\ndiverse single-view images. To be specific, in the first stage, we learn a\n3D-aware feature encoder using a combination of multi-view image synthesis and\n3D feature Gaussian representation, which injects 3D geometry knowledge into\nthe encoder. In the second stage, a novel-view rendering strategy, combined\nwith large-scale synthetic data generation from single-view images, is employed\nto learn a feature decoder for robust feature matching, thus achieving\ngeneralization across diverse domains. Extensive experiments demonstrate that\nour method achieves superior generalization across zero-shot evaluation\nbenchmarks, highlighting the effectiveness of the proposed framework for robust\nfeature matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a L2M \u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u89c4\u6a21\u5355\u89c6\u56fe\u56fe\u50cf\u5c06 2D \u56fe\u50cf\u63d0\u5347\u5230 3D \u7a7a\u95f4\uff0c\u4ece\u800c\u5b9e\u73b0\u9c81\u68d2\u7684\u7279\u5f81\u5339\u914d\u548c\u8de8\u57df\u6cdb\u5316\u3002", "motivation": "\u7279\u5f81\u5339\u914d\u5728\u8bb8\u591a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8d77\u7740 \u0444\u0443\u043d\u0434\u0430\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u0443\u044e \u0440\u043e\u043b\u044c\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e8e\u7a00\u7f3a\u548c\u5e72\u51c0\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u96c6\u5408\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u7684\u6cdb\u5316\u3002\u6b64\u5916\uff0c\u4f20\u7edf\u7684\u7279\u5f81\u7f16\u7801\u5668\u901a\u5e38\u5728\u5355\u89c6\u56fe 2D \u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6355\u83b7 3D \u611f\u77e5\u5bf9\u5e94\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06 2D \u56fe\u50cf\u63d0\u5347\u5230 3D \u7a7a\u95f4\uff0c\u547d\u540d\u4e3a Lift to Match (L2M)\uff0c\u5145\u5206\u5229\u7528\u5927\u89c4\u6a21\u548c\u591a\u6837\u5316\u7684\u5355\u89c6\u56fe\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u4f7f\u7528\u591a\u89c6\u56fe\u56fe\u50cf\u5408\u6210\u548c 3D \u7279\u5f81\u9ad8\u65af\u8868\u793a\u7684\u7ec4\u5408\u6765\u5b66\u4e60 3D \u611f\u77e5\u7279\u5f81\u7f16\u7801\u5668\uff0c\u4ece\u800c\u5c06 3D \u51e0\u4f55\u77e5\u8bc6\u6ce8\u5165\u5230\u7f16\u7801\u5668\u4e2d\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u56fe\u6e32\u67d3\u7b56\u7565\uff0c\u7ed3\u5408\u5355\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\uff0c\u5b66\u4e60\u4e00\u79cd\u7528\u4e8e\u9c81\u68d2\u7279\u5f81\u5339\u914d\u7684\u7279\u5f81\u89e3\u7801\u5668\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u4e0d\u540c\u9886\u57df\u7684\u6cdb\u5316\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\uff0c\u7a81\u51fa\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u9c81\u68d2\u7279\u5f81\u5339\u914d\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\uff0c\u7a81\u51fa\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u9c81\u68d2\u7279\u5f81\u5339\u914d\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.00885", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00885", "abs": "https://arxiv.org/abs/2507.00885", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "comment": null, "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "AI": {"tldr": "\u4e0b\u6e38\u7f29\u653e\u89c4\u5f8b\u5e76\u4e0d\u603b\u662f\u6709\u6548\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u9002\u7528\u6761\u4ef6\u3002", "motivation": "\u4e0b\u6e38\u7f29\u653e\u89c4\u5f8b\u65e8\u5728\u4ece\u5c0f\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u635f\u5931\u9884\u6d4b\u66f4\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u8868\u73b0\u3002\u4f46\u8fd9\u79cd\u9884\u6d4b\u662f\u5426\u53ef\u884c\u5c1a\u4e0d\u6e05\u695a\uff0c\u4e0b\u6e38\u7f29\u653e\u89c4\u5f8b\u9762\u4e34\u6d8c\u73b0\u548c\u9006\u7f29\u653e\u7b49\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u5bf9\u73b0\u6709\u4e0b\u6e38\u7f29\u653e\u89c4\u5f8b\u6570\u636e\u8fdb\u884c\u835f\u8403\u5206\u6790", "result": "\u53ea\u6709 39% \u7684\u60c5\u51b5\u4e0b\u4e0e\u7ebf\u6027\u7f29\u653e\u89c4\u5f8b\u5bc6\u5207\u543b\u5408\uff0c\u5b9e\u9a8c\u73af\u5883\u7684\u5fae\u5c0f\u53d8\u5316\u4f1a\u5b8c\u5168\u6539\u53d8\u7f29\u653e\u8d8b\u52bf\u3002", "conclusion": "\u7ebf\u6027\u7f29\u653e\u89c4\u5f8b\u5e76\u4e0d\u603b\u662f\u6210\u7acb\uff0c\u5b9e\u9a8c\u73af\u5883\u7684\u5fae\u5c0f\u53d8\u5316\u4f1a\u5b8c\u5168\u6539\u53d8\u7f29\u653e\u8d8b\u52bf\u3002\u6211\u4eec\u9700\u8981\u4e86\u89e3\u7f29\u653e\u89c4\u5f8b\u6210\u529f\u7684\u6761\u4ef6\uff0c\u5e76\u63a5\u53d7\u7f29\u653e\u884c\u4e3a\u504f\u79bb\u7ebf\u6027\u8d8b\u52bf\u7684\u60c5\u51b5\u3002"}}
{"id": "2507.00089", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.00089", "abs": "https://arxiv.org/abs/2507.00089", "authors": ["Aho Yapi", "Pierre Latouche", "Arnaud Guillin", "Yan Bailly"], "title": "A new machine learning framework for occupational accidents forecasting with safety inspections integration", "comment": null, "summary": "We propose a generic framework for short-term occupational accident\nforecasting that leverages safety inspections and models accident occurrences\nas binary time series. The approach generates daily predictions, which are then\naggregated into weekly safety assessments to better inform decision making. To\nensure the reliability and operational applicability of the forecasts, we apply\na sliding-window cross-validation procedure specifically designed for time\nseries data, combined with an evaluation based on aggregated period-level\nmetrics. Several machine learning algorithms, including logistic regression,\ntree-based models, and neural networks, are trained and systematically compared\nwithin this framework. Unlike the other approaches, the long short-term memory\n(LSTM) network outperforms the other approaches and detects the upcoming\nhigh-risk periods with a balanced accuracy of 0.86, confirming the robustness\nof our methodology and demonstrating that a binary time series model can\nanticipate these critical periods based on safety inspections. The proposed\nmethodology converts routine safety inspection data into clear weekly risk\nscores, detecting the periods when accidents are most likely. Decision-makers\ncan integrate these scores into their planning tools to classify inspection\npriorities, schedule targeted interventions, and funnel resources to the sites\nor shifts classified as highest risk, stepping in before incidents occur and\ngetting the greatest return on safety investments.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u77ed\u671f\u804c\u4e1a\u4e8b\u6545\u9884\u6d4b\u7684\u901a\u7528\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5b89\u5168\u68c0\u67e5\u5e76\u5c06\u4e8b\u6545\u53d1\u751f\u5efa\u6a21\u4e3a\u4e8c\u5143\u65f6\u95f4\u5e8f\u5217\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u4e13\u95e8\u4e3a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8bbe\u8ba1\u7684\u6ed1\u52a8\u7a97\u53e3\u4ea4\u53c9\u9a8c\u8bc1\u7a0b\u5e8f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u805a\u5408\u5468\u671f\u7ea7\u522b\u6307\u6807\u7684\u8bc4\u4f30\u3002", "method": "\u5229\u7528\u5b89\u5168\u68c0\u67e5\u5e76\u5efa\u6a21\u4e8b\u6545\u53d1\u751f\u4f5c\u4e3a\u4e8c\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u901a\u7528\u6846\u67b6\uff0c\u751f\u6210\u6bcf\u65e5\u9884\u6d4b\uff0c\u7136\u540e\u6c47\u603b\u6210\u6bcf\u5468\u5b89\u5168\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u5e38\u89c4\u5b89\u5168\u68c0\u67e5\u6570\u636e\u8f6c\u6362\u4e3a\u6e05\u6670\u7684\u6bcf\u5468\u98ce\u9669\u8bc4\u5206\uff0c\u68c0\u6d4b\u4e8b\u6545\u6700\u53ef\u80fd\u53d1\u751f\u7684\u65f6\u671f\u3002", "conclusion": "LSTM\u7f51\u7edc\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u591f\u9884\u6d4b\u5373\u5c06\u5230\u6765\u7684\u9ad8\u98ce\u9669\u671f\uff0c\u5e73\u8861\u51c6\u786e\u7387\u8fbe\u52300.86\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u7a33\u5065\u6027\uff0c\u5e76\u8868\u660e\u4e8c\u5143\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u5b89\u5168\u68c0\u67e5\u9884\u6d4b\u8fd9\u4e9b\u5173\u952e\u65f6\u671f\u3002"}}
{"id": "2507.00401", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00401", "abs": "https://arxiv.org/abs/2507.00401", "authors": ["Xin Xu", "Eibe Frank", "Geoffrey Holmes"], "title": "Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains", "comment": null, "summary": "We investigate cross-domain few-shot learning under the constraint that\nfine-tuning of backbones (i.e., feature extractors) is impossible or infeasible\n-- a scenario that is increasingly common in practical use cases. Handling the\nlow-quality and static embeddings produced by frozen, \"black-box\" backbones\nleads to a problem representation of few-shot classification as a series of\nmultiple instance verification (MIV) tasks. Inspired by this representation, we\nintroduce a novel approach to few-shot domain adaptation, named the \"MIV-head\",\nakin to a classification head that is agnostic to any pretrained backbone and\ncomputationally efficient. The core components designed for the MIV-head, when\ntrained on few-shot data from a target domain, collectively yield strong\nperformance on test data from that domain. Importantly, it does so without\nfine-tuning the backbone, and within the \"meta-testing\" phase. Experimenting\nunder various settings and on an extension of the Meta-dataset benchmark for\ncross-domain few-shot image classification, using representative off-the-shelf\nconvolutional neural network and vision transformer backbones pretrained on\nImageNet1K, we show that the MIV-head achieves highly competitive accuracy when\ncompared to state-of-the-art \"adapter\" (or partially fine-tuning) methods\napplied to the same backbones, while incurring substantially lower adaptation\ncost. We also find well-known \"classification head\" approaches lag far behind\nin terms of accuracy. Ablation study empirically justifies the core components\nof our approach. We share our code at https://github.com/xxweka/MIV-head.", "AI": {"tldr": "\u63d0\u51faMIV-head\u7528\u4e8e\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60\uff0c\u65e0\u9700\u5fae\u8c03backbone\uff0c\u9002\u5e94\u6210\u672c\u4f4e\uff0c\u5e76\u5728Meta-dataset\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7814\u7a76\u4e86backbone\u5fae\u8c03\u4e0d\u53ef\u884c\u60c5\u51b5\u4e0b\u7684\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60\uff0c\u8fd9\u79cd\u60c5\u51b5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u5e38\u89c1\u3002\u5904\u7406frozen\u7684\u201c\u9ed1\u76d2\u201dbackbone\u4ea7\u751f\u7684\u4f4e\u8d28\u91cf\u548c\u9759\u6001\u5d4c\u5165\uff0c\u5bfc\u81f4\u5c0f\u6837\u672c\u5206\u7c7b\u7684\u95ee\u9898\u8868\u793a\u4e3a\u4e00\u7cfb\u5217\u591a\u5b9e\u4f8b\u9a8c\u8bc1(MIV)\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cMIV-head\u201d\u7684\u65b0\u578b\u5c0f\u6837\u672c\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7c7b\u4f3c\u4e8e\u4e0e\u4efb\u4f55\u9884\u8bad\u7ec3backbone\u65e0\u5173\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5206\u7c7bhead\u3002", "result": "MIV-head\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u5728\u4f7f\u7528\u5728ImageNet1K\u4e0a\u9884\u8bad\u7ec3\u7684\u4ee3\u8868\u6027off-the-shelf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u89c6\u89c9transformer backbone\u7684\u8de8\u57df\u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7bMeta-dataset\u57fa\u51c6\u6d4b\u8bd5\u7684\u6269\u5c55\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8868\u660e\u4e0e\u5e94\u7528\u4e8e\u76f8\u540cbackbone\u7684\u6700\u5148\u8fdb\u7684\u201cadapter\u201d\u65b9\u6cd5\u76f8\u6bd4\uff0cMIV-head\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u9002\u5e94\u6210\u672c\u5927\u5927\u964d\u4f4e\u3002 \u8fd8\u53d1\u73b0\uff0c\u5728\u51c6\u786e\u6027\u65b9\u9762\uff0c\u4f17\u6240\u5468\u77e5\u7684\u201c\u5206\u7c7bhead\u201d\u65b9\u6cd5\u8fdc\u8fdc\u843d\u540e\u3002", "conclusion": "MIV-head\u5728\u4e0d\u5fae\u8c03backbone\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u8de8\u57df\u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u7684Meta-dataset\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u201cadapter\u201d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u9002\u5e94\u6210\u672c\u66f4\u4f4e\u3002"}}
{"id": "2507.00891", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00891", "abs": "https://arxiv.org/abs/2507.00891", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "comment": null, "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86MemeCMD\uff0c\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684\u4e2d\u6587\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684meme\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u4e3b\u8981\u5c40\u9650\u4e8e\u4eba\u5de5\u6ce8\u91ca\u6216\u7eaf\u6587\u672c\u5bf9\u8bdd\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u4ea4\u4e92\u6240\u63d0\u4f9b\u7684\u8868\u8fbe\u6027\u548c\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u5f15\u5165\u68c0\u7d22\u6846\u67b6\u548c\u81ea\u9002\u5e94\u9608\u503c\uff0c\u4ee5\u786e\u4fdd\u4e0a\u4e0b\u6587\u76f8\u5173\u3001\u81ea\u7136\u95f4\u9694\u7684meme\u4f7f\u7528\u3002\u901a\u8fc7\u53cc\u91cd\u4ee3\u7406\u5728\u4e0d\u540c\u7684\u573a\u666f\u4e2d\u81ea\u52a8\u751f\u6210\u5bf9\u8bdd\uff0c\u5e76\u7ed3\u5408\u5927\u89c4\u6a21\u7684\u3001MLLM\u6ce8\u91ca\u7684meme\u5e93\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u4e0a\u4e0b\u6587\u9002\u5f53\u548c\u591a\u6837\u5316\u7684\u3001\u5305\u542bmeme\u7684\u5bf9\u8bdd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u3001\u591a\u6837\u5316\u7684\u3001\u5305\u542bmeme\u7684\u5bf9\u8bdd\u7684\u65b9\u6cd5\uff0c\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u5bf9\u8bdd\u5f0fAI\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u4fdd\u62a4\u9690\u79c1\u7684\u8d44\u6e90\u3002"}}
{"id": "2507.00090", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00090", "abs": "https://arxiv.org/abs/2507.00090", "authors": ["Corbeau Michael", "Claeys Emmanuelle", "Serrurier Mathieu", "Zarat\u00e9 Pascale"], "title": "Generating Heterogeneous Multi-dimensional Data : A Comparative Study", "comment": "accepted at IEEE SMC 2025 Vienna", "summary": "Allocation of personnel and material resources is highly sensible in the case\nof firefighter interventions. This allocation relies on simulations to\nexperiment with various scenarios. The main objective of this allocation is the\nglobal optimization of the firefighters response. Data generation is then\nmandatory to study various scenarios In this study, we propose to compare\ndifferent data generation methods. Methods such as Random Sampling, Tabular\nVariational Autoencoders, standard Generative Adversarial Networks, Conditional\nTabular Generative Adversarial Networks and Diffusion Probabilistic Models are\nexamined to ascertain their efficacy in capturing the intricacies of\nfirefighter interventions. Traditional evaluation metrics often fall short in\ncapturing the nuanced requirements of synthetic datasets for real-world\nscenarios. To address this gap, an evaluation of synthetic data quality is\nconducted using a combination of domain-specific metrics tailored to the\nfirefighting domain and standard measures such as the Wasserstein distance.\nDomain-specific metrics include response time distribution, spatial-temporal\ndistribution of interventions, and accidents representation. These metrics are\ndesigned to assess data variability, the preservation of fine and complex\ncorrelations and anomalies such as event with a very low occurrence, the\nconformity with the initial statistical distribution and the operational\nrelevance of the synthetic data. The distribution has the particularity of\nbeing highly unbalanced, none of the variables following a Gaussian\ndistribution, adding complexity to the data generation process.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5728\u6d88\u9632\u5458\u5e72\u9884\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u548c\u6807\u51c6\u6307\u6807\u8bc4\u4f30\u4e86\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u3002", "motivation": "\u5728\u6d88\u9632\u5458\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u4eba\u5458\u548c\u7269\u8d44\u8d44\u6e90\u7684\u5206\u914d\u975e\u5e38\u91cd\u8981\u3002\u8fd9\u79cd\u5206\u914d\u4f9d\u8d56\u4e8e\u6a21\u62df\u6765\u8bd5\u9a8c\u5404\u79cd\u573a\u666f\u3002\u8fd9\u79cd\u5206\u914d\u7684\u4e3b\u8981\u76ee\u6807\u662f\u5168\u5c40\u4f18\u5316\u6d88\u9632\u5458\u7684\u54cd\u5e94\u3002\u56e0\u6b64\uff0c\u6570\u636e\u751f\u6210\u5bf9\u4e8e\u7814\u7a76\u5404\u79cd\u573a\u666f\u662f\u5f3a\u5236\u6027\u7684\u3002", "method": "\u7814\u7a76\u8003\u5bdf\u4e86\u968f\u673a\u62bd\u6837\u3001\u8868\u683c\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u3001\u6807\u51c6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u3001\u6761\u4ef6\u8868\u683c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u6269\u6563\u6982\u7387\u6a21\u578b\u7b49\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u7684\u6307\u6807\uff08\u5982\u54cd\u5e94\u65f6\u95f4\u5206\u5e03\u3001\u5e72\u9884\u7684\u65f6\u7a7a\u5206\u5e03\u548c\u4e8b\u6545\u8868\u793a\uff09\u4ee5\u53ca\u6807\u51c6\u5ea6\u91cf\uff08\u5982 Wasserstein \u8ddd\u79bb\uff09\u76f8\u7ed3\u5408\u6765\u8bc4\u4f30\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u3002\u8fd9\u4e9b\u6307\u6807\u65e8\u5728\u8bc4\u4f30\u6570\u636e\u53ef\u53d8\u6027\uff0c\u4fdd\u7559\u7cbe\u7ec6\u548c\u590d\u6742\u7684\u76f8\u5173\u6027\u548c\u5f02\u5e38\uff08\u4f8b\u5982\u53d1\u751f\u7387\u975e\u5e38\u4f4e\u7684\u4e8b\u4ef6\uff09\uff0c\u7b26\u5408\u521d\u59cb\u7edf\u8ba1\u5206\u5e03\u4ee5\u53ca\u5408\u6210\u6570\u636e\u7684\u64cd\u4f5c\u76f8\u5173\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u786e\u5b9a\u5b83\u4eec\u5728\u6355\u6349\u6d88\u9632\u5458\u5e72\u9884\u7684\u590d\u6742\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.00429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00429", "abs": "https://arxiv.org/abs/2507.00429", "authors": ["Jingyi Pan", "Dan Xu", "Qiong Luo"], "title": "DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting", "comment": "ICCV 2025, Project page: https://rorisis.github.io/DiGA3D/", "summary": "Developing a unified pipeline that enables users to remove, re-texture, or\nreplace objects in a versatile manner is crucial for text-guided 3D inpainting.\nHowever, there are still challenges in performing multiple 3D inpainting tasks\nwithin a unified framework: 1) Single reference inpainting methods lack\nrobustness when dealing with views that are far from the reference view. 2)\nAppearance inconsistency arises when independently inpainting multi-view images\nwith 2D diffusion priors; 3) Geometry inconsistency limits performance when\nthere are significant geometric changes in the inpainting regions. To tackle\nthese challenges, we introduce DiGA3D, a novel and versatile 3D inpainting\npipeline that leverages diffusion models to propagate consistent appearance and\ngeometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy\nfor selecting multiple reference views to reduce errors during propagation.\nNext, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that\npropagates attention features from the selected reference views to other views\nvia diffusion models to maintain appearance consistency. Furthermore, DiGA3D\nintroduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to\nfurther improve the geometric consistency of inpainted 3D scenes. Extensive\nexperiments on multiple 3D inpainting tasks demonstrate the effectiveness of\nour method. The project page is available at https://rorisis.github.io/DiGA3D/.", "AI": {"tldr": "DiGA3D is a 3D inpainting pipeline addressing robustness, appearance consistency, and geometry consistency using diffusion models and a coarse-to-fine approach.", "motivation": "Developing a unified pipeline that enables users to remove, re-texture, or replace objects in a versatile manner is crucial for text-guided 3D inpainting. Challenges include single reference inpainting methods lacking robustness, appearance inconsistency when independently inpainting multi-view images, and geometry inconsistency limiting performance.", "method": "DiGA3D, a novel and versatile 3D inpainting pipeline that leverages diffusion models to propagate consistent appearance and geometry in a coarse-to-fine manner. It includes a robust strategy for selecting multiple reference views, an Attention Feature Propagation (AFP) mechanism, and a Texture-Geometry Score Distillation Sampling (TG-SDS) loss.", "result": "The method demonstrates effectiveness on multiple 3D inpainting tasks.", "conclusion": "Extensive experiments on multiple 3D inpainting tasks demonstrate the effectiveness of our method."}}
{"id": "2507.00911", "categories": ["cs.CL", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2507.00911", "abs": "https://arxiv.org/abs/2507.00911", "authors": ["Luise H\u00e4user", "Alexandros Stamatakis"], "title": "The Cognate Data Bottleneck in Language Phylogenetics", "comment": null, "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "AI": {"tldr": "Current computational phylogenetic methods needing large datasets can't be used on cognate data because automatically generated datasets are unreliable, questioning the applicability of these methods in historical linguistics.", "motivation": "To leverage computational phylogenetic methods for cognate data, larger datasets are needed, but no feasible approach exists to automatically generate them.", "method": "Automatically extracting datasets from BabelNet and performing phylogenetic inferences on the resulting character matrices.", "result": "Phylogenetic inferences on character matrices extracted from BabelNet yield trees inconsistent with established ground truth trees.", "conclusion": "Phylogenetic data analysis approaches requiring larger datasets cannot be applied to cognate data, leaving open the question of how computational approaches can be applied in historical linguistics."}}
{"id": "2507.00101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00101", "abs": "https://arxiv.org/abs/2507.00101", "authors": ["Giovanni Ruggieri"], "title": "DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks", "comment": null, "summary": "We introduce DFReg, a physics-inspired regularization method for deep neural\nnetworks that operates on the global distribution of weights. Drawing from\nDensity Functional Theory (DFT), DFReg applies a functional penalty to\nencourage smooth, diverse, and well-distributed weight configurations. Unlike\ntraditional techniques such as Dropout or L2 decay, DFReg imposes global\nstructural regularity without architectural changes or stochastic\nperturbations.", "AI": {"tldr": "DFReg: a physics-inspired regularization method based on Density Functional Theory (DFT) for deep neural networks.", "motivation": "We introduce DFReg, a physics-inspired regularization method for deep neural networks that operates on the global distribution of weights. Drawing from Density Functional Theory (DFT).", "method": "DFReg applies a functional penalty to encourage smooth, diverse, and well-distributed weight configurations.", "result": "DFReg encourages smooth, diverse, and well-distributed weight configurations.", "conclusion": "DFReg imposes global structural regularity without architectural changes or stochastic perturbations."}}
{"id": "2507.00430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00430", "abs": "https://arxiv.org/abs/2507.00430", "authors": ["Huanxin Yang", "Qiwen Wang"], "title": "MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition", "comment": null, "summary": "Handwritten mathematical expression recognition (HMER) suffers from complex\nformula structures and character layouts in sequence prediction. In this paper,\nwe incorporate frequency domain analysis into HMER and propose a method that\nmarries frequency domain with HMER (MFH), leveraging the discrete cosine\ntransform (DCT). We emphasize the structural analysis assistance of frequency\ninformation for recognizing mathematical formulas. When implemented on various\nbaseline models, our network exhibits a consistent performance enhancement,\ndemonstrating the efficacy of frequency domain information. Experiments show\nthat our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on\nthe CROHME 2014/2016/2019 test sets. The source code is available at\nhttps://github.com/Hryxyhe/MFH.", "AI": {"tldr": "This paper introduces a method that marries frequency domain with HMER (MFH), leveraging the discrete cosine transform (DCT) to enhance the structural analysis for recognizing mathematical formulas.", "motivation": "Handwritten mathematical expression recognition (HMER) suffers from complex formula structures and character layouts in sequence prediction.", "method": "incorporate frequency domain analysis into HMER, leveraging the discrete cosine transform (DCT)", "result": "network exhibits a consistent performance enhancement, demonstrating the efficacy of frequency domain information", "conclusion": "MFH-CoMER achieves noteworthy accuracy on CROHME 2014/2016/2019 test sets."}}
{"id": "2507.00985", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00985", "abs": "https://arxiv.org/abs/2507.00985", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "comment": null, "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "AI": {"tldr": "\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u5b58\u5728\u6096\u8bba\uff0c\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u6377\u5f84\uff0c\u5171\u540c\u589e\u5f3a\u81ea\u6211\u7ea0\u6b63\u548c\u81ea\u6211\u8bca\u65ad\u80fd\u529b\u65f6\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u3002", "motivation": "\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u8f93\u51fa\u4e0e\u4eba\u7c7b\u9053\u5fb7\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u3002\u7136\u800c\uff0c\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u6280\u672f\u53d7\u5230\u4e24\u4e2a\u4e3b\u8981\u6096\u8bba\u7684\u652f\u914d\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u6709\u7ecf\u9a8c\u548c\u7406\u8bba\u8bc1\u636e\u652f\u6301\u81ea\u6211\u7ea0\u6b63\u7684\u6709\u6548\u6027\uff0c\u4f46\u8fd9\u79cd LLM \u80fd\u529b\u4ec5\u5728\u8868\u9762\u5c42\u9762\u8fd0\u4f5c\u3002\u5176\u6b21\uff0c\u867d\u7136 LLM \u5177\u6709\u81ea\u6211\u8bca\u65ad\u5176\u8f93\u51fa\u7684\u4e0d\u9053\u5fb7\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u81ea\u6211\u7ea0\u6b63\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u8bc6\u522b\u8fd9\u79cd\u9053\u5fb7\u4e0d\u4e00\u81f4\u7684\u539f\u56e0\u3002", "method": "\u6211\u4eec\u5206\u6790\u4e86\u65e8\u5728\u589e\u5f3a\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u7684\u5fae\u8c03\u8bed\u6599\u5e93\u4e2d\u7684\u8bdd\u8bed\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u6709\u6548\u7ed3\u6784\u80cc\u540e\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5b58\u5728\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u4f9d\u8d56\u4e8e\u53cd\u6620\u542f\u53d1\u5f0f\u6377\u5f84\u7684\u8bba\u8ff0\u7ed3\u6784\uff0c\u5e76\u4e14\u5728\u5c1d\u8bd5\u5171\u540c\u589e\u5f3a\u81ea\u6211\u7ea0\u6b63\u548c\u81ea\u6211\u8bca\u65ad\u80fd\u529b\u65f6\uff0c\u8fd9\u4e9b\u542f\u53d1\u5f0f\u6377\u5f84\u7684\u5b58\u5728\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u3002\u6211\u4eec\u8fd8\u5f3a\u8c03\u4e86\u8fd9\u79cd\u80fd\u529b\u7684\u6cdb\u5316\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4ece\u60c5\u5883\u80cc\u666f\u548c\u6a21\u578b\u89c4\u6a21\u4e2d\u5b66\u4e60\u65b9\u9762\u3002", "conclusion": "\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u4f9d\u8d56\u4e8e\u53cd\u6620\u542f\u53d1\u5f0f\u6377\u5f84\u7684\u8bba\u8ff0\u7ed3\u6784\uff0c\u5e76\u4e14\u5728\u5c1d\u8bd5\u5171\u540c\u589e\u5f3a\u81ea\u6211\u7ea0\u6b63\u548c\u81ea\u6211\u8bca\u65ad\u80fd\u529b\u65f6\uff0c\u8fd9\u4e9b\u542f\u53d1\u5f0f\u6377\u5f84\u7684\u5b58\u5728\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u7b56\u5c55\u6570\u636e\u96c6\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u6539\u8fdb\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00102", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.00102", "abs": "https://arxiv.org/abs/2507.00102", "authors": ["Bernd Hofmann", "Patrick Bruendl", "Huong Giang Nguyen", "Joerg Franke"], "title": "Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series", "comment": null, "summary": "Ensuring consistent product quality in modern manufacturing is crucial,\nparticularly in safety-critical applications. Conventional quality control\napproaches, reliant on manually defined thresholds and features, lack\nadaptability to the complexity and variability inherent in production data and\nnecessitate extensive domain expertise. Conversely, data-driven methods, such\nas machine learning, demonstrate high detection performance but typically\nfunction as black-box models, thereby limiting their acceptance in industrial\nenvironments where interpretability is paramount. This paper introduces a\nmethodology for industrial fault detection, which is both data-driven and\ntransparent. The approach integrates a supervised machine learning model for\nmulti-class fault classification, Shapley Additive Explanations for post-hoc\ninterpretability, and a do-main-specific visualisation technique that maps\nmodel explanations to operator-interpretable features. Furthermore, the study\nproposes an evaluation methodology that assesses model explanations through\nquantitative perturbation analysis and evaluates visualisations by qualitative\nexpert assessment. The approach was applied to the crimping process, a\nsafety-critical joining technique, using a dataset of univariate, discrete time\nseries. The system achieves a fault detection accuracy of 95.9 %, and both\nquantitative selectivity analysis and qualitative expert evaluations confirmed\nthe relevance and inter-pretability of the generated explanations. This\nhuman-centric approach is designed to enhance trust and interpretability in\ndata-driven fault detection, thereby contributing to applied system design in\nindustrial quality control.", "AI": {"tldr": "A data-driven and transparent method for industrial fault detection is introduced, integrating machine learning, Shapley explanations, and domain-specific visualization. It achieves high accuracy and interpretability in a crimping process application.", "motivation": "Conventional quality control approaches lack adaptability and data-driven methods function as black-box models.", "method": "integrates a supervised machine learning model for multi-class fault classification, Shapley Additive Explanations for post-hoc interpretability, and a domain-specific visualisation technique", "result": "achieves a fault detection accuracy of 95.9 %, and both quantitative selectivity analysis and qualitative expert evaluations confirmed the relevance and interpretability of the generated explanations.", "conclusion": "The human-centric approach enhances trust and interpretability in data-driven fault detection, contributing to applied system design in industrial quality control."}}
{"id": "2507.00447", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.00447", "abs": "https://arxiv.org/abs/2507.00447", "authors": ["Xin Luo", "Menglin Zhang", "Yunwei Lan", "Tianyu Zhang", "Rui Li", "Chang Liu", "Dong Liu"], "title": "Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration", "comment": "Code and Models will be publicly available at\n  https://github.com/Luciennnnnnn/Latent-PMRF", "summary": "The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face\nrestoration algorithms must balance perceptual quality and fidelity. To achieve\nminimal distortion while maintaining perfect perceptual quality, Posterior-Mean\nRectified Flow (PMRF) proposes a flow based approach where source distribution\nis minimum distortion estimations. Although PMRF is shown to be effective, its\npixel-space modeling approach limits its ability to align with human\nperception, where human perception is defined as how humans distinguish between\ntwo image distributions. In this work, we propose Latent-PMRF, which\nreformulates PMRF in the latent space of a variational autoencoder (VAE),\nfacilitating better alignment with human perception during optimization. By\ndefining the source distribution on latent representations of minimum\ndistortion estimation, we bound the minimum distortion by the VAE's\nreconstruction error. Moreover, we reveal the design of VAE is crucial, and our\nproposed VAE significantly outperforms existing VAEs in both reconstruction and\nrestoration. Extensive experiments on blind face restoration demonstrate the\nsuperiority of Latent-PMRF, offering an improved PD-tradeoff compared to\nexisting methods, along with remarkable convergence efficiency, achieving a\n5.79X speedup over PMRF in terms of FID. Our code will be available as\nopen-source.", "AI": {"tldr": "Latent-PMRF\u901a\u8fc7\u5728VAE\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91cd\u65b0\u6784\u5efaPMRF\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u76f2\u4eba\u9762\u90e8\u4fee\u590d\u6548\u679c\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "PMRF\u7684\u50cf\u7d20\u7a7a\u95f4\u5efa\u6a21\u65b9\u6cd5\u9650\u5236\u4e86\u5176\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u7684\u80fd\u529b\u3002", "method": "Latent-PMRF\u5728\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE) \u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91cd\u65b0\u6784\u5efa PMRF\u3002", "result": "Latent-PMRF\u4f18\u4e8e\u73b0\u6709\u7684\u76f2\u4eba\u9762\u90e8\u4fee\u590d\u65b9\u6cd5\uff0c\u5728FID\u65b9\u9762\u6bd4PMRF\u63d0\u9ad8\u4e865.79\u500d\u3002", "conclusion": "Latent-PMRF\u5728\u76f2\u4eba\u9762\u90e8\u4fee\u590d\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684PD\u6743\u8861\uff0c\u5e76\u4e14\u5177\u6709\u663e\u7740\u7684\u6536\u655b\u6548\u7387\uff0c\u5728FID\u65b9\u9762\u6bd4PMRF\u63d0\u9ad8\u4e865.79\u500d\u3002"}}
{"id": "2507.00994", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00994", "abs": "https://arxiv.org/abs/2507.00994", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "Andr\u00e9 F. T. Martins", "C\u00e9line Hudelot", "Pierre Colombo"], "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "comment": "23 pages, 10 figures, 17 tables", "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e86 MLM \u548c CLM \u4e24\u79cd\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u6587\u672c\u8868\u793a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0 MLM \u603b\u4f53\u6027\u80fd\u66f4\u597d\uff0cCLM \u6570\u636e\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u53cc\u76f8\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u5b66\u4e60\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u8868\u793a\u662f\u5404\u79cd NLP \u4efb\u52a1\u7684\u57fa\u7840\u3002\u867d\u7136\u7f16\u7801\u5668\u9884\u8bad\u7ec3\u4f20\u7edf\u4e0a\u4f9d\u8d56\u4e8e Masked Language Modeling (MLM)\uff0c\u4f46\u6700\u8fd1\u7684\u8bc1\u636e\u8868\u660e\uff0c\u4f7f\u7528 Causal Language Modeling (CLM) \u9884\u8bad\u7ec3\u7684\u89e3\u7801\u5668\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u91cd\u65b0\u7528\u4f5c\u7f16\u7801\u5668\uff0c\u901a\u5e38\u5728\u6587\u672c\u8868\u793a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8fc7\u4f20\u7edf\u7f16\u7801\u5668\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6536\u76ca\u662f\u5426\u53cd\u6620\u4e86 CLM \u76ee\u6807\u7684\u5185\u5728\u4f18\u52bf\uff0c\u6216\u8005\u662f\u7531\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u7b49\u6df7\u6742\u56e0\u7d20\u5f15\u8d77\u7684\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5927\u89c4\u6a21\u3001\u7cbe\u5fc3\u63a7\u5236\u7684\u9884\u8bad\u7ec3\u6d88\u878d\u5b9e\u9a8c\uff0c\u8bad\u7ec3\u4e86\u603b\u5171 30 \u4e2a\u6a21\u578b\uff0c\u53c2\u6570\u8303\u56f4\u4ece 2.1 \u4ebf\u5230 10 \u4ebf\uff0c\u5e76\u8fdb\u884c\u4e86\u8d85\u8fc7 15,000 \u6b21\u5fae\u8c03\u548c\u8bc4\u4f30\u8fd0\u884c\u3002", "result": "\u867d\u7136\u4f7f\u7528 MLM \u8fdb\u884c\u8bad\u7ec3\u901a\u5e38\u4f1a\u5728\u6587\u672c\u8868\u793a\u4efb\u52a1\u4e2d\u4ea7\u751f\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4f46\u7ecf\u8fc7 CLM \u8bad\u7ec3\u7684\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u5e76\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u5fae\u8c03\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5728\u56fa\u5b9a\u8ba1\u7b97\u8bad\u7ec3\u9884\u7b97\u4e0b\uff0c\u4f9d\u6b21\u5e94\u7528 CLM \u548c MLM \u7684\u53cc\u76f8\u8bad\u7ec3\u7b56\u7565\u53ef\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4ece\u73b0\u6210\u7684\u9884\u8bad\u7ec3 CLM \u6a21\u578b\u521d\u59cb\u5316\u65f6\uff0c\u8be5\u7b56\u7565\u66f4\u5177\u5438\u5f15\u529b\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u8bad\u7ec3\u6700\u4f73\u7f16\u7801\u5668\u6a21\u578b\u6240\u9700\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2507.00105", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.00105", "abs": "https://arxiv.org/abs/2507.00105", "authors": ["Javier Castellano", "Ignacio Villanueva"], "title": "Graph Neural Networks in Wind Power Forecasting", "comment": null, "summary": "We study the applicability of GNNs to the problem of wind energy forecasting.\nWe find that certain architectures achieve performance comparable to our best\nCNN-based benchmark. The study is conducted on three wind power facilities\nusing five years of historical data. Numerical Weather Prediction (NWP)\nvariables were used as predictors, and models were evaluated on a 24 to 36 hour\nahead test horizon.", "AI": {"tldr": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u98ce\u80fd\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u7814\u7a76\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u98ce\u80fd\u9884\u6d4b\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u98ce\u80fd\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u98ce\u7535\u8bbe\u65bd\u4e0a\uff0c\u4f7f\u7528\u4e94\u5e74\u5386\u53f2\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\u8fbe\u5230\u4e86\u4e0e\u57fa\u4e8eCNN\u7684\u57fa\u51c6\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002\u4f7f\u7528\u6570\u503c\u5929\u6c14\u9884\u62a5\uff08NWP\uff09\u53d8\u91cf\u4f5c\u4e3a\u9884\u6d4b\u56e0\u5b50\uff0c\u5e76\u572824\u81f336\u5c0f\u65f6\u7684\u6d4b\u8bd5\u8303\u56f4\u5185\u8bc4\u4f30\u6a21\u578b\u3002", "conclusion": "\u67d0\u4e9b\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u98ce\u80fd\u9884\u6d4b\u95ee\u9898\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u4f73CNN\u57fa\u51c6\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2507.00454", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00454", "abs": "https://arxiv.org/abs/2507.00454", "authors": ["Yihao Zhen", "Qiang Wang", "Yu Qiao", "Liangqiong Qu", "Huijie Fan"], "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales", "comment": null, "summary": "A main challenge of Visual-Language Tracking (VLT) is the misalignment\nbetween visual inputs and language descriptions caused by target movement.\nPrevious trackers have explored many effective feature modification methods to\npreserve more aligned features. However, an important yet unexplored factor\nultimately hinders their capability, which is the inherent differences in the\ntemporal and spatial scale of information between visual and language inputs.\nTo address this issue, we propose a novel visual-language tracker that enhances\nthe effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and\n\\textbf{S}patial scale of different input components, named as\n\\textbf{ATSTrack}. Specifically, we decompose each language description into\nphrases with different attributes based on their temporal and spatial\ncorrespondence with visual inputs, and modify their features in a fine-grained\nmanner. Moreover, we introduce a Visual-Language token that comprises modified\nlinguistic information from the previous frame to guide the model to extract\nvisual features that are more relevant to language description, thereby\nreducing the impact caused by the differences in spatial scale. Experimental\nresults show that our proposed ATSTrack achieves performance comparable to\nexisting methods. Our code will be released.", "AI": {"tldr": "ATSTrack aligns temporal and spatial scales of visual and language inputs by decomposing language descriptions and introducing a Visual-Language token to improve visual-language tracking.", "motivation": "The misalignment between visual inputs and language descriptions caused by target movement and the inherent differences in the temporal and spatial scale of information between visual and language inputs.", "method": "A novel visual-language tracker that enhances the effect of feature modification by aligning temporal and spatial scale of different input components. It decomposes each language description into phrases and introduces a Visual-Language token.", "result": "Achieves performance comparable to existing methods.", "conclusion": "The proposed ATSTrack achieves performance comparable to existing methods."}}
{"id": "2507.00999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.00999", "abs": "https://arxiv.org/abs/2507.00999", "authors": ["Mar\u00eda Grandury", "Javier Aula-Blasco", "J\u00falia Falc\u00e3o", "Cl\u00e9mentine Fourrier", "Miguel Gonz\u00e1lez", "Gonzalo Mart\u00ednez", "Gonzalo Santamar\u00eda", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena G\u00f3mez", "Marta Guerrero", "Guido Ivetta", "Natalia L\u00f3pez", "Flor Miriam Plaza-del-Arco", "Mar\u00eda Teresa Mart\u00edn-Valdivia", "Helena Montoro", "Carmen Mu\u00f1oz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "Mar\u00eda Estrella Vallecillo-Rodr\u00edguez", "Jorge Vallego", "Irune Zubiaga"], "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "comment": "Accepted at ACL 2025 Main", "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "AI": {"tldr": "La Leaderboard\u662f\u9996\u4e2a\u8bc4\u4f30\u897f\u73ed\u7259\u8bed\u751f\u6210LLM\u7684\u5f00\u6e90\u6392\u884c\u699c\uff0c\u65e8\u5728\u4e3a\u897f\u73ed\u7259\u8bed\u793e\u533a\u5efa\u7acb\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u4ee3\u8868\u897f\u73ed\u7259\u8bed\u793e\u533a\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u7684LLM\u7684\u5f00\u53d1\u3002", "method": "\u7ed3\u5408\u4e86\u5df4\u65af\u514b\u8bed\u3001\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u3001\u52a0\u5229\u897f\u4e9a\u8bed\u548c\u4e0d\u540c\u897f\u73ed\u7259\u8bed\u53d8\u4f53\u768466\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u89e3\u91ca\u4e86\u4e3a\u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u9009\u62e9\u6700\u5408\u9002\u8bc4\u4f30\u8bbe\u7f6e\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u6bd4\u6587\u732e\u4e2d\u901a\u5e38\u66f4\u5c11\u7684few-shot\u793a\u4f8b\u3002", "result": "\u5c55\u793a\u4e8650\u4e2a\u6a21\u578b\u7684\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "La Leaderboard\u662f\u9996\u4e2a\u8bc4\u4f30\u897f\u73ed\u7259\u8bed\u793e\u533a\u751f\u6210LLM\u7684\u5f00\u6e90\u6392\u884c\u699c\uff0c\u5c55\u793a\u4e8650\u4e2a\u6a21\u578b\u7684\u8bc4\u4f30\u7ed3\u679c\u3002"}}
{"id": "2507.00184", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00184", "abs": "https://arxiv.org/abs/2507.00184", "authors": ["Jacob Schrum", "Olivia Kilday", "Emilio Salas", "Bess Hagan", "Reid Williams"], "title": "Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros", "comment": null, "summary": "Recent research shows how diffusion models can unconditionally generate\ntile-based game levels, but use of diffusion models for text-to-level\ngeneration is underexplored. There are practical considerations for creating a\nusable model: caption/level pairs are needed, as is a text embedding model, and\na way of generating entire playable levels, rather than individual scenes. We\npresent strategies to automatically assign descriptive captions to an existing\nlevel dataset, and train diffusion models using both pretrained text encoders\nand simple transformer models trained from scratch. Captions are automatically\nassigned to generated levels so that the degree of overlap between input and\noutput captions can be compared. We also assess the diversity and playability\nof the resulting levels. Results are compared with an unconditional diffusion\nmodel and a generative adversarial network, as well as the text-to-level\napproaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model\nuses a simple transformer model for text embedding, and takes less time to\ntrain than diffusion models employing more complex text encoders, indicating\nthat reliance on larger language models is not necessary. We also present a GUI\nallowing designers to construct long levels from model-generated scenes.", "AI": {"tldr": "This paper explores text-to-level generation using diffusion models, presenting strategies for captioning levels, training models, and evaluating results. It finds that simple transformer models can outperform complex text encoders and introduces a GUI for level design.", "motivation": "Use of diffusion models for text-to-level generation is underexplored, and there are practical considerations for creating a usable model, such as caption/level pairs, a text embedding model, and generating entire playable levels.", "method": "Strategies to automatically assign descriptive captions to an existing level dataset, and train diffusion models using both pretrained text encoders and simple transformer models trained from scratch.", "result": "The diversity and playability of the resulting levels are assessed and compared with an unconditional diffusion model, a generative adversarial network, and other text-to-level approaches. Captions are automatically assigned to generated levels to compare the overlap between input and output captions.", "conclusion": "The best diffusion model uses a simple transformer model for text embedding and trains faster than models using complex text encoders, suggesting that large language models are unnecessary. A GUI is presented for constructing long levels from generated scenes."}}
{"id": "2507.00462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00462", "abs": "https://arxiv.org/abs/2507.00462", "authors": ["Jizhou Han", "Chenhao Ding", "SongLin Dong", "Yuhang He", "Xinyuan Gao", "Yihong Gong"], "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation", "comment": null, "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.", "AI": {"tldr": "MS-TTA is a training-free test-time adaptation method that enhances feature representations beyond CLIP's space using kNN Mean-Shift, improving feature compactness and class separability.", "motivation": "Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones.", "method": "a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift", "result": "improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits.", "conclusion": "MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training."}}
{"id": "2507.01001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01001", "abs": "https://arxiv.org/abs/2507.01001", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "comment": null, "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "AI": {"tldr": "SciArena\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u6587\u732e\u4efb\u52a1\u7684\u5f00\u653e\u534f\u4f5c\u5e73\u53f0\uff0c\u5b83\u901a\u8fc7\u793e\u533a\u6295\u7968\u548c\u5143\u8bc4\u4f30\u57fa\u51c6\u6765\u4fc3\u8fdb\u6a21\u578b\u8bc4\u4f30\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u79d1\u5b66\u6587\u732e\u7406\u89e3\u548c\u5408\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u5f00\u653e\u5f0f\u7684\u79d1\u5b66\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u793e\u533a\u9a71\u52a8\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "SciArena\u5e73\u53f0\u91c7\u7528Chatbot Arena\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u793e\u533a\u6295\u7968\u8fdb\u884c\u6a21\u578b\u6bd4\u8f83\uff0c\u5e76\u6536\u96c6\u4e86\u6765\u81ea\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u7684\u8d85\u8fc713,000\u5f20\u9009\u7968\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u63d0\u4ea4\u7684\u95ee\u9898\u662f\u591a\u6837\u5316\u7684\uff0c\u4e0e\u5b9e\u9645\u6587\u732e\u9700\u6c42\u4e00\u81f4\uff0c\u5e76\u4e14\u53c2\u4e0e\u7684\u7814\u7a76\u4eba\u5458\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u81ea\u6211\u4e00\u81f4\u6027\u548c\u6ce8\u91ca\u8005\u95f4\u534f\u8bae\u3002\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u57fa\u4e8e\u793e\u533a\u6295\u7968\u7684\u79d1\u5b66\u6587\u732e\u4efb\u52a1\u8bc4\u4f30\u5e73\u53f0SciArena\uff0c\u5e76\u53d1\u5e03\u4e86\u57fa\u4e8e\u6536\u96c6\u5230\u7684\u504f\u597d\u6570\u636e\u7684\u5143\u8bc4\u4f30\u57fa\u51c6SciArena-Eval\uff0c\u4ee5\u4fc3\u8fdb\u57fa\u4e8e\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2507.00191", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00191", "abs": "https://arxiv.org/abs/2507.00191", "authors": ["Eray Erturk", "Fahad Kamran", "Salar Abbaspourazad", "Sean Jewell", "Harsh Sharma", "Yujie Li", "Sinead Williamson", "Nicholas J Foti", "Joseph Futoma"], "title": "Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions", "comment": "Accepted to ICML 2025", "summary": "Wearable devices record physiological and behavioral signals that can improve\nhealth predictions. While foundation models are increasingly used for such\npredictions, they have been primarily applied to low-level sensor data, despite\nbehavioral data often being more informative due to their alignment with\nphysiologically relevant timescales and quantities. We develop foundation\nmodels of such behavioral signals using over 2.5B hours of wearable data from\n162K individuals, systematically optimizing architectures and tokenization\nstrategies for this unique dataset. Evaluated on 57 health-related tasks, our\nmodel shows strong performance across diverse real-world applications including\nindividual-level classification and time-varying health state prediction. The\nmodel excels in behavior-driven tasks like sleep prediction, and improves\nfurther when combined with representations of raw sensor data. These results\nunderscore the importance of tailoring foundation model design to wearables and\ndemonstrate the potential to enable new health applications.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u884c\u4e3a\u6570\u636e\u7684\u5065\u5eb7\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u7ecf\u9a8c\u8bc1\uff0c\u8be5\u6a21\u578b\u5728\u591a\u79cd\u5065\u5eb7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u7761\u7720\u9884\u6d4b\u7b49\u884c\u4e3a\u9a71\u52a8\u7684\u4efb\u52a1\u4e2d\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u8bb0\u5f55\u7684\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\u53ef\u4ee5\u6539\u5584\u5065\u5eb7\u9884\u6d4b\u3002\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6b64\u7c7b\u9884\u6d4b\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u5e94\u7528\u4e8e\u4f4e\u7ea7\u4f20\u611f\u5668\u6570\u636e\uff0c\u800c\u884c\u4e3a\u6570\u636e\u901a\u5e38\u66f4\u5177\u4fe1\u606f\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0e\u751f\u7406\u76f8\u5173\u7684\u65f6\u95f4\u5c3a\u5ea6\u548c\u6570\u91cf\u5bf9\u9f50\u3002", "method": "\u5229\u7528\u6765\u81ea16.2\u4e07\u4eba\u7684\u8d85\u8fc725\u4ebf\u5c0f\u65f6\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\uff0c\u7cfb\u7edf\u5730\u4f18\u5316\u67b6\u6784\u548ctokenization\u7b56\u7565\uff0c\u5f00\u53d1\u884c\u4e3a\u4fe1\u53f7\u7684\u57fa\u7840\u6a21\u578b\u3002", "result": "\u572857\u9879\u4e0e\u5065\u5eb7\u76f8\u5173\u7684\u4efb\u52a1\u4e2d\uff0c\u8be5\u6a21\u578b\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4e2a\u4f53\u6c34\u5e73\u7684\u5206\u7c7b\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\u3002\u8be5\u6a21\u578b\u64c5\u957f\u4e8e\u884c\u4e3a\u9a71\u52a8\u7684\u4efb\u52a1\uff0c\u5982\u7761\u7720\u9884\u6d4b\uff0c\u5e76\u4e14\u5728\u4e0e\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u7684\u8868\u793a\u76f8\u7ed3\u5408\u65f6\uff0c\u6027\u80fd\u5f97\u5230\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002", "conclusion": "\u9488\u5bf9\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9a\u5236\u7684\u4e13\u7528\u884c\u4e3a\u4fe1\u53f7\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u63d0\u5347\u5065\u5eb7\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u65b0\u7684\u5065\u5eb7\u5e94\u7528\u63d0\u4f9b\u6f5c\u529b\u3002"}}
{"id": "2507.00469", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00469", "abs": "https://arxiv.org/abs/2507.00469", "authors": ["Yue Tan", "Xiaoqian Hu", "Hao Xue", "Celso De Melo", "Flora D. Salim"], "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding", "comment": "23 pages, 12 figures, 10 tables", "summary": "Frontier vision-language models (VLMs) have made remarkable improvements in\nvideo understanding tasks. However, real-world videos typically exist as\ncontinuously evolving data streams (e.g., dynamic scenes captured by wearable\nglasses), necessitating models to continually adapt to shifting data\ndistributions and novel scenarios. Considering the prohibitive computational\ncosts of fine-tuning models on new tasks, usually, a small subset of parameters\nis updated while the bulk of the model remains frozen. This poses new\nchallenges to existing continual learning frameworks in the context of large\nmultimodal foundation models, i.e., catastrophic forgetting and update\nconflict. While the foundation models struggle with parameter-efficient\ncontinual learning, the hippocampus in the human brain has evolved highly\nefficient mechanisms for memory formation and consolidation. Inspired by the\nrapid Binding and pattern separation mechanisms in the hippocampus, in this\nwork, we propose Bisecle for video-language continual learning, where a\nmulti-directional supervision module is used to capture more cross-modal\nrelationships and a contrastive prompt learning scheme is designed to isolate\ntask-specific knowledge to facilitate efficient memory storage. Binding and\nseparation processes further strengthen the ability of VLMs to retain complex\nexperiences, enabling robust and efficient continual learning in video\nunderstanding tasks. We perform a thorough evaluation of the proposed Bisecle,\ndemonstrating its ability to mitigate forgetting and enhance cross-task\ngeneralization on several VideoQA benchmarks.", "AI": {"tldr": "Bisecle\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u8bed\u8a00\u6301\u7eed\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6a21\u4eff\u4eba\u8111\u6d77\u9a6c\u4f53\u7684\u8bb0\u5fc6\u5f62\u6210\u548c\u5de9\u56fa\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u66f4\u65b0\u51b2\u7a81\u7b49\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u800c\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u901a\u5e38\u4ee5\u4e0d\u65ad\u53d1\u5c55\u7684\u6570\u636e\u6d41\u5f62\u5f0f\u5b58\u5728\uff0c\u9700\u8981\u6a21\u578b\u4e0d\u65ad\u9002\u5e94\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\u548c\u65b0\u7684\u573a\u666f\u3002\u8003\u8651\u5230\u5728\u65b0\u4efb\u52a1\u4e0a\u5fae\u8c03\u6a21\u578b\u7684\u5de8\u5927\u8ba1\u7b97\u6210\u672c\uff0c\u901a\u5e38\u53ea\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u53c2\u6570\uff0c\u800c\u6a21\u578b\u7684\u5927\u90e8\u5206\u4fdd\u6301\u51bb\u7ed3\u3002\u8fd9\u5bf9\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u80cc\u666f\u4e0b\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\uff0c\u5373\u707e\u96be\u6027\u9057\u5fd8\u548c\u66f4\u65b0\u51b2\u7a81\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u89c6\u9891\u8bed\u8a00\u6301\u7eed\u5b66\u4e60\u7684Bisecle\uff0c\u5176\u4e2d\u4f7f\u7528\u591a\u5411\u76d1\u7763\u6a21\u5757\u6765\u6355\u83b7\u66f4\u591a\u8de8\u6a21\u6001\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5bf9\u6bd4\u63d0\u793a\u5b66\u4e60\u65b9\u6848\u6765\u9694\u79bb\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u77e5\u8bc6\uff0c\u4ee5\u4fc3\u8fdb\u9ad8\u6548\u7684\u8bb0\u5fc6\u5b58\u50a8\u3002", "result": "\u6211\u4eec\u5bf9\u63d0\u51fa\u7684Bisecle\u8fdb\u884c\u4e86\u5f7b\u5e95\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5b83\u5728\u7f13\u89e3\u9057\u5fd8\u548c\u589e\u5f3a\u8de8\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "Bisecle\u901a\u8fc7\u7f13\u89e3\u9057\u5fd8\u548c\u589e\u5f3a\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2aVideoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u9c81\u68d2\u800c\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u3002"}}
