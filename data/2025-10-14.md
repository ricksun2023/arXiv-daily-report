<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.CV](#cs.CV) [Total: 37]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.IR](#cs.IR) [Total: 29]
- [cs.LG](#cs.LG) [Total: 39]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 本调查全面概述了基于LLM的TQA研究，对现有基准和任务设置进行了分类，并根据其针对的挑战对当前的建模策略进行分组，最后，确定了尚未在先前研究中系统地涵盖的，但尚未充分探索的及时主题。


<details>
  <summary>Details</summary>
Motivation: 该领域仍然缺乏对任务公式、核心挑战和方法趋势的系统组织和理解，尤其是在强化学习等新兴研究方向方面。

Method: 对TQA研究进行全面和结构化的概述，重点是基于LLM的方法。对现有基准和任务设置进行全面分类。根据当前建模策略所针对的挑战对其进行分组，并分析其优势和局限性。

Result: 统一不同的研究思路，并确定未解决的问题，从而为TQA社区提供了一个巩固的基础。

Conclusion: 通过统一不同的研究思路并确定开放性问题，我们的调查为TQA社区提供了一个巩固的基础，从而可以更深入地了解最新技术，并指导该快速发展领域的未来发展。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [2] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: 研究了情感框架如何与谬误和说服力相互作用。


<details>
  <summary>Details</summary>
Motivation: 公共交流中存在逻辑谬误会误导观众，谬误性论点可能仍然具有说服力，因为说服力本质上是主观的。

Method: 使用大型语言模型 (LLM) 系统地改变谬误性论点中的情感诉求。对八个 LLM 进行了基准测试，以将情感诉求注入谬误性论点，同时保留其逻辑结构，然后使用最佳模型生成人类研究的刺激。

Result: LLM 驱动的情感框架使人类的谬误检测率平均降低了 14.5%。与中性或其他情绪状态相比，人类在感知快乐时比恐惧或悲伤时在谬误检测方面表现更好，并且这三种情绪也与明显更高的说服力相关。

Conclusion: 这项工作对谬误论证背景下的人工智能驱动的情感操纵具有影响。

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [3] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)在识别数字序列规律的简单任务中，倾向于生成荒谬的模式，即使这些模式明显不合适。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型(LLM)的逻辑一致性和自我连贯性至关重要，特别是在它们被应用于复杂的现实世界任务时，例如通过检索增强生成提供知识和使用AI代理框架执行多步骤任务。这些方法依赖于LLM的逻辑一致性和自我连贯性，因此需要评估这些方面并考虑潜在的对策。

Method: 我们进行了一项实验，要求LLM解释各种整数序列中的模式，从算术序列到随机生成的整数序列，以识别LLM未能保持逻辑一致性的情况。

Result: 虽然模型成功识别了算术和几何序列中的正确模式，但它们在分析随机生成的序列时，经常过度识别与给定数字不一致的模式。这个问题甚至在多步骤推理模型中也观察到，包括OpenAI o3、o4-mini和Google Gemini 2.5 Flash Preview Thinking。

Conclusion: 这种感知不存在模式的倾向可以被解释为AI模型等同于部落偶像，并突出了它们在需要逻辑推理的应用任务中的潜在局限性，即使采用思维链推理机制。

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [4] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: SeCon-RAG: A two-stage framework for robust and trustworthy retrieval-augmented generation (RAG) that mitigates corpus poisoning and contamination attacks.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems are vulnerable to corpus poisoning and contamination attacks, and current defenses often lead to loss of valuable information.

Method: A two-stage approach involving semantic and cluster-based filtering guided by an Entity-intent-relation extractor (EIRE), followed by an EIRE-guided conflict-aware filtering module.

Result: SeCon-RAG demonstrates significant improvements in generation robustness and output trustworthiness across various LLMs and datasets, outperforming state-of-the-art defense methods.

Conclusion: SeCon-RAG effectively preserves useful knowledge while mitigating conflict contamination in RAG systems.

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [5] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的框架ReaLM，通过残差向量量化的机制，弥合了知识图谱嵌入和LLM分词之间的差距，从而更好地利用结构化语义表示。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的方法难以充分利用结构化语义表示，因为预训练的KG模型的连续嵌入空间与LLM的离散token空间存在根本的不对齐。

Method: 该框架通过残差向量量化将预训练的KG嵌入离散化为紧凑的代码序列，并将它们作为可学习的token集成到LLM词汇表中。此外，还结合了本体指导的类约束来加强语义一致性，并根据类级别的兼容性来细化实体预测。

Result: 在两个广泛使用的基准数据集上的大量实验表明，ReaLM实现了最先进的性能。

Conclusion: ReaLM在将结构化知识与大型语言模型对齐方面是有效的。

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [6] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 本文研究了模型在加密文本中进行推理的能力，发现模型在不熟悉的密码中推理时准确率会显著下降，即使它们能够准确地将密文翻译成英文。这表明使用加密推理来逃避 CoT 监控对于当前的模型可能不是一种有效的策略。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能代理的普及，检测有害的人工智能行为非常重要。链式思考（CoT）监控是广泛用于检测对抗性攻击和人工智能不对齐的一种方法。然而，攻击者和不对齐的模型可能会通过加密推理来逃避 CoT 监控：隐藏在加密、翻译或压缩文本中的推理。

Method: 对于 28 种不同的密码，我们对多达 10 个模型进行了微调和提示，以使用该密码进行推理。我们测量了模型在数学问题上的准确性，以此作为推理能力的代表。

Result: 在我们测试的模型中，我们发现了一种不对称性：当使用加密文本进行推理时，模型准确率会显著下降，即使模型通过能够准确地将其翻译成英语来证明其对加密文本的理解。即使是前沿模型也在不太知名的密码中挣扎，尽管它们可以使用像 rot13 这样的著名密码进行准确的推理。我们表明，加密推理能力与预训练数据中密码的流行程度相关。

Conclusion: 我们的工作表明，使用加密推理来逃避 CoT 监控对于当前的模型可能不是一种有效的策略，并为约束未来前沿模型中这种能力的发展提供了指导。

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [7] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了一种偏好感知记忆更新机制（PAMU），通过融合滑动窗口平均（SW）和指数移动平均（EMA）来捕获短期波动和长期用户趋势，从而实现动态和个性化的记忆优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态优化偏好记忆表示方面存在不足，无法根据不断变化的用户行为和环境进行调整。

Method: 提出了一种偏好感知记忆更新机制（PAMU），通过整合滑动窗口平均（SW）与指数移动平均（EMA）构建融合的偏好感知表示。

Result: 在LoCoMo数据集的五个任务场景中进行的实验表明，PAMU 显著提高了 LLM 在五个基线中的输出质量。

Conclusion: 验证了 PAMU 在长期对话中有效性

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [8] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 本文提出了一个自动化的简历信息提取和评估框架，该框架具有布局感知和效率优化功能，可以应对简历版式和内容的多样性、大语言模型（LLM）的高成本和延迟以及缺乏标准化数据集和评估工具等挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界中自动化的简历信息提取面临三个主要的挑战：简历布局和内容的极端异构性，大型语言模型（LLM）的高成本和延迟，以及缺乏标准化的数据集和评估工具。

Method: 该系统结合了一个微调的布局解析器来标准化不同的文档格式，一个基于并行提示和指令调整的推理高效的LLM提取器，以及一个由新的基准数据集支持的稳健的两阶段自动评估框架。

Result: 大量的实验表明，该框架在准确性和效率方面都明显优于强大的基线。特别地，证明了一个微调的紧凑型0.6B LLM实现了顶级的准确性，同时显著降低了推理延迟和计算成本。

Conclusion: 该系统已完全部署在阿里巴巴的智能人力资源平台中，支持其业务部门的实时应用。

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [9] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种名为EVisRAG的端到端框架，该框架通过学习推理证据引导的多图像来解决视觉检索增强生成（VRAG）系统中存在的问题，该系统旨在通过外部视觉知识来增强视觉语言模型（VLM），从而实现推理并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前的VRAG系统通常无法可靠地感知和整合多个图像中的证据，从而导致较弱的基础和错误的结论。

Method: 该模型首先观察检索到的图像并记录每个图像的证据，然后从汇总的证据中得出最终答案。为了有效地训练EVisRAG，我们引入了奖励范围组相对策略优化（RS-GRPO），它将细粒度的奖励绑定到范围特定的令牌，以共同优化VLM的视觉感知和推理能力。

Result: 在多个视觉问答基准测试上的实验结果表明，EVisRAG与主干VLM相比，实现了显着的端到端收益，平均提高了27%。

Conclusion: 在RS-GRPO的支持下，EVisRAG通过精确地感知和定位多个图像中与问题相关的证据，并从该证据中得出最终答案，从而提高了答案的准确性，就像一个真正的侦探。

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [10] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 提出了一种新的两步法来评估大型语言模型（LLM）在响应准确性评估任务中作为裁判的表现。


<details>
  <summary>Details</summary>
Motivation: 评估 54 个 LLM 在对 RAG 或 Agentic 管道的响应进行评分时，能否很好地复制人类的判断。

Method: 从传统的关联分析到综合 Cohen Kappa 分析，测量实际的一致性模式。包括：(1) 过滤具有强对齐性的裁判的关联测试，以及 (2) 使用 z 分数的人类相似性测试，以识别两种不同的判断模式：类似人类的判断和超一致的判断。

Result: 54 个测试的 LLM 中有 27 个达到了 Tier 1 性能：23 个模型表现出类似人类的模式，而 4 个模型表现出超一致的行为。法官的卓越性并非完全取决于模型大小，而是取决于特定的训练策略。

Conclusion: 建立了相关性不足以进行法官评估，引入了基于一致性模式的“法官图灵测试”，并为将 LLM 法官分类为不同评估需求的不同性能层级提供了标准化基准。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [11] [Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks](https://arxiv.org/abs/2510.10885)
*Jiajing Guo,Kenil Patel,Jorge Piazentin Ono,Wenbin He,Liu Ren*

Main category: cs.CL

TL;DR: 评估了大型语言模型在Text2SQL任务中的实际应用效果，特别是推理模型。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在Text2SQL系统中应用于工业数据库的有效性，特别是最新的推理模型在实际应用中的表现。

Method: 对六种轻量级的、面向工业界的测试时扩展策略和四种大型语言模型进行了基准测试，包括两种推理模型，并在BIRD Mini-Dev基准上评估了它们的性能。

Result: 发现分而治之的提示和少量样本演示能够持续提高通用和推理型大型语言模型的性能。然而，引入额外的工作流程步骤会产生不同的结果，并且基础模型的选择起着关键作用。

Conclusion: 阐明了在部署Text2SQL系统时，准确性、效率和复杂性之间的实际权衡。

Abstract: Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)
systems, enabling non-expert users to query industrial databases using natural
language. While test-time scaling strategies have shown promise in LLM-based
solutions, their effectiveness in real-world applications, especially with the
latest reasoning models, remains uncertain. In this work, we benchmark six
lightweight, industry-oriented test-time scaling strategies and four LLMs,
including two reasoning models, evaluating their performance on the BIRD
Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference
latency and token consumption, providing insights relevant for practical system
deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot
demonstrations consistently enhance performance for both general-purpose and
reasoning-focused LLMs. However, introducing additional workflow steps yields
mixed results, and base model selection plays a critical role. This work sheds
light on the practical trade-offs between accuracy, efficiency, and complexity
when deploying Text2SQL systems.

</details>


### [12] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为 Gold Panning Bandits 的框架，该框架利用大型语言模型在多文档上下文中固有的位置偏差来高效识别最相关的内容，而无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的方法将位置偏差视为需要减轻的噪声，而本文则旨在利用它作为诊断信号。

Method: 该方法将重新排序文档的问题建模为二分匹配问题，并提出了一种贪婪策略，该策略通过优先将最不确定的文档放置在信息量最大的位置来实现可比较的性能。

Result: 该方法在知识密集型 NLP 任务中，与随机置换基线相比，使用更少的语言模型查询（最多减少 65%）来识别相关文档。

Conclusion: 这项工作表明，LLM 固有的偏差可以从劣势转化为优势，以实现高效的推理时优化。

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [13] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: PromptGuard: A few-shot framework for Bengali hate speech classification.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised methods need extensive labeled data, which is expensive for low-resource languages like Bengali.

Method: Combines chi-square keyword extraction with adaptive majority voting.

Result: Achieves a micro-F1 of 67.61, outperforming baselines.

Conclusion: Chi-square keywords provide consistent improvements, and adaptive voting benefits ambiguous cases.

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [14] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: 现代高维文本表示缺乏直接可解释的几何属性，我们引入了Rotor-Invariant Shift Estimation (RISE)，这是一种几何方法，将语义转换表示为嵌入空间中一致的旋转操作，利用了现代语言表示的流形结构。


<details>
  <summary>Details</summary>
Motivation: 理解语言和嵌入模型如何编码语义关系对于模型的可解释性和控制至关重要。

Method: 我们引入了Rotor-Invariant Shift Estimation (RISE)，一种几何方法，将语义转换表示为嵌入空间中一致的旋转操作，利用了现代语言表示的流形结构。

Result: 我们的结果表明，RISE在不同语言和模型中一致地映射具有不同语法特征（例如，否定和条件）的语篇级语义转换。

Conclusion: 这项工作首次系统地证明了语篇级语义转换对应于多语言嵌入空间中一致的几何操作，从经验上支持了句子级别的线性表示假设。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [15] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 本文研究了文本提示注入攻击，旨在误导大型视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的广泛应用引发了安全问题。

Method: 开发了一种文本提示注入攻击算法。

Result: 实验证明了该算法的有效性和效率。

Conclusion: 该方法对于大型模型特别有效，且对计算资源的需求不高。

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [16] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 本研究提出了一种新的营养问答框架，通过知识图引导的多智能体协作来解决个性化饮食指导和预防慢性病问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理能力和多智能体架构设计方面存在局限性，并且面临上下文过载的问题。

Method: 提出了营养图路由（NG-Router）框架，该框架将智能体节点集成到异构知识图中，并使用图神经网络学习任务感知的智能体路由分布，利用来自经验智能体性能的软监督。为了进一步解决上下文过载，提出了一种基于梯度的子图检索机制，该机制在训练期间识别显着证据，从而增强多跳和关系推理。

Result: 在多个基准和骨干模型上的大量实验表明，NG-Router始终优于单智能体和集成基线。

Conclusion: NG-Router为复杂的营养健康任务提供了一种基于领域的知情多智能体推理方法。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [17] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: NarraBench：一个关于叙事理解任务的理论分类，以及对该领域 78 个现有基准的调查。


<details>
  <summary>Details</summary>
Motivation: 发现当前的研究在叙事理解方面存在显著需求，这些需求要么被当前的工作忽视，要么与现有的指标 плохо 对齐。

Method: 构建了一个关于叙事理解任务的理论分类，并调查了该领域中 78 个现有的基准。

Result: 只有 27% 的叙事任务能被现有的基准很好地捕获，并且一些领域（包括叙事事件、风格、视角和揭示）在当前的评估中几乎缺失。需要增加能够评估叙事的构成性主观和视角方面的基准的开发。

Conclusion: 我们的分类、调查和方法论对寻求测试 LLM 叙事理解的 NLP 研究人员具有价值。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [18] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: 论文介绍了一个名为CoBia的轻量级对抗攻击套件，用于系统性地分析大型语言模型(LLM)在对话中偏离规范或伦理行为的情况。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在对话中偶尔会表现出有害行为，例如表达种族主义观点。为了系统地分析这种情况。

Method: 论文使用CoBia创建构造的对话，其中模型会说出关于某个社会群体的有偏见的言论。然后，评估模型是否可以从捏造的偏见声明中恢复并拒绝有偏见的后续问题。

Result: 结果表明，有目的地构建的对话能够可靠地揭示偏见放大，并且LLM在对话过程中通常无法拒绝有偏见的后续问题。

Conclusion: 压力测试突出了可以通过交互浮出水面的深层嵌入的偏见。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [19] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: iBERT: interpretable-BERT creates interpretable embeddings by representing tokens as mixtures of sense vectors, enabling control over representation.


<details>
  <summary>Details</summary>
Motivation: To create interpretable and controllable embeddings that expose discriminative cues in language, such as stylistic and semantic structure.

Method: Represent each token as a sparse, non-negative mixture over context-independent sense vectors.

Result: Improved style representation effectiveness by ~8 points on the STEL benchmark compared to SBERT baselines, while maintaining competitive performance on authorship verification. Specific style attributes can be assigned to specific sense vectors.

Conclusion: iBERT's structural modularity enables interpretable decomposition of discriminative signals, allowing for generalization even when supervision blends stylistic and semantic factors.

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [20] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: 提出了一种名为 DELTA 的免训练稀疏注意力机制，旨在提高大型推理模型 (LRM) 的计算效率，同时保持模型准确性。通过选择性地重用中间注意力图，实现了高效的长文本推理。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法通过修剪键值 (KV) 缓存来减少计算量，但由于累积选择错误和 tokens 在长推导过程中的动态重要性，导致推理任务的准确性严重下降。

Method: DELTA 将 Transformer 层划分为三组：使用全注意力的初始层、通过聚合的头级注意力分数识别显著 tokens 的少量选择层，以及仅关注所选子集的后续稀疏注意力层。

Result: 在 AIME 和 GPQA-Diamond 等推理基准测试中，DELTA 在准确性方面与全注意力相匹配或超过全注意力，同时将参与的 tokens 数量减少了高达 5 倍，并提供了 1.5 倍的端到端加速。

Conclusion: 选择性地重用中间注意力图为高效的长文本推理提供了一条可靠的途径。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [21] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: Masked diffusion large language models (dLLMs) are emerging as a better alternative to autoregressive LLMs (arLLMs) due to better data efficiency and resistance to the reversal curse. The study investigates whether dLLMs can easily acquire new knowledge through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Autoregressive large language models (arLLMs) resist knowledge injection via fine-tuning due to inherent shortcomings such as the reversal curse. Masked diffusion large language models (dLLMs) are emerging as a powerful alternative.

Method: Fine-tune arLLMs and dLLMs on three diverse datasets, evaluating them with forward and backward style Question Answering (QA) to probe knowledge generalization and the reversal curse. Introduce a novel masked fine-tuning paradigm for knowledge injection into pre-trained arLLMs.

Result: ArLLMs critically rely on extensive data augmentation via paraphrases for QA generalization, and paraphrases are only effective when their information order matches the QA style. DllMs achieve high accuracies on both forward and backward QAs without paraphrases; adding paraphrases yields only marginal gains. The proposed masked fine-tuning method successfully and drastically improves the data efficiency of arLLM fine-tuning, effectively closing the performance gap with dLLMs.

Conclusion: dLLMs exhibit advantages in acquiring new knowledge through fine-tuning compared to arLLMs. A novel masked fine-tuning paradigm can improve the data efficiency of arLLM fine-tuning.

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [22] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: 大型语言模型在通过 RLHF 和 DPO 进行对齐后，仍然容易过度自信。现有偏好学习的局限性在于，它强调为给定的提示选择正确的响应，而忽略了应该改变响应的反事实提示。为了解决这个局限性，我们提出了溯因偏好学习，这是一种通过学习给定响应的提示偏好来反转传统条件反射的微调范式。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习强调为给定的提示选择正确的响应，而忽略了应该改变响应的反事实提示。

Method: 我们提出了溯因偏好学习，这是一种通过学习给定响应的提示偏好来反转传统条件反射的微调范式。我们构建了一个来自 HaluEval QA 基准的溯因数据集，包含 1,001 个条目，实现了溯因 DPO 及其变体 DPOP。

Result: 标准方法改进了响应选择，溯因方法改进了提示辨别，而多任务目标统一了两者。在溯因数据集上，多任务 DPOP 将响应选择的准确率从 90.0% 提高到 99.5%，并将提示辨别的准确率从 54.7% 提高到 85.0%。

Conclusion: 溯因偏好学习保留了传统偏好优化的优势，同时解决了反事实提示这一被忽视的挑战。

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [23] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: 本论文提出了一种名为HIPPD的受大脑启发的框架，用于从文本中检测人格。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法难以捕捉跨多个帖子的上下文信息，并且在语义稀疏的环境中提取代表性和鲁棒性特征方面存在不足。

Method: HIPPD利用大型语言模型模拟大脑皮层，实现全局语义推理和深度特征抽象；一个动态记忆模块，模仿前额叶皮层，执行自适应门控和选择性保留关键特征；一组专门的轻量级模型，模仿基底神经节，通过严格的胜者全得机制进行动态路由，以捕捉它们最擅长识别的与人格相关的模式。

Result: 在Kaggle和Pandora数据集上的大量实验表明，HIPPD始终优于最先进的基线。

Conclusion: HIPPD框架在人格检测任务中表现出色。

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [24] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: 对齐训练在推理和指令跟随方面有优势，但在创造力和校准等技能方面有所欠缺。本文旨在通过模型协作，结合不同模型的优势。


<details>
  <summary>Details</summary>
Motivation: 对齐训练后的语言模型在创造力和校准方面不如未对齐的模型。模型协作旨在结合两者的优点。

Method: 提出了一种名为Switch Generation的方法，其中预训练和对齐的模型轮流生成响应序列。通过学习选择不同模型生成下一个片段的结果来训练一个切换器LM。

Result: 模型协作在18个任务中的16个上优于单个模型。Switch Generation平均优于基线12.9%。

Conclusion: Switch Generation能够发现组合技能来解决单个模型难以解决的问题，并推广到未见过的模型和任务。

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [25] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 这篇论文研究了如何通过微调大型语言模型（LLMs）来减少生成摘要中的不忠实信息（幻觉）。


<details>
  <summary>Details</summary>
Motivation: 现有的缓解策略无法完全解决LLM生成摘要中可能出现的各种错误。

Method: 1. 使用多种LLM为训练集中的源文档自动生成摘要；2. 使用GPT-4o在span级别注释其检测到的任何幻觉；3. 使用幻觉自由摘要和带注释的不忠实span微调LLM，以提高模型的可信度。

Result: 所有的微调方法都成功地利用了span级别的注释来提高可信度，其中unlikelihood training是最有效的。

Conclusion: 这篇论文引入了一个新的数据集，其中包含忠实和不忠实的摘要，以及span级别的标签。实验结果表明，使用unlikelihood training微调LLM能够最有效地提高摘要的忠实度。

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [26] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: 本文提出了一种名为SHIELD的框架，用于检测仇恨模因，该框架通过结合PCM和FACT模块来捕捉仇恨的本质。


<details>
  <summary>Details</summary>
Motivation: 当前检测仇恨模因的方法主要依赖于预训练的语言模型，但较少关注模因仇恨的根本原因。

Method: 本文通过哲学和心理学的见解，认为仇恨模因具有两个基本特征：预设语境和虚假声明。为了捕捉预设语境，开发了PCM来建模跨模态的上下文信息。为了检测虚假声明，引入了FACT模块，该模块集成了外部知识并利用了跨模态参考图。

Result: SHIELD在数据集和指标上优于最先进的方法，同时在假新闻检测等其他任务上表现出多功能性。

Conclusion: SHIELD框架能够有效地检测仇恨模因，并在其他相关任务中表现出良好的性能。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [27] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型中分词的重要性，发现标准指标 fertility 存在盲点，并提出了 Single Token Retention Rate (STRR) 指标。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型中分词这一关键步骤，发现标准指标 fertility 无法全面反映词汇在不同语言和领域的分配情况。

Method: 分析了六种常用分词器在七种语言和两个领域的表现，提出了 Single Token Retention Rate (STRR) 指标，用于衡量单词被保留为单个 token 的比例。

Result: 发现英语的 fertility 稳定，中文的 fertility 较高，领域敏感性较低。STRR 指标揭示了对英语的系统性优先、对中文的强大支持以及印地语的分裂情况。

Conclusion: STRR 指标可以补充 fertility 指标，为设计更公平的多语言分词器提供实用指导。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [28] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出了一个统一的框架，用于理解和组织LLM中的 deliberative tree search 算法。


<details>
  <summary>Details</summary>
Motivation: 当前的研究缺乏一个共同的形式化框架，尤其是在奖励信号的角色方面。

Method: 将搜索算法分解为三个核心组件：搜索机制、奖励公式和转移函数。区分了用于TTS的瞬时搜索指导和用于自我改进的持久参数奖励建模。

Result: 建立了一个以组件为中心的分类法，综合了现有技术，并规划了在创建自主、自我改进代理方面取得更系统进展的研究路线图。

Conclusion: 该框架解决了奖励信号的模糊性，并为未来的研究方向提供了指导。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [29] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 提出ExpandSearch，一个通过强化学习训练的LLM搜索代理，用于多跳QA任务，通过查询扩展和文档压缩提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理在多跳QA任务中性能不佳，因为推理和搜索能力有限，难以处理复杂查询。

Method: 训练一个LLM搜索代理，利用强化学习进行查询扩展，并引入预训练的压缩模型来理解检索到的文档。

Result: ExpandSearch在七个QA基准测试中取得了平均4.4%的性能提升，尤其在需要多样证据聚合的多跳推理任务中表现出色。

Conclusion: 即使是小规模的3B LLM，在压缩模型的辅助下，也能展现出强大的查询扩展能力，并在多跳QA基准测试中达到最佳精度。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [30] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 本研究调查了公众对机器翻译(MT)的看法和依赖程度，特别关注了流畅性和充分性错误对双语和非双语用户在日常使用中对MT的依赖的影响。


<details>
  <summary>Details</summary>
Motivation: 了解公众如何看待和依赖不完善的机器翻译对于将机器翻译研究置于真实世界的应用中至关重要。

Method: 在一家公共博物馆进行了一项人类研究(n=452)，调查了流畅性和充分性错误如何影响双语和非双语用户在 casual 使用中对 MT 的依赖。

Result: 研究结果表明，由于缺乏评估策略和替代方案，非双语用户通常过度依赖 MT，而体验到错误的影响会促使用户重新评估未来依赖。

Conclusion: 强调需要 MT 评估和 NLP 解释技术，以提高 MT 质量，并提高用户中的 MT 素养。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [31] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: 本文提出了一种新的无监督文本表示学习框架Text2Token，该框架通过预测目标token来学习文本表示。


<details>
  <summary>Details</summary>
Motivation: 现有的研究发现高质量的文本表示与输入文本的关键token对齐，这揭示了表示空间和词汇空间之间的潜在联系。受此启发，本文重新审视了生成任务，并提出了Text2Token。

Method: Text2Token框架基于token目标预测任务，并利用精心构建的目标token分布作为监督信号。为了构建高质量的目标token分布，本文分析了token对齐属性，并提出了两种方法，即数据驱动和模型驱动，以从数据或LLM主干构建合成token目标。

Result: 在MTEB v2基准测试中，Text2Token取得了与最先进的无监督对比学习嵌入器LLM2Vec具有竞争力的性能。

Conclusion: 词汇和表示空间在训练过程中共同优化并朝着最优解发展，为未来的工作提供了新的思路和见解。

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [32] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种改进的语音到单元翻译(S2UT)模型，通过引入多token预测(MTP)损失，使模型能够在每个位置预测多个后续token，从而捕获更完整的语义。


<details>
  <summary>Details</summary>
Motivation: 当前语音到语音翻译方法主要使用语音token作为中间表示，但单个语音token的语义密度不高，通常需要多个token来表达完整的语义单元。

Method: 将多token预测(MTP)损失引入到语音到单元翻译(S2UT)模型中，并提出MTP-S2UT损失，将MTP损失应用于计算CTC损失的隐藏层表示。

Result: 所有MTP损失变体都能持续提高S2UT翻译的质量，其中MTP-S2UT取得了最佳性能。

Conclusion: 提出的MTP-S2UT方法能够更早、更有效地增强隐藏层表示，从而提升语音翻译质量。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [33] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的共指消解框架ImCoref-CeS，它结合了增强的监督模型和基于LLM的推理。


<details>
  <summary>Details</summary>
Motivation: 当前的研究面临一个关键困境：是进一步探索基于小型语言模型的监督神经方法的潜力，还是采用大型语言模型（LLM）的强大功能。然而，如何有效地结合它们的优势仍未被充分探索。

Method: 该方法首先提出了一个改进的CR方法（ImCoref），通过引入轻量级桥接模块来增强长文本编码能力，设计一个双仿射评分器来全面捕获位置信息，并调用混合提及正则化来提高训练效率。重要的是，该方法采用LLM作为多角色Checker-Splitter代理，以验证ImCoref预测的候选提及（过滤掉无效的提及）和共指结果（拆分错误的集群）。

Result: 大量的实验表明，ImCoref-CeS是有效的，与现有的最先进（SOTA）方法相比，它实现了卓越的性能。

Conclusion: 提出的ImCoref-CeS框架有效地结合了监督模型和LLM的优势，并在共指消解任务上取得了优异的性能。

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [34] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: Long Chain-of-Thought (Long-CoT) 模型在复杂推理任务中被广泛应用，但其推理过程可能偏离对齐路径，导致违反安全限制的内容，这种现象被称为路径漂移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLMs) 在复杂推理任务中被广泛应用，而 Long-CoT 提示已成为结构化推理的关键范例。尽管早期通过 RLHF 等对齐技术实现了初步的安全措施，但 Long-CoT 模型中的推理轨迹可能会偏离对齐路径，从而导致违反安全约束的内容。

Method: 通过实证分析，揭示了路径漂移的三个行为触发因素：(1) 诱导目标驱动推理的第一人称承诺，延迟了拒绝信号；(2) 表面免责声明绕过了对齐检查点的道德蒸发；(3) 分层线索逐渐引导模型走向不安全完成的状态链升级。在此基础上，我们引入了一个三阶段路径漂移诱导框架，包括认知负荷放大、自我角色启动和条件链劫持。

Result: 每个阶段都独立地降低了拒绝率，而它们的组合进一步加剧了这种影响。

Conclusion: 我们的研究结果强调，除了令牌级别的对齐之外，还需要在长格式推理中进行轨迹级别的对齐监督。

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [35] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 提出了一种新的bottom-up方法来线性化树状结构的知识，并通过在每个层级生成隐式的聚合摘要来实现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）擅长根据上下文中的信息生成响应。检索增强生成（RAG）检索相关文档以增强模型的上下文学习。然而，如何最好地表示检索到的知识以生成关于结构化数据的响应，特别是像树这样的层级结构，还没有得到充分的探索。

Method: 提出了一种新的bottom-up方法来线性化来自树状结构的知识（如GitHub存储库），通过在每个层级生成隐式的聚合摘要。

Result: 结果表明，虽然两种方法的响应质量相当，但该方法在检索器中生成的文档数量减少了68%以上，效率显著提高。

Conclusion: 利用隐式的线性化知识可能是处理复杂层级数据结构的一种非常有效和可扩展的策略。

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [36] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 研究了在医疗领域使用轻量级模型进行医学摘要分类的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多 NLP 任务中表现良好，但它们很难在具有严格成本、延迟和隐私限制的医疗环境中部署。

Method: 使用公共医学摘要语料库，我们使用三个目标微调 BERT base 和 DistilBERT：标准交叉熵、类加权交叉熵和焦点损失，保持分词器、序列长度、优化器和时间表不变。

Result: DistilBERT 与普通交叉熵在测试集上给出了最佳平衡，同时使用的参数远少于 BERT base。

Conclusion: 我们的结果提出了一个实用的默认方法：从紧凑的编码器和交叉熵开始，然后在转向更重的模型之前添加校准和特定于任务的检查。

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [37] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 介绍了一个名为HUME的人工评估框架，用于评估文本嵌入模型的性能，并将其与人类表现进行比较。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对文本嵌入模型进行人工评估的可靠方法，难以理解模型性能的优劣。

Method: 通过HUME框架，测量人类在16个MTEB数据集上的表现，涵盖多种任务和语言。

Result: 人类的平均表现为77.6%，最佳嵌入模型为80.1%，但模型在不同数据集上的表现差异很大，表明数据集存在问题，且在低资源语言中存在不足。

Conclusion: 提供人工评估基准，揭示任务难度模式，并提供可扩展的评估框架，以更有效地解读模型，并为模型和基准的开发提供信息。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [38] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: 提出了一种新的神经符号框架，Concept Language Model Network (CLMN)，以提高NLP模型的可解释性，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自然语言处理领域取得了进展，但其可解释性仍然有限，特别是在医疗保健和金融领域。

Method: CLMN将概念表示为连续的、人类可读的嵌入，并应用模糊逻辑推理来学习自适应交互规则，这些规则说明概念如何相互影响以及最终决策。该模型用概念感知表示来增强原始文本特征，并自动推导出可解释的逻辑规则。

Result: 在多个数据集和预训练语言模型上，CLMN实现了比现有基于概念的方法更高的准确率，同时提高了可解释性。

Conclusion: 将神经表示与符号推理集成在一个统一的概念空间中，可以产生实用、透明的NLP系统。

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [39] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: Unilaw-R1 is a 7B parameter LLM tailored for legal reasoning, reducing deployment cost while tackling legal domain challenges.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning-focused LLMs are underexplored in handling complex legal problems.

Method: A two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on a high-quality dataset containing 17K distilled and screened chain-of-thought (CoT) samples.

Result: Unilaw-R1 outperforms all models of similar scale and achieves performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%).

Conclusion: Unilaw-R1 demonstrates strong results on authoritative benchmarks and showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [40] [LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.11358)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成（RAG）中LLM特定效用的概念，发现人工标注的段落对于LLM来说并非最优，且ground-truth的效用段落在不同的LLM之间不可转移。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将效用视为通用属性，忽略了不同LLM由于内部知识和理解能力的不同，可能从同一段落中获益不同的事实。

Method: 通过大规模实验，跨多个数据集和LLM，系统地研究LLM特定效用的概念。

Result: 实验表明，人工标注的段落对于LLM来说并非最优，并且ground-truth的效用段落在不同的LLM之间不可转移。同时发现，困惑度是衡量queries和段落可读性的关键指标。

Conclusion: 研究结果表明，在RAG研究中采用LLM特定的效用是必要的，并提出了一个LLM特定效用判断的基准测试程序。评估了现有utility判断方法，发现使用伪答案的verbalized方法表现稳健，但LLM难以有效评估utility。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

</details>


### [41] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: This paper introduces A-IPO, an adaptive intent-driven preference optimization method, to address the limitations of existing alignment methods like DPO which often overlook minority opinions and fail to capture latent user intentions. A-IPO incorporates an intention module that infers the latent intent behind each user prompt and explicitly incorporates this inferred intent into the reward function.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods often default to majority views, overlooking minority opinions and failing to capture latent user intentions in prompts.

Method: The paper introduces A-IPO, which includes an intention module to infer latent user intents and incorporates this into the reward function. It also introduces two new benchmarks, Real-pref and Attack-pref, along with an extended version of GlobalOpinionQA-Ext, for evaluation.

Result: A-IPO consistently surpasses existing baselines, yielding substantial improvements across key metrics: up to +24.8 win-rate and +45.6 Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention Consistency Score on GlobalOpinionQA-Ext.

Conclusion: A-IPO facilitates pluralistic preference optimization while simultaneously enhancing adversarial robustness in preference alignment.

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [42] [SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping](https://arxiv.org/abs/2510.11599)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: SemCSE-Multi: An unsupervised framework for generating multifaceted embeddings of scientific abstracts.


<details>
  <summary>Details</summary>
Motivation: Capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains.

Method: Unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models. Distill these aspect-specific embedding capabilities into a unified embedding model. Introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects.

Result: Decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.

Conclusion: SemCSE-Multi is a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine.

Abstract: We propose SemCSE-Multi, a novel unsupervised framework for generating
multifaceted embeddings of scientific abstracts, evaluated in the domains of
invasion biology and medicine. These embeddings capture distinct, individually
specifiable aspects in isolation, thus enabling fine-grained and controllable
similarity assessments as well as adaptive, user-driven visualizations of
scientific domains. Our approach relies on an unsupervised procedure that
produces aspect-specific summarizing sentences and trains embedding models to
map semantically related summaries to nearby positions in the embedding space.
We then distill these aspect-specific embedding capabilities into a unified
embedding model that directly predicts multiple aspect embeddings from a
scientific abstract in a single, efficient forward pass. In addition, we
introduce an embedding decoding pipeline that decodes embeddings back into
natural language descriptions of their associated aspects. Notably, we show
that this decoding remains effective even for unoccupied regions in
low-dimensional visualizations, thus offering vastly improved interpretability
in user-centric settings.

</details>


### [43] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一种名为 PerAugy 的数据增强技术，通过交叉轨迹洗牌和摘要内容扰动来提升个性化摘要模型的性能。


<details>
  <summary>Details</summary>
Motivation: 个性化摘要模型训练数据稀缺，现有数据集缺乏用户偏好历史和目标摘要，且主题转换多样性有限。

Method: 提出 PerAugy 数据增强技术，包括交叉轨迹洗牌和摘要内容扰动。

Result: PerAugy 显著提高了四个最先进的用户编码器的准确性（AUC 提升 0.132），并提升了个性化摘要框架的个性化程度（PSE-SU4 指标平均提升 61.2%）。

Conclusion: 数据集多样性是提升个性化摘要模型性能的关键因素。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI](https://arxiv.org/abs/2510.09649)
*Khartik Uppalapati,Bora Yimenicioglu,Shakeel Abdulkareem,Adan Eftekhari,Bhavya Uppalapati,Viraj Kamath*

Main category: cs.CV

TL;DR: 本文提出了一种名为 TinyViT-Batten 的少样本视觉 Transformer (ViT) 框架，用于从儿科脑部 MRI 中检测早期 Batten 病。


<details>
  <summary>Details</summary>
Motivation: Batten 病（神经元蜡样脂褐质沉积症）是一种罕见的儿科神经退行性疾病，其早期 MRI 征象不明显且容易被忽略。

Method: 将大型教师 ViT 提炼成一个 5M 参数的 TinyViT，并使用基于度量的少样本学习（具有 5-shot episode 的原型损失）对其进行微调。

Result: 该模型在包含 79 个基因确认的 Batten 病 MRI（来自 Hochstein 自然历史研究的 27 个 CLN3、来自国际纵向队列的 32 个 CLN2、Cokal 等人报告的 12 个早期表现 CLN2 病例以及 8 个公共 Radiopaedia 扫描）以及 90 个年龄匹配的对照的多站点数据集上实现了高精度（约 91%）和 ROC 曲线下面积至少 0.95，优于 3D-ResNet 和 Swin-Tiny 基线。

Conclusion: 该模型的小尺寸和强大的性能（灵敏度大于 90%，特异性约 90%）证明了一种用于早期 Batten 病检测的实用 AI 解决方案。

Abstract: Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric
neurodegenerative disorder whose early MRI signs are subtle and often missed.
We propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to
detect early Batten disease from pediatric brain MRI with limited training
cases. We distill a large teacher ViT into a 5 M-parameter TinyViT and
fine-tune it using metric-based few-shot learning (prototypical loss with
5-shot episodes). Our model achieves high accuracy (approximately 91%) and area
under ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed
Batten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2
from an international longitudinal cohort, 12 early-manifestation CLN2 cases
reported by Cokal et al., and 8 public Radiopaedia scans) together with 90
age-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We
further integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to
highlight disease-relevant brain regions, enabling explainable predictions. The
model's small size and strong performance (sensitivity greater than 90%,
specificity approximately 90%) demonstrates a practical AI solution for early
Batten disease detection.

</details>


### [45] [Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition](https://arxiv.org/abs/2510.09653)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文全面概述了 Ultralytics YOLO 对象检测器系列，重点关注架构演变、基准测试、部署前景和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在展示 YOLO 系列的发展历程，以及不同版本之间的优劣。

Method: 通过在 MS COCO 数据集上对 YOLOv5、YOLOv8、YOLO11 和 YOLO26 进行基准测试，并与其他模型进行交叉比较，分析精度和效率之间的权衡。

Result: 详细对比了 YOLOv5、YOLOv8、YOLO11 和 YOLO26 在 MS COCO 数据集上的性能，并与其他模型进行了交叉比较，突出了精度和效率之间的权衡。

Conclusion: 总结了 YOLO 系列的优势和局限性，并提出了未来发展方向，包括密集场景限制、混合 CNN-Transformer 集成、开放词汇检测和边缘感知训练方法。

Abstract: This paper presents a comprehensive overview of the Ultralytics YOLO(You Only
Look Once) family of object detectors, focusing the architectural evolution,
benchmarking, deployment perspectives, and future challenges. The review begins
with the most recent release, YOLO26 (YOLOv26), which introduces key
innovations including Distribution Focal Loss (DFL) removal, native NMS-free
inference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label
Assignment (STAL), and the MuSGD optimizer for stable training. The progression
is then traced through YOLO11, with its hybrid task assignment and
efficiency-focused modules; YOLOv8, which advanced with a decoupled detection
head and anchor-free predictions; and YOLOv5, which established the modular
PyTorch foundation that enabled modern YOLO development. Benchmarking on the MS
COCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,
YOLO11, and YOLO26, alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR,
and DEIM. Metrics including precision, recall, F1 score, mean Average
Precision, and inference speed are analyzed to highlight trade-offs between
accuracy and efficiency. Deployment and application perspectives are further
discussed, covering export formats, quantization strategies, and real-world use
in robotics, agriculture, surveillance, and manufacturing. Finally, the paper
identifies challenges and future directions, including dense-scene limitations,
hybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware
training approaches.

</details>


### [46] [TreeNet: Layered Decision Ensembles](https://arxiv.org/abs/2510.09654)
*Zeshan Khan*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种名为TreeNet的新型分层决策集成学习方法，专为医学图像分析设计，尤其适用于实时分析。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分析方法在数据量充足时表现良好，但医学图像分析中常见的问题是数据可用性和数据置信度有限。

Method: 该论文通过整合神经网络、集成学习和基于树的决策模型的关键特征，构建了TreeNet模型。

Result: TreeNet在完整训练数据下F1得分为0.85，在仅使用50%训练数据时F1得分为0.77，同时实现了32帧/秒的速度。

Conclusion: TreeNet在医学图像分析领域，尤其是在实时分析方面，具有效率和实用性。

Abstract: Within the domain of medical image analysis, three distinct methodologies
have demonstrated commendable accuracy: Neural Networks, Decision Trees, and
Ensemble-Based Learning Algorithms, particularly in the specialized context of
genstro institutional track abnormalities detection. These approaches exhibit
efficacy in disease detection scenarios where a substantial volume of data is
available. However, the prevalent challenge in medical image analysis pertains
to limited data availability and data confidence. This paper introduces
TreeNet, a novel layered decision ensemble learning methodology tailored for
medical image analysis. Constructed by integrating pivotal features from neural
networks, ensemble learning, and tree-based decision models, TreeNet emerges as
a potent and adaptable model capable of delivering superior performance across
diverse and intricate machine learning tasks. Furthermore, its interpretability
and insightful decision-making process enhance its applicability in complex
medical scenarios. Evaluation of the proposed approach encompasses key metrics
including Accuracy, Precision, Recall, and training and evaluation time. The
methodology resulted in an F1-score of up to 0.85 when using the complete
training data, with an F1-score of 0.77 when utilizing 50\% of the training
data. This shows a reduction of F1-score of 0.08 while in the reduction of 50\%
of the training data and training time. The evaluation of the methodology
resulted in the 32 Frame per Second which is usable for the realtime
applications. This comprehensive assessment underscores the efficiency and
usability of TreeNet in the demanding landscape of medical image analysis
specially in the realtime analysis.

</details>


### [47] [OmniSAT: Compact Action Token, Faster Auto Regression](https://arxiv.org/abs/2510.09667)
*Huaihai Lyu,Chaofan Chen,Senwei Xie,Pengwei Wang,Xiansheng Chen,Shanghang Zhang,Changsheng Xu*

Main category: cs.CV

TL;DR: OmniSAT通过学习紧凑且可转移的动作表示来改进自回归（AR）模型的效率，实现了更高的压缩率，同时保持了重建质量，从而加速了AR训练的收敛和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型，特别是自回归模型，在处理长序列和高维动作时效率较低，现有的压缩方法存在重建质量差或压缩效率低的问题。

Method: OmniSAT首先对值范围和时间范围进行归一化，以获得与B样条编码一致的表示。然后，将多阶段残差量化应用于位置、旋转和夹持器子空间，从而为每个部分生成具有由粗到细粒度的压缩离散令牌。

Result: OmniSAT在Droid数据集上预训练后，训练序列缩短了6.8倍，并降低了目标熵。在各种真实机器人和仿真实验中，OmniSAT实现了更高的压缩率，同时保持了重建质量，从而加快了AR训练的收敛速度和模型性能。

Conclusion: OmniSAT通过其紧凑的动作表示和跨具身学习策略，提高了VLA模型中AR方法的效率和性能。

Abstract: Existing Vision-Language-Action (VLA) models can be broadly categorized into
diffusion-based and auto-regressive (AR) approaches: diffusion models capture
continuous action distributions but rely on computationally heavy iterative
denoising. In contrast, AR models enable efficient optimization and flexible
sequence construction, making them better suited for large-scale pretraining.
To further improve AR efficiency, particularly when action chunks induce
extended and high-dimensional sequences, prior work applies entropy-guided and
token-frequency techniques to shorten the sequence length. However, such
compression struggled with \textit{poor reconstruction or inefficient
compression}. Motivated by this, we introduce an Omni Swift Action Tokenizer,
which learns a compact, transferable action representation. Specifically, we
first normalize value ranges and temporal horizons to obtain a consistent
representation with B-Spline encoding. Then, we apply multi-stage residual
quantization to the position, rotation, and gripper subspaces, producing
compressed discrete tokens with coarse-to-fine granularity for each part. After
pre-training on the large-scale dataset Droid, the resulting discrete
tokenization shortens the training sequence by 6.8$\times$, and lowers the
target entropy. To further explore the potential of OmniSAT, we develop a
cross-embodiment learning strategy that builds on the unified action-pattern
space and jointly leverages robot and human demonstrations. It enables scalable
auxiliary supervision from heterogeneous egocentric videos. Across diverse
real-robot and simulation experiments, OmniSAT encompasses higher compression
while preserving reconstruction quality, enabling faster AR training
convergence and model performance.

</details>


### [48] [Knowledge-Aware Mamba for Joint Change Detection and Classification from MODIS Times Series](https://arxiv.org/abs/2510.09679)
*Zhengsen Xu,Yimin Zhu,Zack Dewis,Mabel Heffring,Motasem Alkayid,Saeid Taleghanidoozdoozan,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的知识感知Mamba (KAMamba) 用于增强MODIS变化检测。


<details>
  <summary>Details</summary>
Motivation: 由于MODIS混合像素、空间-光谱-时间信息耦合效应和背景类异质性等问题，使用MODIS时间序列进行变化检测非常具有挑战性。

Method: 1. 设计了一种知识驱动的转移矩阵引导方法，生成知识感知转移损失 (KAT-loss)，以提高检测精度。2. 设计了一种多任务学习方法，使用三种损失，即变化前分类损失 (PreC-loss)、变化后分类损失 (PostC-loss) 和变化检测损失 (Chg-loss) 来改进模型学习。3. 设计了新的空间-光谱-时间 Mamba (SSTMamba) 模块，以解耦 MODIS 时间序列中的信息耦合。4. 使用稀疏和可变形的 Mamba (SDMamba) 主干来提高 Mamba 模型的效率并降低计算成本。

Result: 在加拿大萨斯喀彻温省的 MODIS 时间序列数据集上，对土地覆盖变化检测和 LULC 分类进行了评估；结果表明，变化检测的平均 F1 值比基线提高了约 1.5-6%，LULC 分类的 OA、AA 和 Kappa 提高了约 2%。

Conclusion: 本文提出的KAMamba方法在MODIS时序数据变化检测和土地利用分类方面都取得了显著的改进。

Abstract: Although change detection using MODIS time series is critical for
environmental monitoring, it is a highly challenging task due to key MODIS
difficulties, e.g., mixed pixels, spatial-spectral-temporal information
coupling effect, and background class heterogeneity. This paper presents a
novel knowledge-aware Mamba (KAMamba) for enhanced MODIS change detection, with
the following contributions. First, to leverage knowledge regarding class
transitions, we design a novel knowledge-driven transition-matrix-guided
approach, leading to a knowledge-aware transition loss (KAT-loss) that can
enhance detection accuracies. Second, to improve model constraints, a
multi-task learning approach is designed, where three losses, i.e., pre-change
classification loss (PreC-loss), post-change classification loss (PostC-loss),
and change detection loss (Chg-loss) are used for improve model learning.
Third, to disentangle information coupling in MODIS time series, novel
spatial-spectral-temporal Mamba (SSTMamba) modules are designed. Last, to
improve Mamba model efficiency and remove computational cost, a sparse and
deformable Mamba (SDMamba) backbone is used in SSTMamba. On the MODIS
time-series dataset for Saskatchewan, Canada, we evaluate the method on
land-cover change detection and LULC classification; results show about 1.5-6%
gains in average F1 for change detection over baselines, and about 2%
improvements in OA, AA, and Kappa for LULC classification.

</details>


### [49] [NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation](https://arxiv.org/abs/2510.09681)
*Sashank Makanaboyina*

Main category: cs.CV

TL;DR: 提出了一种新的混合框架NNDM，它结合了NN-UNet的鲁棒特征提取和扩散概率模型的生成能力，用于脑肿瘤分割。


<details>
  <summary>Details</summary>
Motivation: 现有的卷积神经网络模型在泛化性、边界精度和有限的数据多样性方面存在不足。

Method: 通过学习预测掩码和真实掩码之间的残差误差分布，扩散模型逐步细化NN-UNet生成的分割掩码。这个迭代去噪过程使模型能够纠正精细的结构不一致性并增强肿瘤边界的描绘。

Result: 在BraTS 2021数据集上进行的实验表明，与传统的U-Net和基于Transformer的基线相比，NNDM取得了优异的性能，在Dice系数和Hausdorff距离指标上都有所提高。此外，扩散引导的细化增强了跨模态和肿瘤亚区域的鲁棒性。

Conclusion: 提出的NNDM为结合确定性分割网络与随机扩散模型建立了一个新的方向，推进了自动脑肿瘤分析的最新技术。

Abstract: Accurate detection and segmentation of brain tumors in magnetic resonance
imaging (MRI) are critical for effective diagnosis and treatment planning.
Despite advances in convolutional neural networks (CNNs) such as U-Net,
existing models often struggle with generalization, boundary precision, and
limited data diversity. To address these challenges, we propose NNDM (NN\_UNet
Diffusion Model)a hybrid framework that integrates the robust feature
extraction of NN-UNet with the generative capabilities of diffusion
probabilistic models. In our approach, the diffusion model progressively
refines the segmentation masks generated by NN-UNet by learning the residual
error distribution between predicted and ground-truth masks. This iterative
denoising process enables the model to correct fine structural inconsistencies
and enhance tumor boundary delineation. Experiments conducted on the BraTS 2021
datasets demonstrate that NNDM achieves superior performance compared to
conventional U-Net and transformer-based baselines, yielding improvements in
Dice coefficient and Hausdorff distance metrics. Moreover, the diffusion-guided
refinement enhances robustness across modalities and tumor subregions. The
proposed NNDM establishes a new direction for combining deterministic
segmentation networks with stochastic diffusion models, advancing the state of
the art in automated brain tumor analysis.

</details>


### [50] [Adaptive Fusion Network with Temporal-Ranked and Motion-Intensity Dynamic Images for Micro-expression Recognition](https://arxiv.org/abs/2510.09730)
*Thi Bich Phuong Man,Luu Tu Nguyen,Vu Tram Anh Khuong,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种新的微表情识别(MER)方法，该方法在三个基准数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 微表情在谎言检测、行为分析和心理评估中具有重要价值，但难以察觉。

Method: 该方法提出了时间排序动态图像和运动强度动态图像两种互补表示，以及自适应融合网络。

Result: 在CASME-II上，AFN实现了93.95%的准确率和0.897的UF1，在SAMM上实现了82.47%的准确率和0.665的UF1，在MMEW上实现了76.00%的准确率。

Conclusion: 该研究表明，输入和所提出的架构在提高MER性能方面都起着重要作用，并为情感计算、谎言检测和人机交互领域的进一步研究和实际应用提供了坚实的基础。

Abstract: Micro-expressions (MEs) are subtle, transient facial changes with very low
intensity, almost imperceptible to the naked eye, yet they reveal a person
genuine emotion. They are of great value in lie detection, behavioral analysis,
and psychological assessment. This paper proposes a novel MER method with two
main contributions. First, we propose two complementary representations -
Temporal-ranked dynamic image, which emphasizes temporal progression, and
Motion-intensity dynamic image, which highlights subtle motions through a frame
reordering mechanism incorporating motion intensity. Second, we propose an
Adaptive fusion network, which automatically learns to optimally integrate
these two representations, thereby enhancing discriminative ME features while
suppressing noise. Experiments on three benchmark datasets (CASME-II, SAMM and
MMEW) demonstrate the superiority of the proposed method. Specifically, AFN
achieves 93.95 Accuracy and 0.897 UF1 on CASME-II, setting a new
state-of-the-art benchmark. On SAMM, the method attains 82.47 Accuracy and
0.665 UF1, demonstrating more balanced recognition across classes. On MMEW, the
model achieves 76.00 Accuracy, further confirming its generalization ability.
The obtained results show that both the input and the proposed architecture
play important roles in improving the performance of MER. Moreover, they
provide a solid foundation for further research and practical applications in
the fields of affective computing, lie detection, and human-computer
interaction.

</details>


### [51] [Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey](https://arxiv.org/abs/2510.09731)
*Muhammad Munsif,Waqas Ahmad,Amjid Ali,Mohib Ullah,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: 本研究对多视角多摄像头（MVMC）系统进行了全面的综述，统一了MVMC跟踪、Re-ID和动作理解。


<details>
  <summary>Details</summary>
Motivation: 现有综述主要关注孤立的任务，忽略了它们集成到一个有凝聚力的系统中。同时，也忽略了多摄像头协作和多视角数据分析所带来的复杂性和机遇。

Method: 提出了一个独特的分类法，将CVS分为四个关键部分：MVMC跟踪、Re-ID、AU和组合方法。系统地整理和总结了最新的数据集、方法、结果和评估指标。

Result: 总结了当前的研究进展，并识别和讨论了开放的研究问题和挑战，以及新兴技术，如终身学习、隐私和联邦学习。

Conclusion: 概述了关键的研究方向，以提高CVS在复杂、现实世界应用中的鲁棒性、效率和适应性。

Abstract: Connected Vision Systems (CVS) are transforming a variety of applications,
including autonomous vehicles, smart cities, surveillance, and human-robot
interaction. These systems harness multi-view multi-camera (MVMC) data to
provide enhanced situational awareness through the integration of MVMC
tracking, re-identification (Re-ID), and action understanding (AU). However,
deploying CVS in real-world, dynamic environments presents a number of
challenges, particularly in addressing occlusions, diverse viewpoints, and
environmental variability. Existing surveys have focused primarily on isolated
tasks such as tracking, Re-ID, and AU, often neglecting their integration into
a cohesive system. These reviews typically emphasize single-view setups,
overlooking the complexities and opportunities provided by multi-camera
collaboration and multi-view data analysis. To the best of our knowledge, this
survey is the first to offer a comprehensive and integrated review of MVMC that
unifies MVMC tracking, Re-ID, and AU into a single framework. We propose a
unique taxonomy to better understand the critical components of CVS, dividing
it into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We
systematically arrange and summarize the state-of-the-art datasets,
methodologies, results, and evaluation metrics, providing a structured view of
the field's progression. Furthermore, we identify and discuss the open research
questions and challenges, along with emerging technologies such as lifelong
learning, privacy, and federated learning, that need to be addressed for future
advancements. The paper concludes by outlining key research directions for
enhancing the robustness, efficiency, and adaptability of CVS in complex,
real-world applications. We hope this survey will inspire innovative solutions
and guide future research toward the next generation of intelligent and
adaptive CVS.

</details>


### [52] [Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping](https://arxiv.org/abs/2510.09741)
*Dwip Dalal,Gautam Vashishtha,Utkarsh Mishra,Jeonghwan Kim,Madhav Kanda,Hyeonjeong Ha,Svetlana Lazebnik,Heng Ji,Unnat Jain*

Main category: cs.CV

TL;DR: AttWarp通过在测试时对输入图像进行attention-guided warping来重新分配空间分辨率，从而提高MLLM的性能。


<details>
  <summary>Details</summary>
Motivation: MLLM通常会忽略杂乱场景中的小细节和空间关系，导致细粒度感知错误。

Method: 使用MLLM的跨模态注意力对输入图像执行rectilinear warping，将空间分辨率重新分配到模型认为重要的区域。

Result: 在五个基准测试和四个MLLM上，AttWarp持续提高准确率，加强组合推理，并减少幻觉，优于其他基线方法。

Conclusion: attention-guided warping可以优先处理与查询相关的信息，同时保留上下文，并且相同的MLLM在获得warped inputs时表现更好。

Abstract: Multimodal large language models (MLLMs) often miss small details and spatial
relations in cluttered scenes, leading to errors in fine-grained perceptual
grounding. We introduce AttWarp, a lightweight method that allocates more
resolution to query-relevant content while compressing less informative areas,
all while preserving global context. At test time, the approach uses an MLLM's
cross-modal attention to perform rectilinear warping of the input image,
reallocating spatial resolution toward regions the model deems important,
without changing model weights or architecture. This attention-guided warping
preserves all original image information but redistributes it non-uniformly, so
small objects and subtle relationships become easier for the same model to read
while the global layout remains intact. Across five benchmarks (TextVQA, GQA,
DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and
InstructBLIP), AttWarp consistently improves accuracy, strengthens
compositional reasoning, and reduces hallucinations, outperforming four
competitive baselines that manipulate raw images at test time. Together, these
results show that attention-guided warping prioritizes information relevant to
the query while preserving context, and that the same MLLMs perform better when
given such warped inputs.

</details>


### [53] [Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning](https://arxiv.org/abs/2510.09815)
*Yufei Wang,Adriana Kovashka,Loretta Fernández,Marc N. Coutanche,Seth Wiener*

Main category: cs.CV

TL;DR: 本研究探索了一种新的外语学习环境，学习者需要在多模态的语境中推断生词的含义，即通过描述配对图像的句子。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探究影响学习者在多模态语境下推断生词含义的因素，以及不同语言背景的学习者与成功率之间的关系。

Method: 通过人类参与者进行实验，使用不同的图像-文本对，分析数据（图像和文本）的特征。

Result: 研究发现只有一些直观的特征与参与者的表现有很强的相关性，这表明需要进一步研究预测任务成功的特征。此外，还分析了人工智能系统对参与者表现的推理能力。

Conclusion: 结果为改进人工智能系统的推理能力提供了有希望的未来方向。

Abstract: We investigate a new setting for foreign language learning, where learners
infer the meaning of unfamiliar words in a multimodal context of a sentence
describing a paired image. We conduct studies with human participants using
different image-text pairs. We analyze the features of the data (i.e., images
and texts) that make it easier for participants to infer the meaning of a
masked or unfamiliar word, and what language backgrounds of the participants
correlate with success. We find only some intuitive features have strong
correlations with participant performance, prompting the need for further
investigating of predictive features for success in these tasks. We also
analyze the ability of AI systems to reason about participant performance, and
discover promising future directions for improving this reasoning ability.

</details>


### [54] [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/abs/2510.09822)
*Weiqing Luo,Zhen Tan,Yifan Li,Xinyu Zhao,Kwonjoon Lee,Behzad Dariush,Tianlong Chen*

Main category: cs.CV

TL;DR: 现有的视觉大语言模型(VLLM)通常为下游任务预设固定的分辨率，导致性能不佳。本文旨在解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 真实世界的视觉语言应用需要不同层次的感知粒度。现有VLLM的分辨率是固定的，无法满足实际需求。

Method: 1. 调查不同视觉语言任务的分辨率偏好，揭示分辨率偏好与图像复杂度和VLLM在不同图像输入分辨率下的不确定性方差之间的相关性；2. 提出了一个经验公式，结合图像复杂度和不确定性方差来确定给定视觉语言任务的最佳分辨率；3. 提出了一种新的参数高效的微调技术，将预训练的VLLM的视觉输入分辨率扩展到所确定的最佳分辨率。

Result: 在各种视觉语言任务上进行了广泛的实验，验证了该方法的有效性。

Conclusion: 该方法能够有效提升VLLM在不同视觉语言任务上的性能。

Abstract: Real-world vision-language applications demand varying levels of perceptual
granularity. However, most existing visual large language models (VLLMs), such
as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to
subpar performance. To address this problem, we first conduct a comprehensive
and pioneering investigation into the resolution preferences of different
vision-language tasks, revealing a correlation between resolution preferences
with image complexity, and uncertainty variance of the VLLM at different image
input resolutions. Building on this insight, we propose an empirical formula to
determine the optimal resolution for a given vision-language task, combining
these two factors. Second, based on rigorous experiments, we propose a novel
parameter-efficient fine-tuning technique to extend the visual input resolution
of pre-trained VLLMs to the identified optimal resolution. Extensive
experiments on various vision-language tasks validate the effectiveness of our
method.

</details>


### [55] [Post Processing of image segmentation using Conditional Random Fields](https://arxiv.org/abs/2510.09833)
*Aashish Dhawan,Pankaj Bodani,Vishal Garg*

Main category: cs.CV

TL;DR: 本文旨在找到一种合适的条件随机场（CRF）以提高分割图像的清晰度。


<details>
  <summary>Details</summary>
Motivation: 卫星图像的低质量特征导致图像分割过程的输出通常不是很清楚。

Method: 研究了不同类型的CRF，并评估了它们对卫星图像和高质量航拍照片的适用性。

Result: 通过实验比较了各种CRF在图像上的结果，并展示了不同方法在这些数据集上的优缺点。

Conclusion: 找到了一种合适的CRF，可以提高分割图像的清晰度。

Abstract: The output of image the segmentation process is usually not very clear due to
low quality features of Satellite images. The purpose of this study is to find
a suitable Conditional Random Field (CRF) to achieve better clarity in a
segmented image. We started with different types of CRFs and studied them as to
why they are or are not suitable for our purpose. We evaluated our approach on
two different datasets - Satellite imagery having low quality features and high
quality Aerial photographs. During the study we experimented with various CRFs
to find which CRF gives the best results on images and compared our results on
these datasets to show the pitfalls and potentials of different approaches.

</details>


### [56] [Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection](https://arxiv.org/abs/2510.09836)
*David Benavente-Rios,Juan Ruiz Rodriguez,Gustavo Gatica*

Main category: cs.CV

TL;DR: 本文研究使用合成面部数据来增强单人脸融合攻击检测 (S-MAD)。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，缺乏大规模真实图像数据集。

Method: 利用各种融合工具和跨数据集评估方案，并实施增量测试协议。

Result: 实验结果表明，通过将受控数量的合成图像谨慎地整合到现有数据集中，或者在训练过程中逐步添加真实图像，可以提高泛化能力。但是，不加区分地使用合成数据可能会导致次优性能。

Conclusion: 在操作场景中，最好的选择不是仅依靠合成数据进行 S-MAD。

Abstract: This paper investigates the use of synthetic face data to enhance
Single-Morphing Attack Detection (S-MAD), addressing the limitations of
availability of large-scale datasets of bona fide images due to privacy
concerns. Various morphing tools and cross-dataset evaluation schemes were
utilized to conduct this study. An incremental testing protocol was implemented
to assess the generalization capabilities as more and more synthetic images
were added. The results of the experiments show that generalization can be
improved by carefully incorporating a controlled number of synthetic images
into existing datasets or by gradually adding bona fide images during training.
However, indiscriminate use of synthetic data can lead to sub-optimal
performance. Evenmore, the use of only synthetic data (morphed and non-morphed
images) achieves the highest Equal Error Rate (EER), which means in operational
scenarios the best option is not relying only on synthetic data for S-MAD.

</details>


### [57] [Cell Instance Segmentation: The Devil Is in the Boundaries](https://arxiv.org/abs/2510.09848)
*Peixian Liang,Yifan Ding,Yizhe Zhang,Jianxu Chen,Hao Zheng,Hongxiao Wang,Yejia Zhang,Guangyu Meng,Tim Weninger,Michael Niemier,X. Sharon Hu,Danny Z Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的像素聚类方法Ceb，利用细胞边界特征和标签将前景像素划分为细胞实例。


<details>
  <summary>Details</summary>
Motivation: 现有的细胞实例分割方法依赖于像素级的目标，可能丢失细胞实例的几何属性，如形状、曲率和凸性。

Method: Ceb首先从语义分割生成的概率图中提取潜在的前景-前景边界，然后构建边界特征表示（称为边界签名），并使用边界分类器预测其二元边界标签，最后基于预测的边界标签分割或合并相邻区域以获得细胞实例。

Result: 在六个数据集上的实验表明，Ceb优于现有的像素聚类方法，并且与SOTA细胞实例分割方法相比，具有高度的竞争力。

Conclusion: Ceb是一种有效的细胞实例分割方法，它利用细胞边界特征来提高分割性能。

Abstract: State-of-the-art (SOTA) methods for cell instance segmentation are based on
deep learning (DL) semantic segmentation approaches, focusing on distinguishing
foreground pixels from background pixels. In order to identify cell instances
from foreground pixels (e.g., pixel clustering), most methods decompose
instance information into pixel-wise objectives, such as distances to
foreground-background boundaries (distance maps), heat gradients with the
center point as heat source (heat diffusion maps), and distances from the
center point to foreground-background boundaries with fixed angles (star-shaped
polygons). However, pixel-wise objectives may lose significant geometric
properties of the cell instances, such as shape, curvature, and convexity,
which require a collection of pixels to represent. To address this challenge,
we present a novel pixel clustering method, called Ceb (for Cell boundaries),
to leverage cell boundary features and labels to divide foreground pixels into
cell instances. Starting with probability maps generated from semantic
segmentation, Ceb first extracts potential foreground-foreground boundaries
with a revised Watershed algorithm. For each boundary candidate, a boundary
feature representation (called boundary signature) is constructed by sampling
pixels from the current foreground-foreground boundary as well as the
neighboring background-foreground boundaries. Next, a boundary classifier is
used to predict its binary boundary label based on the corresponding boundary
signature. Finally, cell instances are obtained by dividing or merging
neighboring regions based on the predicted boundary labels. Extensive
experiments on six datasets demonstrate that Ceb outperforms existing pixel
clustering methods on semantic segmentation probability maps. Moreover, Ceb
achieves highly competitive performance compared to SOTA cell instance
segmentation methods.

</details>


### [58] [Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2510.09867)
*Zhi Chen,Xin Yu,Xiaohui Tao,Yan Li,Zi Huang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的视觉语言模型 (VLM) 提示集成方法，旨在解决传统提示集成方法的次优问题。


<details>
  <summary>Details</summary>
Motivation: 传统提示集成方法通过平均文本特征来表示一个类别，但这会将类别中心从真实的类别分布中移开，导致性能下降。

Method: 该论文提出了聚类感知提示集成学习 (CAPEL) 框架，该框架保留了上下文提示的聚类性质。CAPEL 将图像分类到多个类别簇中的一个，每个簇由不同的提示表示。该框架在分类 logits 空间中执行集成，并引入了聚类保持正则化项和自适应提示权重技术。

Result: 该框架在不同的数据集和任务上实现了鲁棒的性能。

Conclusion: 该论文提出了一种有效的提示集成方法，可以提高视觉语言模型的性能。

Abstract: Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across
various tasks by pre-training on numerous image-text pairs. These models often
benefit from using an ensemble of context prompts to represent a class. Despite
being effective, conventional prompt ensembling that averages textual features
of context prompts often yields suboptimal results. This is because feature
averaging shifts the class centroids away from the true class distribution. To
address this issue, we propose the Cluster-Aware Prompt Ensemble Learning
(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL
classifies images into one of several class clusters, each represented by a
distinct prompt. Instead of ensembling prompts in the feature space, we perform
ensembling in the classification logits space, aligning better with the visual
feature distribution. To further optimize prompt fine-tuning while maintaining
cluster-specific discriminative power, we introduce a cluster-preserving
regularization term. This ensures that prompts remain distinct and specialized
for different clusters, preventing collapse into a uniform direction.
Additionally, we integrate an adaptive prompt weighting technique to
dynamically adjust the attention weights for flawed or ambiguous prompts,
ensuring robust performance across diverse datasets and tasks.

</details>


### [59] [Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking](https://arxiv.org/abs/2510.09878)
*Milad Khanchi,Maria Amer,Charalambos Poullis*

Main category: cs.CV

TL;DR: 提出了一种新的多目标跟踪（MOT）方法，该方法融合了深度和mask特征，并通过自监督编码器生成稳定的对象表示，以提高匹配精度，尤其是在对象相似或遮挡的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的MOT方法依赖于IoU进行关联，但在对象相似或遮挡时变得不可靠，且分割mask的IoU计算成本高昂。

Method: 使用分割mask捕捉对象形状，但不计算分割IoU，而是融合深度和mask特征，通过自监督编码器生成稳定的对象表示，并将其作为额外的相似性线索与bounding box IoU和re-identification特征一起用于匹配。使用zero-shot深度估计器和promptable visual segmentation模型获取深度图和对象mask。

Result: 在具有非线性运动、遮挡和拥挤场景的benchmark（如SportsMOT和DanceTrack）上，该方法优于TBD state-of-the-art，并在具有线性运动的benchmark（如MOT17）上实现了有竞争力的性能。

Conclusion: 该MOT方法是第一个使用自监督编码器来优化分割mask而无需计算mask IoU的方法，并在复杂场景中表现出优越的性能。

Abstract: Multi-object tracking (MOT) methods often rely on Intersection-over-Union
(IoU) for association. However, this becomes unreliable when objects are
similar or occluded. Also, computing IoU for segmentation masks is
computationally expensive. In this work, we use segmentation masks to capture
object shapes, but we do not compute segmentation IoU. Instead, we fuse depth
and mask features and pass them through a compact encoder trained
self-supervised. This encoder produces stable object representations, which we
use as an additional similarity cue alongside bounding box IoU and
re-identification features for matching. We obtain depth maps from a zero-shot
depth estimator and object masks from a promptable visual segmentation model to
obtain fine-grained spatial cues. Our MOT method is the first to use the
self-supervised encoder to refine segmentation masks without computing masks
IoU. MOT can be divided into joint detection-ReID (JDR) and
tracking-by-detection (TBD) models. The latter are computationally more
efficient. Experiments of our TBD method on challenging benchmarks with
non-linear motion, occlusion, and crowded scenes, such as SportsMOT and
DanceTrack, show that our method outperforms the TBD state-of-the-art on most
metrics, while achieving competitive performance on simpler benchmarks with
linear motion, such as MOT17.

</details>


### [60] [CHUG: Crowdsourced User-Generated HDR Video Quality Dataset](https://arxiv.org/abs/2510.09879)
*Shreshth Saini,Alan C. Bovik,Neil Birkbeck,Yilin Wang,Balu Adsumilli*

Main category: cs.CV

TL;DR: 提出了一个名为CHUG的大规模用户生成HDR视频质量数据集，用于解决现有HDR-VQA数据集主要关注专业生成内容（PGC）的问题，填补了理解真实世界UGC-HDR降级的空白。


<details>
  <summary>Details</summary>
Motivation: 现有HDR-VQA数据集主要关注专业生成内容（PGC），缺乏对真实世界UGC-HDR降级的理解。

Method: 构建了包含856个UGC-HDR源视频的数据集，并通过多种分辨率和比特率进行转码，模拟真实场景，共计5,992个视频。通过Amazon Mechanical Turk收集了211,848个感知评级。

Result: CHUG数据集为分析HDR视频中UGC特定失真提供了一个基准。

Conclusion: CHUG数据集的发布预计将通过提供大规模、多样化和真实的UGC数据集来推进无参考（NR）HDR-VQA研究。

Abstract: High Dynamic Range (HDR) videos enhance visual experiences with superior
brightness, contrast, and color depth. The surge of User-Generated Content
(UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR
video quality assessment (VQA) due to diverse capture conditions, editing
artifacts, and compression distortions. Existing HDR-VQA datasets primarily
focus on professionally generated content (PGC), leaving a gap in understanding
real-world UGC-HDR degradations. To address this, we introduce CHUG:
Crowdsourced User-Generated HDR Video Quality Dataset, the first large-scale
subjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos,
transcoded across multiple resolutions and bitrates to simulate real-world
scenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical
Turk collected 211,848 perceptual ratings. CHUG provides a benchmark for
analyzing UGC-specific distortions in HDR videos. We anticipate CHUG will
advance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse,
and real-world UGC dataset. The dataset is publicly available at:
https://shreshthsaini.github.io/CHUG/.

</details>


### [61] [Geometry-Aware Scene Configurations for Novel View Synthesis](https://arxiv.org/abs/2510.09880)
*Minkwan Kim,Changwoon Choi,Young Min Kim*

Main category: cs.CV

TL;DR: 提出场景自适应策略，以有效分配表征能力，从不完整的观测中生成室内环境的沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 室内场景通常呈现出具有不同复杂性的不规则布局，包含杂乱、遮挡和平坦墙壁。

Method: 利用几何先验来指导有限资源的利用，记录估计的几何支架上的观测统计数据，并指导基的最佳放置，并提出场景自适应虚拟视点以补偿输入轨迹中固有的几何缺陷，并施加必要的正则化。

Result: 在几个大型室内场景中进行了全面的分析和讨论，结果表明与采用规则放置的基线相比，有显着增强。

Conclusion: 该方法在渲染质量和内存需求方面均优于现有技术。

Abstract: We propose scene-adaptive strategies to efficiently allocate representation
capacity for generating immersive experiences of indoor environments from
incomplete observations. Indoor scenes with multiple rooms often exhibit
irregular layouts with varying complexity, containing clutter, occlusion, and
flat walls. We maximize the utilization of limited resources with guidance from
geometric priors, which are often readily available after pre-processing
stages. We record observation statistics on the estimated geometric scaffold
and guide the optimal placement of bases, which greatly improves upon the
uniform basis arrangements adopted by previous scalable Neural Radiance Field
(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to
compensate for geometric deficiencies inherent in view configurations in the
input trajectory and impose the necessary regularization. We present a
comprehensive analysis and discussion regarding rendering quality and memory
requirements in several large-scale indoor scenes, demonstrating significant
enhancements compared to baselines that employ regular placements.

</details>


### [62] [LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates](https://arxiv.org/abs/2510.09881)
*Minkwan Kim,Seungmin Lee,Junho Kim,Young Min Kim*

Main category: cs.CV

TL;DR: 提出了一种新的场景表示方法LTGS，可以从稀疏视角更新中适应日常变化，从而实现对真实世界环境的光真实感可视化。


<details>
  <summary>Details</summary>
Motivation: 从常规相机拍摄中获取日常环境面临挑战，因为频繁的场景变化需要密集的空间和时间观测。

Method: 1. 构建对象为模板高斯，作为共享对象轨迹的结构化、可重用先验。 2. 对象模板经过进一步的细化流程，调整先验以适应基于少量观测的时间变化环境。 3. 通过简单的转换，我们的框架可以推广到多个时间步长。

Result: 在真实世界数据集上的实验表明，该框架实现了优于其他基线的重建质量，同时实现了快速和轻量级的更新。

Conclusion: LTGS 是一种有效的场景表示，可以从稀疏视角更新中适应日常变化。

Abstract: Recent advances in novel-view synthesis can create the photo-realistic
visualization of real-world environments from conventional camera captures.
However, acquiring everyday environments from casual captures faces challenges
due to frequent scene changes, which require dense observations both spatially
and temporally. We propose long-term Gaussian scene chronology from sparse-view
updates, coined LTGS, an efficient scene representation that can embrace
everyday changes from highly under-constrained casual captures. Given an
incomplete and unstructured Gaussian splatting representation obtained from an
initial set of input images, we robustly model the long-term chronology of the
scene despite abrupt movements and subtle environmental variations. We
construct objects as template Gaussians, which serve as structural, reusable
priors for shared object tracks. Then, the object templates undergo a further
refinement pipeline that modulates the priors to adapt to temporally varying
environments based on few-shot observations. Once trained, our framework is
generalizable across multiple time steps through simple transformations,
significantly enhancing the scalability for a temporal evolution of 3D
environments. As existing datasets do not explicitly represent the long-term
real-world changes with a sparse capture setup, we collect real-world datasets
to evaluate the practicality of our pipeline. Experiments demonstrate that our
framework achieves superior reconstruction quality compared to other baselines
while enabling fast and light-weight updates.

</details>


### [63] [An uncertainty-aware framework for data-efficient multi-view animal pose estimation](https://arxiv.org/abs/2510.09903)
*Lenny Aharon,Keemin Lee,Karan Sikka,Selmaan Chettih,Cole Hurwitz,Liam Paninski,Matthew R Whiteway*

Main category: cs.CV

TL;DR: 本文提出了一种用于量化动物行为的多视角姿态估计的综合框架，该框架结合了新的训练和后处理技术，以及一种模型蒸馏程序，以生成更高效和有效的姿态估计器。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以在有限的标记数据下实现准确的跟踪，并且不确定性估计较差。

Method: 该方法结合了多视角Transformer（MVT），它利用预训练的backbone并能够同时处理所有视角的信息，同时一种新的patch masking方案学习鲁棒的跨视角对应关系，无需相机校准。对于校准的设置，通过3D增强和三角损失来结合几何一致性。将现有的Ensemble Kalman Smoother（EKS）后处理器扩展到非线性情况，并通过方差膨胀技术来增强不确定性量化。最后，为了利用MVT的缩放特性，设计了一种蒸馏程序，该程序利用改进的EKS预测和不确定性估计来生成高质量的伪标签，从而减少对人工标签的依赖。

Result: 该框架组件在三种不同的动物物种（苍蝇、小鼠、 chickadees）中始终优于现有方法，每个组件都贡献了互补的优势。

Conclusion: 该研究提供了一个实用的、具有不确定性意识的系统，用于可靠的姿态估计，从而能够在实际数据约束下进行下游行为分析。

Abstract: Multi-view pose estimation is essential for quantifying animal behavior in
scientific research, yet current methods struggle to achieve accurate tracking
with limited labeled data and suffer from poor uncertainty estimates. We
address these challenges with a comprehensive framework combining novel
training and post-processing techniques, and a model distillation procedure
that leverages the strengths of these techniques to produce a more efficient
and effective pose estimator. Our multi-view transformer (MVT) utilizes
pretrained backbones and enables simultaneous processing of information across
all views, while a novel patch masking scheme learns robust cross-view
correspondences without camera calibration. For calibrated setups, we
incorporate geometric consistency through 3D augmentation and a triangulation
loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to
the nonlinear case and enhance uncertainty quantification via a variance
inflation technique. Finally, to leverage the scaling properties of the MVT, we
design a distillation procedure that exploits improved EKS predictions and
uncertainty estimates to generate high-quality pseudo-labels, thereby reducing
dependence on manual labels. Our framework components consistently outperform
existing methods across three diverse animal species (flies, mice, chickadees),
with each component contributing complementary benefits. The result is a
practical, uncertainty-aware system for reliable pose estimation that enables
downstream behavioral analyses under real-world data constraints.

</details>


### [64] [SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision](https://arxiv.org/abs/2510.09912)
*D. V. Brovko*

Main category: cs.CV

TL;DR: 本文提出了一种基于高光谱图像的无人机感知深度学习架构，用于导航、目标检测和地形分类。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，传统导航变得不可靠，而高光谱成像(HSI)为无人机计算机视觉提供了独特的机会，实现精细的材料识别和目标区分。

Method: 通过引入提出的 SpectralCA 模块修改 Mobile 3D Vision Transformer (MDvT)，该模块采用双向交叉注意力融合光谱和空间特征。

Result: 在 WHU-Hi-HongHu 数据集上进行的实验评估表明，所提出的架构提高了无人机感知效率，从而能够实时操作导航、目标识别和环境监测任务。

Conclusion: 所提出的架构提高了无人机感知效率，能够实时操作导航、目标识别和环境监测任务。

Abstract: The relevance of this research lies in the growing demand for unmanned aerial
vehicles (UAVs) capable of operating reliably in complex environments where
conventional navigation becomes unreliable due to interference, poor
visibility, or camouflage. Hyperspectral imaging (HSI) provides unique
opportunities for UAV-based computer vision by enabling fine-grained material
recognition and object differentiation, which are critical for navigation,
surveillance, agriculture, and environmental monitoring. The aim of this work
is to develop a deep learning architecture integrating HSI into UAV perception
for navigation, object detection, and terrain classification. Objectives
include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional
architecture with spectral-spatial cross-attention, training, and benchmarking.
The methodology is based on the modification of the Mobile 3D Vision
Transformer (MDvT) by introducing the proposed SpectralCA block. This block
employs bi-directional cross-attention to fuse spectral and spatial features,
enhancing accuracy while reducing parameters and inference time. Experimental
evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed
using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The
findings confirm that the proposed architecture improves UAV perception
efficiency, enabling real-time operation for navigation, object recognition,
and environmental monitoring tasks.
  Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging,
unmanned aerial vehicle, object detection, semi-supervised learning.

</details>


### [65] [HeadsUp! High-Fidelity Portrait Image Super-Resolution](https://arxiv.org/abs/2510.09924)
*Renjie Li,Zihao Zhu,Xiaoyu Wang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 提出了一个名为HeadsUp的单步扩散模型，用于无缝地恢复和放大肖像图像。


<details>
  <summary>Details</summary>
Motivation: 现有的图像超分辨率技术通常只关注通用图像或严格对齐的面部图像，而人像照片同时包含人像和自然背景，将不同模型混合处理人像照片会导致面部区域周围出现混合或边界伪影。

Method: 该模型基于单步扩散模型构建，并开发了一种面部监督机制，以引导模型关注面部区域。同时，集成了一种基于参考的机制，以帮助进行身份恢复，减少低质量面部恢复中的面部模糊。

Result: HeadsUp在PortraitISR任务上实现了最先进的性能，同时在通用图像和对齐面部数据集上保持了相当或更高的性能。

Conclusion: 该研究提出了一种有效的人像图像超分辨率方法，并在PortraitISR任务上取得了显著成果。

Abstract: Portrait pictures, which typically feature both human subjects and natural
backgrounds, are one of the most prevalent forms of photography on social
media. Existing image super-resolution (ISR) techniques generally focus either
on generic real-world images or strictly aligned facial images (i.e., face
super-resolution). In practice, separate models are blended to handle portrait
photos: the face specialist model handles the face region, and the general
model processes the rest. However, these blending approaches inevitably
introduce blending or boundary artifacts around the facial regions due to
different model training recipes, while human perception is particularly
sensitive to facial fidelity. To overcome these limitations, we study the
portrait image supersolution (PortraitISR) problem, and propose HeadsUp, a
single-step diffusion model that is capable of seamlessly restoring and
upscaling portrait images in an end-to-end manner. Specifically, we build our
model on top of a single-step diffusion model and develop a face supervision
mechanism to guide the model in focusing on the facial region. We then
integrate a reference-based mechanism to help with identity restoration,
reducing face ambiguity in low-quality face restoration. Additionally, we have
built a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to
support model training and benchmarking for portrait images. Extensive
experiments show that HeadsUp achieves state-of-the-art performance on the
PortraitISR task while maintaining comparable or higher performance on both
general image and aligned face datasets.

</details>


### [66] [Denoising Diffusion as a New Framework for Underwater Images](https://arxiv.org/abs/2510.09934)
*Nilesh Jain,Elie Alhajjar*

Main category: cs.CV

TL;DR: 本文提出了一种改进水下图像质量的新方案。


<details>
  <summary>Details</summary>
Motivation: 水下图像在海洋研究和环境监测中起着关键作用，但由于环境的复杂性和遥远性，图像质量通常较差，存在低能见度、模糊纹理、颜色失真和噪声等问题。现有图像增强方法存在泛化性差、过度依赖干净数据集等局限性，且数据集缺乏多样性和高质量图像，大多只包含单目图像，限制了对不同光照条件和角度的表示。

Method: 本文提出使用去噪扩散模型扩展数据集，以包含各种图像类型，如立体、广角、微距和特写图像。同时，推荐使用Controlnet增强图像，以评估和提高相应数据集的质量。

Result: 未在摘要中明确说明。

Conclusion: 本文旨在通过扩展数据集和使用Controlnet增强图像来提高水下图像的质量，从而改善海洋生态系统的研究。

Abstract: Underwater images play a crucial role in ocean research and marine
environmental monitoring since they provide quality information about the
ecosystem. However, the complex and remote nature of the environment results in
poor image quality with issues such as low visibility, blurry textures, color
distortion, and noise. In recent years, research in image enhancement has
proven to be effective but also presents its own limitations, like poor
generalization and heavy reliance on clean datasets. One of the challenges
herein is the lack of diversity and the low quality of images included in these
datasets. Also, most existing datasets consist only of monocular images, a fact
that limits the representation of different lighting conditions and angles. In
this paper, we propose a new plan of action to overcome these limitations. On
one hand, we call for expanding the datasets using a denoising diffusion model
to include a variety of image types such as stereo, wide-angled, macro, and
close-up images. On the other hand, we recommend enhancing the images using
Controlnet to evaluate and increase the quality of the corresponding datasets,
and hence improve the study of the marine ecosystem.
  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet

</details>


### [67] [Semi-disentangled spatiotemporal implicit neural representations of longitudinal neuroimaging data for trajectory classification](https://arxiv.org/abs/2510.09936)
*Agampreet Aulakh,Nils D. Forkert,Matthias Wilms*

Main category: cs.CV

TL;DR: 提出了一种新的基于隐式神经表示（INR）的、完全数据驱动的方法，用于表示整个大脑的衰老轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习方法无法表示潜在的连续生物过程，给纵向MRI数据的分析带来了挑战，因为这些数据具有离散性，且个体和人群中存在不同的空间和时间图像采样模式。

Method: 将特定个体的纵向T1加权MRI数据建模为连续函数，使用隐式神经表示（INR）。设计了一个新的INR架构，能够部分解耦空间和时间轨迹参数，并设计了一个直接在INR参数空间上运行的有效框架，以对大脑衰老轨迹进行分类。

Result: 在更真实的非规则采样实验中，基于INR的方法在脑衰老轨迹分类任务中达到了81.3%的准确率，优于标准的深度学习基线模型（73.7%）。

Conclusion: 提出的基于INR的方法在脑衰老轨迹分类任务中表现出色，优于传统的深度学习方法，为分析纵向MRI数据提供了一种新的有效途径。

Abstract: The human brain undergoes dynamic, potentially pathology-driven, structural
changes throughout a lifespan. Longitudinal Magnetic Resonance Imaging (MRI)
and other neuroimaging data are valuable for characterizing trajectories of
change associated with typical and atypical aging. However, the analysis of
such data is highly challenging given their discrete nature with different
spatial and temporal image sampling patterns within individuals and across
populations. This leads to computational problems for most traditional deep
learning methods that cannot represent the underlying continuous biological
process. To address these limitations, we present a new, fully data-driven
method for representing aging trajectories across the entire brain by modelling
subject-specific longitudinal T1-weighted MRI data as continuous functions
using Implicit Neural Representations (INRs). Therefore, we introduce a novel
INR architecture capable of partially disentangling spatial and temporal
trajectory parameters and design an efficient framework that directly operates
on the INRs' parameter space to classify brain aging trajectories. To evaluate
our method in a controlled data environment, we develop a biologically grounded
trajectory simulation and generate T1-weighted 3D MRI data for 450 healthy and
dementia-like subjects at regularly and irregularly sampled timepoints. In the
more realistic irregular sampling experiment, our INR-based method achieves
81.3% accuracy for the brain aging trajectory classification task,
outperforming a standard deep learning baseline model (73.7%).

</details>


### [68] [Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals](https://arxiv.org/abs/2510.09945)
*Pouya Shaeri,Ryan T. Woo,Yasaman Mohammadpour,Ariane Middel*

Main category: cs.CV

TL;DR: 提出了一种人机交互的干预学习框架，通过人工校正分割输出来提升模型在真实场景下的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 分割模型在基准测试中表现良好，但在实际应用中容易过度依赖表面相关性，忽略真实的物体边界。

Method: 利用人工校正作为干预信号，告知模型何时不应依赖表面特征，并通过在视觉相似图像间传播校正信息，引导模型学习鲁棒的、语义相关的特征。

Result: 在具有挑战性的cubemap数据上，分割精度提高了9 mIoU (12-15%的相对改进)，与标准重新训练相比，标注工作量减少了3-4倍，同时在基准数据集上保持了有竞争力的性能。

Conclusion: 该框架为研究人员和实践者构建准确、对数据集偏差具有鲁棒性、数据高效且适应于真实世界领域的分割系统提供了一种实用的方法。

Abstract: Segmentation models achieve high accuracy on benchmarks but often fail in
real-world domains by relying on spurious correlations instead of true object
boundaries. We propose a human-in-the-loop interactive framework that enables
interventional learning through targeted human corrections of segmentation
outputs. Our approach treats human corrections as interventional signals that
show when reliance on superficial features (e.g., color or texture) is
inappropriate. The system learns from these interventions by propagating
correction-informed edits across visually similar images, effectively steering
the model toward robust, semantically meaningful features rather than
dataset-specific artifacts. Unlike traditional annotation approaches that
simply provide more training data, our method explicitly identifies when and
why the model fails and then systematically corrects these failure modes across
the entire dataset. Through iterative human feedback, the system develops
increasingly robust representations that generalize better to novel domains and
resist artifactual correlations. We demonstrate that our framework improves
segmentation accuracy by up to 9 mIoU points (12-15\% relative improvement) on
challenging cubemap data and yields 3-4$\times$ reductions in annotation effort
compared to standard retraining, while maintaining competitive performance on
benchmark datasets. This work provides a practical framework for researchers
and practitioners seeking to build segmentation systems that are accurate,
robust to dataset biases, data-efficient, and adaptable to real-world domains
such as urban climate monitoring and autonomous driving.

</details>


### [69] [A Multi-Strategy Framework for Enhancing Shatian Pomelo Detection in Real-World Orchards](https://arxiv.org/abs/2510.09948)
*Pan Wang,Yihao Hu,Xiaodong Bai,Aiping Yang,Xiangxiang Li,Meiping Ding,Jianguo Yao*

Main category: cs.CV

TL;DR: 本文提出了一种多策略框架，以解决沙田柚检测中的四个关键挑战：成像设备、光照条件、物体尺度变化和遮挡。


<details>
  <summary>Details</summary>
Motivation: 为了满足沙田柚商业化生产的需求，采用自动化检测是必要的。现有的研究方法在实际应用中效果不佳。

Method: 本文构建了一个多场景数据集STP-AgriData，并设计了一个REAS-Det网络，该网络包含RFAConv、C3RFEM模块、MultiSEAM结构和soft-NMS。

Result: 实验结果表明，该方法的精确率为87.6%，召回率为74.9%，mAP@.50为82.8%，mAP@.50:.95为53.3%。

Conclusion: 所提出的网络比其他最先进的检测方法表现出更优越的性能。

Abstract: As a specialty agricultural product with a large market scale, Shatian pomelo
necessitates the adoption of automated detection to ensure accurate quantity
and meet commercial demands for lean production. Existing research often
involves specialized networks tailored for specific theoretical or dataset
scenarios, but these methods tend to degrade performance in real-world. Through
analysis of factors in this issue, this study identifies four key challenges
that affect the accuracy of Shatian pomelo detection: imaging devices, lighting
conditions, object scale variation, and occlusion. To mitigate these
challenges, a multi-strategy framework is proposed in this paper. Firstly, to
effectively solve tone variation introduced by diverse imaging devices and
complex orchard environments, we utilize a multi-scenario dataset,
STP-AgriData, which is constructed by integrating real orchard images with
internet-sourced data. Secondly, to simulate the inconsistent illumination
conditions, specific data augmentations such as adjusting contrast and changing
brightness, are applied to the above dataset. Thirdly, to address the issues of
object scale variation and occlusion in fruit detection, an REAS-Det network is
designed in this paper. For scale variation, RFAConv and C3RFEM modules are
designed to expand and enhance the receptive fields. For occlusion variation, a
multi-scale, multi-head feature selection structure (MultiSEAM) and soft-NMS
are introduced to enhance the handling of occlusion issues to improve detection
accuracy. The results of these experiments achieved a precision(P) of 87.6%, a
recall (R) of 74.9%, a mAP@.50 of 82.8%, and a mAP@.50:.95 of 53.3%. Our
proposed network demonstrates superior performance compared to other
state-of-the-art detection methods.

</details>


### [70] [J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented Joint Training](https://arxiv.org/abs/2510.09953)
*Salma J. Ahmed,Emad A. Mohammed,Azam Asilian Bidgoli*

Main category: cs.CV

TL;DR: 提出了一种名为 J-RAS 的联合训练方法，用于引导图像分割，该方法集成了分割模型和检索模型。


<details>
  <summary>Details</summary>
Motivation: 医疗应用中的图像分割对于准确诊断、治疗计划和疾病监测至关重要。尽管人工分割结果精确，但耗时、成本高且容易因人类专业知识的差异而产生变异。人工智能 (AI) 方法已被开发出来以解决这些限制，但它们通常需要大型带注释的数据集，而这些数据集在实践中很少可用，并且由于患者间的差异和罕见的病理病例，常常难以推广到不同的成像条件。

Method: 提出 Joint Retrieval Augmented Segmentation (J-RAS)，这是一种用于引导图像分割的联合训练方法，该方法集成了分割模型和检索模型。两个模型经过联合优化，使分割模型能够利用检索到的图像-掩码对来丰富其解剖学理解，而检索模型则学习分割相关的特征，而不仅仅是简单的视觉相似性。

Result: 在 ACDC 数据集上，没有 J-RAS 的 SegFormer 的平均 Dice 得分为 0.8708，平均 Hausdorff 距离 (HD) 为 1.8130，而使用 J-RAS 后，性能显着提高到平均 Dice 得分为 0.9115，平均 HD 为 1.1489。

Conclusion: 验证了 J-RAS 在多个分割骨干网络（包括 U-Net、TransUNet、SAM 和 SegFormer）在两个基准数据集 ACDC 和 M&Ms 上的有效性和泛化性。

Abstract: Image segmentation, the process of dividing images into meaningful regions,
is critical in medical applications for accurate diagnosis, treatment planning,
and disease monitoring. Although manual segmentation by healthcare
professionals produces precise outcomes, it is time-consuming, costly, and
prone to variability due to differences in human expertise. Artificial
intelligence (AI)-based methods have been developed to address these
limitations by automating segmentation tasks; however, they often require
large, annotated datasets that are rarely available in practice and frequently
struggle to generalize across diverse imaging conditions due to inter-patient
variability and rare pathological cases. In this paper, we propose Joint
Retrieval Augmented Segmentation (J-RAS), a joint training method for guided
image segmentation that integrates a segmentation model with a retrieval model.
Both models are jointly optimized, enabling the segmentation model to leverage
retrieved image-mask pairs to enrich its anatomical understanding, while the
retrieval model learns segmentation-relevant features beyond simple visual
similarity. This joint optimization ensures that retrieval actively contributes
meaningful contextual cues to guide boundary delineation, thereby enhancing the
overall segmentation performance. We validate J-RAS across multiple
segmentation backbones, including U-Net, TransUNet, SAM, and SegFormer, on two
benchmark datasets: ACDC and M&Ms, demonstrating consistent improvements. For
example, on the ACDC dataset, SegFormer without J-RAS achieves a mean Dice
score of 0.8708$\pm$0.042 and a mean Hausdorff Distance (HD) of
1.8130$\pm$2.49, whereas with J-RAS, the performance improves substantially to
a mean Dice score of 0.9115$\pm$0.031 and a mean HD of 1.1489$\pm$0.30. These
results highlight the method's effectiveness and its generalizability across
architectures and datasets.

</details>


### [71] [Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making](https://arxiv.org/abs/2510.09981)
*Fan Zuo,Donglin Zhou,Jingqin Gao,Kaan Ozbay*

Main category: cs.CV

TL;DR: This paper introduces an AI framework using traffic cameras for large-scale traffic monitoring.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of traditional traffic monitoring methods, which are costly and difficult to scale, and existing video analytics, which struggle with dynamic viewpoints and large data volumes.

Method: The method involves a fine-tuned YOLOv11 model for real-time traffic density and classification, a graph-based viewpoint normalization method for non-stationary cameras, and a domain-specific large language model for automated summaries.

Result: The system was validated with 9 million images from 1,000 cameras during NYC's congestion pricing rollout in 2025, showing a 9% decline in passenger vehicle density, early truck volume reductions, and increases in pedestrian/cyclist activity.

Conclusion: The framework offers a practical solution for large-scale, policy-relevant traffic monitoring with minimal human intervention.

Abstract: Accurate, scalable traffic monitoring is critical for real-time and long-term
transportation management, particularly during disruptions such as natural
disasters, large construction projects, or major policy changes like New York
City's first-in-the-nation congestion pricing program. However, widespread
sensor deployment remains limited due to high installation, maintenance, and
data management costs. While traffic cameras offer a cost-effective
alternative, existing video analytics struggle with dynamic camera viewpoints
and massive data volumes from large camera networks. This study presents an
end-to-end AI-based framework leveraging existing traffic camera infrastructure
for high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11
model, trained on localized urban scenes, extracts multimodal traffic density
and classification metrics in real time. To address inconsistencies from
non-stationary pan-tilt-zoom cameras, we introduce a novel graph-based
viewpoint normalization method. A domain-specific large language model was also
integrated to process massive data from a 24/7 video stream to generate
frequent, automated summaries of evolving traffic patterns, a task far
exceeding manual capabilities. We validated the system using over 9 million
images from roughly 1,000 traffic cameras during the early rollout of NYC
congestion pricing in 2025. Results show a 9% decline in weekday passenger
vehicle density within the Congestion Relief Zone, early truck volume
reductions with signs of rebound, and consistent increases in pedestrian and
cyclist activity at corridor and zonal scales. Experiments showed that
example-based prompts improved LLM's numerical accuracy and reduced
hallucinations. These findings demonstrate the framework's potential as a
practical, infrastructure-ready solution for large-scale, policy-relevant
traffic monitoring with minimal human intervention.

</details>


### [72] [FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering](https://arxiv.org/abs/2510.09995)
*Lishen Qu,Zhihao Liu,Jinshan Pan,Shihao Zhou,Jinglei Shi,Duosheng Chen,Jufeng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的物理信息方法用于生成镜头光晕数据，包括参数化模板创建、光照感知 2D 合成和基于物理引擎的 3D 渲染，从而生成混合的 FlareX 数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的镜头光晕数据集通常在 2D 中合成，缺乏光晕多样性，忽略了物理原理，导致模型泛化能力差。

Method: 该方法包括三个阶段：参数化模板创建、光照感知 2D 合成和基于物理引擎的 3D 渲染，生成包含 2D 和 3D 视角的混合 FlareX 数据集，并设计了一种掩蔽方法从真实世界的图像中获得无光晕图像。

Result: 该数据集提供了 9,500 个 2D 模板和 3,000 个 3D 渲染的图像对。大量实验证明了该方法和数据集的有效性。

Conclusion: 本文提出了一种新的物理信息方法用于生成镜头光晕数据，并构建了一个混合的 FlareX 数据集，为镜头光晕去除研究提供了新的资源。

Abstract: Lens flare occurs when shooting towards strong light sources, significantly
degrading the visual quality of images. Due to the difficulty in capturing
flare-corrupted and flare-free image pairs in the real world, existing datasets
are typically synthesized in 2D by overlaying artificial flare templates onto
background images. However, the lack of flare diversity in templates and the
neglect of physical principles in the synthesis process hinder models trained
on these datasets from generalizing well to real-world scenarios. To address
these challenges, we propose a new physics-informed method for flare data
generation, which consists of three stages: parameterized template creation,
the laws of illumination-aware 2D synthesis, and physical engine-based 3D
rendering, which finally gives us a mixed flare dataset that incorporates both
2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates
derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D
scenes. Furthermore, we design a masking approach to obtain real-world
flare-free images from their corrupted counterparts to measure the performance
of the model on real-world images. Extensive experiments demonstrate the
effectiveness of our method and dataset.

</details>


### [73] [BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes](https://arxiv.org/abs/2510.09996)
*Lishen Qu,Zhihao Liu,Shihao Zhou,Yaqi Luo,Jie Liang,Hui Zeng,Lei Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: 本文介绍了一个名为BurstDeflicker的大规模数据集，用于解决短曝光图像中由交流供电照明引起的闪烁伪影问题。


<details>
  <summary>Details</summary>
Motivation: 闪烁伪影会降低图像质量，并影响目标检测和跟踪等高级任务。缺乏大规模、真实的数据集是闪烁去除研究的主要障碍。

Method: 1. 基于Retinex的合成流水线，可控地操纵关键的闪烁相关属性，从而生成不同的闪烁模式。2. 从不同场景中捕获4,000张真实世界的闪烁图像。3. 提出了一种绿屏方法，将运动融入到图像对中，同时保留真实的闪烁退化。

Result: 综合实验表明，该数据集的有效性及其在推进闪烁去除研究方面的潜力。

Conclusion: 本文构建了一个可扩展的基准数据集BurstDeflicker，利用三种互补的数据采集策略，解决了闪烁去除研究中缺乏大规模真实数据集的问题。

Abstract: Flicker artifacts in short-exposure images are caused by the interplay
between the row-wise exposure mechanism of rolling shutter cameras and the
temporal intensity variations of alternating current (AC)-powered lighting.
These artifacts typically appear as uneven brightness distribution across the
image, forming noticeable dark bands. Beyond compromising image quality, this
structured noise also affects high-level tasks, such as object detection and
tracking, where reliable lighting is crucial. Despite the prevalence of
flicker, the lack of a large-scale, realistic dataset has been a significant
barrier to advancing research in flicker removal. To address this issue, we
present BurstDeflicker, a scalable benchmark constructed using three
complementary data acquisition strategies. First, we develop a Retinex-based
synthesis pipeline that redefines the goal of flicker removal and enables
controllable manipulation of key flicker-related attributes (e.g., intensity,
area, and frequency), thereby facilitating the generation of diverse flicker
patterns. Second, we capture 4,000 real-world flicker images from different
scenes, which help the model better understand the spatial and temporal
characteristics of real flicker artifacts and generalize more effectively to
wild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we
propose a green-screen method to incorporate motion into image pairs while
preserving real flicker degradation. Comprehensive experiments demonstrate the
effectiveness of our dataset and its potential to advance research in flicker
removal.

</details>


### [74] [MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output](https://arxiv.org/abs/2510.10011)
*Yanyuan Chen,Dexuan Xu,Yu Huang,Songkun Zhan,Hanpin Wang,Dongxue Chen,Xueping Wang,Meikang Qiu,Hang Li*

Main category: cs.CV

TL;DR: 提出了一个统一的医学视觉语言模型MIMO，具有视觉参考多模态输入和像素接地多模态输出。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉语言模型存在两个问题：1) 输入端，模型只依赖于文本指令，缺乏对图像中视觉线索的直接理解；2) 输出端，模型只给出文本答案，缺乏与图像中关键区域的联系。

Method: 提出了一个统一的医学视觉语言模型MIMO，具有视觉参考多模态输入和像素接地多模态输出。为了克服医学领域相关数据的稀缺性，提出了一个综合性的医学多模态数据集MIMOSeg，包含895K个样本。

Result: 在几个下游医学多模态任务上进行了实验。大量的实验结果验证了MIMO可以独特地结合视觉参考和像素接地能力，这在以前的模型中是不可用的。

Conclusion: MIMO模型能够结合视觉线索和文本指令来理解复杂的医学图像和语义，并且能够将文本输出中的医学术语与图像中的像素区域对应起来。

Abstract: Currently, medical vision language models are widely used in medical vision
question answering tasks. However, existing models are confronted with two
issues: for input, the model only relies on text instructions and lacks direct
understanding of visual clues in the image; for output, the model only gives
text answers and lacks connection with key areas in the image. To address these
issues, we propose a unified medical vision language model MIMO, with visual
referring Multimodal Input and pixel grounding Multimodal Output. MIMO can not
only combine visual clues and textual instructions to understand complex
medical images and semantics, but can also ground medical terminologies in
textual output within the image. To overcome the scarcity of relevant data in
the medical field, we propose MIMOSeg, a comprehensive medical multimodal
dataset including 895K samples. MIMOSeg is constructed from four different
perspectives, covering basic instruction following and complex question
answering with multimodal input and multimodal output. We conduct experiments
on several downstream medical multimodal tasks. Extensive experimental results
verify that MIMO can uniquely combine visual referring and pixel grounding
capabilities, which are not available in previous models.

</details>


### [75] [Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning](https://arxiv.org/abs/2510.10022)
*Junan Chen,Trung Thanh Nguyen,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 提出了一种轻量级的视觉适配器模块（Q-Adapter），用于增强多模态大型语言模型（MLLMs）在视频字幕任务中的性能，同时实现高效的微调。


<details>
  <summary>Details</summary>
Motivation: 现有的视频字幕模型微调方法计算成本高昂，且参数高效微调（PEFT）在多模态任务中仍有探索空间，尤其是在微调过程中对视觉信息的理解不足。

Method: 在视觉编码器中引入可学习的查询令牌和门控层，以有效提取稀疏的、与字幕相关的特征，无需外部文本监督。

Result: 在MSR-VTT和MSVD视频字幕数据集上，Q-Adapter在BLEU@4、METEOR、ROUGE-L和CIDEr指标上取得了参数高效微调方法中的最优性能，并且与全参数微调方法相比，仅需1.4%的参数即可达到具有竞争力的性能。

Conclusion: Q-Adapter在平衡字幕质量和参数效率方面具有强大的潜力，展示了其在视频语言建模中的可扩展性。

Abstract: Recent advances in video captioning are driven by large-scale pretrained
models, which follow the standard "pre-training followed by fine-tuning"
paradigm, where the full model is fine-tuned for downstream tasks. Although
effective, this approach becomes computationally prohibitive as the model size
increases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a
promising alternative, but primarily focuses on the language components of
Multimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains
underexplored in multimodal tasks and lacks sufficient understanding of visual
information during fine-tuning the model. To bridge this gap, we propose
Query-Adapter (Q-Adapter), a lightweight visual adapter module designed to
enhance MLLMs by enabling efficient fine-tuning for the video captioning task.
Q-Adapter introduces learnable query tokens and a gating layer into Vision
Encoder, enabling effective extraction of sparse, caption-relevant features
without relying on external textual supervision. We evaluate Q-Adapter on two
well-known video captioning datasets, MSR-VTT and MSVD, where it achieves
state-of-the-art performance among the methods that take the PEFT approach
across BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves
competitive performance compared to methods that take the full fine-tuning
approach while requiring only 1.4% of the parameters. We further analyze the
impact of key hyperparameters and design choices on fine-tuning effectiveness,
providing insights into optimization strategies for adapter-based learning.
These results highlight the strong potential of Q-Adapter in balancing caption
quality and parameter efficiency, demonstrating its scalability for
video-language modeling.

</details>


### [76] [P-4DGS: Predictive 4D Gaussian Splatting with 90$\times$ Compression](https://arxiv.org/abs/2510.10030)
*Henan Wang,Hanxin Zhu,Xinliang Gong,Tianyu He,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出了一种新的动态3D高斯溅射表示方法P-4DGS，用于紧凑的4D场景建模，显著降低了存储空间占用。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D高斯溅射算法忽略了动态场景中固有的时间和空间冗余，导致内存消耗过大。

Method: 设计了一个基于3D锚点的时空预测模块，充分利用不同3D高斯基元之间的时空相关性。采用自适应量化策略结合基于上下文的熵编码，进一步减小3D锚点的大小。

Result: 在合成和真实世界数据集上进行了大量实验，结果表明该方法实现了最先进的重建质量和最快的渲染速度，平均存储空间占用约为1MB，在合成和真实场景中分别实现了高达40倍和90倍的压缩。

Conclusion: P-4DGS在动态场景重建方面具有显著优势，实现了高质量、快速渲染和低存储占用。

Abstract: 3D Gaussian Splatting (3DGS) has garnered significant attention due to its
superior scene representation fidelity and real-time rendering performance,
especially for dynamic 3D scene reconstruction (\textit{i.e.}, 4D
reconstruction). However, despite achieving promising results, most existing
algorithms overlook the substantial temporal and spatial redundancies inherent
in dynamic scenes, leading to prohibitive memory consumption. To address this,
we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene
modeling. Inspired by intra- and inter-frame prediction techniques commonly
used in video compression, we first design a 3D anchor point-based
spatial-temporal prediction module to fully exploit the spatial-temporal
correlations across different 3D Gaussian primitives. Subsequently, we employ
an adaptive quantization strategy combined with context-based entropy coding to
further reduce the size of the 3D anchor points, thereby achieving enhanced
compression efficiency. To evaluate the rate-distortion performance of our
proposed P-4DGS in comparison with other dynamic 3DGS representations, we
conduct extensive experiments on both synthetic and real-world datasets.
Experimental results demonstrate that our approach achieves state-of-the-art
reconstruction quality and the fastest rendering speed, with a remarkably low
storage footprint (around \textbf{1MB} on average), achieving up to
\textbf{40$\times$} and \textbf{90$\times$} compression on synthetic and
real-world scenes, respectively.

</details>


### [77] [Complementary and Contrastive Learning for Audio-Visual Segmentation](https://arxiv.org/abs/2510.10051)
*Sitong Gong,Yunzhi Zhuge,Lu Zhang,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: CCFormer: a novel framework adept at processing both local and global information and capturing spatial-temporal context comprehensively for Audio-Visual Segmentation (AVS).


<details>
  <summary>Details</summary>
Motivation: Traditional CNN and Transformer-based methods struggle to extract multimodal coefficients and temporal dynamics adequately.

Method: Complementary and Contrastive Transformer (CCFormer) with Early Integration Module (EIM), Multi-query Transformer Module (MTM), and Bi-modal Contrastive Learning (BCL).

Result: achieves state-of-the-art benchmarks across the S4, MS3 and AVSS datasets.

Conclusion: The proposed method effectively combines local and global information and captures spatial-temporal context comprehensively, advancing the field of AVS.

Abstract: Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps
that correlate with the auditory signals of objects. This field has seen
significant progress with numerous CNN and Transformer-based methods enhancing
the segmentation accuracy and robustness. Traditional CNN approaches manage
audio-visual interactions through basic operations like padding and
multiplications but are restricted by CNNs' limited local receptive field. More
recently, Transformer-based methods treat auditory cues as queries, utilizing
attention mechanisms to enhance audio-visual cooperation within frames.
Nevertheless, they typically struggle to extract multimodal coefficients and
temporal dynamics adequately. To overcome these limitations, we present the
Complementary and Contrastive Transformer (CCFormer), a novel framework adept
at processing both local and global information and capturing spatial-temporal
context comprehensively. Our CCFormer initiates with the Early Integration
Module (EIM) that employs a parallel bilateral architecture, merging
multi-scale visual features with audio data to boost cross-modal
complementarity. To extract the intra-frame spatial features and facilitate the
perception of temporal coherence, we introduce the Multi-query Transformer
Module (MTM), which dynamically endows audio queries with learning capabilities
and models the frame and video-level relations simultaneously. Furthermore, we
propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across
both modalities in the unified feature space. Through the effective combination
of those designs, our method sets new state-of-the-art benchmarks across the
S4, MS3 and AVSS datasets. Our source code and model weights will be made
publicly available at https://github.com/SitongGong/CCFormer

</details>


### [78] [Think Twice to See More: Iterative Visual Reasoning in Medical VLMs](https://arxiv.org/abs/2510.10052)
*Kaitao Chen,Shaohao Rui,Yankai Jiang,Jiamin Wu,Qihao Zheng,Chunfeng Song,Xiaosong Wang,Mu Zhou,Mianxin Liu*

Main category: cs.CV

TL;DR: ViTAR是一个新的VLM框架，它模拟人类专家通过“思考-行动-再思考-回答”的认知链进行迭代推理的过程，从而弥合机器与人类的感知差距。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉-语言模型(VLM)擅长图像-文本理解，但通常依赖于忽略局部视觉线索的单次推理。为了弥合这种机器与人类的感知差距。

Method: ViTAR将医学图像视为交互对象，使模型能够进行多步骤视觉推理。为了支持这种方法，我们整理了一个高质量的指令数据集，其中包含1K个交互示例，这些示例编码了类似专家的诊断行为。此外，还整理了一个16K视觉问答训练数据，用于细粒度的视觉诊断。我们引入了一种两阶段训练策略，首先进行监督微调以指导认知轨迹，然后进行强化学习以优化决策。

Result: ViTAR优于强大的state-of-the-art模型。视觉注意力分析表明，从“思考”到“再思考”的循环中，ViTAR越来越多地将视觉基础锚定到临床关键区域，并在推理过程中保持对视觉标记的高注意力分配，从而为改进的性能提供了机制性见解。

Conclusion: 将专家式迭代思维链嵌入到VLM中，可以提高医学AI的性能和可信度。

Abstract: Medical vision-language models (VLMs) excel at image-text understanding but
typically rely on a single-pass reasoning that neglects localized visual cues.
In clinical practice, however, human experts iteratively scan, focus, and
refine the regions of interest before reaching a final diagnosis. To narrow
this machine-human perception gap, we introduce ViTAR, a novel VLM framework
that emulates the iterative reasoning process of human experts through a
cognitive chain of "think-act-rethink-answer". ViTAR treats medical images as
interactive objects, enabling models to engage multi-step visual reasoning. To
support this approach, we curate a high-quality instruction dataset comprising
1K interactive examples that encode expert-like diagnostic behaviors. In
addition, a 16K visual question answering training data has been curated
towards fine-grained visual diagnosis. We introduce a two-stage training
strategy that begins with supervised fine-tuning to guide cognitive
trajectories, followed by the reinforcement learning to optimize
decision-making. Extensive evaluations demonstrate that ViTAR outperforms
strong state-of-the-art models. Visual attention analysis reveals that from the
"think" to "rethink" rounds, ViTAR increasingly anchors visual grounding to
clinically critical regions and maintains high attention allocation to visual
tokens during reasoning, providing mechanistic insight into its improved
performance. These findings demonstrate that embedding expert-style iterative
thinking chains into VLMs enhances both performance and trustworthiness of
medical AI.

</details>


### [79] [DREAM: A Benchmark Study for Deepfake REalism AssessMent](https://arxiv.org/abs/2510.10053)
*Bo Peng,Zichuan Wang,Sheng Yu,Xiaochuan Jin,Wei Wang,Jing Dong*

Main category: cs.CV

TL;DR: 本文着眼于深度伪造视频的视觉逼真度评估，旨在模仿人类对深度伪造的感知。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在客观的深度伪造检测，而对深度伪造的主观感知及其计算建模研究不足。评估深度伪造的质量和欺骗性，预测其对互联网的影响，并改进深度伪造生成过程具有重要意义。

Method: 构建了一个名为DREAM的综合基准，包含多样质量的深度伪造视频数据集，大规模的人工标注（14万个逼真度评分和文本描述，来自3500名标注者），以及对16种代表性逼真度评估方法的全面评估和分析，包括视觉语言模型和一种新的描述对齐CLIP方法。

Result: 构建了DREAM基准，包含大规模人工标注和多种评估方法。

Conclusion: 本研究提出的基准和见解可以为未来在该方向和其他相关领域的研究奠定基础。

Abstract: Deep learning based face-swap videos, widely known as deepfakes, have drawn
wide attention due to their threat to information credibility. Recent works
mainly focus on the problem of deepfake detection that aims to reliably tell
deepfakes apart from real ones, in an objective way. On the other hand, the
subjective perception of deepfakes, especially its computational modeling and
imitation, is also a significant problem but lacks adequate study. In this
paper, we focus on the visual realism assessment of deepfakes, which is defined
as the automatic assessment of deepfake visual realism that approximates human
perception of deepfakes. It is important for evaluating the quality and
deceptiveness of deepfakes which can be used for predicting the influence of
deepfakes on Internet, and it also has potentials in improving the deepfake
generation process by serving as a critic. This paper prompts this new
direction by presenting a comprehensive benchmark called DREAM, which stands
for Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of
diverse quality, a large scale annotation that includes 140,000 realism scores
and textual descriptions obtained from 3,500 human annotators, and a
comprehensive evaluation and analysis of 16 representative realism assessment
methods, including recent large vision language model based methods and a newly
proposed description-aligned CLIP method. The benchmark and insights included
in this study can lay the foundation for future research in this direction and
other related areas.

</details>


### [80] [Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels](https://arxiv.org/abs/2510.10055)
*Zhi-Fen He,Ren-Dong Xie,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 本文提出了一种用于不完整标签多标签图像识别的协同学习语义感知特征学习和标签恢复 (CLSL) 方法。


<details>
  <summary>Details</summary>
Motivation: 解决语义感知特征学习和缺失标签恢复这两大挑战。

Method: 设计了语义相关特征学习模块和语义引导的特征增强模块，并引入了协同学习框架。

Result: 在 MS-COCO、VOC2007 和 NUS-WIDE 三个数据集上的实验表明，CLSL 优于现有技术。

Conclusion: CLSL 方法能够有效地进行多标签图像识别，并在不完整标签的情况下表现出色。

Abstract: Multi-label image recognition with incomplete labels is a critical learning
task and has emerged as a focal topic in computer vision. However, this task is
confronted with two core challenges: semantic-aware feature learning and
missing label recovery. In this paper, we propose a novel Collaborative
Learning of Semantic-aware feature learning and Label recovery (CLSL) method
for multi-label image recognition with incomplete labels, which unifies the two
aforementioned challenges into a unified learning framework. More specifically,
we design a semantic-related feature learning module to learn robust
semantic-related features by discovering semantic information and label
correlations. Then, a semantic-guided feature enhancement module is proposed to
generate high-quality discriminative semantic-aware features by effectively
aligning visual and semantic feature spaces. Finally, we introduce a
collaborative learning framework that integrates semantic-aware feature
learning and label recovery, which can not only dynamically enhance the
discriminability of semantic-aware features but also adaptively infer and
recover missing labels, forming a mutually reinforced loop between the two
processes. Extensive experiments on three widely used public datasets (MS-COCO,
VOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art
multi-label image recognition methods with incomplete labels.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLM）如何通过其表示空间进行“思考”，并提出了一个新颖的几何框架，将LLM的推理建模为流——嵌入轨迹随着逻辑的发展而演变。该框架通过采用具有不同语义载体的相同自然演绎命题，将逻辑结构与语义分离，从而能够测试LLM是否在表面形式之外内化了逻辑。这种视角将推理与位置、速度和曲率等几何量联系起来，从而能够在表示和概念空间中进行形式分析。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(LLM)的推理过程，理解其内在机理

Method: 提出一种新的几何框架，将LLM的推理建模为流，并采用具有不同语义载体的相同自然演绎命题，将逻辑结构与语义分离。

Result: LLM推理对应于表示空间中的平滑流，并且逻辑语句充当这些流的速度的局部控制器。通过使用学习的表示代理，设计受控实验来可视化和量化推理流，从而为理论框架提供实证验证。

Conclusion: 这项工作既是研究推理现象的概念基础，又是实用工具，为LLM行为的可解释性和形式分析提供了新的视角。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [82] [How can we assess human-agent interactions? Case studies in software agent design](https://arxiv.org/abs/2510.09801)
*Valerie Chen,Rohit Malhotra,Xingyao Wang,Juan Michelini,Xuhui Zhou,Aditya Bharat Soni,Hoang H. Tran,Calvin Smith,Ameet Talwalkar,Graham Neubig*

Main category: cs.AI

TL;DR: 这篇论文提出了PULSE框架，用于更有效地评估人机交互的LLM Agent设计。


<details>
  <summary>Details</summary>
Motivation: 现有的Agent评估基准主要假设完全自动化，忽略了真实世界用例中的协作性。

Method: 该框架包括收集用户反馈、训练机器学习模型预测用户满意度，并将用户满意度评分与模型生成的伪标签相结合。

Result: 通过大规模Web平台收集了超过15k用户的真实使用数据，案例研究表明LLM骨干、规划策略和记忆机制的选择会影响开发者满意度。

Conclusion: 研究结果表明，与标准A/B测试相比，该框架可以更稳健地评估Agent设计，并将置信区间降低40%。此外，真实数据与基准测试结果存在显著差异，强调了基准驱动评估的局限性。

Abstract: LLM-powered agents are both a promising new technology and a source of
complexity, where choices about models, tools, and prompting can affect their
usefulness. While numerous benchmarks measure agent accuracy across domains,
they mostly assume full automation, failing to represent the collaborative
nature of real-world use cases. In this paper, we make two major steps towards
the rigorous assessment of human-agent interactions. First, we propose PULSE, a
framework for more efficient human-centric evaluation of agent designs, which
comprises collecting user feedback, training an ML model to predict user
satisfaction, and computing results by combining human satisfaction ratings
with model-generated pseudo-labels. Second, we deploy the framework on a
large-scale web platform built around the open-source software agent OpenHands,
collecting in-the-wild usage data across over 15k users. We conduct case
studies around how three agent design decisions -- choice of LLM backbone,
planning strategy, and memory mechanisms -- impact developer satisfaction
rates, yielding practical insights for software agent design. We also show how
our framework can lead to more robust conclusions about agent design, reducing
confidence intervals by 40\% compared to a standard A/B test. Finally, we find
substantial discrepancies between in-the-wild results and benchmark performance
(e.g., the anti-correlation between results comparing claude-sonnet-4 and
gpt-5), underscoring the limitations of benchmark-driven evaluation. Our
findings provide guidance for evaluations of LLM agents with humans and
identify opportunities for better agent designs.

</details>


### [83] [AI and Consciousness](https://arxiv.org/abs/2510.09858)
*Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 对人工智能意识文献的怀疑性概述。根据某些有影响力的主流意识理论，我们很快将创造出有意识的人工智能系统，但根据其他有影响力的主流意识理论，这些系统没有意识。我们将无法知道哪些理论是正确的，也无法知道我们是被像人类一样丰富和有意义的有意识的人工智能系统所包围，还是被像烤面包机一样在体验上空白的系统所包围。对于或反对人工智能意识的标准论点都没有使我们走得很远。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能意识的可能性，并指出不同意识理论对人工智能意识的判断存在差异。

Method: 回顾和分析了关于人工智能意识的文献，并探讨了各种支持和反对人工智能意识的论点。

Result: 强调了当前对人工智能意识的理解的局限性，以及我们无法确定人工智能系统是否真正具有意识。

Conclusion: 标准论点在判断人工智能意识方面作用有限，我们可能无法知道人工智能系统是否具有意识。

Abstract: This is a skeptical overview of the literature on AI consciousness. We will
soon create AI systems that are conscious according to some influential,
mainstream theories of consciousness but are not conscious according to other
influential, mainstream theories of consciousness. We will not be in a position
to know which theories are correct and whether we are surrounded by AI systems
as richly and meaningfully conscious as human beings or instead only by systems
as experientially blank as toasters. None of the standard arguments either for
or against AI consciousness takes us far.
  Table of Contents
  Chapter One: Hills and Fog
  Chapter Two: What Is Consciousness? What Is AI?
  Chapter Three: Ten Possibly Essential Features of Consciousness
  Chapter Four: Against Introspective and Conceptual Arguments for Essential
Features
  Chapter Five: Materialism and Functionalism
  Chapter Six: The Turing Test and the Chinese Room
  Chapter Seven: The Mimicry Argument Against AI Consciousness
  Chapter Eight: Global Workspace Theories and Higher Order Theories
  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,
and Iterative Natural Kinds
  Chapter Ten: Does Biological Substrate Matter?
  Chapter Eleven: The Problem of Strange Intelligence
  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution

</details>


### [84] [Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning](https://arxiv.org/abs/2510.09894)
*Junyuan Liu,Quan Qin,Guangsheng Dong,Xinglei Wang,Jiazhuang Feng,Zichao Zeng,Tao Cheng*

Main category: cs.AI

TL;DR: 提出了AETHER，一个通过兴趣点(POI)引导的多模态对齐框架，用于改进AlphaEarth (AE)的城市分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的地球观测(EO)驱动的地理空间基础模型在捕捉城市的功能和社会经济维度方面存在局限性，因为它们主要编码物理和光谱模式，而不是人类活动或空间功能。

Method: AETHER通过将AE嵌入与POI的文本表示对齐，利用轻量级框架将AlphaEarth调整到以人为中心的城市分析。

Result: 在伦敦，AETHER在土地利用分类F1指标上实现了7.2%的相对提升，在社会经济映射的Kullback-Leibler散度上实现了23.6%的相对降低。

Conclusion: AETHER通过耦合EO与以人为中心的语义，推动地理空间基础模型朝着整合物理形式和功能意义的通用城市表示发展。

Abstract: General-purpose spatial representations are essential for building
transferable geospatial foundation models (GFMs). Among them, the AlphaEarth
Foundation (AE) represents a major step toward a global, unified representation
of the Earth's surface, learning 10-meter embeddings from multi-source Earth
Observation (EO) data that capture rich physical and environmental patterns
across diverse landscapes. However, such EO-driven representations remain
limited in capturing the functional and socioeconomic dimensions of cities, as
they primarily encode physical and spectral patterns rather than human
activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched
Representation Learning), a lightweight framework that adapts AlphaEarth to
human-centered urban analysis through multimodal alignment guided by Points of
Interest (POIs). AETHER aligns AE embeddings with textual representations of
POIs, enriching physically grounded EO features with semantic cues about urban
functions and socioeconomic contexts. In Greater London, AETHER achieves
consistent gains over the AE baseline, with a 7.2% relative improvement in
land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler
divergence for socioeconomic mapping. Built upon pretrained AE, AETHER
leverages a lightweight multimodal alignment to enrich it with human-centered
semantics while remaining computationally efficient and scalable for urban
applications. By coupling EO with human-centered semantics, it advances
geospatial foundation models toward general-purpose urban representations that
integrate both physical form and functional meaning.

</details>


### [85] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）正在改变科学发现的方式，引入了可以加速不同自主程度发现的自主系统，即智能代理。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）在科学发现中的作用，以及它们如何通过与科学家、自然语言、计算机语言和物理学的交互来改变科学发现的生命周期。

Method: 分析当前基于LLM的科学智能代理的方法，重点关注关键创新、实际成就和存在的局限性。

Result: 强调了自主智能代理在加速不同领域科学发现方面的变革潜力。

Conclusion: 总结了构建更强大、更通用和更具适应性的科学智能代理所面临的开放研究挑战，并概述了有希望的发展方向。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [86] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 用户记忆会影响大型语言模型的情感智能，导致对不同用户产生偏差。


<details>
  <summary>Details</summary>
Motivation: 研究用户记忆如何影响大型语言模型（LLM）中的情感智能。

Method: 在经过人工验证的情感智能测试中评估了 15 个模型，将相同的场景与不同的用户资料配对。

Result: 不同的用户资料会导致系统性不同的情感解读。在多个高性能的LLM中，优势群体的情感解读更为准确，并且LLM在情感理解和支持性建议任务中表现出显著的差距。

Conclusion: 为个性化设计的系统可能会无意中强化社会不平等。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [87] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: LLMs are bad at reasoning and tend to hallucinate.


<details>
  <summary>Details</summary>
Motivation: LLMs' System 1 processing is not sufficient for reliable reasoning, which requires System 2.

Method: A low-cost, instruction-based intervention is introduced, which decomposes fallacy classification into a series of atomic procedural steps. A relational knowledge graph is consulted for verification.

Result: The procedural, rule-based intervention yields a significant improvement in LLM logical fallacy classification.

Conclusion: The approach provides enhanced transparency into the LLMs' decision-making, highlighting a practical pathway for Neuro-symbolic architectures to address LLM reasoning deficits.

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [88] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 研究大型语言模型（LLM）在多轮对话中进行道德推理和价值对齐的表现。


<details>
  <summary>Details</summary>
Motivation: 单轮提示评估可能无法完全捕捉LLM在实际应用中的价值表现，尤其是在涉及对话、修改和共识的多轮环境中。

Method: 使用LLM辩论，要求GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash这三种模型对来自Reddit“Am I the Asshole”社区的1000个日常困境进行集体责任分配。采用同步（并行响应）和循环（顺序响应）两种形式，以测试顺序效应和判决修改。

Result: 在同步设置中，GPT表现出很强的惯性，而Claude和Gemini则灵活得多。价值模式也不同：GPT强调个人自主和直接沟通，而Claude和Gemini优先考虑共情对话。某些价值观在推动判决改变方面特别有效。GPT和Gemini相对于Claude来说，表现出高度的一致性，它们的判决行为受到顺序效应的强烈影响。

Conclusion: 多轮互动中的道德推理受到对话结构和模型特定行为的影响，社会技术对齐取决于系统构建对话的方式，以及它们的输出。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [89] [RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning](https://arxiv.org/abs/2510.10008)
*Meng Xi,Sihan Lv,Yechen Jin,Guanjie Cheng,Naibo Wang,Ying Li,Jianwei Yin*

Main category: cs.AI

TL;DR: 本文研究了针对复杂RAG系统的黑盒投毒攻击，提出了一种名为RIPRAG的攻击框架，该框架利用强化学习优化恶意文档的生成，实验表明该方法能有效攻击复杂的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中于针对简化RAG架构的白盒攻击。本文旨在研究更复杂和真实的场景：攻击者缺乏对RAG系统内部组成和实现细节的了解，并且RAG系统包含的组件超出了单纯的检索器。

Method: 提出RIPRAG攻击框架，这是一个端到端的攻击流程，将目标RAG系统视为黑盒，攻击者唯一可访问的信息是投毒是否成功。该方法利用强化学习来优化恶意文档生成模型，确保生成的恶意文档符合目标RAG系统的偏好。

Result: 实验结果表明，该方法可以有效地对大多数复杂的RAG系统执行投毒攻击，与基线方法相比，攻击成功率（ASR）提高了0.72。

Conclusion: 强调了当前防御方法中普遍存在的缺陷，并为LLM安全研究提供了重要的见解。

Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.

</details>


### [90] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的优化LLM工作流程的范式，通过最小化预期失败质量来直接优化失败分布，而不是最大化标量分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多步骤执行跟踪简化为简单的成功/失败信号，导致信息崩溃，无法对工作流程的失败分布进行建模。

Method: 提出 CE-Graph 框架，通过 failure-driven 的优化过程，从 counterexamples 池中近似失败分布，识别其最密集区域作为 recurring failure modes，并通过 Propose-and-Verify 机制应用有针对性的图编辑来减少失败质量。

Result: 在数学、代码和 QA 基准测试中，CE-Graph 以显着更低的成本实现了比强基线更高的鲁棒性。

Conclusion: 系统的可靠性不是来自于避免失败，而是来自于系统地学习和重塑其失败分布的几何结构。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [91] [Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation](https://arxiv.org/abs/2510.10042)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 提出了一种新的图论框架，将可信度（外部的先验信任）与置信度（内部的涌现评估）区分开来，用于矛盾容忍推理。


<details>
  <summary>Details</summary>
Motivation: 信念系统很少全局一致，但有效的推理通常在局部持续存在。

Method: 信念表示为有向、有符号、加权图中的节点，边表示支持和矛盾。通过收缩传播过程获得置信度，该过程混合了先验知识和结构感知的影响，并保证了独特的稳定解。定义了推理区域：高置信度、结构平衡的子图，即使存在全局矛盾，经典推理也是安全的。提供了一个近似线性的过程，通过置信度播种区域，使用基于奇偶校验的着色测试平衡，并应用贪婪的、局部性保留的修复与Jaccard去重来构建紧凑的图集。为了模拟信念变化，引入了冲击更新，该更新在局部缩小支持并提升目标矛盾，同时通过简单的回溯规则保持收缩性。重新传播会产生局部重新配置——区域可能会缩小、分裂或崩溃——而不会破坏整个图。

Result: 在具有植入区域的合成有符号图上概述了一个经验协议，报告了区域恢复、冲击下的稳定性和运行时。

Conclusion: 为矛盾容忍推理奠定了原则性基础，该基础精确地在结构支持它的地方激活经典逻辑。

Abstract: Belief systems are rarely globally consistent, yet effective reasoning often
persists locally. We propose a novel graph-theoretic framework that cleanly
separates credibility--external, a priori trust in sources--from confidence--an
internal, emergent valuation induced by network structure. Beliefs are nodes in
a directed, signed, weighted graph whose edges encode support and
contradiction. Confidence is obtained by a contractive propagation process that
mixes a stated prior with structure-aware influence and guarantees a unique,
stable solution. Within this dynamics, we define reasoning zones:
high-confidence, structurally balanced subgraphs on which classical inference
is safe despite global contradictions. We provide a near-linear procedure that
seeds zones by confidence, tests balance using a parity-based coloring, and
applies a greedy, locality-preserving repair with Jaccard de-duplication to
build a compact atlas. To model belief change, we introduce shock updates that
locally downscale support and elevate targeted contradictions while preserving
contractivity via a simple backtracking rule. Re-propagation yields localized
reconfiguration-zones may shrink, split, or collapse--without destabilizing the
entire graph. We outline an empirical protocol on synthetic signed graphs with
planted zones, reporting zone recovery, stability under shocks, and runtime.
The result is a principled foundation for contradiction-tolerant reasoning that
activates classical logic precisely where structure supports it.

</details>


### [92] [Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval](https://arxiv.org/abs/2510.10942)
*Nilima Rao,Jagriti Srivastava,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AI

TL;DR: 本文提出了一个用于自适应企业信息访问的模块化混合检索框架，该框架集成了知识库语言增强模型（KBLam）、DeepGraph表示和嵌入驱动的语义搜索。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法基于关键词搜索或静态嵌入，通常无法回答需要跨artifacts进行上下文推理和多跳推理的复杂查询。

Method: 该框架构建了一个统一的知识图谱，该知识图谱来自解析的存储库，包括代码、pull request和提交历史，从而实现语义相似性搜索、结构推理和多跳推理。查询分析动态地确定最佳检索策略，通过独立或融合处理来支持结构化和非结构化数据源。

Result: 在大型Git存储库上的实验表明，与独立的基于GPT的检索管道相比，统一的推理层将答案相关性提高了高达80%。

Conclusion: 通过结合图构建、混合推理和交互式可视化，所提出的框架为企业环境中的智能知识助手提供了可扩展、可解释和以用户为中心的基础。

Abstract: Modern enterprises manage vast knowledge distributed across heterogeneous
systems such as Jira, Git repositories, Confluence, and wikis. Conventional
retrieval methods based on keyword search or static embeddings often fail to
answer complex queries that require contextual reasoning and multi-hop
inference across artifacts. We present a modular hybrid retrieval framework for
adaptive enterprise information access that integrates Knowledge Base
Language-Augmented Models (KBLam), DeepGraph representations, and
embedding-driven semantic search. The framework builds a unified knowledge
graph from parsed repositories including code, pull requests, and commit
histories, enabling semantic similarity search, structural inference, and
multi-hop reasoning. Query analysis dynamically determines the optimal
retrieval strategy, supporting both structured and unstructured data sources
through independent or fused processing. An interactive interface provides
graph visualizations, subgraph exploration, and context-aware query routing to
generate concise and explainable answers. Experiments on large-scale Git
repositories show that the unified reasoning layer improves answer relevance by
up to 80 percent compared with standalone GPT-based retrieval pipelines. By
combining graph construction, hybrid reasoning, and interactive visualization,
the proposed framework offers a scalable, explainable, and user-centric
foundation for intelligent knowledge assistants in enterprise environments.

</details>


### [93] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: SwarmSys is a closed-loop framework for distributed multi-agent reasoning inspired by swarm intelligence.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning.

Method: Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and a pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision.

Result: SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability across symbolic reasoning, research synthesis, and scientific programming tasks.

Conclusion: Swarm-inspired coordination is a promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence.

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [94] [SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation](https://arxiv.org/abs/2510.10069)
*Zeyu Ling,Xiaodong Gu,Jiangnan Tang,Changqing Zou*

Main category: cs.AI

TL;DR: SyncLipMAE：一个用于说话人脸视频的自监督预训练框架，学习同步感知和可迁移的面部动态。


<details>
  <summary>Details</summary>
Motivation: 从无标签的视听流中学习同步感知和可迁移的面部动态。

Method: 该方法将掩蔽视觉建模与跨模态对比对齐相结合，并采用三个逐帧提示令牌，这些令牌显式地编码了说话人脸帧的基本因素——身份、声音运动（语音同步的面部动态）和环境运动（与音频无关的运动，如眨眼和头部姿势）。

Result: SyncLipMAE 在需要不同能力的四个任务系列中实现了最先进的结果。

Conclusion: 强调了同步感知、分解的自监督预训练的有效性。

Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.

</details>


### [95] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个用于自动化故障排除指南（TSG）的端到端agent框架，通过三阶段工作流程，解决TSG质量问题、复杂控制流、数据密集型查询和并行执行等挑战，在真实世界的TSG和事件集合上实现了约94%的成功率，并显著减少了执行时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的解决方案缺乏对TSG质量管理、复杂控制流解释、数据密集型查询处理和并行执行利用等关键挑战的专门支持，导致大规模IT系统中事件管理的效率低下。

Method: StepFly采用三阶段工作流程：TSG Mentor辅助SRE提高TSG质量；离线预处理提取结构化执行DAG并创建QPP；在线使用DAG引导的调度器-执行器框架和内存系统，保证工作流正确并支持并行执行。

Result: StepFly在GPT-4.1上实现了约94%的成功率，优于基线方法，并减少了时间和token消耗。对于可并行化的TSG，执行时间减少了32.9%到70.4%。

Conclusion: StepFly能够有效解决现有LLM在自动化TSG执行中面临的挑战，显著提升了事件管理的效率和成功率。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [96] [DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay](https://arxiv.org/abs/2510.10117)
*Yunxiang Mo,Tianshi Zheng,Qing Zong,Jiayu Liu,Baixuan Xu,Yauwai Yim,Chunkit Chan,Jiaxin Bai,Yangqiu Song*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个新的综合评估套件DixitWorld，用于评估视觉语言模型中的多模态溯因推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前对视觉语言模型中这种能力的评估主要局限于静态、单智能体任务，为了弥补这一不足。

Method: DixitWorld包含两个核心组件：DixitArena（一个动态、多智能体环境，评估假设生成和选择）和DixitBench（一个静态QA基准，用于有效、受控地评估听者的任务）。

Result: DixitArena的结果表明，较小的开源模型通常擅长作为创造性的讲故事者，而较大的专有模型表现出卓越的整体性能，尤其是在听者方面。DixitBench的性能与DixitArena中的听者结果密切相关。

Conclusion: 研究结果揭示了多模态溯因推理中生成创造力和判别理解之间的关键权衡，这是开发更平衡和更有能力的视觉语言代理的关键挑战。

Abstract: Multimodal abductive reasoning--the generation and selection of explanatory
hypotheses from partial observations--is a cornerstone of intelligence. Current
evaluations of this ability in vision-language models (VLMs) are largely
confined to static, single-agent tasks. Inspired by Dixit, we introduce
DixitWorld, a comprehensive evaluation suite designed to deconstruct this
challenge. DIXITWORLD features two core components: DixitArena, a dynamic,
multi-agent environment that evaluates both hypothesis generation (a
"storyteller" crafting cryptic clues) and hypothesis selection ("listeners"
choosing the target image from decoys) under imperfect information; and
DixitBench, a static QA benchmark that isolates the listener's task for
efficient, controlled evaluation. Results from DixitArena reveal distinct,
role-dependent behaviors: smaller open-source models often excel as creative
storytellers, producing imaginative yet less discriminative clues, whereas
larger proprietary models demonstrate superior overall performance,
particularly as listeners. Performance on DixitBench strongly correlates with
listener results in DixitArena, validating it as a reliable proxy for
hypothesis selection. Our findings reveal a key trade-off between generative
creativity and discriminative understanding in multimodal abductive reasoning,
a central challenge for developing more balanced and capable vision-language
agents.

</details>


### [97] [CharCom: Composable Identity Control for Multi-Character Story Illustration](https://arxiv.org/abs/2510.10135)
*Zhongsheng Wang,Ming Lin,Zhedong Lin,Yaser Shakib,Qian Liu,Jiamou Liu*

Main category: cs.AI

TL;DR: 提出了一种名为CharCom的框架，用于在扩散模型中实现角色一致的图像生成。


<details>
  <summary>Details</summary>
Motivation: 确保在不同的提示下角色身份的一致性是基于扩散的文本到图像生成中的一个基本限制。

Method: CharCom通过可组合的LoRA适配器实现角色一致的故事图例，从而无需重新训练基本模型即可实现高效的单角色定制。CharCom建立在冻结的扩散骨干网上，在推理时使用提示感知控制动态地组合适配器。

Result: 在多场景叙事上的实验表明，CharCom显著提高了角色保真度、语义对齐和时间连贯性。它在拥挤的场景中保持稳健，并能够以最小的开销实现可扩展的多角色生成。

Conclusion: CharCom框架适用于故事插图和动画等实际应用。

Abstract: Ensuring character identity consistency across varying prompts remains a
fundamental limitation in diffusion-based text-to-image generation. We propose
CharCom, a modular and parameter-efficient framework that achieves
character-consistent story illustration through composable LoRA adapters,
enabling efficient per-character customization without retraining the base
model. Built on a frozen diffusion backbone, CharCom dynamically composes
adapters at inference using prompt-aware control. Experiments on multi-scene
narratives demonstrate that CharCom significantly enhances character fidelity,
semantic alignment, and temporal coherence. It remains robust in crowded scenes
and enables scalable multi-character generation with minimal overhead, making
it well-suited for real-world applications such as story illustration and
animation.

</details>


### [98] [Concise Reasoning in the Lens of Lagrangian Optimization](https://arxiv.org/abs/2510.10168)
*Chengqian Gao,Haonan Li,Taylor W. Killian,Jianshu She,Renxi Wang,Liqun Ma,Zhoujun Cheng,Shibo Hao,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出了一种名为性能感知长度更新（PALU）的简洁推理策略，旨在减少大型语言模型中的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于手工设计的启发式方法，难以在简洁性和性能之间取得平衡，且难以适应不同的领域和模型规模。

Method: 将简洁推理建模为一个约束优化问题，最小化响应长度，同时满足性能约束，并应用拉格朗日优化将其转换为易于处理的无约束问题。通过三个近似简化更新规则：使用非策略rollout估计性能，将拉格朗日乘数截断为两个极端，并用分位数驱动的长度调整代替基于梯度的更新。

Result: 在DeepSeek-Distill-Qwen-1.5B上应用PALU，在五个基准测试中，输出长度减少了65%，同时准确率提高了15%，优于其他方法。PALU还证明了可以适应不同的领域（逻辑、STEM和数学）和模型规模（1.5B、7B、14B）。

Conclusion: PALU是一种实用且有效的简洁推理方法。

Abstract: Concise reasoning in large language models seeks to generate only essential
intermediate steps needed to arrive at a final answer, thereby alleviating
issues of overthinking. Most proposed approaches hinge on carefully
hand-crafted heuristics, struggling to balance concision with performance,
often failing to adapt across domains and model scales. In this work, we
address these challenges by introducing a principled and pragmatic strategy,
performance-aware length updating (PALU). As a principled algorithm, PALU
formulates concise reasoning as a constrained optimization problem, minimizing
response length subject to a performance constraint, and then applies
Lagrangian optimization to convert it into a tractable unconstrained problem.
As a pragmatic solution, PALU streamlines complicated update rules through
three approximations: (i) estimating performance with off-policy rollouts, (ii)
truncating the Lagrange multiplier to two extremes, and (iii) replacing
gradient-based updates with quantile-driven length adjustments. PALU reduces
output length by 65% while improving accuracy by 15% when applied to
DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a
range of alternative methods. Furthermore, PALU is demonstrated to adapt across
both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching
the algorithm as a practical and effective concise reasoning approach.

</details>


### [99] [SAFER: Risk-Constrained Sample-then-Filter in Large Language Models](https://arxiv.org/abs/2510.10193)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出了一种新的两阶段风险控制框架SAFER，用于提高大型语言模型在开放式问答等风险敏感应用中的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的选择性共形预测(SCP)方法在构建预测集时，对正确答案的错误覆盖率进行了约束，从而提供了统计保证。然而，先前的工作不切实际地假设所有实例的可接受答案都可以通过有限采样获得，即使对于缺乏固定和有限解空间的开放式QA场景也是如此。

Method: SAFER包括两个阶段：abstention-aware sampling和conformalized filtering。首先，在保留的校准集上，SAFER在最大采样上限内校准采样预算，使用Clopper-Pearson精确方法在用户期望的风险水平下进行校准。然后，我们使用校准实例，在校准预算下可以获得正确的答案，并应用共形风险控制方法来确定一个统计上有效的不确定性阈值，该阈值从每个测试数据点的候选集中过滤掉不可靠的干扰因素。

Result: SAFER与各种特定于任务的准入标准和校准测试拆分比率兼容，突出了其鲁棒性和高数据效率。

Conclusion: 该论文提出了一种新的两阶段风险控制框架SAFER，用于提高大型语言模型在开放式问答等风险敏感应用中的可信度。该框架通过 abstention-aware sampling 和 conformalized filtering 两个阶段，实现了在约束的错误覆盖率下构建预测集，并且具有鲁棒性和高数据效率。

Abstract: As large language models (LLMs) are increasingly deployed in risk-sensitive
applications such as real-world open-ended question answering (QA), ensuring
the trustworthiness of their outputs has become critical. Existing selective
conformal prediction (SCP) methods provide statistical guarantees by
constructing prediction sets with a constrained miscoverage rate for correct
answers. However, prior works unrealistically assume that admissible answers
for all instances can be obtained via finite sampling, even for open-ended QA
scenarios that lack a fixed and finite solution space. To address this, we
introduce a two-stage risk control framework comprising abstention-aware
sampling and conformalized filtering (SAFER). Firstly, on a held-out
calibration set, SAFER calibrates a sampling budget within the maximum sampling
cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,
the maximum allowable miscoverage rate of the sampling sets). If the risk level
cannot be satisfied within the cap, we abstain; otherwise, the calibrated
sampling budget becomes the minimum requirements at test time. Then, we employ
calibration instances where correct answers are attainable under the calibrated
budget and apply the conformal risk control method to determine a statistically
valid uncertainty threshold, which filters unreliable distractors from the
candidate set for each test data point. In this stage, SAFER introduces an
additional risk level to guide the calculation of the threshold, thereby
controlling the risk of correct answers being excluded. Furthermore, we show
that SAFER is compatible with various task-specific admission criteria and
calibration-test split ratios, highlighting its robustness and high data
efficiency.

</details>


### [100] [Don't Just Fine-tune the Agent, Tune the Environment](https://arxiv.org/abs/2510.10197)
*Siyuan Lu,Zechuan Wang,Hongxuan Zhang,Qintong Wu,Leilei Gan,Chenyi Zhuang,Jinjie Gu,Tao Lin*

Main category: cs.AI

TL;DR: 提出了Environment Tuning方法，一种新的训练范式，使agent能够直接从问题实例中学习复杂的行为，而无需依赖预先收集的专家轨迹。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据的极度稀缺阻碍了LLM agent的开发；在合成数据上进行监督微调（SFT）导致过拟合，而标准强化学习（RL）则面临着严重的冷启动问题和训练不稳定。

Method: 通过结构化课程、可操作的环境增强（提供纠正反馈）和细粒度的进度奖励来协调学习过程，确保稳定和高效的探索。

Result: 仅使用BFCL的400个问题实例，该方法不仅实现了与强大基线相比具有竞争力的同分布性能，而且表现出卓越的异分布泛化能力，克服了基于SFT的方法常见的性能崩溃。

Conclusion: 从静态轨迹上的监督微调到基于动态环境的探索的范式转变，为训练更鲁棒和数据高效的agent铺平了道路。

Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn
tool-use tasks, but their development is often hampered by the extreme scarcity
of high-quality training data. Supervised fine-tuning (SFT) on synthetic data
leads to overfitting, whereas standard reinforcement learning (RL) struggles
with a critical cold-start problem and training instability. To address these
challenges, we introduce $\textbf{Environment Tuning}$, a novel training
paradigm that enables agents to learn complex behaviors directly from problem
instances without relying on pre-collected expert trajectories.
$\textbf{Environment Tuning}$ orchestrates this learning process through a
structured curriculum, actionable environment augmentation that provides
corrective feedback, and fine-grained progress rewards to ensure stable and
efficient exploration. Using only 400 problem instances from Berkeley
Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves
competitive in-distribution performance against strong baselines but also
demonstrates superior out-of-distribution generalization, overcoming the
performance collapse common to SFT-based approaches. Our work presents a
paradigm shift from supervised fine-tuning on static trajectories to dynamic,
environment-based exploration, paving the way for training more robust and
data-efficient agents.

</details>


### [101] [PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration](https://arxiv.org/abs/2510.10205)
*Manjiang Yu,Hongji Li,Priyanka Singh,Xue Li,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 提出了一种名为 PIXEL 的激活 steering 框架，用于提升大型语言模型 (LLM) 的可靠行为控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于粗略的启发式方法，缺乏对在哪里进行 steering 以及如何有力干预的原则性解释。

Method: PIXEL 从双重视角（尾部平均和结束 token）学习属性对齐的子空间，并通过具有闭式解的约束几何目标选择干预强度，从而适应 token 级别的敏感性，而无需全局超参数调整。PIXEL 进一步执行样本级别正交残差校准以细化全局属性方向，并采用轻量级位置扫描程序来识别接受性注入站点。

Result: PIXEL 在各种模型和评估范例中，始终如一地提高属性对齐，同时保持模型的一般能力。

Conclusion: PIXEL 提供了一种实用且有原则的方法，用于 LLM 的可控生成。

Abstract: Reliable behavior control is central to deploying large language models
(LLMs) on the web. Activation steering offers a tuning-free route to align
attributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing
approaches rely on coarse heuristics and lack a principled account of where to
steer and how strongly to intervene. To this end, we propose Position-wise
Injection with eXact Estimated Levels (PIXEL), a position-wise activation
steering framework that, in contrast to prior work, learns a property-aligned
subspace from dual views (tail-averaged and end-token) and selects intervention
strength via a constrained geometric objective with a closed-form solution,
thereby adapting to token-level sensitivity without global hyperparameter
tuning. PIXEL further performs sample-level orthogonal residual calibration to
refine the global attribute direction and employs a lightweight
position-scanning routine to identify receptive injection sites. We
additionally provide representation-level guarantees for the
minimal-intervention rule, supporting reliable alignment. Across diverse models
and evaluation paradigms, PIXEL consistently improves attribute alignment while
preserving model general capabilities, offering a practical and principled
method for LLMs' controllable generation. Our code is available at
https://github.com/V1centNevwake/PIXEL-Adaptive-Steering

</details>


### [102] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出了一种名为自适应双重推理器（ADR）的模型，该模型支持快速思考和慢速思考两种推理模式，并根据上下文复杂性动态切换。


<details>
  <summary>Details</summary>
Motivation: 长推理模型（LRM）在各种推理场景中表现出色，但计算成本和推理延迟较高。

Method: ADR的训练分为两个阶段：(1) 使用监督微调（SFT）进行冷启动，使模型具备整合快速和慢速推理模式的能力；(2) 强化学习阶段，优化推理工作量，引入熵引导混合策略优化EHPO。

Result: 在具有挑战性的数学推理基准测试中，ADR在推理性能和效率之间实现了有效平衡，性能提升高达6.1%，同时推理输出长度减少了49.5%至59.3%。

Conclusion: ADR在推理性能和效率之间取得了平衡。

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [103] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: LLMs, similar to the human brain, may contain a small subset of critical neurons. Disrupting these neurons can severely impact performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs contain a small subset of critical neurons, similar to the human brain.

Method: Proposes a Perturbation-based Causal Identification of Critical Neurons method.

Result: LLMs contain ultra-sparse critical neuron sets, concentrated in outer layers (MLP down_proj components), and their disruption leads to sharp performance degradation.

Conclusion: Findings offer guidance for developing more robust model architectures and improving deployment security in safety-critical applications.

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [104] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 多模态大型推理模型(MLRM)在视觉语言推理方面迅速发展，但幻觉仍然是一个问题。


<details>
  <summary>Details</summary>
Motivation: 观察到注意力头表现出阶段性划分，浅层头主要用于感知，而深层头转向符号推理，揭示了幻觉的两个主要原因：感知偏差和推理漂移。

Method: 提出了一个轻量级和可解释的两步插件，功能头识别和类条件重新缩放，它可以定位面向感知和推理的头，并在不重新训练的情况下调节它们的贡献。

Result: 在三个真实世界的MLRM（Kimi-VL、Ocean-R1、R1-Onevision），跨三个领域的六个基准和四个基线上进行的评估表明，我们的插件实现了平均5%和高达15%的改进，只有<1%的额外计算和9%的基线延迟。

Conclusion: 该方法是完全模型无关的，并显著提高了现成的MLRM的可靠性和可解释性，从而使其能够在高风险应用中安全部署。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [105] [LLM-Friendly Knowledge Representation for Customer Support](https://arxiv.org/abs/2510.10331)
*Hanchen Su,Wei Luo,Wei Han,Yu Elaine Liu,Yufeng Wayne Zhang,Cen Mia Zhao,Ying Joy Zhang,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了一种将大型语言模型（LLM）与框架相结合的实用方法，以应对 Airbnb 客户支持操作的复杂性。


<details>
  <summary>Details</summary>
Motivation: 旨在提升 Airbnb 客户支持运营效率和效果。

Method: 采用了一种新颖的重格式化技术，即意图、上下文和行动（ICA）格式，并将策略和工作流程转换为 LLM 更容易理解的结构。此外，还开发了一种合成数据生成策略，以最少的人工干预创建训练数据，从而能够经济高效地微调模型。

Result: 内部实验表明，重构工作流程和使用合成数据微调 LLM 的方法显著提高了其性能，为 LLM 在客户支持中的应用树立了新的基准。

Conclusion: 该解决方案不仅经济高效，而且通过准确性和人工处理时间评估指标证明，它可以改善客户支持。

Abstract: We propose a practical approach by integrating Large Language Models (LLMs)
with a framework designed to navigate the complexities of Airbnb customer
support operations. In this paper, our methodology employs a novel reformatting
technique, the Intent, Context, and Action (ICA) format, which transforms
policies and workflows into a structure more comprehensible to LLMs.
Additionally, we develop a synthetic data generation strategy to create
training data with minimal human intervention, enabling cost-effective
fine-tuning of our model. Our internal experiments (not applied to Airbnb
products) demonstrate that our approach of restructuring workflows and
fine-tuning LLMs with synthetic data significantly enhances their performance,
setting a new benchmark for their application in customer support. Our solution
is not only cost-effective but also improves customer support, as evidenced by
both accuracy and manual processing time evaluation metrics.

</details>


### [106] [Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI](https://arxiv.org/abs/2510.10338)
*Balagopal Unnikrishnan,Ariel Guerra Adames,Amin Adibi,Sameer Peesapati,Rafal Kocielnik,Shira Fischer,Hillary Clinton Kasimbazi,Rodrigo Gameiro,Alina Peluso,Chrystinne Oliveira Fernandes,Maximin Lange,Lovedeep Gondara,Leo Anthony Celi*

Main category: cs.AI

TL;DR: 包容性设计在医疗AI领域具有经济和战略价值，能带来超额回报。


<details>
  <summary>Details</summary>
Motivation: 探讨包容性设计在医疗AI领域的经济和战略价值，指出其重要性未被充分认识。

Method: 通过分析辅助技术发展为主流产业的案例，论证包容性医疗AI开发能创造商业价值。

Result: 提出了医疗AI包容性创新框架（HAIIF），用于评估AI投资的包容性潜力，并确定了包容性创新驱动回报的四个机制：市场扩张、风险降低、性能提升和竞争优势。

Conclusion: 投资于包容性设计的组织能够获得更广阔的市场和持续的竞争优势，而忽视包容性的组织将面临累积劣势。

Abstract: While ethical arguments for fairness in healthcare AI are well-established,
the economic and strategic value of inclusive design remains underexplored.
This perspective introduces the ``inclusive innovation dividend'' -- the
counterintuitive principle that solutions engineered for diverse, constrained
use cases generate superior economic returns in broader markets. Drawing from
assistive technologies that evolved into billion-dollar mainstream industries,
we demonstrate how inclusive healthcare AI development creates business value
beyond compliance requirements. We identify four mechanisms through which
inclusive innovation drives returns: (1) market expansion via geographic
scalability and trust acceleration; (2) risk mitigation through reduced
remediation costs and litigation exposure; (3) performance dividends from
superior generalization and reduced technical debt, and (4) competitive
advantages in talent acquisition and clinical adoption. We present the
Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring
system that enables organizations to evaluate AI investments based on their
potential to capture these benefits. HAIIF provides structured guidance for
resource allocation, transforming fairness and inclusivity from regulatory
checkboxes into sources of strategic differentiation. Our findings suggest that
organizations investing incrementally in inclusive design can achieve expanded
market reach and sustained competitive advantages, while those treating these
considerations as overhead face compounding disadvantages as network effects
and data advantages accrue to early movers.

</details>


### [107] [Trace Length is a Simple Uncertainty Signal in Reasoning Models](https://arxiv.org/abs/2510.10409)
*Siddartha Devic,Charlotte Peale,Arwen Bradley,Sinead Williamson,Preetum Nakkiran,Aravind Gollakota*

Main category: cs.AI

TL;DR: 推理轨迹长度可以作为大型推理模型中的置信度评估器。


<details>
  <summary>Details</summary>
Motivation: 量化大型语言模型的不确定性是解决幻觉和其他限制可靠部署问题的关键研究方向。

Method: 通过多个模型、数据集和提示进行综合实验，比较推理轨迹长度与其他零样本置信度评估器的性能。

Result: 推理轨迹长度的性能与诸如语言置信度等其他零样本置信度评估器相当但具有互补性。推理后训练从根本上改变了轨迹长度和准确性之间的关系。即使在调整了问题难度和 GRPO 引起的长度偏差等混杂因素后，这种效应仍然存在。高熵或“分叉” tokens 在该机制中起着关键作用。

Conclusion: 推理后训练增强了不确定性量化，超越了口头表达，并将轨迹长度确立为大型推理模型的实用置信度度量。

Abstract: Uncertainty quantification for LLMs is a key research direction towards
addressing hallucination and other issues that limit their reliable deployment.
In this work, we show that reasoning trace length is a simple and useful
confidence estimator in large reasoning models. Through comprehensive
experiments across multiple models, datasets, and prompts, we show that trace
length performs in comparable but complementary ways to other zero-shot
confidence estimators such as verbalized confidence. Our work reveals that
reasoning post-training fundamentally alters the relationship between trace
length and accuracy, going beyond prior work that had shown that post-training
causes traces to grow longer in general (e.g., "overthinking"). We investigate
the mechanisms behind trace length's performance as a confidence signal,
observing that the effect remains even after adjusting for confounders such as
problem difficulty and GRPO-induced length bias. We identify high-entropy or
"forking" tokens as playing a key role in the mechanism. Our findings
demonstrate that reasoning post-training enhances uncertainty quantification
beyond verbal expressions, and establish trace length as a practical confidence
measure for large reasoning models.

</details>


### [108] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: Traj-CoA is a multi-agent system that models patient trajectories using LLMs. It processes EHR data sequentially, distills critical events, and uses a manager agent to make predictions.


<details>
  <summary>Details</summary>
Motivation: LLMs can model patient trajectories but struggle with long and noisy EHR data.

Method: Traj-CoA uses a chain of worker agents to process EHR data in chunks, distilling critical events into a shared memory module (EHRMem). A manager agent then synthesizes the summaries and timeline to make predictions.

Result: Traj-CoA outperforms baselines in a zero-shot one-year lung cancer risk prediction task.

Conclusion: Traj-CoA is a robust and generalizable approach for modeling complex patient trajectories, exhibiting clinically aligned temporal reasoning.

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [109] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: 提出了一个信心感知多智能体框架MedCoAct，通过整合医生和药剂师智能体来模拟临床协作。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗AI系统在孤立的环境中处理任务，缺乏临床团队中的交叉验证和知识整合，降低了它们在实际医疗场景中的有效性。

Method: 提出了MedCoAct，一个信心感知多智能体框架，通过整合专业的医生和药剂师智能体来模拟临床协作，并提出了一个基准测试DrugCareQA，以评估医疗AI在综合诊断和治疗工作流程中的能力。

Result: MedCoAct实现了67.58%的诊断准确率和67.58%的药物推荐准确率，分别优于单智能体框架7.04%和7.08%。

Conclusion: 这种协作方法可以很好地推广到不同的医疗领域，尤其是在远程医疗咨询和常规临床场景中，同时提供可解释的决策路径。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [110] [Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning](https://arxiv.org/abs/2510.10494)
*Martina G. Vilas,Safoora Yousefi,Besmira Nushi,Eric Horvitz,Vidhisha Balachandran*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的方法，通过分析模型在推理过程中内部表征的演变来预测推理轨迹的成功率，从而提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 推理模型可以通过分配更多计算资源来提高解决问题的能力，但识别哪些推理轨迹可能成功仍然是一个关键问题。

Method: 该方法引入了潜在轨迹信号，通过测量模型在生成中间推理 tokens 期间内部表征的随时间演变来表征推理过程。

Result: 实验表明，潜在轨迹信号比跨层指标和基于输出的置信度测量方法更可靠地预测解决方案的准确性。当用于指导跨多个抽样世代的答案选择时，与多数投票相比，潜在轨迹信号可以更有效地利用测试时间缩放，减少高达 70% 的 token 使用量，同时平均保持甚至提高 2.6% 的准确率。

Conclusion: 该研究结果不仅为推理时间效率提供了实用的策略，而且还为推理过程如何在潜在空间中表示和区分提供了更深入的可解释性视角。

Abstract: Reasoning models improve their problem-solving ability through inference-time
scaling, allocating more compute via longer token budgets. Identifying which
reasoning traces are likely to succeed remains a key opportunity: reliably
predicting productive paths can substantially reduce wasted computation and
improve overall efficiency. We introduce Latent-Trajectory signals that
characterize the temporal evolution of a model's internal representations
during the generation of intermediate reasoning tokens. By measuring the
overall change in latent representations between the start and end of
reasoning, the change accumulated across intermediate steps, and the extent to
which these changes advance toward the final state, we show that these signals
predict solution accuracy more reliably than both cross-layer metrics and
output-based confidence measures. When used to guide answer selection across
multiple sampled generations, Latent-Trajectory signals make test-time scaling
more effective and efficient than majority voting, reducing token usage by up
to 70% while preserving and even improving accuracy by 2.6% on average.
Moreover, these predictive signals often emerge early in the reasoning trace,
enabling early selection and allocation of compute to the most promising
candidates. Our findings contribute not only practical strategies for
inference-time efficiency, but also a deeper interpretability perspective on
how reasoning processes are represented and differentiated in latent space.

</details>


### [111] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: 论文提出了 ELAIPBench，一个用于评估大型语言模型对人工智能研究论文的理解能力的基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分评估大型语言模型对完整学术论文的深度理解和推理能力。

Method: 领域专家策划了一个包含 403 个选择题的基准，这些问题来自 137 篇论文，涵盖三个难度级别，并强调非浅层检索的推理。

Result: 性能最佳的 LLM 的准确率仅为 39.95%，远低于人类水平。配备思维模式或检索增强生成 (RAG) 系统的 LLM 未能提高最终结果，甚至因过度思考或检索噪声而损害了准确性。

Conclusion: 当前 LLM 的能力与对学术论文的真正理解之间存在显著差距。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [112] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 这篇论文提出了一个统一的直觉-方法分层模型，通过范围扩展来系统地解决间接（未见）问题，并提出了熵方法扩展来评估该过程。


<details>
  <summary>Details</summary>
Motivation: 现有研究已经引入了基于方法推理和范围扩展的方法来增强大型语言模型（LLM）的性能，超越了直接矩阵映射。本文在此基础上，旨在总结和整合这些思想。

Method: 该模型包括基于直觉的思考、基于方法的思考和范围扩展，范围扩展包括垂直（原因分析）、水平（平行和概括问题）以及时间和空间扩展。这些扩展被组织成知识树，相互连接成知识网络。

Result: 论文提出了方法扩展的熵，用于衡量扩展的独立性和多样性，以此作为系统解决未见问题的能力指标。

Conclusion: 这项工作通过将现有方法与新的扩展连接起来，并引入基于熵的评估框架，从而朝着为LLM在实际问题解决中提供更强大和可扩展的推理范例迈进了一步。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [113] [A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective](https://arxiv.org/abs/2510.10596)
*Ruolan Cheng,Yong Deng,Serafín Moral,José Ramón Trillo*

Main category: cs.AI

TL;DR: 本文提出了一种基于累积Jaccard指数矩阵的随机置换集（RPS）距离测量新方法，该方法克服了现有方法的缺点，具有更高的灵敏度和灵活性。


<details>
  <summary>Details</summary>
Motivation: 研究RPS理论中置换质量函数之间距离的测量问题。

Method: 从随机有限集（RFS）和可转移信念模型（TBM）两个角度深入分析RPS之间的距离，引入累积Jaccard指数的新定义来量化两个置换之间的相似性，并提出基于累积Jaccard指数矩阵的RPS距离测量方法。

Result: 所提出的方法具有自然的顶权性，较高排名元素之间的不一致往往导致更大的距离值。 提供了两个参数供决策者调整权重和截断深度。 实验结果表明，该方法克服了现有方法的缺点，并且与Jousselme距离兼容，同时具有更高的灵敏度和灵活性。

Conclusion: 本文提出的RPS距离测量方法具有良好的性能和应用前景。

Abstract: Random permutation set (RPS) is a recently proposed framework designed to
represent order-structured uncertain information. Measuring the distance
between permutation mass functions is a key research topic in RPS theory
(RPST). This paper conducts an in-depth analysis of distances between RPSs from
two different perspectives: random finite set (RFS) and transferable belief
model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we
regard RPST as a refinement of TBM, where the order in the ordered focus set
represents qualitative propensity. Starting from the permutation, we introduce
a new definition of the cumulative Jaccard index to quantify the similarity
between two permutations and further propose a distance measure method for RPSs
based on the cumulative Jaccard index matrix. The metric and structural
properties of the proposed distance measure are investigated, including the
positive definiteness analysis of the cumulative Jaccard index matrix, and a
correction scheme is provided. The proposed method has a natural
top-weightiness property: inconsistencies between higher-ranked elements tend
to result in greater distance values. Two parameters are provided to the
decision-maker to adjust the weight and truncation depth. Several numerical
examples are used to compare the proposed method with the existing method. The
experimental results show that the proposed method not only overcomes the
shortcomings of the existing method and is compatible with the Jousselme
distance, but also has higher sensitivity and flexibility.

</details>


### [114] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为DRIFT的框架，用于改进大型语言模型(llm)在定理证明中自动形式化数学语句的能力。该框架将非正式的数学语句分解为更小的子组件，以便从数学库中检索相关的先验知识，并检索说明性的定理来帮助模型更有效地使用这些先验知识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在识别和利用数学知识及其在Lean等语言中的形式化表示方面存在困难。现有的检索增强自动形式化方法直接使用非正式语句查询外部库，忽略了非正式数学语句通常很复杂且提供的上下文有限这一基本限制。

Method: 该论文介绍了一种新颖的框架DRIFT，它可以将非正式的数学语句分解为更小的、更容易处理的“子组件”。这有助于从Mathlib等数学库中进行有针对性的前提检索。此外，DRIFT还会检索说明性定理，以帮助模型更有效地在形式化任务中使用前提。

Result: DRIFT在不同的基准测试(ProofNet、ConNF和MiniF2F-test)中进行评估，结果表明，与ProofNet上的DPR基线相比，它始终能提高前提检索能力，并使F1得分几乎翻了一番。值得注意的是，DRIFT在out-of-distribution ConNF基准测试中表现出强大的性能，使用GPT-4.1和DeepSeek-V3.1时，BEq+@10的改进分别为37.14%和42.25%。

Conclusion: 数学自动形式化中的检索有效性在很大程度上取决于特定于模型的知识边界，这突出了需要与每个模型的功能相一致的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [115] [EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms](https://arxiv.org/abs/2510.10603)
*WenTao Liu,Siyu Song,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: 提出了一种使用进化算法优化大型语言模型的方法(EA4LLM)，并首次成功地证明了其从预训练阶段训练一个拥有10亿参数的LLM的能力。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化方法对硬件要求高，并且需要所有神经网络操作都是可微的，从而排除了许多有前途的不可微架构的实际应用。为了解决这些限制。

Method: 使用进化算法 (EA4LLM) 优化LLM。

Result: 成功地训练了一个拥有10亿参数的LLM，并提供了进化算法如何有效优化神经网络的关键见解。

Conclusion: 挑战了基于梯度的优化是训练神经网络唯一可行方法的普遍假设。它还有可能降低训练大型语言模型的计算成本，从而使计算资源有限的团队能够参与深度学习研究。

Abstract: In recent years, large language models (LLMs) have made remarkable progress,
with model optimization primarily relying on gradient-based optimizers such as
Adam. However, these gradient-based methods impose stringent hardware
requirements, demanding high-concurrency, high-memory GPUs. Moreover, they
require all neural network operations to be differentiable, thereby excluding
many promising non-differentiable architectures from practical use. To address
these limitations, we propose a method for optimizing LLMs using evolutionary
algorithms (EA4LLM) and, for the first time, successfully demonstrate its
capability to train a 1-billion-parameter LLM from the pre-trained stage. We
conduct extensive experiments and provide key insights into how evolutionary
algorithms can effectively optimize neural networks. Our work challenges the
prevailing assumption that gradient-based optimization is the only viable
approach for training neural networks. It also holds significant potential to
reduce the computational cost of training large language models, thereby
enabling groups with limited computational resources to participate in deep
learning research.

</details>


### [116] [FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems](https://arxiv.org/abs/2510.11003)
*Takuma Fujiu,Sho Okazaki,Kohei Kaminishi,Yuji Nakata,Shota Hamamoto,Kenshin Yokose,Tatsunori Hara,Yasushi Umeda,Jun Ota*

Main category: cs.AI

TL;DR: 本文构建了诊断知识本体，并提出了一种基于功能-行为-结构（FBS）模型的维护记录积累方法。


<details>
  <summary>Details</summary>
Motivation: 在制造系统中，识别故障原因是维持和提高生产效率的关键。在基于知识的故障原因推断中，知识库需要明确地构建关于目标系统和故障的知识，并包含足够长的故障因果链。

Method: 构建诊断知识本体，并提出一种基于功能-行为-结构（FBS）模型的维护记录积累方法。

Result: 使用该方法积累的维护记录进行故障原因推断，与专家列举的候选原因集合有更好的一致性，尤其是在相关案例数量少且词汇使用不同的困难情况下。

Conclusion: 该方法利用设计阶段对目标的理解和知识，以支持维护阶段的知识积累和问题解决，并有望成为未来整个工程链知识共享的基础。

Abstract: In manufacturing systems, identifying the causes of failures is crucial for
maintaining and improving production efficiency. In knowledge-based
failure-cause inference, it is important that the knowledge base (1) explicitly
structures knowledge about the target system and about failures, and (2)
contains sufficiently long causal chains of failures. In this study, we
constructed Diagnostic Knowledge Ontology and proposed a
Function-Behavior-Structure (FBS) model-based maintenance-record accumulation
method based on it. Failure-cause inference using the maintenance records
accumulated by the proposed method showed better agreement with the set of
candidate causes enumerated by experts, especially in difficult cases where the
number of related cases is small and the vocabulary used differs. In the
future, it will be necessary to develop inference methods tailored to these
maintenance records, build a user interface, and carry out validation on larger
and more diverse systems. Additionally, this approach leverages the
understanding and knowledge of the target in the design phase to support
knowledge accumulation and problem solving during the maintenance phase, and it
is expected to become a foundation for knowledge sharing across the entire
engineering chain in the future.

</details>


### [117] [Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion](https://arxiv.org/abs/2510.10633)
*Jiabao Shi,Minfeng Qi,Lefeng Zhang,Di Wang,Yingjie Zhao,Ziying Li,Yalong Xing,Ningran Li*

Main category: cs.AI

TL;DR: 提出了一种多智能体强化学习框架，用于多模态文本到图像的生成，以保持语义对齐和专业水平的细节。


<details>
  <summary>Details</summary>
Motivation: 多模态文本到图像生成难以保持语义对齐和专业水平的细节。

Method: 使用多智能体强化学习框架，协调领域专家智能体，通过近端策略优化（PPO）进行训练，并结合对比学习、双向注意力和迭代反馈来增强跨模态对齐。

Result: 显著丰富了生成内容（单词数增加1614%），同时降低了ROUGE-1分数69.7%。Transformer策略取得了最高的综合得分（0.521）。

Conclusion: 协作的、专业化驱动的架构有希望推进可靠的多模态生成系统。

Abstract: Multimodal text-to-image generation remains constrained by the difficulty of
maintaining semantic alignment and professional-level detail across diverse
visual domains. We propose a multi-agent reinforcement learning framework that
coordinates domain-specialized agents (e.g., focused on architecture,
portraiture, and landscape imagery) within two coupled subsystems: a text
enhancement module and an image generation module, each augmented with
multimodal integration components. Agents are trained using Proximal Policy
Optimization (PPO) under a composite reward function that balances semantic
similarity, linguistic visual quality, and content diversity. Cross-modal
alignment is enforced through contrastive learning, bidirectional attention,
and iterative feedback between text and image. Across six experimental
settings, our system significantly enriches generated content (word count
increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion
methods, Transformer-based strategies achieve the highest composite score
(0.521), despite occasional stability issues. Multimodal ensembles yield
moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent
challenges of cross-modal semantic grounding. These findings underscore the
promise of collaborative, specialization-driven architectures for advancing
reliable multimodal generative systems.

</details>


### [118] [Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction](https://arxiv.org/abs/2510.10639)
*Haemin Choi,Gayathri Nadarajan*

Main category: cs.AI

TL;DR: 本研究探索了使用可解释机器学习预测学生学习满意度的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少使用可解释机器学习和神经网络分析学生学习满意度。

Method: 使用结合boosting和可解释性的APLR模型。

Result: 时间管理、专注力、帮助同学和参与线下课程对学习满意度有显著积极影响；参与创造性活动没有积极影响。

Conclusion: APLR模型可以为教育者提供个性化教学方案。

Abstract: Although student learning satisfaction has been widely studied, modern
techniques such as interpretable machine learning and neural networks have not
been sufficiently explored. This study demonstrates that a recent model that
combines boosting with interpretability, automatic piecewise linear
regression(APLR), offers the best fit for predicting learning satisfaction
among several state-of-the-art approaches. Through the analysis of APLR's
numerical and visual interpretations, students' time management and
concentration abilities, perceived helpfulness to classmates, and participation
in offline courses have the most significant positive impact on learning
satisfaction. Surprisingly, involvement in creative activities did not
positively affect learning satisfaction. Moreover, the contributing factors can
be interpreted on an individual level, allowing educators to customize
instructions according to student profiles.

</details>


### [119] [Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany](https://arxiv.org/abs/2510.10640)
*Piyush Pant,Marcellius William Suntoro,Ayesha Siddiqua,Muhammad Shehryaar Sharif,Daniyal Ahmed*

Main category: cs.AI

TL;DR: EA-GeoAI框架用于德国到2030年的需求预测和公平医院规划。


<details>
  <summary>Details</summary>
Motivation: 结合区域人口变化、老龄化人口密度和基础设施平衡，形成统一的公平指数。

Method: 使用可解释的Agentic AI优化器来分配床位和识别新的设施地点，以最大限度地减少预算和旅行时间限制下未满足的需求。

Result: 为决策者提供可操作的建议。

Conclusion: 该方法桥接了GeoAI、长期预测和公平性测量。

Abstract: This paper presents EA-GeoAI, an integrated framework for demand forecasting
and equitable hospital planning in Germany through 2030. We combine
district-level demographic shifts, aging population density, and infrastructure
balances into a unified Equity Index. An interpretable Agentic AI optimizer
then allocates beds and identifies new facility sites to minimize unmet need
under budget and travel-time constraints. This approach bridges GeoAI,
long-term forecasting, and equity measurement to deliver actionable
recommendations for policymakers.

</details>


### [120] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: 提出了一种新的混合框架，该框架集成了大型语言模型（LLM）和动态分层系统中的数学优化，以解决在线叫车平台中供需平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常分为两类：强化学习（RL）方法和分解的在线优化方法，但都存在不足。强化学习方法存在数据效率低、对现实世界动态的建模过于简单以及难以执行操作约束等问题；分解的在线优化方法依赖于手动设计的高级目标，缺乏对低级路由动态的感知。

Method: 该框架利用LLM来弥补问题分解造成的认知限制，自适应地生成高级目标。LLM作为元优化器，产生语义启发式，指导负责约束执行和实时决策执行的低级优化器。通过闭环进化过程（由和谐搜索驱动）改进这些启发式方法，该过程基于优化层的可行性和性能反馈迭代地调整LLM提示。

Result: 基于纽约和芝加哥出租车数据集的场景进行的大量实验表明，该方法是有效的，与最先进的基线相比，平均提高了16%。

Conclusion: 该研究提出了一种有前景的混合框架，通过将LLM与数学优化相结合，可以有效地解决在线叫车平台中的供需平衡问题。

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [121] [Real-Time Health Analytics Using Ontology-Driven Complex Event Processing and LLM Reasoning: A Tuberculosis Case Study](https://arxiv.org/abs/2510.09646)
*Ritesh Chandra,Sonali Agarwal,Navjot Singh*

Main category: cs.DB

TL;DR: 本研究提出了一种基于本体的实时分析框架，该框架集成了复杂事件处理（CEP）和大型语言模型（LLM），以实现智能健康事件检测和对异构、高速健康数据流的语义推理。


<details>
  <summary>Details</summary>
Motivation: 在大数据环境中，及时检测关键健康状况仍然是一个主要的挑战，尤其是在临床数据量大、速度快、种类多的情况下。

Method: 该架构利用基本形式本体（BFO）和语义网规则语言（SWRL）来建模诊断规则和领域知识。患者数据使用Apache Kafka和Spark Streaming进行摄取和处理，其中CEP引擎检测临床上重要的事件模式。LLM支持自适应推理、事件解释和本体改进。临床信息在Graph DB中被语义结构化为资源描述框架（RDF）三元组，从而实现基于SPARQL的查询和知识驱动的决策支持。

Result: 该框架使用包含1,000名结核病（TB）患者的数据集作为用例进行评估，证明了低延迟事件检测、可扩展的推理和高模型性能（在精确率、召回率和F1分数方面）。

Conclusion: 这些结果验证了该系统在复杂大数据场景中进行通用、实时健康分析的潜力。

Abstract: Timely detection of critical health conditions remains a major challenge in
public health analytics, especially in Big Data environments characterized by
high volume, rapid velocity, and diverse variety of clinical data. This study
presents an ontology-enabled real-time analytics framework that integrates
Complex Event Processing (CEP) and Large Language Models (LLMs) to enable
intelligent health event detection and semantic reasoning over heterogeneous,
high-velocity health data streams. The architecture leverages the Basic Formal
Ontology (BFO) and Semantic Web Rule Language (SWRL) to model diagnostic rules
and domain knowledge. Patient data is ingested and processed using Apache Kafka
and Spark Streaming, where CEP engines detect clinically significant event
patterns. LLMs support adaptive reasoning, event interpretation, and ontology
refinement. Clinical information is semantically structured as Resource
Description Framework (RDF) triples in Graph DB, enabling SPARQL-based querying
and knowledge-driven decision support. The framework is evaluated using a
dataset of 1,000 Tuberculosis (TB) patients as a use case, demonstrating
low-latency event detection, scalable reasoning, and high model performance (in
terms of precision, recall, and F1-score). These results validate the system's
potential for generalizable, real-time health analytics in complex Big Data
scenarios.

</details>


### [122] [Targeted Sequential Pattern Mining with High Average Utility](https://arxiv.org/abs/2510.10115)
*Kai Cao,Yucong Duan,Wensheng Gan*

Main category: cs.DB

TL;DR: 本文提出了一种新的算法TAUSQ-PG，用于在大型长序列数据集中挖掘目标高平均效用序列模式。


<details>
  <summary>Details</summary>
Motivation: 传统的基于频率的方法存在局限性，而基于效用的方法又会生成大量冗长复杂的序列。为了提高模式的相关性和可解释性，并增强可用性和交互性，本文将平均效用引入到目标序列模式挖掘中。

Method: 设计了一种名为TAUSQ-PG的新算法，该算法结合了有效的过滤和剪枝策略、更严格的上限模型以及专门的评估指标和查询标志。

Result: 在不同数据集上的大量对比实验表明，TAUSQ-PG有效地控制了候选集大小，从而减少了冗余序列的生成，并显着提高了运行时和内存效率。

Conclusion: 本文研究解决了在大型长序列数据集中挖掘目标高平均效用序列模式的挑战，提出的TAUSQ-PG算法在效率方面表现出色。

Abstract: Incorporating utility into targeted pattern mining can address the practical
limitations of traditional frequency-based approaches. However, utility-based
methods often suffer from generating a large number of long and complicated
sequences. To improve pattern relevance and interpretability, average utility
provides a more balanced metric by considering both utility and sequence
length. Moreover, incorporating user-defined query targets into the mining
process enhances usability and interactivity by retaining only patterns
containing user-specified goals. To address challenges related to mining
efficiency in large-scale, long-sequence datasets, this study introduces
average utility into targeted sequential pattern mining. A novel algorithm,
TAUSQ-PG, is designed to find targeted high average utility sequential
patterns. It incorporates efficient filtering and pruning strategies, tighter
upper bound models, as well as novel specialized evaluation metrics and query
flags tailored to this task. Extensive comparative experiments on different
datasets demonstrate that TAUSQ-PG effectively controls the candidate set size,
thereby reducing redundant sequence generation and significantly improving
runtime and memory efficiency.

</details>


### [123] [The Hybrid Multimodal Graph Index (HMGI): A Comprehensive Framework for Integrated Relational and Vector Search](https://arxiv.org/abs/2510.10123)
*Joydeep Chandra,Satyam Kumar Navneet,Yong Zhang*

Main category: cs.DB

TL;DR: 提出了一种新的混合多模图索引（HMGI），旨在弥合向量数据库和传统图数据库之间的差距，以实现对多模数据的有效混合查询。


<details>
  <summary>Details</summary>
Motivation: 专门的向量数据库和传统图数据库之间存在一个关键差距。向量数据库擅长语义相似性搜索，但缺乏深度关系查询的能力。图数据库擅长复杂遍历，但未针对高维向量搜索进行原生优化。

Method: HMGI利用原生图数据库架构和集成的向量搜索能力（例如 Neo4j）将近似最近邻搜索 (ANNS) 与表达图遍历查询相结合。HMGI 框架的关键创新包括：模态感知嵌入分区，以优化索引结构和查询性能；以及自适应、低开销索引更新系统，以支持动态数据提取，从 TigerVector 等系统的架构原则中汲取灵感。

Result: 通过将语义相似性搜索与关系上下文直接集成，HMGI 旨在在复杂、关系密集的查询场景中优于 Milvus 等纯向量数据库，并实现混合任务的亚线性查询时间。

Conclusion: HMGI弥合向量数据库和传统图数据库之间的差距，实现对多模数据的有效混合查询。

Abstract: The proliferation of complex, multimodal datasets has exposed a critical gap
between the capabilities of specialized vector databases and traditional graph
databases. While vector databases excel at semantic similarity search, they
lack the capacity for deep relational querying. Conversely, graph databases
master complex traversals but are not natively optimized for high-dimensional
vector search. This paper introduces the Hybrid Multimodal Graph Index (HMGI),
a novel framework designed to bridge this gap by creating a unified system for
efficient, hybrid queries on multimodal data. HMGI leverages the native graph
database architecture and integrated vector search capabilities, exemplified by
platforms like Neo4j, to combine Approximate Nearest Neighbor Search (ANNS)
with expressive graph traversal queries. Key innovations of the HMGI framework
include modality-aware partitioning of embeddings to optimize index structure
and query performance, and a system for adaptive, low-overhead index updates to
support dynamic data ingestion, drawing inspiration from the architectural
principles of systems like TigerVector. By integrating semantic similarity
search directly with relational context, HMGI aims to outperform pure vector
databases like Milvus in complex, relationship-heavy query scenarios and
achieve sub-linear query times for hybrid tasks.

</details>


### [124] [Efficient Mining of Low-Utility Sequential Patterns](https://arxiv.org/abs/2510.10243)
*Jian Zhu,Zhidong Lin,Wensheng Gan,Ruichu Cai,Zhifeng Hao,Philip S. Yu*

Main category: cs.DB

TL;DR: 本研究针对低效用序列模式挖掘(LUSPM)问题，提出了三种新算法LUSPM_b, LUSPM_s和LUSPM_e，并通过实验验证了LUSPM_s和LUSPM_e的有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于效用的SPM研究主要集中在高效用序列模式，而适用于高效用SPM的定义和策略不能直接应用于LUSPM。目前还没有专门为挖掘低效用序列模式而开发的算法。

Method: 1. 形式化LUSPM问题，重新定义序列效用。 2. 引入一种称为序列效用链的紧凑数据结构，以有效记录效用信息。 3. 提出了三种新算法LUSPM_b, LUSPM_s和LUSPM_e来发现完整的低效用序列模式集。

Result: LUSPM_s和LUSPM_e的性能明显优于LUSPM_b，并且表现出良好的可扩展性。LUSPM_e具有更高的效率，比LUSPM_s需要更少的运行时间和内存消耗。

Conclusion: 本研究提出的LUSPM算法能够有效地解决低效用序列模式挖掘问题，并在效率和可扩展性方面表现出色。

Abstract: Discovering valuable insights from rich data is a crucial task for
exploratory data analysis. Sequential pattern mining (SPM) has found widespread
applications across various domains. In recent years, low-utility sequential
pattern mining (LUSPM) has shown strong potential in applications such as
intrusion detection and genomic sequence analysis. However, existing research
in utility-based SPM focuses on high-utility sequential patterns, and the
definitions and strategies used in high-utility SPM cannot be directly applied
to LUSPM. Moreover, no algorithms have yet been developed specifically for
mining low-utility sequential patterns. To address these problems, we formalize
the LUSPM problem, redefine sequence utility, and introduce a compact data
structure called the sequence-utility chain to efficiently record utility
information. Furthermore, we propose three novel algorithm--LUSPM_b, LUSPM_s,
and LUSPM_e--to discover the complete set of low-utility sequential patterns.
LUSPM_b serves as an exhaustive baseline, while LUSPM_s and LUSPM_e build upon
it, generating subsequences through shrinkage and extension operations,
respectively. In addition, we introduce the maximal non-mutually contained
sequence set and incorporate multiple pruning strategies, which significantly
reduce redundant operations in both LUSPM_s and LUSPM_e. Finally, extensive
experimental results demonstrate that both LUSPM_s and LUSPM_e substantially
outperform LUSPM_b and exhibit excellent scalability. Notably, LUSPM_e achieves
superior efficiency, requiring less runtime and memory consumption than
LUSPM_s. Our code is available at https://github.com/Zhidong-Lin/LUSPM.

</details>


### [125] [Regular Expression Indexing for Log Analysis. Extended Version](https://arxiv.org/abs/2510.10348)
*Ling Zhang,Shaleen Deep,Jignesh M. Patel,Karthikeyan Sankaralingam*

Main category: cs.DB

TL;DR: REI是一个用于索引日志数据以进行正则表达式查询的新系统。


<details>
  <summary>Details</summary>
Motivation: 现有的文本处理库中的倒排索引等策略在日志分析任务的正则表达式索引方面可能导致次优性能。

Method: 基于n-gram的索引策略和高效的存储机制。

Result: 与不使用索引的最先进的正则表达式处理引擎相比，速度提高了14倍，仅使用了2.1%的额外空间。

Conclusion: REI方法在评估日志数据的正则表达式查询时提供了显着提升，并且是模块化的，可以与现有的正则表达式包一起使用。

Abstract: In this paper, we present the design and architecture of REI, a novel system
for indexing log data for regular expression queries. Our main contribution is
an $n$-gram-based indexing strategy and an efficient storage mechanism that
results in a speedup of up to 14x compared to state-of-the-art regex processing
engines that do not use indexing, using only 2.1% of extra space. We perform a
detailed study that analyzes the space usage of the index and the improvement
in workload execution time, uncovering interesting insights. Specifically, we
show that even an optimized implementation of strategies such as inverted
indexing, which are widely used in text processing libraries, may lead to
suboptimal performance for regex indexing on log analysis tasks. Overall, the
REI approach presented in this paper provides a significant boost when
evaluating regular expression queries on log data. REI is also modular and can
work with existing regular expression packages, making it easy to deploy in a
variety of settings. The code of REI is available at
https://github.com/mush-zhang/REI-Regular-Expression-Indexing.

</details>


### [126] [AQORA: A Learned Adaptive Query Optimizer for Spark SQL](https://arxiv.org/abs/2510.10580)
*Jiahao He,Yutao Cui,Cuiping Li,Jikang Jiang,Yuheng Hou,Hong Chen*

Main category: cs.DB

TL;DR: AQORA结合了学习型查询优化（LQO）和自适应查询处理（AQP）的优点，以克服它们各自的局限性。


<details>
  <summary>Details</summary>
Motivation: LQO在没有实际基数的情况下必须提交一个固定的计划，并且通常依赖于单一的端到端反馈信号，这使得学习效率低下。另一方面，AQP严重依赖于基于规则的启发式方法，并且缺乏从经验中学习的能力。

Method: AQORA采用强化学习架构，并通过四个核心策略来解决上述挑战：（1）现实的特征编码，（2）查询阶段级别的反馈和干预，（3）自动策略适应，以及（4）低成本集成。

Result: 实验表明，与其他学习方法相比，AQORA最多可减少90%的端到端执行时间，与Spark SQL的默认配置（具有自适应查询执行）相比，最多可减少70%。

Conclusion: AQORA是一种有效的自适应查询优化器，它结合了LQO和AQP的优点，并在实验中表现出显著的性能提升。

Abstract: Recent studies have identified two main approaches to improve query
optimization: learned query optimization (LQO), which generates or selects
better query plans before execution based on models trained in advance, and
adaptive query processing (AQP), which adapts the query plan during execution
based on statistical feedback collected at runtime. Although both approaches
have shown promise, they also face critical limitations. LQO must commit to a
fixed plan without access to actual cardinalities and typically rely on a
single end-to-end feedback signal, making learning inefficient. On the other
hand, AQP depends heavily on rule-based heuristics and lacks the ability to
learn from experience. In this paper, we present AQORA, an adaptive query
optimizer with a reinforcement learning architecture that combines the
strengths of both LQO and AQP. AQORA addresses the above challenges through
four core strategies: (1) realistic feature encoding, (2) query stage-level
feedback and intervention, (3) automatic strategy adaptation, and (4) low-cost
integration. Experiments show that AQORA reduces end-to-end execution time by
up to 90% compared to other learned methods and by up to 70% compared to Spark
SQL's default configuration with adaptive query execution.

</details>


### [127] [DriftBench: Defining and Generating Data and Query Workload Drift for Benchmarking](https://arxiv.org/abs/2510.10858)
*Guanli Liu,Renata Borovica-Gajic*

Main category: cs.DB

TL;DR: 论文提出了一个统一的框架来模拟和生成数据库基准测试中的数据和工作负载漂移。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试是静态的，无法支持漂移建模，缺乏清晰的定义和工具来生成数据和工作负载漂移。

Method: 提出了一个统一的数据和工作负载漂移分类法，并在此基础上引入了DriftBench，一个轻量级和可扩展的框架，用于在基准测试输入中生成数据和工作负载漂移。

Result: 通过案例研究证明了其有效性，包括数据漂移、工作负载漂移和漂移感知基数估计。

Conclusion: 该分类法和DriftBench提供了一个标准化的词汇表和机制，用于在基准测试中建模和生成漂移。

Abstract: Data and workload drift are key to evaluating database components such as
caching, cardinality estimation, indexing, and query optimization. Yet,
existing benchmarks are static, offering little to no support for modeling
drift. This limitation stems from the lack of clear definitions and tools for
generating data and workload drift. Motivated by this gap, we propose a unified
taxonomy for data and workload drift, grounded in observations from both
academia and industry. Building on this foundation, we introduce DriftBench, a
lightweight and extensible framework for generating data and workload drift in
benchmark inputs. Together, the taxonomy and DriftBench provide a standardized
vocabulary and mechanism for modeling and generating drift in benchmarking. We
demonstrate their effectiveness through case studies involving data drift,
workload drift, and drift-aware cardinality estimation.

</details>


### [128] [GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable Transactional and Analytical Workloads](https://arxiv.org/abs/2510.11011)
*Farzaneh Zirak,Farhana Choudhury,Renata Borovica-Gajic*

Main category: cs.DB

TL;DR: 本文提出了一种名为GrASP的基于学习的预取器，旨在提高预取准确性和可扩展性，适用于分析和事务工作负载。


<details>
  <summary>Details</summary>
Motivation: 传统预取器侧重于顺序模式，而最近的基于学习的方法在复杂访问模式下实现了更高的准确性。然而，这些方法通常难以处理当今动态的、不断增长的数据集，需要频繁、及时地进行微调。隐私约束也可能限制对完整数据集的访问，因此需要能够从样本中有效学习的预取器。

Method: GrASP利用逻辑块地址增量，并将查询表示与结果编码相结合。它将预取视为上下文感知的多标签分类任务，使用多层LSTM从嵌入上下文中预测增量模式。

Result: 在真实世界数据集和工业基准测试上的实验表明，GrASP可以推广到比训练数据大250倍的数据集，与现有基线相比，命中率提高了45%，I/O时间降低了60%，端到端查询执行延迟降低了55%。平均而言，GrASP实现了91.4%的命中率，90.8%的I/O时间减少和57.1%的执行延迟减少。

Conclusion: GrASP是一种有效的预取器，可以在各种工作负载和数据集大小上实现高性能。

Abstract: Data prefetching--loading data into the cache before it is requested--is
essential for reducing I/O overhead and improving database performance. While
traditional prefetchers focus on sequential patterns, recent learning-based
approaches, especially those leveraging data semantics, achieve higher accuracy
for complex access patterns. However, these methods often struggle with today's
dynamic, ever-growing datasets and require frequent, timely fine-tuning.
Privacy constraints may also restrict access to complete datasets,
necessitating prefetchers that can learn effectively from samples. To address
these challenges, we present GrASP, a learning-based prefetcher designed for
both analytical and transactional workloads. GrASP enhances prefetching
accuracy and scalability by leveraging logical block address deltas and
combining query representations with result encodings. It frames prefetching as
a context-aware multi-label classification task, using multi-layer LSTMs to
predict delta patterns from embedded context. This delta modeling approach
enables GrASP to generalize predictions from small samples to larger, dynamic
datasets without requiring extensive retraining. Experiments on real-world
datasets and industrial benchmarks demonstrate that GrASP generalizes to
datasets 250 times larger than the training data, achieving up to 45% higher
hit ratios, 60% lower I/O time, and 55% lower end-to-end query execution
latency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a
90.8% I/O time reduction, and a 57.1% execution latency reduction.

</details>


### [129] [Poseidon: A OneGraph Engine](https://arxiv.org/abs/2510.11166)
*Brad Bebee,Ümit V. Çatalyürek,Olaf Hartig,Ankesh Khandelwal,Simone Rondelli,Michael Schmidt,Lefteris Sidirourgos,Bryan Thompson*

Main category: cs.DB

TL;DR: Poseidon是Neptune Analytics图数据库服务背后的引擎，它使用openCypher查询语言，支持广泛的图工作负载。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图应用需要反映动态图的当前状态，例如实时欺诈检测。

Method: Poseidon结合了最先进的事务处理和新型图数据索引，包括无锁邻接表维护、二级简洁索引、具有统一放置的分区堆和用于基于成本的查询优化的创新统计。

Result: 批量数据加载在许多数据集上每秒实现超过1000万个属性值，而简单事务可以在20毫秒内针对存储引擎执行。

Conclusion: Poseidon引擎使用逻辑日志来实现持久性，从而能够快速演进内存数据结构。

Abstract: We present the Poseidon engine behind the Neptune Analytics graph database
service. Customers interact with Poseidon using the declarative openCypher
query language, which enables requests that seamlessly combine traditional
querying paradigms (such as graph pattern matching, variable length paths,
aggregation) with algorithm invocations and has been syntactically extended to
facilitate OneGraph interoperability, such as the disambiguation between
globally unique IRIs (as exposed via RDF) vs. local identifiers (as encountered
in LPG data). Poseidon supports a broad range of graph workloads, from simple
transactions, to top-k beam search algorithms on dynamic graphs, to whole graph
analytics requiring multiple full passes over the data. For example, real-time
fraud detection, like many other use cases, needs to reflect current committed
state of the dynamic graph. If a users cell phone is compromised, then all
newer actions by that user become immediately suspect. To address such dynamic
graph use cases, Poseidon combines state-of-the-art transaction processing with
novel graph data indexing, including lock-free maintenance of adjacency lists,
secondary succinct indices, partitioned heaps for data tuple storage with
uniform placement, and innovative statistics for cost-based query optimization.
The Poseidon engine uses a logical log for durability, enabling rapid evolution
of in-memory data structures. Bulk data loads achieve more than 10 million
property values per second on many data sets while simple transactions can
execute in under 20ms against the storage engine.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [130] [MTMD: A Multi-Task Multi-Domain Framework for Unified Ad Lightweight Ranking at Pinterest](https://arxiv.org/abs/2510.09857)
*Xiao Yang,Peifeng Yin,Abe Engle,Jinfeng Zhuang,Ling Leng*

Main category: cs.IR

TL;DR: 提出了一个多任务多域（MTMD）架构，用于在轻量级广告排序器中进行联合整体优化，以最大化平台、广告商和用户的价值。


<details>
  <summary>Details</summary>
Motivation: 在级联广告推荐系统中，轻量级广告排序层至关重要。由于广告域存在多个优化任务（例如点击率和转化率），以及广告投放的多个表面和不同的广告产品，因此在轻量级排序器中进行联合整体优化是一个具有挑战性的问题。

Method: 提出了一个多任务多域（MTMD）架构，该架构基于经典的双塔模式，并结合了混合专家架构来学习领域专业知识和领域之间的共享知识，以及一个领域自适应模块来促进专家之间的知识转移。

Result: MTMD将离线损失值提高了12%到36%，并在线降低了2%的每次点击成本。

Conclusion: MTMD框架已部署到Pinterest广告推荐的生产环境中，取代了9个生产模型。

Abstract: The lightweight ad ranking layer, living after the retrieval stage and before
the fine ranker, plays a critical role in the success of a cascaded ad
recommendation system. Due to the fact that there are multiple optimization
tasks depending on the ad domain, e.g., Click Through Rate (CTR) for click ads
and Conversion Rate (CVR) for conversion ads, as well as multiple surfaces
where an ad is served (home feed, search, or related item recommendation) with
diverse ad products (shopping or standard ad); it is an essentially challenging
problem in industry on how to do joint holistic optimization in the lightweight
ranker, such that the overall platform's value, advertiser's value, and user's
value are maximized.
  Deep Neural Network (DNN)-based multitask learning (MTL) can handle multiple
goals naturally, with each prediction head mapping to a particular optimization
goal. However, in practice, it is unclear how to unify data from different
surfaces and ad products into a single model. It is critical to learn
domain-specialized knowledge and explicitly transfer knowledge between domains
to make MTL effective. We present a Multi-Task Multi-Domain (MTMD) architecture
under the classic Two-Tower paradigm, with the following key contributions: 1)
handle different prediction tasks, ad products, and ad serving surfaces in a
unified framework; 2) propose a novel mixture-of-expert architecture to learn
both specialized knowledge each domain and common knowledge shared between
domains; 3) propose a domain adaption module to encourage knowledge transfer
between experts; 4) constrain the modeling of different prediction tasks. MTMD
improves the offline loss value by 12% to 36%, mapping to 2% online reduction
in cost per click. We have deployed this single MTMD framework into production
for Pinterest ad recommendation replacing 9 production models.

</details>


### [131] [PairSem: LLM-Guided Pairwise Semantic Matching for Scientific Document Retrieval](https://arxiv.org/abs/2510.09897)
*Wonbin Kweon,Runchu Tian,SeongKu Kang,Pengcheng Jiang,Zhiyong Lu,Jiawei Han,Hwanjo Yu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的科学文档检索框架，Pairwise Semantic Matching (PairSem)，通过将语义表示为实体-方面对，捕捉复杂的多方面科学概念，从而提高检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的密集检索方法难以捕捉文本中细粒度的科学概念，并且通常将实体视为独立的片段，忽略了科学概念的多方面性质。

Method: PairSem框架将语义表示为实体-方面对，是无监督的、与基础检索器无关的，并且是即插即用的，无需查询-文档标签或实体注释。

Result: 在多个数据集和检索器上的大量实验表明，PairSem显著提高了检索性能。

Conclusion: 建模多方面语义对于科学信息检索至关重要。

Abstract: Scientific document retrieval is a critical task for enabling knowledge
discovery and supporting research across diverse domains. However, existing
dense retrieval methods often struggle to capture fine-grained scientific
concepts in texts due to their reliance on holistic embeddings and limited
domain understanding. Recent approaches leverage large language models (LLMs)
to extract fine-grained semantic entities and enhance semantic matching, but
they typically treat entities as independent fragments, overlooking the
multi-faceted nature of scientific concepts. To address this limitation, we
propose Pairwise Semantic Matching (PairSem), a framework that represents
relevant semantics as entity-aspect pairs, capturing complex, multi-faceted
scientific concepts. PairSem is unsupervised, base retriever-agnostic, and
plug-and-play, enabling precise and context-aware matching without requiring
query-document labels or entity annotations. Extensive experiments on multiple
datasets and retrievers demonstrate that PairSem significantly improves
retrieval performance, highlighting the importance of modeling multi-aspect
semantics in scientific information retrieval.

</details>


### [132] [CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms](https://arxiv.org/abs/2510.10095)
*Peiyuan Gong,Feiran Zhu,Yaqi Yin,Chenglei Dai,Chao Zhang,Kai Zheng,Wentian Bao,Jiaxin Mao,Yi Zhang*

Main category: cs.IR

TL;DR: 这篇论文提出了一种名为CardRewriter的框架，用于改进短视频平台上的长尾查询重写，该框架通过整合多源知识并将其总结为知识卡片，以指导LLM更好地理解用户意图并生成更有效的查询重写。


<details>
  <summary>Details</summary>
Motivation: 用户查询（尤其是长尾查询）经常出现拼写错误、短语不完整和意图模糊等问题，导致用户期望与检索结果不匹配。大型语言模型(LLM)在电子商务中的长尾查询重写方面取得了成功，但在短视频平台上却表现不佳，因为短视频、直播、微短剧和用户社交网络等专有内容不在它们的训练分布范围内。

Method: 该方法为每个查询聚合与查询相关的多源知识，并将其总结为信息丰富且与查询相关的知识卡片。然后，该卡片指导LLM更好地捕捉用户意图并产生更有效的查询重写。使用两阶段训练管道优化CardRewriter：监督式微调，然后是群体相对策略优化，并采用定制的奖励系统来平衡查询相关性和检索有效性。

Result: 离线实验表明，CardRewriter显着提高了针对专有内容的查询的重写质量。在线A/B测试进一步证实了长观看率(LVR)和点击率(CTR)的显着提高，以及主动查询重新表述率(IQRR)的显着降低。

Conclusion: CardRewriter已于2025年9月部署在快手上，每天为数亿用户提供服务。

Abstract: Short-video platforms have rapidly become a new generation of information
retrieval systems, where users formulate queries to access desired videos.
However, user queries, especially long-tail ones, often suffer from spelling
errors, incomplete phrasing, and ambiguous intent, resulting in mismatches
between user expectations and retrieved results. While large language models
(LLMs) have shown success in long-tail query rewriting within e-commerce, they
struggle on short-video platforms, where proprietary content such as short
videos, live streams, micro dramas, and user social networks falls outside
their training distribution. To address this challenge, we introduce
\textbf{CardRewriter}, an LLM-based framework that incorporates domain-specific
knowledge to enhance long-tail query rewriting. For each query, our method
aggregates multi-source knowledge relevant to the query and summarizes it into
an informative and query-relevant knowledge card. This card then guides the LLM
to better capture user intent and produce more effective query rewrites. We
optimize CardRewriter using a two-stage training pipeline: supervised
fine-tuning followed by group relative policy optimization, with a tailored
reward system balancing query relevance and retrieval effectiveness. Offline
experiments show that CardRewriter substantially improves rewriting quality for
queries targeting proprietary content. Online A/B testing further confirms
significant gains in long-view rate (LVR) and click-through rate (CTR), along
with a notable reduction in initiative query reformulation rate (IQRR). Since
September 2025, CardRewriter has been deployed on Kuaishou, one of China's
largest short-video platforms, serving hundreds of millions of users daily.

</details>


### [133] [Integrating Structure-Aware Attention and Knowledge Graphs in Explainable Recommendation Systems](https://arxiv.org/abs/2510.10109)
*Shuangquan Lyu,Ming Wang,Huajun Zhang,Jiasen Zheng,Junjiang Lin,Xiaoxuan Sun*

Main category: cs.IR

TL;DR: 这篇论文设计并实现了一个可解释的推荐模型，该模型集成了知识图谱和结构感知注意力机制。


<details>
  <summary>Details</summary>
Motivation: 该模型旨在通过整合知识图谱的结构信息并动态地分配不同邻居的重要性，从而增强模型捕获隐式偏好关系的能力。

Method: 该模型基于图神经网络，并结合了多跳邻居聚合策略。用户和项目被嵌入到一个统一的图结构中，并构建多层次语义路径以提取更丰富的上下文信息。在评分预测阶段，通过用户和目标项目表示之间的交互生成推荐。

Result: 在Amazon Books数据集上进行的实验验证了所提出的模型在各种评估指标上的优越性能。该模型还表现出良好的收敛性和稳定性。

Conclusion: 这些结果进一步证明了结构感知注意力机制在知识图谱增强推荐中的有效性和实用性。

Abstract: This paper designs and implements an explainable recommendation model that
integrates knowledge graphs with structure-aware attention mechanisms. The
model is built on graph neural networks and incorporates a multi-hop neighbor
aggregation strategy. By integrating the structural information of knowledge
graphs and dynamically assigning importance to different neighbors through an
attention mechanism, the model enhances its ability to capture implicit
preference relationships. In the proposed method, users and items are embedded
into a unified graph structure. Multi-level semantic paths are constructed
based on entities and relations in the knowledge graph to extract richer
contextual information. During the rating prediction phase, recommendations are
generated through the interaction between user and target item representations.
The model is optimized using a binary cross-entropy loss function. Experiments
conducted on the Amazon Books dataset validate the superior performance of the
proposed model across various evaluation metrics. The model also shows good
convergence and stability. These results further demonstrate the effectiveness
and practicality of structure-aware attention mechanisms in knowledge
graph-enhanced recommendation.

</details>


### [134] [Breaking the Likelihood Trap: Consistent Generative Recommendation with Graph-structured Model](https://arxiv.org/abs/2510.10127)
*Qiya Yang,Xiaoxi Liang,Zeping Xiao,Yingjie Deng,Yalong Wang,Yongqi Liu,Han Li*

Main category: cs.IR

TL;DR: 本文提出了一种名为Congrats的生成式重排序框架，旨在解决传统方法中的“可能性陷阱”问题，提高推荐的多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式重排序方法容易陷入“可能性陷阱”，导致推荐列表同质化，限制用户参与度。

Method: 设计了一个图结构的解码器，可以捕捉多个路径上的不同序列，扩大了解码空间，并通过顶点转换推导出的隐式项目依赖关系提高预测精度。此外，还设计了一个可微的级联系统，该系统包含一个评估器，使模型能够直接从用户偏好中学习。

Result: 大量的离线实验验证了Congrats优于最先进的重排序方法。在快手上的大规模评估表明，该方法显著提高了推荐质量和多样性。

Conclusion: Congrats在实际工业环境中有效，可以显著提高推荐质量和多样性。

Abstract: Reranking, as the final stage of recommender systems, demands real-time
inference, accuracy, and diversity. It plays a crucial role in determining the
final exposure, directly influencing user experience. Recently, generative
reranking has gained increasing attention for its strong ability to model
complex dependencies among items. However, most existing methods suffer from
the "likelihood trap", where high-likelihood sequences are often perceived as
low-quality by humans. These models tend to repeatedly recommend a set of
high-frequency items, resulting in list homogeneity, thereby limiting user
engagement. In this work, we propose Consistent Graph-structured Generative
Recommendation (Congrats), a novel generative reranking framework. To break the
likelihood trap, we introduce a novel graph-structured decoder that can capture
diverse sequences along multiple paths. This design not only expands the
decoding space to promote diversity, but also improves prediction accuracy by
implicit item dependencies derived from vertex transitions. Furthermore, we
design a differentiable cascade system that incorporates an evaluator, enabling
the model to learn directly from user preferences as the training objective.
Extensive offline experiments validate the superior performance of Congrats
over state-of-the-art reranking methods. Moreover, Congrats has been evaluated
on a large-scale video-sharing app, Kuaishou, with over 300 million daily
active users, demonstrating that our approach significantly improves both
recommendation quality and diversity, validating our effectiveness in practical
industrial environments.

</details>


### [135] [ZeroGR: A Generalizable and Scalable Framework for Zero-Shot Generative Retrieval](https://arxiv.org/abs/2510.10419)
*Weiwei Sun,Keyi Kong,Xinyu Ma,Shuaiqiang Wang,Dawei Yin,Maarten de Rijke,Zhaochun Ren,Yiming Yang*

Main category: cs.IR

TL;DR: 本文提出了一种名为ZeroGR的零样本生成检索框架，该框架利用自然语言指令将GR扩展到各种IR任务。


<details>
  <summary>Details</summary>
Motivation: 现有的生成检索(GR)在有监督训练下取得了显著进展，但难以推广到实际应用中普遍存在的零样本IR场景。

Method: ZeroGR包括三个关键组件：(i) 基于LM的docid生成器，将异构文档统一为语义上有意义的docid；(ii) 指令调优的查询生成器，从自然语言任务描述生成不同类型的查询，以增强语料库索引；(iii) 反向退火解码策略，以平衡docid生成期间的精度和召回率。

Result: 在BEIR和MAIR基准测试上的实验结果表明，ZeroGR在零样本设置中优于强大的密集检索和生成基线，为指令驱动的GR建立了新的技术水平。

Conclusion: 指令微调规模的增加可以持续提高性能。

Abstract: Generative retrieval (GR) reformulates information retrieval (IR) by framing
it as the generation of document identifiers (docids), thereby enabling an
end-to-end optimization and seamless integration with generative language
models (LMs). Despite notable progress under supervised training, GR still
struggles to generalize to zero-shot IR scenarios, which are prevalent in
real-world applications. To tackle this challenge, we propose \textsc{ZeroGR},
a zero-shot generative retrieval framework that leverages natural language
instructions to extend GR across a wide range of IR tasks. Specifically,
\textsc{ZeroGR} is composed of three key components: (i) an LM-based docid
generator that unifies heterogeneous documents (e.g., text, tables, code) into
semantically meaningful docids; (ii) an instruction-tuned query generator that
generates diverse types of queries from natural language task descriptions to
enhance corpus indexing; and (iii) a reverse annealing decoding strategy to
balance precision and recall during docid generation. We investigate the impact
of instruction fine-tuning scale and find that performance consistently
improves as the number of IR tasks encountered during training increases.
Empirical results on the BEIR and MAIR benchmarks demonstrate that
\textsc{ZeroGR} outperforms strong dense retrieval and generative baselines in
zero-shot settings, establishing a new state-of-the-art for instruction-driven
GR.

</details>


### [136] [Does Weighting Improve Matrix Factorization for Recommender Systems?](https://arxiv.org/abs/2510.10440)
*Alex Ayoub,Samuel Robertson,Dawen Liang,Harald Steck,Nathan Kallus*

Main category: cs.IR

TL;DR: 该论文研究了矩阵分解中隐式反馈数据的权重方案，发现非加权数据有时优于加权数据，尤其是在大型模型中。


<details>
  <summary>Details</summary>
Motivation: 研究加权方案和矩阵分解算法。

Method: 对各种加权方案和矩阵分解算法进行了系统的研究。

Result: 发现使用非加权数据进行训练与使用加权数据进行训练相比，性能相当甚至更好，尤其是在大型模型中。确定了加权可能有利的情况，特别是对于容量较低的模型和特定的正则化方案。

Conclusion: 对推荐系统中矩阵分解的权重、正则化和模型容量之间的相互作用进行了全面的分析。

Abstract: Matrix factorization is a widely used approach for top-N recommendation and
collaborative filtering. When implemented on implicit feedback data (such as
clicks), a common heuristic is to upweight the observed interactions. This
strategy has been shown to improve performance for certain algorithms. In this
paper, we conduct a systematic study of various weighting schemes and matrix
factorization algorithms. Somewhat surprisingly, we find that training with
unweighted data can perform comparably to, and sometimes outperform, training
with weighted data, especially for large models. This observation challenges
the conventional wisdom. Nevertheless, we identify cases where weighting can be
beneficial, particularly for models with lower capacity and specific
regularization schemes. We also derive efficient algorithms for exactly
minimizing several weighted objectives that were previously considered
computationally intractable. Our work provides a comprehensive analysis of the
interplay between weighting, regularization, and model capacity in matrix
factorization for recommender systems.

</details>


### [137] [Towards Long-Term User Welfare in Recommender Systems via Creator-Oriented Information Revelation](https://arxiv.org/abs/2510.10511)
*Xu Zhao,Xiaopeng Ye,Chen Xu,Weiran Shen,Jun Xu*

Main category: cs.IR

TL;DR: 提出了一种通过信息披露优化长期用户福利 (LoRe) 的框架，以改善推荐系统生态。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于重新排序算法，但与提高推荐准确性的短期目标相冲突，导致性能下降和次优的长期福利。通过揭示信息来引导创作者行为，促进更健康的 RS 生态系统。

Method: 将信息披露过程建模为马尔可夫决策过程 (MDP)，并提出了一种在具有有限理性创造者的环境中训练和推断的学习算法。

Result: 在两个真实世界的 RS 数据集上进行的大量实验表明，该方法在改善长期用户福利方面优于现有的公平重新排序方法和信息披露策略。

Conclusion: 该方法 (LoRe) 可以有效地优化推荐系统的长期用户福利。

Abstract: Improving the long-term user welfare (e.g., sustained user engagement) has
become a central objective of recommender systems (RS). In real-world
platforms, the creation behaviors of content creators plays a crucial role in
shaping long-term welfare beyond short-term recommendation accuracy, making the
effective steering of creator behavior essential to foster a healthier RS
ecosystem. Existing works typically rely on re-ranking algorithms that
heuristically adjust item exposure to steer creators' behavior. However, when
embedded within recommendation pipelines, such a strategy often conflicts with
the short-term objective of improving recommendation accuracy, leading to
performance degradation and suboptimal long-term welfare. The well-established
economics studies offer us valuable insights for an alternative approach
without relying on recommendation algorithmic design: revealing information
from an information-rich party (sender) to a less-informed party (receiver) can
effectively change the receiver's beliefs and steer their behavior. Inspired by
this idea, we propose an information-revealing framework, named Long-term
Welfare Optimization via Information Revelation (LoRe). In this framework, we
utilize a classical information revelation method (i.e., Bayesian persuasion)
to map the stakeholders in RS, treating the platform as the sender and creators
as the receivers. To address the challenge posed by the unrealistic assumption
of traditional economic methods, we formulate the process of information
revelation as a Markov Decision Process (MDP) and propose a learning algorithm
trained and inferred in environments with boundedly rational creators.
Extensive experiments on two real-world RS datasets demonstrate that our method
can effectively outperform existing fair re-ranking methods and information
revealing strategies in improving long-term user welfare.

</details>


### [138] [Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation](https://arxiv.org/abs/2510.10556)
*Donglin Zhou,Weike Pan,Zhong Ming*

Main category: cs.IR

TL;DR: SICSRec模型通过LLM驱动的样本构建、Transformer序列模型和内容感知的对比学习，实现了ID和内容模态对齐的自监督表示学习，从而提升序列推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型在交互历史有限时表现不佳，而内容感知的序列推荐是很有潜力的方向，但面临着模态语义鸿沟、用户行为和内容偏好联合建模以及ID和内容表示对齐的挑战。

Method: 1. 提出LLM驱动的样本构建方法和监督微调方法对齐item-level的模态表示。
2. 设计Transformer序列模型，使用ID-模态序列编码器捕获用户行为偏好，内容模态序列编码器学习用户内容偏好，混合模态序列解码器把握两者关系。
3. 提出两步训练策略，使用内容感知的对比学习任务对齐模态表示和ID表示。

Result: 在四个视频流数据集上，SICSRec模型在NDCG@5和NDCG@10上分别平均超过现有最佳ID-模态序列推荐器8.04%和内容-模态序列推荐器6.62%。

Conclusion: SICSRec模型有效地解决了内容感知序列推荐中的模态语义鸿沟、用户行为和内容偏好联合建模以及ID和内容表示对齐问题，并在实验中取得了显著的性能提升。

Abstract: Sequential recommendation (SR) models often capture user preferences based on
the historically interacted item IDs, which usually obtain sub-optimal
performance when the interaction history is limited. Content-based sequential
recommendation has recently emerged as a promising direction that exploits
items' textual and visual features to enhance preference learning. However,
there are still three key challenges: (i) how to reduce the semantic gap
between different content modality representations; (ii) how to jointly model
user behavior preferences and content preferences; and (iii) how to design an
effective training strategy to align ID representations and content
representations. To address these challenges, we propose a novel model,
self-supervised representation learning with ID-Content modality alignment,
named SICSRec. Firstly, we propose a LLM-driven sample construction method and
develop a supervised fine-tuning approach to align item-level modality
representations. Secondly, we design a novel Transformer-based sequential
model, where an ID-modality sequence encoder captures user behavior
preferences, a content-modality sequence encoder learns user content
preferences, and a mix-modality sequence decoder grasps the intrinsic
relationship between these two types of preferences. Thirdly, we propose a
two-step training strategy with a content-aware contrastive learning task to
align modality representations and ID representations, which decouples the
training process of content modality dependency and item collaborative
dependency. Extensive experiments conducted on four public video streaming
datasets demonstrate our SICSRec outperforms the state-of-the-art ID-modality
sequential recommenders and content-modality sequential recommenders by 8.04%
on NDCG@5 and 6.62% on NDCD@10 on average, respectively.

</details>


### [139] [Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation](https://arxiv.org/abs/2510.10564)
*Liang Li,Zhou Yang,Xiaofei Zhu*

Main category: cs.IR

TL;DR: 提出了一种名为MGSD-WSS的多粒度序列去噪框架，用于序列推荐，旨在解决历史交互序列中无关噪声项目的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法容易将用户感兴趣的项目误认为噪声，且忽略了兴趣粒度的噪声。

Method: 1. 引入多高斯核感知器模块，将原始序列和增强序列映射到公共表示空间。
2. 利用弱监督信号准确识别历史交互序列中的噪声项目。
3. 采用带噪声加权对比学习的项目粒度去噪模块，获得去噪的项目表示。
4. 从ground truth项目中提取目标兴趣表示，并应用噪声加权对比学习获得去噪的兴趣表示。
5. 基于去噪的项目和兴趣表示，预测下一个项目。

Result: 在五个数据集上的大量实验表明，该方法显著优于现有序列推荐和去噪模型。

Conclusion: MGSD-WSS有效地提升了序列推荐的性能，并在去噪方面表现出色。

Abstract: Sequential recommendation aims to predict the next item based on user
interests in historical interaction sequences. Historical interaction sequences
often contain irrelevant noisy items, which significantly hinders the
performance of recommendation systems. Existing research employs unsupervised
methods that indirectly identify item-granularity irrelevant noise by
predicting the ground truth item. Since these methods lack explicit noise
labels, they are prone to misidentify users' interested items as noise.
Additionally, while these methods focus on removing item-granularity noise
driven by the ground truth item, they overlook interest-granularity noise,
limiting their ability to perform broader denoising based on user interests. To
address these issues, we propose Multi-Granularity Sequence Denoising with
Weakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSS
first introduces the Multiple Gaussian Kernel Perceptron module to map the
original and enhance sequence into a common representation space and utilizes
weakly supervised signals to accurately identify noisy items in the historical
interaction sequence. Subsequently, it employs the item-granularity denoising
module with noise-weighted contrastive learning to obtain denoised item
representations. Then, it extracts target interest representations from the
ground truth item and applies noise-weighted contrastive learning to obtain
denoised interest representations. Finally, based on the denoised item and
interest representations, MGSD-WSS predicts the next item. Extensive
experiments on five datasets demonstrate that the proposed method significantly
outperforms state-of-the-art sequence recommendation and denoising models. Our
code is available at https://github.com/lalunex/MGSD-WSS.

</details>


### [140] [VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering](https://arxiv.org/abs/2510.10828)
*Zhenghan Tai,Hanwei Wu,Qingchen Hu,Jijun Chi,Hailin He,Lei Ding,Tung Sum Thomas Kwok,Bohuai Xiao,Yuchen Hua,Suyuchen Wang,Peng Lu,Muzhi Li,Yihong Wu,Liheng Ma,Jerry Huang,Jiayi Zhang,Gonghao Zhang,Chaolong Jiang,Jingrui Tian,Sicheng Lyu,Zeyu Li,Boyu Han,Fengran Mo,Xinyue Yu,Yufei Cui,Ling Zhou,Xinyu Wang*

Main category: cs.IR

TL;DR: VeritasFi is a hybrid RAG framework for financial QA that tackles heterogeneous data and balances general and company-specific applicability.


<details>
  <summary>Details</summary>
Motivation: Existing financial RAG systems struggle with heterogeneous data formats and balancing general-domain applicability with company-specific adaptation.

Method: The paper presents VeritasFi, a hybrid RAG framework with a multi-modal preprocessing pipeline and a two-stage training strategy for its re-ranking component. It uses a tripartite hybrid retrieval engine combining deep multi-path retrieval, real-time data acquisition, and an expert-curated memory bank.

Result: VeritasFi enhances the adaptability and robustness of financial RAG systems, providing a scalable solution for both general-domain and company-specific QA tasks.

Conclusion: VeritasFi is a groundbreaking framework that greatly enhances financial RAG systems' adaptability and robustness.

Abstract: Retrieval-Augmented Generation (RAG) is becoming increasingly essential for
Question Answering (QA) in the financial sector, where accurate and
contextually grounded insights from complex public disclosures are crucial.
However, existing financial RAG systems face two significant challenges: (1)
they struggle to process heterogeneous data formats, such as text, tables, and
figures; and (2) they encounter difficulties in balancing general-domain
applicability with company-specific adaptation. To overcome these challenges,
we present VeritasFi, an innovative hybrid RAG framework that incorporates a
multi-modal preprocessing pipeline alongside a cutting-edge two-stage training
strategy for its re-ranking component. VeritasFi enhances financial QA through
three key innovations: (1) A multi-modal preprocessing pipeline that seamlessly
transforms heterogeneous data into a coherent, machine-readable format. (2) A
tripartite hybrid retrieval engine that operates in parallel, combining deep
multi-path retrieval over a semantically indexed document corpus, real-time
data acquisition through tool utilization, and an expert-curated memory bank
for high-frequency questions, ensuring comprehensive scope, accuracy, and
efficiency. (3) A two-stage training strategy for the document re-ranker, which
initially constructs a general, domain-specific model using anonymized data,
followed by rapid fine-tuning on company-specific data for targeted
applications. By integrating our proposed designs, VeritasFi presents a
groundbreaking framework that greatly enhances the adaptability and robustness
of financial RAG systems, providing a scalable solution for both general-domain
and company-specific QA tasks. Code accompanying this work is available at
https://github.com/simplew4y/VeritasFi.git.

</details>


### [141] [Comparative Explanations via Counterfactual Reasoning in Recommendations](https://arxiv.org/abs/2510.10920)
*Yi Yu,Zhenxing Hu*

Main category: cs.IR

TL;DR: 本文提出了一种新的可解释推荐方法CoCountER，通过比较反事实推理来识别推荐中物品的影响因素，并生成更准确的解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解释推荐时，为了最小化产品方面的变化并反转推荐决策，常常导致解释不准确。

Method: CoCountER基于软交换操作创建反事实数据，从而能够解释任意比较项目的推荐。

Result: 实验验证了该方法的有效性。

Conclusion: CoCountER能够生成更准确的推荐解释。

Abstract: Explainable recommendation through counterfactual reasoning seeks to identify
the influential aspects of items in recommendations, which can then be used as
explanations. However, state-of-the-art approaches, which aim to minimize
changes in product aspects while reversing their recommended decisions
according to an aggregated decision boundary score, often lead to factual
inaccuracies in explanations. To solve this problem, in this work we propose a
novel method of Comparative Counterfactual Explanations for Recommendation
(CoCountER). CoCountER creates counterfactual data based on soft swap
operations, enabling explanations for recommendations of arbitrary pairs of
comparative items. Empirical experiments validate the effectiveness of our
approach.

</details>


### [142] [HatLLM: Hierarchical Attention Masking for Enhanced Collaborative Modeling in LLM-based Recommendation](https://arxiv.org/abs/2510.10955)
*Yu Cui,Feng Liu,Jiawei Chen,Canghong Jin,Xingyu Lou,Changwang Zhang,Jun Wang,Yuegang Sun,Can Wang*

Main category: cs.IR

TL;DR: 提出了一种新的基于LLM的推荐方法，名为HatLLM，通过分层注意力掩蔽策略来提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在顺序推荐中被广泛应用，但其在有效建模协同信号方面存在局限性，并且注意力机制倾向于过度关注同一项目内的tokens，阻碍了跨项目相关性的捕获。

Method: 提出了一种分层注意力掩蔽策略，浅层屏蔽不同项目之间的tokens注意力，深层屏蔽项目内的tokens注意力，从而联合建模token级别和项目级别的依赖关系。

Result: 在三个真实世界数据集上进行的广泛实验表明，HatLLM相比现有的基于LLM的方法，性能显著提升（平均提升9.13%）。

Conclusion: HatLLM通过分层注意力掩蔽策略，有效提升了LLM在顺序推荐中的性能。

Abstract: Recent years have witnessed a surge of research on leveraging large language
models (LLMs) for sequential recommendation. LLMs have demonstrated remarkable
potential in inferring users' nuanced preferences through fine-grained semantic
reasoning. However, they also exhibit a notable limitation in effectively
modeling collaborative signals, i.e., behavioral correlations inherent in
users' historical interactions. Our empirical analysis further reveals that the
attention mechanisms in LLMs tend to disproportionately focus on tokens within
the same item, thereby impeding the capture of cross-item correlations.
  To address this limitation, we propose a novel hierarchical attention masking
strategy for LLM-based recommendation, termed HatLLM. Specifically, in shallow
layers, HatLLM masks attention between tokens from different items,
facilitating intra-item semantic understanding; in contrast, in deep layers,
HatLLM masks attention within items, thereby compelling the model to capture
cross-item correlations. This progressive, layer-wise approach enables LLMs to
jointly model both token-level and item-level dependencies. Extensive
experiments on three real-world datasets demonstrate that HatLLM achieves
significant performance gains (9.13% on average) over existing LLM-based
methods.

</details>


### [143] [Does LLM Focus on the Right Words? Diagnosing Language Bias in LLM-based Recommenders](https://arxiv.org/abs/2510.10978)
*Bohao Wang,Jiawei Chen,Feng Liu,Changwang Zhang,Jun Wang,Canghong Jin,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: 大型语言模型（LLM）被越来越多地集成到推荐系统（RS）中。为了弥补LLM的预训练目标和推荐任务的特定要求之间的差距，通常在专门策划的推荐数据集上执行监督微调（SFT）以进一步提高其预测能力。但是，SFT会带来语言偏差，GDRT通过对token进行分组，自适应地增加表现不佳的组的权重，从而减轻语言偏差。


<details>
  <summary>Details</summary>
Motivation: SFT存在一个关键限制：它会引起语言偏差，即模型过度依赖辅助token，而未充分利用编码用户特定偏好的核心用户交互token。这种偏差不仅会损害推荐准确性，还会引起不公平的问题。

Method: 提出了一种基于群体分布鲁棒优化（GDRT）的新型微调范例，该范例强制模型在与辅助token具有不同相关程度的token组之间保持一致的性能。

Result: 在三个公共数据集上进行的大量实验表明，GDRT可以有效地缓解语言偏差，从而显着提高推荐准确性（NDCG@10平均增益为24.29%），并显着提高推荐公平性。

Conclusion: GDRT通过将模型的注意力从肤浅的辅助线索转移到信息丰富的用户交互token，从而减轻语言偏差。

Abstract: Large language models (LLMs), owing to their extensive open-domain knowledge
and semantic reasoning capabilities, have been increasingly integrated into
recommender systems (RS). However, a substantial gap remains between the
pre-training objectives of LLMs and the specific requirements of recommendation
tasks. To address this gap, supervised fine-tuning (SFT) is commonly performed
on specially curated recommendation datasets to further enhance their
predictive ability. Despite its success, SFT exhibits a critical limitation: it
induces Language Bias, whereby the model over-relies on auxiliary tokens-such
as task descriptions and prefix-generated tokens-while underutilizing core user
interaction tokens that encode user-specific preferences. This bias not only
undermines recommendation accuracy but also raises unfairness concerns.
  To address this issue, we propose Group Distributionally Robust
Optimization-based Tuning (GDRT), a novel fine-tuning paradigm that enforces
consistent model performance across token groups with varying degrees of
relevance to auxiliary tokens. By adaptively upweighting underperforming
groups, typically those weakly correlated with auxiliary tokens, GDRT shifts
the model's attention from superficial auxiliary cues to informative user
interaction tokens, thereby mitigating language bias. Extensive experiments
conducted on three public datasets demonstrate that GDRT effectively mitigates
language bias, yielding substantial improvements in recommendation accuracy
(with an average NDCG@10 gain of 24.29%) and significantly enhancing
recommendation fairness.

</details>


### [144] [From Reasoning LLMs to BERT: A Two-Stage Distillation Framework for Search Relevance](https://arxiv.org/abs/2510.11056)
*Runze Xia,Yupeng Ji,Yuxi Zhou,Haodong Liu,Teng Zhang,Piji Li*

Main category: cs.IR

TL;DR: 提出了一种两阶段推理蒸馏框架，将大型语言模型（LLM）的推理能力转移到轻量级模型，以满足电商搜索系统的严格延迟要求。


<details>
  <summary>Details</summary>
Motivation: 直接应用大型语言模型（LLM）到电商搜索系统的查询服务相关性预测中，面临严格的延迟要求。

Method: 1. 构建领域自适应的教师模型：通过领域自适应预训练注入平台知识，监督微调引发推理技能，以及使用多维奖励模型进行偏好优化。2. 引入对比推理自蒸馏（CRSD）：通过模拟同一学生模型在“标准”和“推理增强”输入下的行为，使轻量级模型能够内化教师的复杂决策机制。

Result: 离线评估和美团搜索广告系统上的在线A/B测试表明，该框架在多个指标上取得了显著改进。

Conclusion: 验证了该框架的有效性和实用价值。

Abstract: Query-service relevance prediction in e-commerce search systems faces strict
latency requirements that prevent the direct application of Large Language
Models (LLMs). To bridge this gap, we propose a two-stage reasoning
distillation framework to transfer reasoning capabilities from a powerful
teacher LLM to a lightweight, deployment-friendly student model. In the first
stage, we address the limitations of general-purpose LLMs by constructing a
domain-adapted teacher model. This is achieved through a three-step process:
domain-adaptive pre-training to inject platform knowledge, supervised
fine-tuning to elicit reasoning skills, and preference optimization with a
multi-dimensional reward model to ensure the generation of reliable and
preference-aligned reasoning paths. This teacher can then automatically
annotate massive query-service pairs from search logs with both relevance
labels and reasoning chains. In the second stage, to address the challenges of
architectural heterogeneity in standard distillation, we introduce Contrastive
Reasoning Self-Distillation (CRSD). By modeling the behavior of the same
student model under "standard" and "reasoning-augmented" inputs as a
teacher-student relationship, CRSD enables the lightweight model to internalize
the teacher's complex decision-making mechanisms without needing the explicit
reasoning path at inference. Offline evaluations and online A/B testing in the
Meituan search advertising system demonstrate that our framework achieves
significant improvements across multiple metrics, validating its effectiveness
and practical value.

</details>


### [145] [Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction](https://arxiv.org/abs/2510.11066)
*Alin Fan,Hanqing Li,Sihan Lu,Jingsong Yuan,Jiandong Zhang*

Main category: cs.IR

TL;DR: 提出了解耦多模态融合（DMF）方法，用于提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分捕捉内容语义和行为信号之间的细粒度交互。

Method: 构建目标感知特征来弥合不同嵌入空间之间的语义差距，并设计了一种推理优化的注意力机制。

Result: 在公开和工业数据集上进行了离线实验，证明了DMF的有效性。在Lazada的产品推荐系统中，CTCVR相对提高了5.30%，GMV相对提高了7.43%，且计算开销可忽略不计。

Conclusion: DMF方法有效提升了推荐系统性能，并在实际电商平台取得显著效果。

Abstract: Modern industrial recommendation systems improve recommendation performance
by integrating multimodal representations from pre-trained models into ID-based
Click-Through Rate (CTR) prediction frameworks. However, existing approaches
typically adopt modality-centric modeling strategies that process ID-based and
multimodal embeddings independently, failing to capture fine-grained
interactions between content semantics and behavioral signals. In this paper,
we propose Decoupled Multimodal Fusion (DMF), which introduces a
modality-enriched modeling strategy to enable fine-grained interactions between
ID-based collaborative representations and multimodal representations for user
interest modeling. Specifically, we construct target-aware features to bridge
the semantic gap across different embedding spaces and leverage them as side
information to enhance the effectiveness of user interest modeling.
Furthermore, we design an inference-optimized attention mechanism that
decouples the computation of target-aware features and ID-based embeddings
before the attention layer, thereby alleviating the computational bottleneck
introduced by incorporating target-aware features. To achieve comprehensive
multimodal integration, DMF combines user interest representations learned
under the modality-centric and modality-enriched modeling strategies. Offline
experiments on public and industrial datasets demonstrate the effectiveness of
DMF. Moreover, DMF has been deployed on the product recommendation system of
the international e-commerce platform Lazada, achieving relative improvements
of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.

</details>


### [146] [HoMer: Addressing Heterogeneities by Modeling Sequential and Set-wise Contexts for CTR Prediction](https://arxiv.org/abs/2510.11100)
*Shuwei Chen,Jiajun Cui,Zhengqi Xu,Fan Zhang,Jiangke Fan,Teng Zhang,Xingxing Wang*

Main category: cs.IR

TL;DR: HoMer: A Homogeneous-Oriented Transformer to address feature, context, and architecture heterogeneity in CTR prediction.


<details>
  <summary>Details</summary>
Motivation: Existing CTR prediction methods suffer from feature, context, and architecture heterogeneity, which degrades performance.

Method: Proposes HoMer, which aligns sequence side features, shifts to set-wise prediction, and uses a unified encoder-decoder architecture.

Result: HoMer outperforms the industrial baseline by 0.0099 AUC, improves online business metrics (CTR/RPM) by 1.99%/2.46%, and saves 27% of GPU resources.

Conclusion: HoMer effectively addresses the limitations of existing CTR prediction methods and demonstrates superior performance and practicality.

Abstract: Click-through rate (CTR) prediction, which models behavior sequence and
non-sequential features (e.g., user/item profiles or cross features) to infer
user interest, underpins industrial recommender systems. However, most methods
face three forms of heterogeneity that degrade predictive performance: (i)
Feature Heterogeneity persists when limited sequence side features provide less
granular interest representation compared to extensive non-sequential features,
thereby impairing sequence modeling performance; (ii) Context Heterogeneity
arises because a user's interest in an item will be influenced by other items,
yet point-wise prediction neglects cross-item interaction context from the
entire item set; (iii) Architecture Heterogeneity stems from the fragmented
integration of specialized network modules, which compounds the model's
effectiveness, efficiency and scalability in industrial deployments. To tackle
the above limitations, we propose HoMer, a Homogeneous-Oriented TransforMer for
modeling sequential and set-wise contexts. First, we align sequence side
features with non-sequential features for accurate sequence modeling and
fine-grained interest representation. Second, we shift the prediction paradigm
from point-wise to set-wise, facilitating cross-item interaction in a highly
parallel manner. Third, HoMer's unified encoder-decoder architecture achieves
dual optimization through structural simplification and shared computation,
ensuring computational efficiency while maintaining scalability with model
size. Without arduous modification to the prediction pipeline, HoMer
successfully scales up and outperforms our industrial baseline by 0.0099 in the
AUC metric, and enhances online business metrics like CTR/RPM by 1.99%/2.46%.
Additionally, HoMer saves 27% of GPU resources via preliminary engineering
optimization, further validating its superiority and practicality.

</details>


### [147] [DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval-Augmented Generation in E-commerce Search Relevance](https://arxiv.org/abs/2510.11122)
*Tingqiao Xu,Shaowei Yao,Chenhe Dong,Yiming Jin,Zerui Huang,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: DyKnow-RAG is a dynamic noisy-RAG framework that adaptively reweights the contributions of parametric knowledge and external context based on per-query correctness gap, outperforming existing methods in e-commerce ranking.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling query-item relevance in e-commerce is challenging due to long-tail queries and the limitations of parametric LLMs. External context can help but is noisy, and latency constraints prevent clean-then-summarize approaches.

Method: DyKnow-RAG uses Group Relative Policy Optimization to train two rollout groups (with and without external context) and adaptively reweights their contributions based on a posterior-driven inter-group advantage scaling. It is trained with supervised initialization, RL pool prioritized by SFT uncertainty and optional DPO warm start.

Result: DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing.

Conclusion: DyKnow-RAG is a single-pass RAG solution for e-commerce relevance that effectively utilizes noisy external signals without adding online complexity, and is deployed in Taobao's production relevance system.

Abstract: Accurately modeling query-item relevance drives e-commerce ranking, yet
long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM
coverage. External context (reviews, attribute encyclopedias, UGC) can help but
is noisy, and single-pass latency and cost forbid any clean-then-summarize
step. The model must, per query, judge relevance and decide whether to use,
partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG
framework built on Group Relative Policy Optimization. It trains two rollout
groups (no external context vs a single retrieved chunk) and applies
posterior-driven inter-group advantage scaling that adaptively reweights their
contributions by the per-query correctness gap. This teaches when to trust
retrieval versus fall back to parametric knowledge, without process labels,
value networks, or extra inference passes, preserving single-pass, single-chunk
deployment under production latency. Training combines: (1) supervised
initialization with a structured rationale that explicitly records the
context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus
where context choice is most consequential; and (3) an optional lightweight DPO
warm start to stabilize with-context calibration. Under a unified
retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and
vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query
Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's
production relevance system, serving live traffic. To our knowledge, it is
among the first single-pass RAG solutions for e-commerce relevance, turning
noisy external signals into reliable gains without added online complexity.

</details>


### [148] [Next Interest Flow: A Generative Pre-training Paradigm for Recommender Systems by Modeling All-domain Movelines](https://arxiv.org/abs/2510.11317)
*Chen Gao,Zixin Zhao,Lv Shao,Tong Liu*

Main category: cs.IR

TL;DR: 提出了一种新的生成式预训练范式，用于点击率 (CTR) 预测，通过预测 Next Interest Flow 来建模用户意图。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么反应式地依赖过去行为，要么存在语义不匹配或记忆/冷启动问题。

Method: 模型学习预测 Next Interest Flow，同时建模兴趣多样性和演变速度。通过双向对齐策略和时间序列配对机制来解决目标不匹配和捕捉时间因果关系。

Result: 离线实验和大规模在线 A/B 测试验证了 AMEN 的优越性和实际效果。

Conclusion: AMEN 显著提升了关键业务指标，具有重要的现实意义。

Abstract: Click-Through Rate (CTR) prediction, a cornerstone of modern recommender
systems, has been dominated by discriminative models that react to past user
behavior rather than proactively modeling user intent. Existing generative
paradigms attempt to address this but suffer from critical limitations: Large
Language Model (LLM) based methods create a semantic mismatch by forcing
e-commerce signals into a linguistic space, while ID-based generation is
constrained by item memorization and cold-start issues. To overcome these
limitations, we propose a novel generative pre-training paradigm. Our model
learns to predict the Next Interest Flow, a dense vector sequence representing
a user's future intent, while simultaneously modeling its internal Interest
Diversity and Interest Evolution Velocity to ensure the representation is both
rich and coherent. However, this two-stage approach introduces a critical
objective mismatch between the generative and discriminative stages. We resolve
this via a bidirectional alignment strategy, which harmonizes the two stages
through cross-stage weight initialization and a dynamic Semantic Alignment
Module for fine-tuning. Additionally, we enhance the underlying discriminative
model with a Temporal Sequential Pairwise (TSP) mechanism to better capture
temporal causality. We present the All-domain Moveline Evolution Network
(AMEN), a unified framework implementing our entire pipeline. Extensive offline
experiments validate AMEN's superiority over strong baselines, and a
large-scale online A/B test demonstrates its significant real-world impact,
delivering substantial improvements in key business metrics.

</details>


### [149] [Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate Marketing](https://arxiv.org/abs/2510.11323)
*Zhe Wang,Yaming Yang,Ziyu Guan,Bin Tong,Rui Wang,Wei Zhao,Hongbo Deng*

Main category: cs.IR

TL;DR: 本文提出了一种新的评估推广者贡献的指标，称为传播规模，并设计了一个两阶段解决方案来预测它。


<details>
  <summary>Details</summary>
Motivation: 现有的联盟营销中，准确评估和预测推广者在产品推广中的贡献是一个关键但未被充分探索的挑战。

Method: 该方法将网络结构与节点信号解耦，并提出了一个两阶段的解决方案：首先分别进行基本的自销和网络结构预测，然后综合传播规模。具体来说，设计了一种基于后代邻居的图卷积编码方案，并结合超图卷积来有效地捕获复杂的促销动态。此外，还采用了三个辅助任务：自销预测用于基本估计，后代预测用于合成传播规模，以及启动者激活预测以减轻高波动性问题。

Result: 在大型工业数据集上的大量离线实验验证了该方法的优越性。该模型已部署在拥有超过 10 万美元启动者的 Alimama 平台上，GMV 提高了 9.29%，销量提高了 5.89%。

Conclusion: 本文提出的方法能够有效地评估和预测推广者在产品推广中的贡献，并在实际应用中取得了显著的效果。

Abstract: In recent years, affiliate marketing has emerged as a revenue-sharing
strategy where merchants collaborate with promoters to promote their products.
It not only increases product exposure but also allows promoters to earn a
commission. This paper addresses the pivotal yet under-explored challenge in
affiliate marketing: accurately assessing and predicting the contributions of
promoters in product promotion. We design a novel metric for evaluating the
indirect contributions of the promoter, called propagation scale.
Unfortunately, existing time series forecasting techniques fail to deliver
accurate predictions due to the propagation scale being influenced by multiple
factors and the inherent complexities arising from dynamic scenarios. To
address this issue, we decouple the network structure from the node signals and
propose a two-stage solution: initially, the basic self-sales and network
structure prediction are conducted separately, followed by the synthesis of the
propagation scale. Specifically, we design a graph convolution encoding scheme
based on descendant neighbors and incorporate hypergraph convolution to
efficiently capture complex promotional dynamics. Additionally, three auxiliary
tasks are employed: self-sales prediction for base estimations, descendant
prediction to synthesize propagation scale, and promoter activation prediction
to mitigate high volatility issues. Extensive offline experiments on
large-scale industrial datasets validate the superiority of our method. We
further deploy our model on Alimama platform with over $100,000$ promoters,
achieving a $9.29\%$ improvement in GMV and a $5.89\%$ increase in sales
volume.

</details>


### [150] [VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification](https://arxiv.org/abs/2510.11394)
*Haosheng Qian,Yixing Fan,Jiafeng Guo,Ruqing Zhang,Qi Chen,Dawei Yin,Xueqi Cheng*

Main category: cs.IR

TL;DR: 本文介绍了一种名为 VeriCite 的新框架，旨在验证支持性证据并加强答案归属，以减少大型语言模型中的幻觉。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成 (RAG) 在利用外部知识源增强大型语言模型的响应方面至关重要，但仍然存在幻觉问题。内联引用 RAG 生成的内容已显示出减少幻觉和促进人工验证的潜力。现有的引文生成方法主要依赖于微调生成器或采用后处理方法进行引文匹配，但是前者需要大量的注释数据和计算资源，而后者在管理多个引文时经常遇到困难，并且经常产生次优结果。

Method: VeriCite 分解为三个阶段的生成：1) 初始答案生成；2) 支持证据选择；3) 最终答案细化。

Result: 在五个开源 LLM 和四个数据集上进行的实验表明，VeriCite 可以显着提高引文质量，同时保持答案的正确性。

Conclusion: VeriCite 是一种用于减少大型语言模型幻觉的新框架，通过验证支持性证据并加强答案归属来提高引文质量。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for
enhancing the responses of large language models (LLMs) with external knowledge
sources. Despite the impressive performance in complex question-answering
tasks, RAG still struggles with hallucinations. Attributing RAG-generated
content through in-line citations has demonstrated potential in reducing
hallucinations and facilitating human verification. Existing citation
generation methods primarily rely on either fine-tuning the generator or
employing post-processing approaches for citation matching. However, the former
approach demands substantial annotated data and computational resources, while
the latter often encounters difficulties in managing multiple citations and
frequently produces suboptimal results. In this paper, we introduce a novel
framework, called VeriCite, designed to rigorously validate supporting evidence
and enhance answer attribution. Specifically, VeriCite breaks down into a
three-stage generation: 1) The initial answer generation first generates a
response based on all available contexts and has its claims verified through
the NLI model; 2) the supporting evidence selection assesses the utility of
each document and extracts useful supporting evidences; 3) the final answer
refinement integrates the initial response and collected evidences to produce
the final, refined answer.We conduct experiments across five open-source LLMs
and four datasets, demonstrating that VeriCite can significantly improve
citation quality while maintaining the correctness of the answers.

</details>


### [151] [On Inherited Popularity Bias in Cold-Start Item Recommendation](https://arxiv.org/abs/2510.11402)
*Gregor Meehan,Johan Pauwels*

Main category: cs.IR

TL;DR: 冷启动推荐系统旨在解决未见过物品的预测问题，但会继承热门物品的偏差，导致不公平。


<details>
  <summary>Details</summary>
Motivation: 冷启动系统会学习模仿 CF 方法，可能也会模仿它们的预测偏差。本文研究表明，冷启动系统会继承热门度偏差，并且比 warm 模型更严重，因为它们无法从交互数据中推断热门度，而是试图仅基于内容特征来估计。

Method: 通过在三个多媒体数据集上进行实验，分析了这种行为对三种生成式冷启动方法的影响。然后，描述了一种简单的后处理偏差缓解方法，该方法通过使用嵌入幅度作为预测热门度的代理，可以产生更平衡的推荐，且对面向用户的冷启动准确性的损害有限。

Result: 冷启动推荐系统不仅反映了 warm 模型的热门度偏差，而且受到的影响实际上更为严重。因为他们无法从互动数据中推断出受欢迎程度，因此他们试图仅根据内容特征来估计受欢迎程度。这导致对某些与流行的热门商品具有相似内容的冷门商品的大量过度预测，即使它们的真实受欢迎程度非常低。

Conclusion: 冷启动系统会继承 warm 模型的 popularity 偏差，并且比 warm 模型更严重。提出了一种简单的后处理偏差缓解方法，可以产生更平衡的推荐。

Abstract: Collaborative filtering (CF) recommender systems struggle with making
predictions on unseen, or 'cold', items. Systems designed to address this
challenge are often trained with supervision from warm CF models in order to
leverage collaborative and content information from the available interaction
data. However, since they learn to replicate the behavior of CF methods,
cold-start models may therefore also learn to imitate their predictive biases.
In this paper, we show that cold-start systems can inherit popularity bias, a
common cause of recommender system unfairness arising when CF models overfit to
more popular items, thereby maximizing user-oriented accuracy but neglecting
rarer items. We demonstrate that cold-start recommenders not only mirror the
popularity biases of warm models, but are in fact affected more severely:
because they cannot infer popularity from interaction data, they instead
attempt to estimate it based solely on content features. This leads to
significant over-prediction of certain cold items with similar content to
popular warm items, even if their ground truth popularity is very low. Through
experiments on three multimedia datasets, we analyze the impact of this
behavior on three generative cold-start methods. We then describe a simple
post-processing bias mitigation method that, by using embedding magnitude as a
proxy for predicted popularity, can produce more balanced recommendations with
limited harm to user-oriented cold-start accuracy.

</details>


### [152] [What Generative Search Engines Like and How to Optimize Web Content Cooperatively](https://arxiv.org/abs/2510.11438)
*Yujiang Wu,Shanshan Zhong,Yubin Kim,Chenyan Xiong*

Main category: cs.IR

TL;DR: 本文介绍了一种名为 AutoGEO 的框架，用于自动学习生成式引擎在使用检索内容生成响应时的偏好，并重写 Web 内容以获得更多关注。


<details>
  <summary>Details</summary>
Motivation: 内容提供商希望从生成式引擎中获得更多关注，因此需要生成式引擎优化 (GEO)。

Method: AutoGEO 首先提示前沿 LLM 解释生成式引擎偏好，并从这些解释中提取有意义的偏好规则。然后，它使用偏好规则作为 AutoGEO$_{\text{API}}$（一种基于提示的 GEO 系统）的上下文工程，并作为基于规则的奖励来训练 AutoGEO$_{\text{Mini}}$（一种经济高效的 GEO 模型）。

Result: 在标准 GEO-Bench 和两个使用真实用户查询新建的基准上的实验表明，AutoGEO 在增强内容吸引力的同时保留了搜索效用。

Conclusion: 分析证实了学习到的规则的鲁棒性以及捕获变体域中独特偏好的能力，以及 AutoGEO 系统将它们嵌入内容优化的能力。

Abstract: By employing large language models (LLMs) to retrieve documents and generate
natural language responses, Generative Engines, such as Google AI overview and
ChatGPT, provide significantly enhanced user experiences and have rapidly
become the new form of search. Their rapid adoption also drives the needs of
Generative Engine Optimization (GEO), as content providers are eager to gain
more traction from them. In this paper, we introduce AutoGEO, a framework to
automatically learn generative engine preferences when using retrieved contents
for response generation, and rewrite web contents for more such traction.
AutoGEO first prompts frontier LLMs to explain generative engine preferences
and extract meaningful preference rules from these explanations. Then it uses
preference rules as context engineering for AutoGEO$_\text{API}$, a
prompt-based GEO system, and as rule-based rewards to train
AutoGEO$_\text{Mini}$, a cost-effective GEO model. Experiments on the standard
GEO-Bench and two newly constructed benchmarks using real user queries
demonstrate the effectiveness of AutoGEO in enhancing content traction while
preserving search utility. Analyses confirm the learned rules' robustness and
abilities to capture unique preferences in variant domains, and AutoGEO
systems' ability to embed them in content optimization. The code is released at
https://github.com/cxcscmu/AutoGEO.

</details>


### [153] [Uncertainty Quantification for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2510.11483)
*Heydar Soudani,Hamed Zamani,Faegheh Hasibi*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的不确定性量化（UQ）方法，名为Retrieval-Augmented Reasoning Consistency (R2C)，用于检索增强推理（RAR）系统。R2C通过扰动多步推理过程来捕获检索和生成过程中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化方法无法很好地处理RAR系统，因为它们通常只处理简单的查询或单步检索，而忽略了检索和生成过程中的所有不确定性来源。

Method: R2C的核心思想是通过对推理步骤应用各种操作来扰动多步推理过程，从而改变检索器的输入，并最终改变生成器的输入。通过这种迭代反馈循环，R2C可以捕获来自检索器和生成器的不确定性。

Result: 在五个流行的RAR系统和各种QA数据集上的实验表明，R2C的AUROC比最先进的UQ基线平均提高了5%以上。此外，在两个下游任务（Abstention和Model Selection）中的外在评估也证实了R2C的有效性。

Conclusion: R2C是一种有效的不确定性量化方法，可以提高RAR系统的可靠性和性能。

Abstract: Retrieval-augmented reasoning (RAR) is a recent evolution of
retrieval-augmented generation (RAG) that employs multiple reasoning steps for
retrieval and generation. While effective for some complex queries, RAR remains
vulnerable to errors and misleading outputs. Uncertainty quantification (UQ)
offers methods to estimate the confidence of systems' outputs. These methods,
however, often handle simple queries with no retrieval or single-step
retrieval, without properly handling RAR setup. Accurate estimation of UQ for
RAR requires accounting for all sources of uncertainty, including those arising
from retrieval and generation. In this paper, we account for all these sources
and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ
method for RAR. The core idea of R2C is to perturb the multi-step reasoning
process by applying various actions to reasoning steps. These perturbations
alter the retriever's input, which shifts its output and consequently modifies
the generator's input at the next step. Through this iterative feedback loop,
the retriever and generator continuously reshape one another's inputs, enabling
us to capture uncertainty arising from both components. Experiments on five
popular RAR systems across diverse QA datasets show that R2C improves AUROC by
over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic
evaluations using R2C as an external signal further confirm its effectiveness
for two downstream tasks: in Abstention, it achieves ~5% gains in both
F1Abstain and AccAbstain; in Model Selection, it improves the exact match by
~7% over single models and ~3% over selection methods.

</details>


### [154] [Characterizing Web Search in The Age of Generative AI](https://arxiv.org/abs/2510.11560)
*Elisabeth Kirsten,Jost Grosse Perdekamp,Mihir Upadhyay,Krishna P. Gummadi,Muhammad Bilal Zafar*

Main category: cs.IR

TL;DR: LLMs 驱动了一种新的网络搜索方式：生成式搜索，它与传统的返回独立网页列表的网络搜索形成对比。本文旨在比较生成式搜索与传统网络搜索的不同之处。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式搜索与传统网络搜索在网络搜索领域的差异，以及在生成式人工智能时代重新评估网络搜索评估标准的需求。

Method: 通过对比 Google 和来自 Google 及 OpenAI 的四个生成式搜索引擎，分析它们在四个领域查询中的表现。

Result: 生成式搜索覆盖更广泛的来源，对模型内部知识和外部知识的依赖程度不同，并呈现不同的概念集合，从而创造了增强搜索多样性和偶然性的新机会。

Conclusion: 强调在生成式人工智能时代，需要重新审视网络搜索的评估标准。

Abstract: The advent of LLMs has given rise to a new type of web search: Generative
search, where LLMs retrieve web pages related to a query and generate a single,
coherent text as a response. This output modality stands in stark contrast to
traditional web search, where results are returned as a ranked list of
independent web pages. In this paper, we ask: Along what dimensions do
generative search outputs differ from traditional web search? We compare
Google, a traditional web search engine, with four generative search engines
from two providers (Google and OpenAI) across queries from four domains. Our
analysis reveals intriguing differences. Most generative search engines cover a
wider range of sources compared to web search. Generative search engines vary
in the degree to which they rely on internal knowledge contained within the
model parameters v.s. external knowledge retrieved from the web. Generative
search engines surface varying sets of concepts, creating new opportunities for
enhancing search diversity and serendipity. Our results also highlight the need
for revisiting evaluation criteria for web search in the age of Generative AI.

</details>


### [155] [QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking](https://arxiv.org/abs/2510.11589)
*Shubham Chatterjee,Jeff Dalton*

Main category: cs.IR

TL;DR: QDER: a neural re-ranking model unifying knowledge graph semantics and multi-vector models for improved information retrieval.


<details>
  <summary>Details</summary>
Motivation: Combining knowledge graph approaches with multi-vector models to advance neural IR.

Method: Integrating knowledge graph semantics into a multi-vector model with late aggregation of token and entity representations, learned attention patterns, and mathematical operations for precise matches.

Result: Significant performance gains across five benchmarks, including a 36% improvement in nDCG@20 on TREC Robust 2004.

Conclusion: QDER excels on difficult queries, setting a foundation for future work in entity-aware retrieval.

Abstract: Neural IR has advanced through two distinct paths: entity-oriented approaches
leveraging knowledge graphs and multi-vector models capturing fine-grained
semantics. We introduce QDER, a neural re-ranking model that unifies these
approaches by integrating knowledge graph semantics into a multi-vector model.
QDER's key innovation lies in its modeling of query-document relationships:
rather than computing similarity scores on aggregated embeddings, we maintain
individual token and entity representations throughout the ranking process,
performing aggregation only at the final scoring stage - an approach we call
"late aggregation." We first transform these fine-grained representations
through learned attention patterns, then apply carefully chosen mathematical
operations for precise matches. Experiments across five standard benchmarks
show that QDER achieves significant performance gains, with improvements of 36%
in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar
improvements on other datasets. QDER particularly excels on difficult queries,
achieving an nDCG@20 of 0.70 where traditional approaches fail completely
(nDCG@20 = 0.0), setting a foundation for future work in entity-aware
retrieval.

</details>


### [156] [REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking](https://arxiv.org/abs/2510.11592)
*Shubham Chatterjee*

Main category: cs.IR

TL;DR: 这篇论文介绍了一种名为REGENT的神经重排序模型，该模型通过模仿人类的理解方式，利用实体作为“语义骨架”来引导注意力，从而在处理复杂信息需求和内容丰富的长文档时表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的神经重排序模型在处理复杂信息需求和长文档时存在困难，主要问题在于无法有效地选择关键内容。这些模型在固定token窗口内处理文本，忽略了重要的语义信号。

Method: REGENT模型将实体作为语义骨架来引导注意力，并将相关性指导直接整合到注意力机制中，结合了细粒度的词汇匹配和高层次的语义推理。

Result: REGENT在三个具有挑战性的数据集上取得了新的state-of-the-art性能，相比BM25提高了高达108%，并且持续优于包括ColBERT和RankVicuna在内的强基线模型。

Conclusion: 这项工作首次成功地将实体语义直接整合到神经注意力中，为实体感知的信息检索建立了一个新的范例。

Abstract: Current neural re-rankers often struggle with complex information needs and
long, content-rich documents. The fundamental issue is not computational--it is
intelligent content selection: identifying what matters in lengthy,
multi-faceted texts. While humans naturally anchor their understanding around
key entities and concepts, neural models process text within rigid token
windows, treating all interactions as equally important and missing critical
semantic signals. We introduce REGENT, a neural re-ranking model that mimics
human-like understanding by using entities as a "semantic skeleton" to guide
attention. REGENT integrates relevance guidance directly into the attention
mechanism, combining fine-grained lexical matching with high-level semantic
reasoning. This relevance-guided attention enables the model to focus on
conceptually important content while maintaining sensitivity to precise term
matches. REGENT achieves new state-of-the-art performance in three challenging
datasets, providing up to 108% improvement over BM25 and consistently
outperforming strong baselines including ColBERT and RankVicuna. To our
knowledge, this is the first work to successfully integrate entity semantics
directly into neural attention, establishing a new paradigm for entity-aware
information retrieval.

</details>


### [157] [OneRec-Think: In-Text Reasoning for Generative Recommendation](https://arxiv.org/abs/2510.11639)
*Zhanyu Liu,Shiyao Wang,Xingmei Wang,Rongzhou Zhang,Jiaxin Deng,Honghui Bao,Jinghao Zhang,Wuchao Li,Pengfei Zheng,Xiangyu Wu,Yifei Hu,Qigen Hu,Xinchen Luo,Lejian Ren,Zixing Zhang,Qianqian Wang,Kuo Cai,Yunfan Wu,Hongtao Cheng,Zexuan Cheng,Lu Ren,Huanjie Wang,Yi Su,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 提出了OneRec-Think，一个统一的框架，无缝集成了对话、推理和个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型作为隐式预测器运行，严重缺乏显式和可控的推理能力，而这正是LLM的一个关键优势。

Method: 该框架包含：(1)项目对齐：用于语义接地的跨模态项目-文本对齐；(2)推理激活：推理支架，以激活推荐上下文中的LLM推理；(3)推理增强：设计了一个推荐特定的奖励函数，该函数考虑了用户偏好的多重有效性。

Result: 在公共基准测试中表现出最先进的性能。提出的“Think-Ahead”架构实现了在快手上的有效工业部署，在APP停留时间上实现了0.159%的增长。

Conclusion: 验证了模型显式推理能力的实际效果。

Abstract: The powerful generative capacity of Large Language Models (LLMs) has
instigated a paradigm shift in recommendation. However, existing generative
models (e.g., OneRec) operate as implicit predictors, critically lacking the
capacity for explicit and controllable reasoning-a key advantage of LLMs. To
bridge this gap, we propose OneRec-Think, a unified framework that seamlessly
integrates dialogue, reasoning, and personalized recommendation. OneRec-Think
incorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for
semantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate
LLM reasoning within the recommendation context; and (3) Reasoning Enhancement,
where we design a recommendation-specific reward function that accounts for the
multi-validity nature of user preferences. Experiments across public benchmarks
show state-of-the-art performance. Moreover, our proposed "Think-Ahead"
architecture enables effective industrial deployment on Kuaishou, achieving a
0.159\% gain in APP Stay Time and validating the practical efficacy of the
model's explicit reasoning capability.

</details>


### [158] [FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection](https://arxiv.org/abs/2510.11654)
*Daniel Berhane Araya,Duoduo Liao*

Main category: cs.IR

TL;DR: FinVet: A multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism.


<details>
  <summary>Details</summary>
Motivation: Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources.

Method: FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis.

Result: FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.

Conclusion: FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient.

Abstract: Financial markets face growing threats from misinformation that can trigger
billions in losses in minutes. Most existing approaches lack transparency in
their decision-making and provide limited attribution to credible sources. We
introduce FinVet, a novel multi-agent framework that integrates two
Retrieval-Augmented Generation (RAG) pipelines with external fact-checking
through a confidence-weighted voting mechanism. FinVet employs adaptive
three-tier processing that dynamically adjusts verification strategies based on
retrieval confidence, from direct metadata extraction to hybrid reasoning to
full model-based analysis. Unlike existing methods, FinVet provides
evidence-backed verdicts, source attribution, confidence scores, and explicit
uncertainty flags when evidence is insufficient. Experimental evaluation on the
FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a
10.4% improvement over the best individual pipeline (fact-check pipeline) and
37% improvement over standalone RAG approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [159] [Direct Routing Gradient (DRGrad): A Personalized Information Surgery for Multi-Task Learning (MTL) Recommendations](https://arxiv.org/abs/2510.09643)
*Yuguang Liu,Yiyun Miao,Luyao Xia*

Main category: cs.LG

TL;DR: 本文提出了一种名为DRGrad的个性化直接路由梯度框架，用于解决推荐系统中多任务学习的负迁移和跷跷板现象。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中复杂且经常矛盾的任务相关性给多任务学习模型带来了负迁移和跷跷板现象的挑战。

Method: DRGrad包含三个关键组件：路由器、更新器和个性化门网络，用于判断训练过程中任务之间的利害关系，从而利用所有有效梯度来减少冲突。

Result: 在包含150亿个样本的真实推荐数据集上，DRGrad的性能优于最先进的多任务学习模型，尤其是在AUC指标方面，表明它能有效管理多任务学习环境中的任务冲突，且不增加模型复杂性。在Census-income数据集和Synthetic数据集上的实验也证明了DRGrad在判断和路由具有不同程度相关性和个性化的任务之间的利害关系的能力。

Conclusion: DRGrad能有效管理多任务学习环境中的任务冲突，且不增加模型复杂性，同时解决了噪声处理方面的缺陷。

Abstract: Multi-task learning (MTL) has emerged as a successful strategy in
industrial-scale recommender systems, offering significant advantages such as
capturing diverse users' interests and accurately detecting different behaviors
like ``click" or ``dwell time". However, negative transfer and the seesaw
phenomenon pose challenges to MTL models due to the complex and often
contradictory task correlations in real-world recommendations. To address the
problem while making better use of personalized information, we propose a
personalized Direct Routing Gradient framework (DRGrad), which consists of
three key components: router, updater and personalized gate network. DRGrad
judges the stakes between tasks in the training process, which can leverage all
valid gradients for the respective task to reduce conflicts. We evaluate the
efficiency of DRGrad on complex MTL using a real-world recommendation dataset
with 15 billion samples. The results show that DRGrad's superior performance
over competing state-of-the-art MTL models, especially in terms of AUC (Area
Under the Curve) metrics, indicating that it effectively manages task conflicts
in multi-task learning environments without increasing model complexity, while
also addressing the deficiencies in noise processing. Moreover, experiments on
the public Census-income dataset and Synthetic dataset, have demonstrated the
capability of DRGrad in judging and routing the stakes between tasks with
varying degrees of correlation and personalization.

</details>


### [160] [Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining](https://arxiv.org/abs/2510.09644)
*Shaharyar Alam Ansari,Mohammad Luqman,Aasim Zafar,Savir Ali*

Main category: cs.LG

TL;DR: 本文提出了一种统一的框架，该框架集成了闭路电视监控视频与多源数据描述符，以增强实时城市交通预测。


<details>
  <summary>Details</summary>
Motivation: 传统的交通系统依赖于静态信号和人工监控，不足以应对现代交通的动态特性。因此，需要智能和自适应的交通管理解决方案。

Method: 该方法结合了时空特征融合、用于序列交通模式发现的频繁 эпизод 挖掘以及用于稳健交通状态预测的混合 LSTM-Transformer 模型。

Result: 该框架在 CityFlowV2 数据集上进行了评估，实现了 98.46% 的高预测精度，宏精度为 0.9800，宏召回率为 0.9839，宏 F1 分数为 0.9819。频繁 эпизод 挖掘分析揭示了显着的序列模式，例如置信水平超过 55% 的中度拥堵过渡。

Conclusion: 强调需要将视频流分析与来自多个来源的数据相结合，以设计实时、响应迅速、适应性强的多级智能交通系统，从而使城市交通更智能、更安全。

Abstract: Rapid urbanization has intensified traffic congestion, environmental strain,
and inefficiencies in transportation systems, creating an urgent need for
intelligent and adaptive traffic management solutions. Conventional systems
relying on static signals and manual monitoring are inadequate for the dynamic
nature of modern traffic. This research aims to develop a unified framework
that integrates CCTV surveillance videos with multi-source data descriptors to
enhance real-time urban traffic prediction. The proposed methodology
incorporates spatio-temporal feature fusion, Frequent Episode Mining for
sequential traffic pattern discovery, and a hybrid LSTM-Transformer model for
robust traffic state forecasting. The framework was evaluated on the CityFlowV2
dataset comprising 313,931 annotated bounding boxes across 46 cameras. It
achieved a high prediction accuracy of 98.46 percent, with a macro precision of
0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis
revealed significant sequential patterns such as moderate-congested transitions
with confidence levels exceeding 55 percent. The 46 sustained congestion alerts
are system-generated, which shows practical value for proactive congestion
management. This emphasizes the need for the incorporation of video stream
analytics with data from multiple sources for the design of real-time,
responsive, adaptable multi-level intelligent transportation systems, which
makes urban mobility smarter and safer.

</details>


### [161] [Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials](https://arxiv.org/abs/2510.09657)
*Riccardo Fosco Gramaccioni,Christian Marinoni,Fabrizio Frezza,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 本研究介绍了一个包含 31,000 种声学材料的数据集 HA30K，旨在利用数据驱动的方法学习亥姆霍兹方程的解。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在处理大规模或实时场景时计算成本高昂。

Method: 提出了一种基于 Stable Diffusion with ControlNet 的深度学习方法，将解表示为图像，从而绕过复杂的仿真软件和显式方程求解。

Result: 该方法利用 GPU 并行化来同时处理多个模拟，从而显著减少计算时间。扩散步骤的数量可以在推理时调整，从而平衡速度和质量。

Conclusion: 证明了基于深度学习的方法在早期研究中特别有用，在这些研究中，快速探索比绝对精度更重要。

Abstract: Accurate simulation of wave propagation in complex acoustic materials is
crucial for applications in sound design, noise control, and material
engineering. Traditional numerical solvers, such as finite element methods, are
computationally expensive, especially when dealing with large-scale or
real-time scenarios. In this work, we introduce a dataset of 31,000 acoustic
materials, named HA30K, designed and simulated solving the Helmholtz equations.
For each material, we provide the geometric configuration and the corresponding
pressure field solution, enabling data-driven approaches to learn Helmholtz
equation solutions. As a baseline, we explore a deep learning approach based on
Stable Diffusion with ControlNet, a state-of-the-art model for image
generation. Unlike classical solvers, our approach leverages GPU
parallelization to process multiple simulations simultaneously, drastically
reducing computation time. By representing solutions as images, we bypass the
need for complex simulation software and explicit equation-solving.
Additionally, the number of diffusion steps can be adjusted at inference time,
balancing speed and quality. We aim to demonstrate that deep learning-based
methods are particularly useful in early-stage research, where rapid
exploration is more critical than absolute accuracy.

</details>


### [162] [Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models](https://arxiv.org/abs/2510.09658)
*Filippo Rinaldi,Aniello Panariello,Giacomo Salici,Fengyuan Liu,Marco Ciccone,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: 提出GradFix，一种仅使用少量标记样本即可迁移知识的新方法，无需额外微调，通过计算目标模型上的一些梯度并相应地屏蔽源任务向量来实现适应。


<details>
  <summary>Details</summary>
Motivation: 当发布新的基础模型时，从业者通常需要重复完整的微调，即使之前的版本已经解决了相同的任务。一个有希望的替代方案是重用参数变化（即，任务向量），这些参数变化捕捉了模型如何适应特定任务。然而，由于它们未对齐的参数空间，它们经常无法在不同的预训练模型之间转移。

Method: 该方法近似理想的梯度符号结构，并利用它来转移知识。通过计算目标模型上的一些梯度并相应地屏蔽源任务向量来实现适应。

Result: 在视觉和语言基准测试中取得了显著的性能提升，始终优于朴素的任务向量加法和小样本微调。

Conclusion: 知识迁移的关键在于新模型梯度的符号结构。

Abstract: When a new release of a foundation model is published, practitioners
typically need to repeat full fine-tuning, even if the same task has already
been solved in the previous version. A promising alternative is to reuse the
parameter changes (i.e., task vectors) that capture how a model adapts to a
specific task. However, they often fail to transfer across different
pre-trained models due to their misaligned parameter space. In this work, we
show that the key to successful transfer lies in the sign structure of the
gradients of the new model. Based on this insight, we propose GradFix, a novel
method that approximates the ideal gradient sign structure and leverages it to
transfer knowledge using only a handful of labeled samples. Notably, this
requires no additional fine-tuning: the adaptation is achieved by computing a
few gradients at the target model and masking the source task vector
accordingly. This yields an update that is locally aligned with the target loss
landscape, effectively rebasing the task vector onto the new pre-training. We
provide a theoretical guarantee that our method ensures first-order descent.
Empirically, we demonstrate significant performance gains on vision and
language benchmarks, consistently outperforming naive task vector addition and
few-shot fine-tuning.

</details>


### [163] [Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors](https://arxiv.org/abs/2510.09659)
*Edgar E. Robles,Dikshant Sagar,Alejandro Yankelevich,Jianming Bian,Pierre Baldi,NOvA Collaboration*

Main category: cs.LG

TL;DR: 提出了一个点集神经网络，用于匹配探测器中的原始命中，并识别每个粒子的类型。


<details>
  <summary>Details</summary>
Motivation: 在NOvA实验中，需要将探测器中的原始命中与其源粒子匹配，并识别每个粒子的类型。传统方法使用聚类和卷积神经网络的混合方法，但内存需求较高。

Method: 提出了一个点集神经网络，它可以处理稀疏矩阵，并混合来自两个视图的信息。该模型直接在稀疏矩阵上操作，并混合来自 XZ 和 YZ 视图的信息。

Result: 该模型实现了 96.8% 的 AUC 分数，高于独立处理两个视图时获得的分数 (85.4%)，同时使用的内存比以前的方法少 90% 以上。

Conclusion: 提出的点集神经网络在粒子识别任务中表现出色，并在内存使用方面具有显著优势。

Abstract: NOvA is a long-baseline neutrino oscillation experiment that detects neutrino
particles from the NuMI beam at Fermilab. Before data from this experiment can
be used in analyses, raw hits in the detector must be matched to their source
particles, and the type of each particle must be identified. This task has
commonly been done using a mix of traditional clustering approaches and
convolutional neural networks (CNNs). Due to the construction of the detector,
the data is presented as two sparse 2D images: an XZ and a YZ view of the
detector, rather than a 3D representation. We propose a point set neural
network that operates on the sparse matrices with an operation that mixes
information from both views. Our model uses less than 10% of the memory
required using previous methods while achieving a 96.8% AUC score, a higher
score than obtained when both views are processed independently (85.4%).

</details>


### [164] [Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise](https://arxiv.org/abs/2510.09660)
*Luca Scimeca,Thomas Jiralerspong,Berton Earnshaw,Jason Hartford,Yoshua Bengio*

Main category: cs.LG

TL;DR: 本文通过构建各向异性噪声算子，将归纳偏置引入扩散模型的训练和采样过程中，以更好地适应目标数据分布。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型(DPMs) 具有强大的生成性能，但它们的归纳偏置在很大程度上仍然是隐含的。本文旨在将归纳偏置构建到扩散模型的训练和采样中，以更好地适应要建模的数据的目标分布。

Method: 我们引入了一个各向异性噪声算子，通过用结构化的频率对角协方差代替各向同性前向协方差来塑造这些偏差。该算子统一了带通掩模和幂律权重，允许我们强调或抑制指定的频带，同时保持前向过程为高斯分布。我们将其称为谱各向异性高斯扩散 (SAGD)。

Result: 我们推导了各向异性协方差的得分关系，并表明在完全支持下，当 $t\!\\to\!0$ 时，学习到的得分收敛到真实数据得分，而各向异性重塑了从噪声到数据的概率流路径。在经验上，我们表明，在多个视觉数据集中，感应各向异性优于标准扩散，并且能够进行选择性省略：在忽略限制在特定频带的已知损坏的同时进行学习。

Conclusion: 这些结果共同表明，精心设计的各向异性前向噪声提供了一个简单而有原则的处理方法，可以在 DPM 中定制归纳偏置。

Abstract: Diffusion Probabilistic Models (DPMs) have achieved strong generative
performance, yet their inductive biases remain largely implicit. In this work,
we aim to build inductive biases into the training and sampling of diffusion
models to better accommodate the target distribution of the data to model. We
introduce an anisotropic noise operator that shapes these biases by replacing
the isotropic forward covariance with a structured, frequency-diagonal
covariance. This operator unifies band-pass masks and power-law weightings,
allowing us to emphasize or suppress designated frequency bands, while keeping
the forward process Gaussian. We refer to this as spectrally anisotropic
Gaussian diffusion (SAGD). In this work, we derive the score relation for
anisotropic covariances and show that, under full support, the learned score
converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the
probability-flow path from noise to data. Empirically, we show the induced
anisotropy outperforms standard diffusion across several vision datasets, and
enables selective omission: learning while ignoring known corruptions confined
to specific bands. Together, these results demonstrate that carefully designed
anisotropic forward noise provides a simple, yet principled, handle to tailor
inductive bias in DPMs.

</details>


### [165] [Assessment of different loss functions for fitting equivalent circuit models to electrochemical impedance spectroscopy data](https://arxiv.org/abs/2510.09662)
*Ali Jaberi,Amin Sadeghi,Runze Zhang,Zhaoyang Zhao,Qiuyu Shi,Robert Black,Zoya Sadighi,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 本文研究了电化学阻抗谱(EIS)数据建模中损失函数的选择问题，提出了两种新的损失函数log-B和log-BW，并与现有损失函数进行了比较。


<details>
  <summary>Details</summary>
Motivation: 在电化学阻抗谱(EIS)数据建模中，选择合适的损失函数对于参数拟合至关重要，不同的损失函数会影响收敛性、计算效率和拟合质量。

Method: 通过生成大量的EIS数据，评估了提出的损失函数以及现有损失函数在R2分数、卡方、计算效率以及预测分量值与原始值之间的平均绝对百分比误差(MAPE)方面的性能。

Result: 研究表明，损失函数的选择会影响收敛性、计算效率、拟合质量和MAPE。X2损失函数在多个拟合质量指标上表现最佳，而log-B在计算效率上更有优势，且对于大多数电路组件产生较低的MAPE。

Conclusion: X2损失函数在追求拟合质量时是首选，而log-B是另一种有竞争力的选择，尤其是在大规模数据驱动应用中，例如在大量数据集或迭代上训练机器学习模型时。

Abstract: Electrochemical impedance spectroscopy (EIS) data is typically modeled using
an equivalent circuit model (ECM), with parameters obtained by minimizing a
loss function via nonlinear least squares fitting. This paper introduces two
new loss functions, log-B and log-BW, derived from the Bode representation of
EIS. Using a large dataset of generated EIS data, the performance of proposed
loss functions was evaluated alongside existing ones in terms of R2 scores,
chi-squared, computational efficiency, and the mean absolute percentage error
(MAPE) between the predicted component values and the original values.
Statistical comparisons revealed that the choice of loss function impacts
convergence, computational efficiency, quality of fit, and MAPE. Our analysis
showed that X2 loss function (squared sum of residuals with proportional
weighting) achieved the highest performance across multiple quality of fit
metrics, making it the preferred choice when the quality of fit is the primary
goal. On the other hand, log-B offered a slightly lower quality of fit while
being approximately 1.4 times faster and producing lower MAPE for most circuit
components, making log-B as a strong alternative. This is a critical factor for
large-scale least squares fitting in data-driven applications, such as training
machine learning models on extensive datasets or iterations.

</details>


### [166] [Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing](https://arxiv.org/abs/2510.09664)
*Changchang Sun,Vickie Chen,Yan Yan*

Main category: cs.LG

TL;DR: 本文提出了一种新的语义凝聚知识蒸馏方案（SODA）用于深度跨模态哈希。


<details>
  <summary>Details</summary>
Motivation: 现有深度监督跨模态哈希方法未能显式地与原始多模态数据交互，导致学习到的语义信息与异构多模态数据不兼容，阻碍了跨模态桥接的性能。

Method: 将多标签信息作为新的文本模态引入，并将其重新表述为一组ground-truth标签提示，然后设计了一个跨模态教师网络，以有效地提取图像和标签模态之间的跨模态语义特征，从而为图像模态学习一个良好映射的Hamming空间。该Hamming空间可以被视为一种先验知识，以指导跨模态学生网络的学习，并全面保留图像和文本模态之间的语义相似性。

Result: 在两个基准数据集上的大量实验表明，该模型优于现有技术。

Conclusion: SODA模型在跨模态哈希任务中表现出色

Abstract: Recently, deep supervised cross-modal hashing methods have achieve compelling
success by learning semantic information in a self-supervised way. However,
they still suffer from the key limitation that the multi-label semantic
extraction process fail to explicitly interact with raw multimodal data, making
the learned representation-level semantic information not compatible with the
heterogeneous multimodal data and hindering the performance of bridging
modality gap. To address this limitation, in this paper, we propose a novel
semantic cohesive knowledge distillation scheme for deep cross-modal hashing,
dubbed as SODA. Specifically, the multi-label information is introduced as a
new textual modality and reformulated as a set of ground-truth label prompt,
depicting the semantics presented in the image like the text modality. Then, a
cross-modal teacher network is devised to effectively distill cross-modal
semantic characteristics between image and label modalities and thus learn a
well-mapped Hamming space for image modality. In a sense, such Hamming space
can be regarded as a kind of prior knowledge to guide the learning of
cross-modal student network and comprehensively preserve the semantic
similarities between image and text modality. Extensive experiments on two
benchmark datasets demonstrate the superiority of our model over the
state-of-the-art methods.

</details>


### [167] [LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference](https://arxiv.org/abs/2510.09665)
*Yihua Cheng,Yuhan Liu,Jiayi Yao,Yuwei An,Xiaokun Chen,Shaoting Feng,Yuyang Huang,Samuel Shen,Kuntai Du,Junchen Jiang*

Main category: cs.LG

TL;DR: LMCache is an open-source KV caching solution that improves LLM inference efficiency by sharing KV caches across engines and queries.


<details>
  <summary>Details</summary>
Motivation: Current LLM inference systems treat engines and queries independently, leading to resource inefficiencies. Existing proposals to avoid redundant computation and increase GPU utilization require efficient KV cache offloading and communication.

Method: LMCache extracts and stores KV caches, exposing them in the LLM engine interface and supporting cache offloading and prefill-decode disaggregation. It features optimized data movement, a modular connector, and a control API.

Result: LMCache achieves up to 15x throughput improvement when combined with vLLM.

Conclusion: LMCache is widely adopted by enterprise inference systems and provides valuable lessons for future KV caching solutions.

Abstract: Today's LLM inference systems treat individual engines and queries
independently for simplicity, but this causes significant resource
inefficiencies. While there are proposals to avoid redundant computation by
reusing KV caches across queries and to increase GPU utilization by
disaggregating a single query to different engines, their promises cannot be
realized without efficiently offloading and communicating KV cache across LLM
inference engines and queries.
  We present LMCache, the first and so far the most efficient open-source KV
caching solution, which extracts and stores KV caches generated by modern LLM
engines (vLLM and SGLang) and shares the KV caches across engines and queries.
LMCache exposes KV caches in the LLM engine interface, effectively transforming
LLM engines from individual token processors to a collection of engines with KV
cache as the storage and communication medium. In particular, it supports both
cache offloading (prefix reuse across queries) and prefill-decode
disaggregation (cross-engine cache transfer). LMCache's high performance and
wide adoption stem from the following contributions: highly optimized KV cache
data movement with performance optimizations including batched data movement
operations, compute and I/O pipelining; a modular KV cache connector component,
decoupling LMCache from the rapid evolution of inference engines; a first-class
control API, such as pinning, lookup, cleanup, movement, and compression, for
flexible cache orchestration across GPU, CPU, storage, and network layers.
Evaluation shows that combining LMCache with vLLM achieves up to 15x
improvement in throughput across diverse workloads. With a growing community,
LMCache has seen dramatic growth in adoption by enterprise inference systems,
which provides valuable lessons for future KV caching solutions. The source
code of LMCache is at: https://github.com/LMCache/LMCache.

</details>


### [168] [Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems](https://arxiv.org/abs/2510.10810)
*Omar Islam Laskar,Fatemeh Ramezani Khozestani,Ishika Nankani,Sohrab Namazi Nia,Senjuti Basu Roy,Kaustubh Beedkar*

Main category: cs.LG

TL;DR: 提出了AEGIS中间件框架，用于识别机器学习数据集的最佳掩蔽配置，以最大限度地提高数据集的效用。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，数据提供者通常在共享数据集之前对其进行匿名化，但多种掩蔽配置的存在导致掩蔽数据集的效用各不相同。因此，一个关键的挑战在于有效地确定最大化数据集效用的最佳掩蔽配置。

Method: 利用有限的数据摘要（如1D直方图）或不使用任何数据摘要来估计特征-标签联合分布，提出了一种基于迭代比例拟合的联合分布估计器，支持各种特征-标签相关量化方法，如g3、互信息或卡方。

Result: 在真实世界数据集上的实验评估表明，AEGIS识别最佳掩蔽配置的速度提高了一个数量级，而生成的掩蔽数据集在下游ML任务中实现了与基线方法相当的预测性能。

Conclusion: AEGIS框架能够快速有效地识别最佳掩蔽配置，并在保护隐私的同时保持数据的效用。

Abstract: Data-sharing ecosystems enable entities -- such as providers, consumers, and
intermediaries -- to access, exchange, and utilize data for various downstream
tasks and applications. Due to privacy concerns, data providers typically
anonymize datasets before sharing them; however, the existence of multiple
masking configurations results in masked datasets with varying utility.
Consequently, a key challenge lies in efficiently determining the optimal
masking configuration that maximizes a dataset's utility. This paper presents
AEGIS, a middleware framework for identifying the optimal masking configuration
for machine learning datasets that consist of features and a class label. We
introduce a utility optimizer that minimizes predictive utility deviation -- a
metric based on the changes in feature-label correlations before and after
masking. Our framework leverages limited data summaries (such as 1D histograms)
or none to estimate the feature-label joint distribution, making it suitable
for scenarios where raw data is inaccessible due to privacy restrictions. To
achieve this, we propose a joint distribution estimator based on iterative
proportional fitting, which allows supporting various feature-label correlation
quantification methods such as g3, mutual information, or chi-square. Our
experimental evaluation on real-world datasets shows that AEGIS identifies
optimal masking configurations over an order of magnitude faster, while the
resulting masked datasets achieve predictive performance on downstream ML tasks
that is on par with baseline approaches.

</details>


### [169] [Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning](https://arxiv.org/abs/2510.09666)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: 论文研究了利用机器学习和地球观测数据预测野火蔓延，但现有方法缺乏不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了全球野火风险，可靠的预测对于适应策略至关重要，现有方法缺乏不确定性量化。

Method: 论文首次系统地分析了使用多模态地球观测输入进行野火蔓延预测中的空间不确定性。

Result: 预测不确定性在空间结构上表现出一致性，集中在火灾 perímetro 附近。高不确定性区域在预测的火线周围形成 20-60 米的缓冲区。

Conclusion: 这项工作支持社群适应气候变化下日益增加的火灾风险，从而实现更强大的野火管理系统。

Abstract: Climate change is intensifying wildfire risks globally, making reliable
forecasting critical for adaptation strategies. While machine learning shows
promise for wildfire prediction from Earth observation data, current approaches
lack uncertainty quantification essential for risk-aware decision making. We
present the first systematic analysis of spatial uncertainty in wildfire spread
forecasting using multimodal Earth observation inputs. We demonstrate that
predictive uncertainty exhibits coherent spatial structure concentrated near
fire perimeters. Our novel distance metric reveals high-uncertainty regions
form consistent 20-60 meter buffer zones around predicted firelines - directly
applicable for emergency planning. Feature attribution identifies vegetation
health and fire activity as primary uncertainty drivers. This work enables more
robust wildfire management systems supporting communities adapting to
increasing fire risk under climate change.

</details>


### [170] [A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction](https://arxiv.org/abs/2510.09668)
*Maryam Abdollahi Shamami,Babak Teimourpour,Farshad Sharifi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可解释且高效的框架，该框架融合了现代机器学习和领域知识，以改进 DDI 预测。


<details>
  <summary>Details</summary>
Motivation: 药物-药物相互作用 (DDI) 是导致可预防不良事件的主要原因，通常会使治疗复杂化并增加医疗保健成本。同时，了解哪些药物不相互作用也同样重要，因为这些知识可以支持更安全的处方和更好的患者预后。

Method: 该方法结合了两种互补的分子嵌入：Mol2Vec（捕获片段级结构模式）和 SMILES-BERT（学习上下文化学特征），以及无泄漏的、基于规则的临床评分 (RBScore)，该评分注入药理学知识而不依赖于相互作用标签。然后使用一种新颖的三阶段元启发式策略 (RSmpl-ACO-PSO) 优化一个轻量级神经分类器，该策略平衡了全局探索和局部细化，以实现稳定的性能。

Result: 在真实世界数据集上的实验表明，该模型实现了高预测精度（在 DrugBank 上 ROC-AUC 为 0.911，PR-AUC 为 0.867），并且可以很好地推广到临床相关的 2 型糖尿病队列。

Conclusion: 这些结果共同突出了构建可靠、可解释且计算高效的模型的实用途径，这些模型可以支持更安全的药物治疗和临床决策。

Abstract: Drug-drug interactions (DDIs) are a leading cause of preventable adverse
events, often complicating treatment and increasing healthcare costs. At the
same time, knowing which drugs do not interact is equally important, as such
knowledge supports safer prescriptions and better patient outcomes. In this
study, we propose an interpretable and efficient framework that blends modern
machine learning with domain knowledge to improve DDI prediction. Our approach
combines two complementary molecular embeddings - Mol2Vec, which captures
fragment-level structural patterns, and SMILES-BERT, which learns contextual
chemical features - together with a leakage-free, rule-based clinical score
(RBScore) that injects pharmacological knowledge without relying on interaction
labels. A lightweight neural classifier is then optimized using a novel
three-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global
exploration and local refinement for stable performance. Experiments on
real-world datasets demonstrate that the model achieves high predictive
accuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a
clinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,
studies show how embedding fusion, RBScore, and the optimizer each contribute
to precision and robustness. Together, these results highlight a practical
pathway for building reliable, interpretable, and computationally efficient
models that can support safer drug therapies and clinical decision-making.

</details>


### [171] [Population synthesis with geographic coordinates](https://arxiv.org/abs/2510.09669)
*Jacopo Lenti,Lorenzo Costantini,Ariadna Fosch,Anna Monticelli,David Scala,Marco Pangallo*

Main category: cs.LG

TL;DR: 提出了一种新的生成具有显式坐标的合成人群的方法，以解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏生成具有显式坐标的合成人群的成熟方法，因为经纬度与其他连续变量不同，存在大量空白区域和高度不均匀的密度。

Method: 提出一种人口合成算法，该算法首先使用 Normalizing Flows (NF) 将空间坐标映射到更规则的潜在空间，然后将它们与变分自编码器 (VAE) 中的其他特征结合以生成合成人群。

Result: 通过生成具有与 121 个数据集中真实房屋相同统计属性的合成房屋来展示该方法，并提出了一个评估框架，该框架可测量空间准确性和实用性，同时确保隐私保护。结果表明，NF+VAE 架构优于流行的基准。

Conclusion: 生成具有精细空间分辨率的地理定位合成人群的能力为需要详细地理信息的应用打开了大门，从家庭对洪水的反应到流行病传播、疏散计划和交通建模。

Abstract: It is increasingly important to generate synthetic populations with explicit
coordinates rather than coarse geographic areas, yet no established methods
exist to achieve this. One reason is that latitude and longitude differ from
other continuous variables, exhibiting large empty spaces and highly uneven
densities. To address this, we propose a population synthesis algorithm that
first maps spatial coordinates into a more regular latent space using
Normalizing Flows (NF), and then combines them with other features in a
Variational Autoencoder (VAE) to generate synthetic populations. This approach
also learns the joint distribution between spatial and non-spatial features,
exploiting spatial autocorrelations. We demonstrate the method by generating
synthetic homes with the same statistical properties of real homes in 121
datasets, corresponding to diverse geographies. We further propose an
evaluation framework that measures both spatial accuracy and practical utility,
while ensuring privacy preservation. Our results show that the NF+VAE
architecture outperforms popular benchmarks, including copula-based methods and
uniform allocation within geographic areas. The ability to generate geolocated
synthetic populations at fine spatial resolution opens the door to applications
requiring detailed geography, from household responses to floods, to epidemic
spread, evacuation planning, and transport modeling.

</details>


### [172] [A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials](https://arxiv.org/abs/2510.09670)
*Xinlun Cheng,Bingzhe Chen,Joseph Choi,Yen T. Nguyen,Pradeep Seshadri,Mayank Verma,H. S. Udaykumar,Stephen Baek*

Main category: cs.LG

TL;DR: 本研究关注能量材料在弱到中等冲击载荷下的热点形成，这对于能量材料的安全存储和处理至关重要，但与强冲击条件相比，这方面的研究仍然不足。


<details>
  <summary>Details</summary>
Motivation: 模拟能量材料中的冲击到爆轰现象需要捕捉复杂的物理过程，例如强冲击、微观结构形态的快速变化和化学反应前沿的非线性动力学。这些过程参与热点的能量 локализация，从而引发化学能释放，最终导致爆轰。

Method: 我们改进了 Physics-Aware Recurrent Convolutional Neural Network (PARCv2) 的架构，以快速预测剪切 локализации 和塑性加热，这在弱到中等冲击状态下起着重要作用。

Result: PARCv2 在捕捉剪切带形成的时空动态方面表现出优异的性能。虽然所有模型都表现出某些失效模式，但我们的发现强调了在开发用于反应材料的鲁棒的 AI 加速模拟工具时，领域特定考虑因素的重要性。

Conclusion: 本研究推进了 Physics-Aware Recurrent Convolutional Neural Network (PARCv2)，并证明了其在预测弱到中等冲击状态下剪切 локализации 和塑性加热方面的优越性能。

Abstract: Modeling shock-to-detonation phenomena in energetic materials (EMs) requires
capturing complex physical processes such as strong shocks, rapid changes in
microstructural morphology, and nonlinear dynamics of chemical reaction fronts.
These processes participate in energy localization at hotspots, which initiate
chemical energy release leading to detonation. This study addresses the
formation of hotspots in crystalline EMs subjected to weak-to-moderate shock
loading, which, despite its critical relevance to the safe storage and handling
of EMs, remains underexplored compared to the well-studied strong shock
conditions. To overcome the computational challenges associated with direct
numerical simulations, we advance the Physics-Aware Recurrent Convolutional
Neural Network (PARCv2), which has been shown to be capable of predicting
strong shock responses in EMs. We improved the architecture of PARCv2 to
rapidly predict shear localizations and plastic heating, which play important
roles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two
widely used physics-informed models, namely, Fourier neural operator and neural
ordinary differential equation; we demonstrate its superior performance in
capturing the spatiotemporal dynamics of shear band formation. While all models
exhibit certain failure modes, our findings underscore the importance of
domain-specific considerations in developing robust AI-accelerated simulation
tools for reactive materials.

</details>


### [173] [Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling](https://arxiv.org/abs/2510.09676)
*Shayan Mohajer Hamidi,En-Hui Yang,Ben Liang*

Main category: cs.LG

TL;DR: 提出了一种新的逆问题求解框架C-DPS，它不需要约束调整或似然近似，通过在数据空间和测量空间中耦合扩散过程，导出一个闭合形式的后验分布，实现精确和递归采样。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的逆问题求解方法依赖于基于投影的技术或近似似然函数，导致伪影和在高噪声条件下不稳定。

Method: 提出C-DPS框架，在测量空间引入前向随机过程，与数据空间扩散过程并行演化，导出闭合形式的后验分布。

Result: 在多个逆问题基准测试中，C-DPS在定性和定量方面均优于现有基线。

Conclusion: C-DPS框架能够有效地解决逆问题，且性能优于现有方法。

Abstract: Inverse problems, where the goal is to recover an unknown signal from noisy
or incomplete measurements, are central to applications in medical imaging,
remote sensing, and computational biology. Diffusion models have recently
emerged as powerful priors for solving such problems. However, existing methods
either rely on projection-based techniques that enforce measurement consistency
through heuristic updates, or they approximate the likelihood $p(\boldsymbol{y}
\mid \boldsymbol{x})$, often resulting in artifacts and instability under
complex or high-noise conditions. To address these limitations, we propose a
novel framework called \emph{coupled data and measurement space diffusion
posterior sampling} (C-DPS), which eliminates the need for constraint tuning or
likelihood approximation. C-DPS introduces a forward stochastic process in the
measurement space $\{\boldsymbol{y}_t\}$, evolving in parallel with the
data-space diffusion $\{\boldsymbol{x}_t\}$, which enables the derivation of a
closed-form posterior $p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t,
\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive
sampling based on a well-defined posterior distribution. Empirical results
demonstrate that C-DPS consistently outperforms existing baselines, both
qualitatively and quantitatively, across multiple inverse problem benchmarks.

</details>


### [174] [Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation](https://arxiv.org/abs/2510.09684)
*Chris Engh,P. M. Aronow*

Main category: cs.LG

TL;DR: 利用大型语言模型（LLM）驱动的AI工具来改进因果估计。


<details>
  <summary>Details</summary>
Motivation: 在高维混杂因素存在的情况下，治疗对结果影响的因果估计的准确性取决于条件期望函数估计器的性能。

Method: 使用在历史数据上训练的生成模型所做的预测来提高这些估计器的性能，相对于那些仅依赖于从这些模型中提取的嵌入进行调整的方法。

Result: 在在线珠宝拍卖的小型数据集的案例研究中，证明了包含LLM生成的猜测作为预测变量可以提高估计效率。

Conclusion: 历史知识和与这些生成模型相关的推理能力可以帮助克服因果推理问题中的维度诅咒问题。

Abstract: We propose a simple yet effective use of LLM-powered AI tools to improve
causal estimation. In double machine learning, the accuracy of causal estimates
of the effect of a treatment on an outcome in the presence of a
high-dimensional confounder depends on the performance of estimators of
conditional expectation functions. We show that predictions made by generative
models trained on historical data can be used to improve the performance of
these estimators relative to approaches that solely rely on adjusting for
embeddings extracted from these models. We argue that the historical knowledge
and reasoning capacities associated with these generative models can help
overcome curse-of-dimensionality problems in causal inference problems. We
consider a case study using a small dataset of online jewelry auctions, and
demonstrate that inclusion of LLM-generated guesses as predictors can improve
efficiency in estimation.

</details>


### [175] [Deep Neural Networks Inspired by Differential Equations](https://arxiv.org/abs/2510.09685)
*Yongshuai Liu,Lianfang Wang,Kuilin Qin,Qinghua Zhang,Faqiang Wang,Li Cui,Jun Liu,Yuping Duan,Tieyong Zeng*

Main category: cs.LG

TL;DR: 本文综述了受微分方程启发的深度神经网络架构和动态建模方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在计算机视觉、科学计算和动态系统等领域已成为关键技术，但神经网络在理论理解、可解释性和泛化方面仍然面临挑战。

Method: 本文考察了基于常微分方程 (ODE) 的深度神经网络模型和确定性动态网络结构，以及基于随机微分方程 (SDE) 的正则化技术和随机动态网络模型。并对这些模型进行了数值比较，以说明它们的特性和性能。

Result: 通过数值比较展示了不同模型的特性和性能。

Conclusion: 探讨了将微分方程与深度学习相结合的有前景的研究方向，旨在为开发具有增强的可解释性和泛化能力的智能计算方法提供新的见解。

Abstract: Deep learning has become a pivotal technology in fields such as computer
vision, scientific computing, and dynamical systems, significantly advancing
these disciplines. However, neural Networks persistently face challenges
related to theoretical understanding, interpretability, and generalization. To
address these issues, researchers are increasingly adopting a differential
equations perspective to propose a unified theoretical framework and systematic
design methodologies for neural networks. In this paper, we provide an
extensive review of deep neural network architectures and dynamic modeling
methods inspired by differential equations. We specifically examine deep neural
network models and deterministic dynamical network constructs based on ordinary
differential equations (ODEs), as well as regularization techniques and
stochastic dynamical network models informed by stochastic differential
equations (SDEs). We present numerical comparisons of these models to
illustrate their characteristics and performance. Finally, we explore promising
research directions in integrating differential equations with deep learning to
offer new insights for developing intelligent computational methods that boast
enhanced interpretability and generalization capabilities.

</details>


### [176] [On the Occurence of Critical Learning Periods in Neural Networks](https://arxiv.org/abs/2510.09687)
*Stanisław Pawlak*

Main category: cs.LG

TL;DR: 本研究表明，通过调整学习超参数可以避免神经网络的关键学习期和warm-starting性能损失。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决神经网络训练中的关键学习期和warm-starting性能损失问题。

Method: 该研究通过实验，复制了关键学习期的主要发现，并扩大了实验范围。此外，研究还考虑了一种warm-starting方法，并表明它可以被看作是一种 deficit 预训练的形式。该研究采用循环学习率计划。

Result: 研究结果表明，通过采用循环学习率计划，可以避免关键学习期和warm-starting性能损失问题。

Conclusion: 研究结果不仅影响神经网络训练实践，而且在关键学习期和warm-starting神经网络训练的持续研究之间建立了重要的联系。

Abstract: This study delves into the plasticity of neural networks, offering empirical
support for the notion that critical learning periods and warm-starting
performance loss can be avoided through simple adjustments to learning
hyperparameters. The critical learning phenomenon emerges when training is
initiated with deficit data. Subsequently, after numerous deficit epochs, the
network's plasticity wanes, impeding its capacity to achieve parity in accuracy
with models trained from scratch, even when extensive clean data training
follows deficit epochs. Building upon seminal research introducing critical
learning periods, we replicate key findings and broaden the experimental scope
of the main experiment from the original work. In addition, we consider a
warm-starting approach and show that it can be seen as a form of deficit
pretraining. In particular, we demonstrate that these problems can be averted
by employing a cyclic learning rate schedule. Our findings not only impact
neural network training practices but also establish a vital link between
critical learning periods and ongoing research on warm-starting neural network
training.

</details>


### [177] [Evaluation of Differential Privacy Mechanisms on Federated Learning](https://arxiv.org/abs/2510.09691)
*Tejash Varsani*

Main category: cs.LG

TL;DR: 联邦学习在保护数据隐私方面面临挑战，尤其是在模型收敛时，固定隐私预算会引入过多噪声，影响性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决固定隐私预算的问题，研究人员开始探索自适应隐私预算。

Method: 本文实现了基于 Laplace 和 Gaussian 机制的 DP 方法，并采用自适应隐私预算，扩展了 SelecEval 模拟器。在高斯机制中，引入了一种自适应剪裁方法，以动态更新模型梯度。

Result: 实验结果表明，自适应隐私预算和自适应剪裁有助于在保护隐私的同时保持模型精度。

Conclusion: 自适应隐私预算和自适应剪裁是提高联邦学习中隐私保护和模型性能的有效方法。

Abstract: Federated learning is distributed model training across several clients
without disclosing raw data. Despite advancements in data privacy, risks still
remain. Differential Privacy (DP) is a technique to protect sensitive data by
adding noise to model updates, usually controlled by a fixed privacy budget.
However, this approach can introduce excessive noise, particularly when the
model converges, which compromises performance. To address this problem,
adaptive privacy budgets have been investigated as a potential solution. This
work implements DP methods using Laplace and Gaussian mechanisms with an
adaptive privacy budget, extending the SelecEval simulator. We introduce an
adaptive clipping approach in the Gaussian mechanism, ensuring that gradients
of the model are dynamically updated rather than using a fixed sensitivity. We
conduct extensive experiments with various privacy budgets, IID and non-IID
datasets, and different numbers of selected clients per round. While our
experiments were limited to 200 training rounds, the results suggest that
adaptive privacy budgets and adaptive clipping can help maintain model accuracy
while preserving privacy.

</details>


### [178] [Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs](https://arxiv.org/abs/2510.09693)
*Jiakang Chen*

Main category: cs.LG

TL;DR: 本论文对比了三种无网格神经偏微分方程求解器（PINNs、DRM 和 WANs）在泊松问题和薛定谔方程上的表现，并提供方法选择的实用指南。


<details>
  <summary>Details</summary>
Motivation: 传统的基于网格的求解器在高维情况下成本高昂，而解析解并不常见。

Method: 使用统一的协议，在泊松问题（高达 5D）和与时间无关的薛定谔方程在 1D/2D（无限阱和简谐振子）上，比较了三种无网格神经 PDE 求解器，即物理信息神经网络 (PINN)、深度 Ritz 方法 (DRM) 和弱对抗网络 (WAN)。

Result: 所有方法在强制边界条件 (FBC)、强制节点 (FN) 和正交正则化 (OG) 的配合下，均实现了较低的 $L_2$ 误差（$10^{-6}$-$10^{-9}$）。PINN 在准确性和激发光谱恢复方面最可靠；DRM 在静态问题上提供了最佳的准确性-运行时间权衡；WAN 更敏感，但当弱形式约束和 FN/OG 有效使用时，也具有竞争力。

Conclusion: 物理引导的神经求解器是解决复杂偏微分方程的可信、可扩展的工具。

Abstract: Partial differential equations (PDEs) underpin models across science and
engineering, yet analytical solutions are atypical and classical mesh-based
solvers can be costly in high dimensions. This dissertation presents a unified
comparison of three mesh-free neural PDE solvers, physics-informed neural
networks (PINNs), the deep Ritz method (DRM), and weak adversarial networks
(WANs), on Poisson problems (up to 5D) and the time-independent Schr\"odinger
equation in 1D/2D (infinite well and harmonic oscillator), and extends the
study to a laser-driven case of Schr\"odinger's equation via the
Kramers-Henneberger (KH) transformation.
  Under a common protocol, all methods achieve low $L_2$ errors
($10^{-6}$-$10^{-9}$) when paired with forced boundary conditions (FBCs),
forced nodes (FNs), and orthogonality regularization (OG). Across tasks, PINNs
are the most reliable for accuracy and recovery of excited spectra; DRM offers
the best accuracy-runtime trade-off on stationary problems; WAN is more
sensitive but competitive when weak-form constraints and FN/OG are used
effectively. Sensitivity analyses show that FBC removes boundary-loss tuning,
network width matters more than depth for single-network solvers, and most
gains occur within 5000-10,000 epochs. The same toolkit solves the KH case,
indicating transfer beyond canonical benchmarks.
  We provide practical guidelines for method selection and outline the
following extensions: time-dependent formulations for DRM and WAN, adaptive
residual-driven sampling, parallel multi-state training, and neural domain
decomposition. These results support physics-guided neural solvers as credible,
scalable tools for solving complex PDEs.

</details>


### [179] [Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection](https://arxiv.org/abs/2510.09694)
*Xiaodan Li,Mengjie Wu,Yao Zhu,Yunna Lv,YueFeng Chen,Cen Chen,Jianmei Guo,Hui Xue*

Main category: cs.LG

TL;DR: Kelp是一个在LM生成过程中进行流式风险检测的插件框架，它利用中间LM隐藏状态和时间一致性损失来提高风险检测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有防护措施主要执行事后检测，可能在捕获之前暴露不安全内容，并且延迟约束进一步将它们推向轻量级模型，从而限制了检测精度。

Method: 提出Kelp，一种新颖的插件框架，它通过流式潜在动态头部（SLD）在LM生成管道中实现流式风险检测，该头部对生成序列中风险的时间演变进行建模，以实现更准确的实时风险检测。为了确保实际应用中可靠的流式审核，引入锚定时间一致性（ATC）损失，通过嵌入良性到有害的时间先验来强制执行单调的有害预测。

Result: Kelp在各种模型和数据集上始终优于最先进的事后防护措施和先前的插件探针（平均F1高出15.61%），同时仅使用20M参数并增加不到0.5毫秒的每token延迟。

Conclusion: Kelp是一种有效的流式风险检测框架，可以在不显著增加计算负担的情况下提高风险检测的准确性和可靠性。

Abstract: Large models (LMs) are powerful content generators, yet their open-ended
nature can also introduce potential risks, such as generating harmful or biased
content. Existing guardrails mostly perform post-hoc detection that may expose
unsafe content before it is caught, and the latency constraints further push
them toward lightweight models, limiting detection accuracy. In this work, we
propose Kelp, a novel plug-in framework that enables streaming risk detection
within the LM generation pipeline. Kelp leverages intermediate LM hidden states
through a Streaming Latent Dynamics Head (SLD), which models the temporal
evolution of risk across the generated sequence for more accurate real-time
risk detection. To ensure reliable streaming moderation in real applications,
we introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic
harm predictions by embedding a benign-then-harmful temporal prior. Besides,
for a rigorous evaluation of streaming guardrails, we also present
StreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from
each protected model, reflecting real-world streaming scenarios in both text
and vision-language tasks. Across diverse models and datasets, Kelp
consistently outperforms state-of-the-art post-hoc guardrails and prior plug-in
probes (15.61% higher average F1), while using only 20M parameters and adding
less than 0.5 ms of per-token latency.

</details>


### [180] [Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form](https://arxiv.org/abs/2510.09696)
*Lorenzo Nikiforos,Charalampos Antoniadis,Luciano Prono,Fabio Pareschi,Riccardo Rovatti,Gianluca Setti*

Main category: cs.LG

TL;DR: VCON通过在微调期间平滑过渡到压缩形式来提高压缩模型的精度。


<details>
  <summary>Details</summary>
Motivation: 压缩技术在减少内存、计算和能源消耗方面非常有效，但通常会引入严重的精度下降。

Method: VCON并行执行原始模型和压缩模型，逐渐减少原始模型的贡献，同时逐渐增加压缩模型的贡献。

Result: VCON在计算机视觉和自然语言处理基准测试中，结合多种压缩策略，始终如一地带来改进：典型增益超过3%，某些配置表现出20%的精度提升。

Conclusion: VCON提供了一种通用方法，可以应用于现有的压缩技术，并在多个基准测试中证明了一致的收益。

Abstract: The increasing scale of deep neural networks has led to a growing need for
compression techniques such as pruning, quantization, and low-rank
decomposition. While these methods are very effective in reducing memory,
computation and energy consumption, they often introduce severe accuracy
degradation when applied directly. We introduce Vanishing Contributions (VCON),
a general approach for smoothly transitioning neural models into compressed
form. Rather than replacing the original network directly with its compressed
version, VCON executes the two in parallel during fine-tuning. The contribution
of the original (uncompressed) model is progressively reduced, while that of
the compressed model is gradually increased. This smooth transition allows the
network to adapt over time, improving stability and mitigating accuracy
degradation. We evaluate VCON across computer vision and natural language
processing benchmarks, in combination with multiple compression strategies.
Across all scenarios, VCON leads to consistent improvements: typical gains
exceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus
provides a generalizable method that can be applied to the existing compression
techniques, with evidence of consistent gains across multiple benchmarks.

</details>


### [181] [Operator Learning for Power Systems Simulation](https://arxiv.org/abs/2510.09704)
*Matthew Schlegel,Matthew E. Taylor,Mostafa Farrokhabadi*

Main category: cs.LG

TL;DR: 这篇论文探索了使用算子学习作为替代模型，以加速可再生能源电网中耗时的时域仿真。


<details>
  <summary>Details</summary>
Motivation: 由于可再生能源快速的动态现象，可再生能源电网的时域仿真计算量巨大，需要快速且可扩展的解决方案。

Method: 研究了仿真时间步长不变性的基本概念，并对三种算子学习方法进行了基准测试。

Result: 验证了时间步长不变性的可行性，模型在零样本超分辨率和稳定/不稳定动态状态之间的泛化能力上进行了评估。

Conclusion: 该研究通过对算子学习方法进行基准测试，以模拟物理系统，从而应对了可再生能源并网以减缓气候变化的关键挑战。

Abstract: Time domain simulation, i.e., modeling the system's evolution over time, is a
crucial tool for studying and enhancing power system stability and dynamic
performance. However, these simulations become computationally intractable for
renewable-penetrated grids, due to the small simulation time step required to
capture renewable energy resources' ultra-fast dynamic phenomena in the range
of 1-50 microseconds. This creates a critical need for solutions that are both
fast and scalable, posing a major barrier for the stable integration of
renewable energy resources and thus climate change mitigation. This paper
explores operator learning, a family of machine learning methods that learn
mappings between functions, as a surrogate model for these costly simulations.
The paper investigates, for the first time, the fundamental concept of
simulation time step-invariance, which enables models trained on coarse time
steps to generalize to fine-resolution dynamics. Three operator learning
methods are benchmarked on a simple test system that, while not incorporating
practical complexities of renewable-penetrated grids, serves as a first
proof-of-concept to demonstrate the viability of time step-invariance. Models
are evaluated on (i) zero-shot super-resolution, where training is performed on
a coarse simulation time step and inference is performed at super-resolution,
and (ii) generalization between stable and unstable dynamic regimes. This work
addresses a key challenge in the integration of renewable energy for the
mitigation of climate change by benchmarking operator learning methods to model
physical systems.

</details>


### [182] [A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation](https://arxiv.org/abs/2510.09705)
*Sudip Khadka,L. S. Paudel*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习（RL）的框架，用于在特征选择过程中减轻偏差，该框架将预测性能与公平性考虑相结合。


<details>
  <summary>Details</summary>
Motivation: 静态特征排除策略在隐藏依赖关系影响模型预测时通常无法防止偏差。

Method: 使用强化学习（RL）agent自适应地选择特征，该agent使用明确整合预测性能和公平性考虑的奖励信号。

Result: 提供了一种灵活且通用的方法来选择预测变量相关的环境中的特征，并且偏差可能会无意中重新出现。

Conclusion: 该模型可以在整个训练过程中平衡泛化、准确性和公平性，而不是仅仅依赖于预处理调整或事后校正机制。

Abstract: Static feature exclusion strategies often fail to prevent bias when hidden
dependencies influence the model predictions. To address this issue, we explore
a reinforcement learning (RL) framework that integrates bias mitigation and
automated feature selection within a single learning process. Unlike
traditional heuristic-driven filter or wrapper approaches, our RL agent
adaptively selects features using a reward signal that explicitly integrates
predictive performance with fairness considerations. This dynamic formulation
allows the model to balance generalization, accuracy, and equity throughout the
training process, rather than rely exclusively on pre-processing adjustments or
post hoc correction mechanisms. In this paper, we describe the construction of
a multi-component reward function, the specification of the agents action space
over feature subsets, and the integration of this system with ensemble
learning. We aim to provide a flexible and generalizable way to select features
in environments where predictors are correlated and biases can inadvertently
re-emerge.

</details>


### [183] [Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments](https://arxiv.org/abs/2510.09712)
*Zhao Tong,Chunlin Gong,Yimeng Gu,Haichao Shi,Qiang Liu,Shu Wu,Xiao-Yu Zhang*

Main category: cs.LG

TL;DR: 该论文研究了在线虚假新闻检测模型在对抗性评论攻击下的脆弱性，并提出了一种基于群体自适应对抗训练的策略来提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有虚假新闻检测模型容易受到对抗性评论的影响，这些评论会微妙地改变模型的决策。

Method: 该方法包括三个步骤：将对抗性评论分为感知、认知和社会三个心理类别；通过大型语言模型生成多样化的、特定类别的攻击；应用Dirichlet-based自适应采样机制，动态调整训练过程中不同评论类别的学习重点。

Result: 实验结果表明，该方法在保持较强检测精度的同时，显著提高了对各种对抗性评论扰动的鲁棒性。

Conclusion: 该研究提出了一种有效的对抗训练策略，可以提高虚假新闻检测模型在对抗性评论攻击下的鲁棒性。

Abstract: The spread of fake news online distorts public judgment and erodes trust in
social media platforms. Although recent fake news detection (FND) models
perform well in standard settings, they remain vulnerable to adversarial
comments-authored by real users or by large language models (LLMs)-that subtly
shift model decisions. In view of this, we first present a comprehensive
evaluation of comment attacks to existing fake news detectors and then
introduce a group-adaptive adversarial training strategy to improve the
robustness of FND models. To be specific, our approach comprises three steps:
(1) dividing adversarial comments into three psychologically grounded
categories: perceptual, cognitive, and societal; (2) generating diverse,
category-specific attacks via LLMs to enhance adversarial training; and (3)
applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting
Mechanism) that dynamically adjusts the learning focus across different comment
categories during training. Experiments on benchmark datasets show that our
method maintains strong detection accuracy while substantially increasing
robustness to a wide range of adversarial comment perturbations.

</details>


### [184] [High-Power Training Data Identification with Provable Statistical Guarantees](https://arxiv.org/abs/2510.09717)
*Zhenlong Liu,Hao Zeng,Weiran Huang,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出了一种新的训练数据识别方法，能够在严格控制错误发现率（FDR）的同时，显著提高识别能力。


<details>
  <summary>Details</summary>
Motivation: 在大规模模型中识别训练数据对于版权诉讼、隐私审计和确保公平评估至关重要。传统方法将其视为简单的二元分类任务，缺乏统计保证。现有方法依赖于容易被违反的强假设。

Method: 提出了一种名为“可证明的训练数据识别”（PTDI）的严格方法，该方法使用一组已知的未见数据计算每个数据点的p值，并构建测试集数据使用比例的保守估计器，从而缩放这些p值。然后，通过识别所有缩放后的p值低于数据相关阈值的点来选择最终的训练数据集。

Result: 在各种模型（LLM和VLM）和数据集上的大量实验表明，PTDI严格控制了FDR，并实现了更高的能力。

Conclusion: PTDI方法能够在可证明的、严格的FDR控制下发现训练数据，并显著提高识别能力。

Abstract: Identifying training data within large-scale models is critical for copyright
litigation, privacy auditing, and ensuring fair evaluation. The conventional
approaches treat it as a simple binary classification task without statistical
guarantees. A recent approach is designed to control the false discovery rate
(FDR), but its guarantees rely on strong, easily violated assumptions. In this
paper, we introduce Provable Training Data Identification (PTDI), a rigorous
method that identifies a set of training data with strict false discovery rate
(FDR) control. Specifically, our method computes p-values for each data point
using a set of known unseen data, and then constructs a conservative estimator
for the data usage proportion of the test set, which allows us to scale these
p-values. Our approach then selects the final set of training data by
identifying all points whose scaled p-values fall below a data-dependent
threshold. This entire procedure enables the discovery of training data with
provable, strict FDR control and significantly boosted power. Extensive
experiments across a wide range of models (LLMs and VLMs), and datasets
demonstrate that PTDI strictly controls the FDR and achieves higher power.

</details>


### [185] [Federated k-Means via Generalized Total Variation Minimization](https://arxiv.org/abs/2510.09718)
*A. Jung*

Main category: cs.LG

TL;DR: 本文提出了一种联邦k-means聚类算法，旨在保护本地数据隐私的同时，对分散的数据集进行联合聚类。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中，设备需要在不共享本地数据的情况下，对整体数据集进行聚类的问题。重点是基于k-means原则的硬聚类。

Method: 将联邦k-means聚类问题转化为GTVMin的实例。每个设备通过解决一个改进的本地k-means问题来更新本地聚类中心，该改进包括增加一个惩罚项来衡量相邻设备聚类中心之间的差异。

Result: 提出了一个联邦k-means算法，该算法是隐私友好的，因为它只需要在互连设备之间共享聚合信息。

Conclusion: 提出了一种在保护隐私的同时进行联邦k-means聚类的有效方法。

Abstract: We consider the problem of federated clustering, where interconnected devices
have access to private local datasets and need to jointly cluster the overall
dataset without sharing their local dataset. Our focus is on hard clustering
based on the k-means principle. We formulate federated k-means clustering as an
instance of GTVMin. This formulation naturally lends to a federated k-means
algorithm where each device updates local cluster centroids by solving a
modified local k-means problem. The modification involves adding a penalty term
to measure the discrepancy between the cluster centroid of neighbouring
devices. Our federated k-means algorithm is privacy-friendly as it only
requires sharing aggregated information among interconnected devices.

</details>


### [186] [ICL-Router: In-Context Learned Model Representations for LLM Routing](https://arxiv.org/abs/2510.09719)
*Chenxu Wang,Hao Li,Yiqun Zhang,Linyao Chen,Jianhao Chen,Ping Jian,Peng Ye,Qiaosheng Zhang,Shuyue Hu*

Main category: cs.LG

TL;DR: 提出了一种新的模型路由方法，该方法使用上下文向量来表示模型的能力，从而可以在不重新训练路由器的情况下无缝集成新模型。


<details>
  <summary>Details</summary>
Motivation: 模型路由可以利用大型语言模型 (LLM) 的互补优势，但是路由性能依赖于准确的模型表示，并且添加新模型通常需要重新训练，从而限制了可扩展性。

Method: 该方法分两个阶段进行。首先，查询被嵌入并投影到向量中，投影仪和基于 LLM 的路由器经过训练以重建原始查询，从而使向量表示与路由器的语义空间对齐。其次，在查询集上分析每个候选模型，并且路由器学习——基于查询和模型性能的上下文向量——预测每个模型是否可以正确回答新查询。

Result: 大量实验表明，我们的方法在同分布和异分布任务中都实现了最先进的路由性能。此外，我们的方法允许无缝集成新模型，而无需重新训练路由器。

Conclusion: 该论文提出了一种有效的模型路由方法，可以在不重新训练的情况下集成新模型，并且在各种任务中都表现出色。

Abstract: Large language models (LLMs) often exhibit complementary strengths. Model
routing harnesses these strengths by dynamically directing each query to the
most suitable model, given a candidate model pool. However, routing performance
relies on accurate model representations, and adding new models typically
requires retraining, limiting scalability. To address these challenges, we
propose a novel routing method using in-context vectors to represent model
capabilities. The method proceeds in two stages. First, queries are embedded
and projected into vectors, with a projector and LLM-based router trained to
reconstruct the original queries, aligning vector representations with the
router's semantic space. Second, each candidate model is profiled on a query
set, and the router learns -- based on in-context vectors of query and model
performance -- to predict whether each model can correctly answer new queries.
Extensive experiments demonstrate that our method achieves state-of-the-art
routing performance in both in-distribution and out-of-distribution tasks.
Moreover, our method allows for seamless integration of new models without
retraining the router. The code is available at
https://github.com/lalalamdbf/ICL-Router.

</details>


### [187] [It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning](https://arxiv.org/abs/2510.09723)
*Gregory D. Baker*

Main category: cs.LG

TL;DR: 提出了一种名为“叙事学习”的方法，该方法完全用自然语言定义模型，并使用解释性提示而不是传统的数值优化来迭代地改进其分类标准。


<details>
  <summary>Details</summary>
Motivation: 探索使用自然语言定义和改进模型的方法，而非传统的数值优化。

Method: 使用解释性提示迭代改进模型的分类标准，并使用3个合成数据集和3个自然数据集进行实验评估。

Result: 在6个数据集中的5个上，叙事学习比基线可解释模型更准确。

Conclusion: 叙事学习是一种有潜力的方法，其准确性优于基线可解释模型。

Abstract: In this paper, we introduce Narrative Learning, a methodology where models
are defined entirely in natural language and iteratively refine their
classification criteria using explanatory prompts rather than traditional
numerical optimisation. We report on experiments to evaluate the accuracy and
potential of this approach using 3 synthetic and 3 natural datasets and compare
them against 7 baseline explainable machine learning models. We demonstrate
that on 5 out of 6 of these datasets, Narrative Learning became more accurate
than the baseline explainable models in 2025 or earlier because of improvements
in language models. We also report on trends in the lexicostatistics of these
models' outputs as a proxy for the comprehensibility of the explanations.

</details>


### [188] [Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction](https://arxiv.org/abs/2510.09732)
*P. van Oerle,R. H. Bemthuis,F. A. Bukhsh*

Main category: cs.LG

TL;DR: 研究了使用大型语言模型（LLM）根据事件日志生成的流程模型文本解释。通过逐步缩减行为输入，在保证解释质量的前提下，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 从大型行为抽象（例如直接跟随图或Petri网）生成解释的计算成本可能很高。

Method: 该研究通过逐步缩小固定日志的前缀来发现模型，然后提示LLM生成解释，并使用第二个LLM评估完整性、瓶颈识别和改进建议。

Result: 在合成日志上，解释质量在适度缩减的情况下基本保持不变，这表明在实际应用中可以实现成本与质量的平衡。

Conclusion: 结果表明，在资源受限的环境中，可以利用LLM辅助进行更高效的流程分析。

Abstract: Large Language Models (LLMs) are increasingly used to generate textual
explanations of process models discovered from event logs. Producing
explanations from large behavioral abstractions (e.g., directly-follows graphs
or Petri nets) can be computationally expensive. This paper reports an
exploratory evaluation of explanation quality under progressive
behavioral-input reduction, where models are discovered from progressively
smaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple
input sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a
second LLM to assess completeness, bottleneck identification, and suggested
improvements. On synthetic logs, explanation quality is largely preserved under
moderate reduction, indicating a practical cost-quality trade-off. The study is
exploratory, as the scores are LLM-based (comparative signals rather than
ground truth) and the data are synthetic. The results suggest a path toward
more computationally efficient, LLM-assisted process analysis in
resource-constrained settings.

</details>


### [189] [ARROW: An Adaptive Rollout and Routing Method for Global Weather Forecasting](https://arxiv.org/abs/2510.09734)
*Jindong Tian,Yifei Ding,Ronghui Xu,Hao Miao,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: ARROW: 一种用于全球天气预报的自适应 rollout 多尺度时间路由方法，通过多区间预测模型和基于强化学习的自适应 rollout 调度器，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的预测方法无法充分模拟全球天气系统中的空间和多尺度时间依赖性，并且 rollout 策略难以平衡误差累积与精细大气变化的捕获。

Method: 构建了一个多区间预测模型，该模型通过共享-私有混合专家捕获不同时间尺度的共享模式和特定特征，并采用环形位置编码来准确编码地球的圆形纬度结构。此外，还开发了一个基于强化学习的自适应 rollout 调度器，用于根据当前天气状态选择最合适的时间间隔进行预测。

Result: ARROW 在全球天气预报中取得了最先进的性能。

Conclusion: ARROW 建立了一个有前景的全球天气预报范例。

Abstract: Weather forecasting is a fundamental task in spatiotemporal data analysis,
with broad applications across a wide range of domains. Existing data-driven
forecasting methods typically model atmospheric dynamics over a fixed short
time interval (e.g., 6 hours) and rely on naive autoregression-based rollout
for long-term forecasting (e.g., 138 hours). However, this paradigm suffers
from two key limitations: (1) it often inadequately models the spatial and
multi-scale temporal dependencies inherent in global weather systems, and (2)
the rollout strategy struggles to balance error accumulation with the capture
of fine-grained atmospheric variations. In this study, we propose ARROW, an
Adaptive-Rollout Multi-scale temporal Routing method for Global Weather
Forecasting. To contend with the first limitation, we construct a
multi-interval forecasting model that forecasts weather across different time
intervals. Within the model, the Shared-Private Mixture-of-Experts captures
both shared patterns and specific characteristics of atmospheric dynamics
across different time scales, while Ring Positional Encoding accurately encodes
the circular latitude structure of the Earth when representing spatial
information. For the second limitation, we develop an adaptive rollout
scheduler based on reinforcement learning, which selects the most suitable time
interval to forecast according to the current weather state. Experimental
results demonstrate that ARROW achieves state-of-the-art performance in global
weather forecasting, establishing a promising paradigm in this field.

</details>


### [190] [Hierarchical LoRA MoE for Efficient CTR Model Scaling](https://arxiv.org/abs/2510.10432)
*Zhichen Zeng,Mengyue Hang,Xiaolong Liu,Xiaoyi Liu,Xiao Lin,Ruizhong Qiu,Tianxin Wei,Zhining Liu,Siyang Yuan,Chaofei Yang,Yiqun Liu,Hang Yin,Jiyan Yang,Hanghang Tong*

Main category: cs.LG

TL;DR: 提出了一种名为HiLoMoE的框架，它结合了垂直和水平扩展的优势，以实现参数高效的方式进行整体扩展。


<details>
  <summary>Details</summary>
Motivation: 为了突破投资回报率（ROI）的界限。

Method: HiLoMoE采用轻量级的rank-1专家进行参数高效的水平扩展，并堆叠具有分层路由的多个MoE层，以实现组合多样性的专家组合。与传统的堆叠不同，HiLoMoE基于先前的层得分而不是输出进行路由，允许所有层并行执行。一个有原则的三阶段训练框架确保了稳定的优化和专家多样性。

Result: 在四个公共数据集上的实验表明，与非MoE基线相比，HiLoMoE实现了更好的性能-效率权衡，平均AUC提高了0.20％，FLOPs减少了18.5％。

Conclusion: HiLoMoE框架能够以参数高效的方式进行整体扩展，并在CTR预测任务中实现了更好的性能-效率权衡。

Abstract: Deep models have driven significant advances in click-through rate (CTR)
prediction. While vertical scaling via layer stacking improves model
expressiveness, the layer-by-layer sequential computation poses challenges to
efficient scaling. Conversely, horizontal scaling through Mixture of Experts
(MoE) achieves efficient scaling by activating a small subset of experts in
parallel, but flat MoE layers may struggle to capture the hierarchical
structure inherent in recommendation tasks. To push the Return-On-Investment
(ROI) boundary, we explore the complementary strengths of both directions and
propose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic
scaling in a parameter-efficient manner. Specifically, HiLoMoE employs
lightweight rank-1 experts for parameter-efficient horizontal scaling, and
stacks multiple MoE layers with hierarchical routing to enable combinatorially
diverse expert compositions. Unlike conventional stacking, HiLoMoE routes based
on prior layer scores rather than outputs, allowing all layers to execute in
parallel. A principled three-stage training framework ensures stable
optimization and expert diversity. Experiments on four public datasets show
that HiLoMoE achieving better performance-efficiency tradeoff, achieving an
average AUC improvement of 0.20\% in AUC and 18.5\% reduction in FLOPs compared
to the non-MoE baseline.

</details>


### [191] [InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models](https://arxiv.org/abs/2510.09735)
*Qianyou Sun,Jiexin Zheng,Bohan Jin,Lihua Chen,Yijie Peng*

Main category: cs.LG

TL;DR: 提出InterCorpRel-LLM，一个结合GNN和LLM的跨模态框架，用于识别公司间关系。


<details>
  <summary>Details</summary>
Motivation: 识别公司间的供应和竞争关系对于财务分析和公司治理至关重要，但由于公司数据的规模、稀疏性和上下文依赖性，这仍然具有挑战性。

Method: 该方法结合了GNN和LLM，并使用从FactSet供应链记录中获得的专有数据集以及三个定制的训练任务：公司图匹配、行业分类和供应关系预测。

Result: InterCorpRel-LLM在供应关系识别任务上显著优于包括GPT-5在内的强大基线，实现了0.8543的F-score，而GPT-5仅为0.2287。该模型还可以推广到零样本竞争者识别。

Conclusion: 该框架为分析师和战略家提供了一个强大的工具，用于绘制和推理复杂的公司网络，从而加强动态市场中的决策和风险管理。

Abstract: Identifying inter-firm relationships such as supply and competitive ties is
critical for financial analysis and corporate governance, yet remains
challenging due to the scale, sparsity, and contextual dependence of corporate
data. Graph-based methods capture structure but miss semantic depth, while
large language models (LLMs) excel at text but remain limited in their ability
to represent relational dependencies. To address this, we propose
InterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,
supported by a proprietary dataset derived from FactSet supply chain records
and three tailored training tasks: company graph matching, industry
classification, and supply relation prediction. This design enables effective
joint modeling of structure and semantics. Experiments show that
InterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,
on a supply relation identification task, achieving an F-score of 0.8543 vs.
0.2287 with only a 7B-parameter backbone and lightweight training. The model
also generalizes to zero-shot competitor identification, underscoring its
ability to capture nuanced inter-firm dynamics. Our framework thus provides
analysts and strategists with a robust tool for mapping and reasoning about
complex corporate networks, enhancing decision-making and risk management in
dynamic markets.

</details>


### [192] [Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings](https://arxiv.org/abs/2510.09739)
*Ayoub Bouguettaya,Elizabeth M. Stuart*

Main category: cs.LG

TL;DR: 这篇论文构建了一个基于机器学习的人格模型，并将其与大五人格模型进行了比较。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验词汇假设，即人格特质编码在语言中。

Method: 使用机器学习方法，从经典形容词列表中构建了一个自下而上的人格模型，并通过分析一百万条Reddit评论，将其描述效用与大五人格模型进行比较。

Result: 研究结果表明，大五人格模型（特别是宜人性、尽责性和神经质）能够更强大、更易于理解地描述这些在线社区。相比之下，机器学习聚类没有提供有意义的区分，未能恢复外向性特质，并且缺乏大五人格的心理测量一致性。

Conclusion: 研究结果证实了大五人格模型的稳健性，并表明人格的语义结构是上下文相关的。机器学习可以帮助检查已建立的心理学理论的生态有效性，但可能无法取代它们。

Abstract: The lexical hypothesis posits that personality traits are encoded in language
and is foundational to models like the Big Five. We created a bottom-up
personality model from a classic adjective list using machine learning and
compared its descriptive utility against the Big Five by analyzing one million
Reddit comments. The Big Five, particularly Agreeableness, Conscientiousness,
and Neuroticism, provided a far more powerful and interpretable description of
these online communities. In contrast, our machine-learning clusters provided
no meaningful distinctions, failed to recover the Extraversion trait, and
lacked the psychometric coherence of the Big Five. These results affirm the
robustness of the Big Five and suggest personality's semantic structure is
context-dependent. Our findings show that while machine learning can help check
the ecological validity of established psychological theories, it may not be
able to replace them.

</details>


### [193] [Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry](https://arxiv.org/abs/2510.09740)
*Atharv Goel,Sharat Agarwal,Saket Anand,Chetan Arora*

Main category: cs.LG

TL;DR: 提出了一种名为NCAL-R的可靠主动学习框架，通过利用深度网络的几何规律来减少噪声标签和数据分布偏移对主动学习的影响。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习方法在标签噪声或数据分布偏移的情况下，会放大错误，选择错误标记或冗余样本。因此，需要一种更可靠的主动学习方法。

Method: NCAL-R框架引入了两个互补的信号：类均值对齐扰动分数和特征波动分数。前者量化了候选样本在结构上稳定或扭曲类间几何结构的能力，后者捕捉了表征在训练检查点上的时间不稳定性。通过结合这些信号，NCAL-R能够优先选择既能保持类分离又能突出模糊区域的样本。

Result: 在ImageNet-100和CIFAR100上的实验表明，NCAL-R始终优于标准主动学习基线，以更少的标签实现了更高的准确率，在合成标签噪声下提高了鲁棒性，并增强了对分布外数据的泛化能力。

Conclusion: 将几何可靠性标准纳入获取决策可以使主动学习对标注错误和分布偏移的敏感性降低，这是在现实世界标注管道中实现可信部署的关键一步。

Abstract: Active Learning (AL) promises to reduce annotation cost by prioritizing
informative samples, yet its reliability is undermined when labels are noisy or
when the data distribution shifts. In practice, annotators make mistakes, rare
categories are ambiguous, and conventional AL heuristics (uncertainty,
diversity) often amplify such errors by repeatedly selecting mislabeled or
redundant samples. We propose Reliable Active Learning via Neural Collapse
Geometry (NCAL-R), a framework that leverages the emergent geometric
regularities of deep networks to counteract unreliable supervision. Our method
introduces two complementary signals: (i) a Class-Mean Alignment Perturbation
score, which quantifies how candidate samples structurally stabilize or distort
inter-class geometry, and (ii) a Feature Fluctuation score, which captures
temporal instability of representations across training checkpoints. By
combining these signals, NCAL-R prioritizes samples that both preserve class
separation and highlight ambiguous regions, mitigating the effect of noisy or
redundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R
consistently outperforms standard AL baselines, achieving higher accuracy with
fewer labels, improved robustness under synthetic label noise, and stronger
generalization to out-of-distribution data. These results suggest that
incorporating geometric reliability criteria into acquisition decisions can
make Active Learning less brittle to annotation errors and distribution shifts,
a key step toward trustworthy deployment in real-world labeling pipelines. Our
code is available at https://github.com/Vision-IIITD/NCAL.

</details>


### [194] [Patentformer: A demonstration of AI-assisted automated patent drafting](https://arxiv.org/abs/2510.09752)
*Sai Krishna Reddy Mudhiganti,Juanyan Wang,Ruo Yang,Manali Sharma*

Main category: cs.LG

TL;DR: 本文介绍了一种名为 Patentformer 的人工智能专利撰写平台，旨在通过快速生成符合法律写作标准的高质量专利申请来支持专利律师。


<details>
  <summary>Details</summary>
Motivation: 专利撰写依赖于专利律师的丰富经验和专业知识，他们必须同时具备法律敏锐性和对发明的技术理解，才能以正式的法律写作风格撰写专利申请。

Method: 展示 Patentformer 平台

Result: 快速生成高质量专利申请

Conclusion: AI 驱动的自动化专利撰写平台可以支持专利律师

Abstract: Patent drafting presents significant challenges due to its reliance on the
extensive experience and specialized expertise of patent attorneys, who must
possess both legal acumen and technical understanding of an invention to craft
patent applications in a formal legal writing style. This paper presents a
demonstration of Patentformer, an AI-powered automated patent drafting platform
designed to support patent attorneys by rapidly producing high-quality patent
applications adhering to legal writing standards.

</details>


### [195] [ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces](https://arxiv.org/abs/2510.11168)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.LG

TL;DR: 提出了一种名为ELMO的纯低精度训练框架，用于训练具有数百万标签的超大规模多标签分类（XMC）模型。


<details>
  <summary>Details</summary>
Motivation: 当前XMC方法主要依赖FP16-FP32混合精度训练，这可能不稳定且效率低下。现有的低精度方法通常为分类层保留较高精度。

Method: 利用Kahan求和和随机舍入，在Float8数据类型中有效训练XMC模型，无需单精度master权重或张量缩放。结合梯度融合和分块的内存优化。

Result: 使用ELMO，仅用6.6 GiB的GPU内存即可训练一个具有300万标签的XMC模型，而优化的SOTA方法Renee需要39.7 GiB，且不影响准确性。

Conclusion: ELMO框架通过纯低精度训练和内存优化，显著降低了XMC模型的GPU内存使用量，同时保持了模型精度。

Abstract: Large output spaces, also referred to as Extreme multilabel classification
(XMC), is a setting that arises, e.g., in large-scale tagging and
product-to-product recommendation, and is characterized by the number of labels
ranging from hundreds of thousands to millions. This means that the linear
classification head, usually only a tiny fraction of the overall model, turns
into the main driver for compute and memory demand. Current state-of-the-art
XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we
show can be unstable, and inefficient in terms of memory usage and
computational overhead. Meanwhile, existing low-precision methods typically
retain higher precision for the classification layer. In this work, we propose
ELMO, a pure low-precision training framework for XMC models using BFloat16 and
Float8 data types. By leveraging Kahan summation and stochastic rounding, we
demonstrate that XMC models can be effectively trained entirely in Float8,
without relying on single-precision master weights or tensor scaling.
Low-precision training, combined with our proposed memory optimizations --
gradient fusion and chunking -- enables significant reductions in GPU memory
usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of
GPU memory, compared to the 39.7 GiB required by the optimized SOTA method,
Renee without compromising accuracy.

</details>


### [196] [PatentVision: A multimodal method for drafting patent applications](https://arxiv.org/abs/2510.09762)
*Ruo Yang,Sai Krishna Reddy Mudhiganti,Manali Sharma*

Main category: cs.LG

TL;DR: 本研究提出了一种名为PatentVision的多模态框架，利用大型视觉语言模型（LVLMs）结合文本和视觉输入，自动生成完整的专利说明书。


<details>
  <summary>Details</summary>
Motivation: 专利撰写因需要详细的技术描述、法律合规性和视觉元素而非常复杂，而大型视觉语言模型在自动化专利写作方面的应用尚未被充分探索。

Method: 构建于先进的LVLMs之上，通过结合微调的视觉语言模型与针对专利的领域特定训练，PatentVision提高了准确性。

Result: 实验表明，PatentVision超越了仅使用文本的方法，生成了与人工撰写标准相比具有更高保真度和一致性的输出。它对视觉数据的整合使其能够更好地表示复杂的设计特征和功能连接，从而产生更丰富和更精确的结果。

Conclusion: 这项研究强调了多模态技术在专利自动化中的价值，提供了一个可扩展的工具，以减少手动工作量并提高一致性。PatentVision不仅推进了专利起草，还为LVLMs在专门领域的更广泛使用奠定了基础，有可能改变知识产权管理和创新流程。

Abstract: Patent drafting is complex due to its need for detailed technical
descriptions, legal compliance, and visual elements. Although Large Vision
Language Models (LVLMs) show promise across various tasks, their application in
automating patent writing remains underexplored. In this paper, we present
PatentVision, a multimodal framework that integrates textual and visual inputs
such as patent claims and drawings to generate complete patent specifications.
Built on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned
vision language models with domain specific training tailored to patents.
Experiments reveal it surpasses text only methods, producing outputs with
greater fidelity and alignment with human written standards. Its incorporation
of visual data allows it to better represent intricate design features and
functional connections, leading to richer and more precise results. This study
underscores the value of multimodal techniques in patent automation, providing
a scalable tool to reduce manual workloads and improve consistency.
PatentVision not only advances patent drafting but also lays the groundwork for
broader use of LVLMs in specialized areas, potentially transforming
intellectual property management and innovation processes.

</details>


### [197] [Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model](https://arxiv.org/abs/2510.09764)
*Wanting Mao,Maxwell A Xu,Harish Haresamudram,Mithun Saha,Santosh Kumar,James Matthew Rehg*

Main category: cs.LG

TL;DR: 提出了一种新的自监督学习框架ProtoMM，用于建模多模态时间序列数据，特别是在生物信号领域。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方法依赖于CLIP式的对比目标，容易过拟合到容易对齐的特征，并将有效的跨模态关系错误分类为负样本，导致嵌入分散且不具有泛化性。

Method: 引入了一个共享原型字典，以将异构模态锚定在公共嵌入空间中。通过围绕共享原型聚类表示，而不是显式的负采样，来捕获跨模态的互补信息。

Result: 在脉搏运动基础模型的开发中，ProtoMM优于仅对比方法和先前的多模态SSL方法，实现了最先进的性能。

Conclusion: ProtoMM在多模态生物信号建模中表现出色，并提高了学习特征的可解释性。

Abstract: Modeling multi-modal time-series data is critical for capturing system-level
dynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,
and accelerometry provide complementary perspectives on interconnected
physiological processes. While recent self-supervised learning (SSL) advances
have improved unimodal representation learning, existing multi-modal approaches
often rely on CLIP-style contrastive objectives that overfit to easily aligned
features and misclassify valid cross-modal relationships as negatives,
resulting in fragmented and non-generalizable embeddings. To overcome these
limitations, we propose ProtoMM, a novel SSL framework that introduces a shared
prototype dictionary to anchor heterogeneous modalities in a common embedding
space. By clustering representations around shared prototypes rather than
explicit negative sampling, our method captures complementary information
across modalities and provides a coherent "common language" for physiological
signals. In this work, we focus on developing a Pulse Motion foundation model
with ProtoMM and demonstrate that our approach outperforms contrastive-only and
prior multimodal SSL methods, achieving state-of-the-art performance while
offering improved interpretability of learned features.

</details>
