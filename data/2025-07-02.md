<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 42]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 43]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: This paper investigates the effectiveness of text-based and multimodal LLMs on table understanding tasks through a cross-domain and cross-modality evaluation. The paper finds that while LLMs maintain robustness across table modalities, they face significant challenges when processing scientific tables.


<details>
  <summary>Details</summary>
Motivation: Although LLMs demonstrate strong performance in downstream tasks, their efficiency in processing tabular data remains underexplored.

Method: Compare the performance of text-based and multimodal LLMs on tables from scientific vs. non-scientific contexts and examine their robustness on tables represented as images vs. text. Conduct an interpretability analysis to measure context usage and input relevance. Introduce the TableEval benchmark, comprising 3017 tables from scholarly publications, Wikipedia, and financial reports, where each table is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.

Result: LLMs maintain robustness across table modalities, but face significant challenges when processing scientific tables.

Conclusion: LLMs maintain robustness across table modalities, but face challenges when processing scientific tables.

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [2] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: Prompting should be treated as behavioral science and a key component in the science of LLMs, rather than alchemy.


<details>
  <summary>Details</summary>
Motivation: Prompting is the primary method by which we study and control large language models, and it unlocked major capabilities of LLMs. However, prompting is rarely treated as science.

Method: Treating LLMs as a new kind of complex and opaque organism and probing the model in its native interface: language.

Result: Prompting is behavioral science.

Conclusion: Prompting is a key component in the science of LLMs.

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [3] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 提出LineRetriever，一种用于web导航任务的新方法，该方法利用语言模型来识别和检索与未来导航步骤最相关的观察线，从而在上下文限制内保持一致的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在网络导航任务中表现出了令人印象深刻的能力，但是网页的广泛上下文，通常表示为DOM或可访问性树（AxTree）结构，经常超过模型上下文的限制。像自下而上的截断或基于嵌入的检索之类的方法会丢失有关页面状态和操作历史记录的关键信息。这对于Web代理中的自适应计划尤其成问题，在Web代理中，理解当前状态对于确定将来的操作至关重要。

Method: 提出了一种名为LineRetriever的新方法，该方法利用语言模型来识别和检索与未来导航步骤最相关的观察线。

Result: 实验表明，LineRetriever可以减少web agent每一步的观察范围，同时在上下文限制内保持一致的性能。

Conclusion: LineRetriever可以减少web agent每一步的观察范围，同时在上下文限制内保持一致的性能。

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [4] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 利用大型语言模型（LLM）生成的推理来增强文本分类，实验表明，经过训练以输出推理和情感的生成模型，在情感预测的准确性方面取得了显着提高。


<details>
  <summary>Details</summary>
Motivation: 标准分类模型通常将输入直接映射到标签，而没有明确的推理，这可能会限制它们的性能、鲁棒性和可解释性。

Method: 使用Llama-3.2-1B-Instruct模型在通用推理数据集上进行微调，以生成文本推理（R），然后使用该模型离线创建增强的训练数据集，用于下游生成模型。

Result: 生成模型在输出推理和情感方面，与仅输出情感的基线生成模型相比，在准确率上显着提高了8.7个百分点。

Conclusion: 使用LLM生成的推理可以创建更丰富的训练数据集，从而提高各种下游NLP任务的性能，并提供明确的解释。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [5] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: LLM翻译在风格上存在偏差，RASTA方法可以通过学习风格概念来改进LLM的翻译风格，使其更好地传达文化交流规范。


<details>
  <summary>Details</summary>
Motivation: 文化差异经常导致说话者的预期风格与听者的理解风格不一致，例如，礼貌经常在翻译中丢失。

Method: RASTA（检索增强风格对齐）

Result: RASTA方法可以有效缓解LLM在翻译风格上的偏差，使其能够更好地传达文化交流规范。

Conclusion: LLMs在翻译风格上存在偏差，倾向于中立，并且在非西方语言中表现更差。使用RASTA（检索增强风格对齐）方法可以缓解这些问题，该方法利用学习到的风格概念来鼓励LLM翻译适当地传达文化交流规范并对齐风格。

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [6] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 越狱模型拒绝的信息可以通过线性探针解码，instruction-tuning只是抑制了有害信息的直接表达


<details>
  <summary>Details</summary>
Motivation: 研究通过越狱提示访问的信息在多大程度上可解码

Method: 使用在线性探针解码LM隐藏状态访问的信息

Result: 大量最初拒绝的信息可以通过线性解码；探针预测值与LM生成的成对比较相关

Conclusion: instruction-tuning并不能完全消除或重新定位表征空间中的有害信息，而只是抑制了其直接表达，使其在线性可访问且间接影响下游行为。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [7] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: COLDSELECT 是一种联合 verbalizer 和实例选择方法，可对数据多样性进行建模，从而在冷启动场景中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 基于提示的方法对模板、verbalizer 和少样本实例选择敏感，尤其是在没有标记数据的冷启动设置中。现有研究忽略了实例和 verbalizer 之间的依赖关系。

Method: COLDSELECT，一种联合 verbalizer 和实例选择方法，可对数据多样性进行建模。

Result: 在八个基准测试上的实验表明，COLDSELECT 在减少不确定性和增强泛化方面表现出色，优于 verbalizer 和少样本实例选择的基线模型。

Conclusion: COLDSELECT在减少不确定性和增强泛化方面表现出色，优于基线模型。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [8] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: We present a mathematical model of the morphology-syntax interface.


<details>
  <summary>Details</summary>
Motivation: Within the context of the mathematical formulation of Merge and the Strong Minimalist Thesis,

Method: We present a mathematical model of the morphology-syntax interface. In this setting, morphology has compositional properties responsible for word formation, organized into a magma of morphological trees. However, unlike syntax, we do not have movement within morphology. A coproduct decomposition exists, but it requires extending the set of morphological trees beyond those which are generated solely by the magma, to a larger set of possible morphological inputs to syntactic trees. These participate in the formation of morphosyntactic trees as an algebra over an operad, and a correspondence between algebras over an operad.

Result: The process of structure formation for morphosyntactic trees can then be described in terms of this operadic correspondence that pairs syntactic and morphological data and the morphology coproduct.

Conclusion: We reinterpret in this setting certain operations of Distributed Morphology as transformation that allow for flexibility in moving the boundary between syntax and morphology within the morphosyntactic objects.

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [9] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: Non-English languages can be more token-efficient for reasoning while preserving accuracy, suggesting genuine shifts in reasoning behavior.


<details>
  <summary>Details</summary>
Motivation: Most research focuses solely on English, even though many models are pretrained on multilingual data. This work investigates: Is English the most token-efficient language for reasoning?

Method: Evaluate three open-source RLMs: DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven typologically diverse languages.

Result: Reasoning in non-English languages reduces token usage, and also preserves accuracy. These gains persist even after translating the reasoning traces into English, suggesting genuine shifts in reasoning behavior rather than surface-level linguistic effects.

Conclusion: Reasoning in non-English languages reduces token usage and preserves accuracy. These gains persist even after translating the reasoning traces into English, suggesting genuine shifts in reasoning behavior rather than surface-level linguistic effects. The extent of improvement depends on the models multilingual strength.

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [10] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: privacy risks from memorization during fine-tuning are not well addressed. Compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs.


<details>
  <summary>Details</summary>
Motivation: the privacy risks arising from memorization during fine-tuning have received relatively little attention.

Method: categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs).

Result: prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale.

Conclusion: parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option.

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [11] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: This paper focuses on improving NLP for low-resource African languages by creating datasets, analyzing data quality, and adapting multilingual language models.


<details>
  <summary>Details</summary>
Motivation: Multilingual models face challenges with low-resource languages due to data noise and lack of labeled datasets for evaluation, especially for languages in Sub-Saharan Africa.

Method: The paper curates a high-quality corpus and adapts/specializes multilingual PLMs to unseen African languages using a small amount of monolingual texts. It also developed large scale human-annotated labelled datasets for 21 African languages.

Result: The quality of semantic representations learned in word embeddings depends on the quality of pre-training data. The paper demonstrates the limitations of word embeddings and the opportunities of multilingual PLMs for unseen languages and low-resource scenarios.

Conclusion: This paper addresses the under-representation of African languages in NLP by developing large scale human-annotated labelled datasets for 21 African languages in named entity recognition and machine translation tasks. The paper conducts an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [12] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: This paper studies why LMs struggle with simple syntactic tasks and introduces RASteer, a steering method to improve model performance by identifying and increasing the contribution of reliable components.


<details>
  <summary>Details</summary>
Motivation: Language models still struggle with simple syntactic tasks such as generating balanced parentheses. Errors occur when the faulty mechanisms overshadow the sound ones and dominantly affect the predictions.

Method: RASteer, a steering method to systematically identify and increase the contribution of reliable components

Result: LMs rely on a number of components (attention heads and FF neurons) that independently make their own predictions. Some components reliably promote correct answers, others are less reliable and introduce noise by promoting incorrect tokens.

Conclusion: RASteer, a steering method is introduced to identify and increase the contribution of reliable components for improving model performance. RASteer substantially improves performance on balanced parentheses tasks, boosting accuracy of some models from $0$% to around $100$%, and achieving performance gains of up to around $20$% in arithmetic reasoning tasks.

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [13] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 该论文提出了一种基于问题分解的RAG方法，用于解决多跳问题，实验结果表明该方法有效提高了检索和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 标准RAG在处理多跳问题时面临挑战，因为相关事实通常分布在多个文档中，难以检索到足够的信息。

Method: 该方法包括：(i) 使用LLM将原始查询分解为子问题；(ii) 检索每个子问题的相关段落；(iii) 重新排序合并后的候选池，以提高检索证据的覆盖率和准确性。

Result: 在MultiHop-RAG和HotpotQA数据集上，该方法在检索（MRR@10: +36.7%）和答案准确性（F1: +11.6%）方面均优于标准RAG基线。

Conclusion: 该论文提出了一种结合问题分解的RAG流水线，并通过实验证明了其在多跳问题上的有效性，在检索和答案准确性方面均优于标准RAG基线。

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [14] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: searching for an optimal unsupervised segmentation of chant melody and achieves state-of-the-art performance in mode classification


<details>
  <summary>Details</summary>
Motivation: Gregorian melodies are constructed from some vocabulary of segments. Frequent re-use of certain melodic segments has been observed in chant melodies, and the intractable number of possible segmentations allowed the option that some undiscovered segmentation exists that will yet prove the value of centonisation, and recent empirical results have shown that segmentations can outperform music-theoretical features in mode classification. Inspired by the fact that Gregorian chant was memorised

Method: search for an optimal unsupervised segmentation of chant melody using nested hierarchical Pitman-Yor language models

Result: achieves state-of-the-art performance in mode classification. find empirical evidence for the link between mode classification and memory efficiency, and observe more formulaic areas at the beginnings and ends of melodies corresponding to the practical role of modality in performance

Conclusion: even a memory-optimal segmentation is not what is understood as centonisation.

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [15] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: CAPITAL is a causal prompting framework for implicit sentiment analysis that uses front-door adjustment to improve accuracy and robustness by addressing biases in chain-of-thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing prompting-based methods for implicit sentiment analysis rely on majority voting over chain-of-thought reasoning paths without evaluating their causal validity, making them susceptible to biases and spurious correlations.

Method: The CAPITAL framework decomposes the causal effect into the influence of the input prompt on reasoning chains and the impact of chains on the final output, estimated using encoder-based clustering and NWGM approximation, with a contrastive learning objective for better alignment.

Result: CAPITAL consistently outperforms strong prompting baselines in both accuracy and robustness on benchmark ISA datasets, particularly under adversarial conditions.

Conclusion: This work introduces CAPITAL, a causal prompting framework that integrates front-door adjustment into chain-of-thought reasoning for implicit sentiment analysis. Experiments show CAPITAL outperforms strong baselines in accuracy and robustness, especially under adversarial conditions, demonstrating the benefits of causal inference for bias-aware sentiment reasoning.

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [16] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: 本研究表明，简单的监督可以显著提高语言模型与不同人群的对齐度，并提供了一个基准以供未来研究。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同人群对主观问题的回答具有重要价值。

Method: 使用相对简单的监督

Result: 结果表明，使用相对简单的监督可以大大提高语言模型与不同人群的对齐度，这是通过跨越各种主题的三个数据集来衡量的。

Conclusion: 该研究通过对多个LLM和prompt策略进行评估，并开源其工作，为一个有用的基准，以激发未来的研究。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [17] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: 该研究通过构建在公共测试集上进行微调的“作弊”模型，揭示了开放基准测试的弱点，这些模型在开放基准测试中表现出色，但实际效果不佳，强调需要重新评估当前的基准测试实践。


<details>
  <summary>Details</summary>
Motivation: 开放的大型语言模型 (LLM) 基准（例如 HELM 和 BIG-bench）提供标准化、透明的协议，有助于语言模型 (LM) 的公平比较、可重复性和迭代进步。然而，它们的开放性也带来了关键且未被充分探索的缺陷。

Method: 通过在公共测试集上直接微调 BART、T5 和 GPT-2 的较小变体来系统地构建“作弊”模型

Result: 构建的“作弊”模型在著名的开放式整体基准 (HELM) 上取得了最高的排名，尽管泛化能力较差且实用性有限。

Conclusion: 开放基准测试中的高排行榜性能可能并不总是反映真实世界的有效性；私有或动态基准测试必须补充开放评估以保障完整性；必须从根本上重新评估当前的基准测试实践，以确保稳健和可信的 LM 评估。

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [18] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 该研究提出了一种在基于RAG的对话系统中管理广告的模块化管道，并利用对抗性协同进化框架来开发更复杂的广告感知生成搜索系统和强大的广告分类器。


<details>
  <summary>Details</summary>
Motivation: 随着会话搜索引擎越来越多地采用由大型语言模型（LLM）和检索增强生成（RAG）驱动的基于生成的范例，将广告集成到生成的响应中既带来了商业机会，也给用户体验带来了挑战。与广告明确划分的传统搜索不同，生成系统模糊了信息内容和促销材料之间的界限，引发了对透明度和信任的担忧。

Method: 该研究提出了一个基于RAG的对话系统中广告管理的模块化管道，包括用于无缝广告集成的广告重写器和用于检测的鲁棒广告分类器。利用合成数据来训练高性能分类器，然后用于指导两种互补的广告集成策略：广告重写器的监督微调和best-of-N抽样方法，该方法在多个候选者中选择最不明显的广告集成响应。

Result: 实验结果表明，该研究的广告分类器在营销策略的启发下，通过课程学习增强的合成广告数据训练，实现了强大的检测性能。此外，该研究表明，通过微调和best-of-N抽样，分类器引导的优化显著提高了广告的隐蔽性，从而实现了更无缝的集成。

Conclusion: 该研究提出了一种对抗性协同进化框架，用于开发更复杂的广告感知生成搜索系统和强大的广告分类器，通过微调和best-of-N抽样，显著提高了广告的隐蔽性，从而实现了更无缝的集成。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [19] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Nirantar 是一个用于评估多语言和多领域 ASR 中持续学习的综合框架，它利用在印度收集的增量数据，可以评估语言增量 (LIL)、领域增量 (DIL) 和新的语言增量领域增量学习 (LIDIL) 场景。


<details>
  <summary>Details</summary>
Motivation: 为了反映现实世界的 CL 挑战，Nirantar 利用通过自然事件在印度 22 种语言和 208 个地区收集的增量数据。

Method: Nirantar，一个全面的框架，用于评估多语言和多领域 ASR 中的持续学习 (CL)。

Result: 通过对现有方法的评估表明，没有一种方法能始终如一地表现良好。

Conclusion: 现有方法表现不佳，需要更强大的 CL 策略。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [20] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于 Capsule 网络的语义意图建模算法，用于提高人机交互中意图识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了解决人机交互中意图识别准确率不足的问题。

Method: 提出了一种基于 Capsule 网络的语义意图建模算法。

Result: 提出的模型在准确率、F1 值和意图检测率方面优于传统方法和其他深度学习结构。

Conclusion: 提出了一种新的结构化建模方法，以提高复杂语义条件下的意图识别。

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [21] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: 本文讨论了在主题建模中确保方法论严谨性的问题，并提供了一套指南，这些指南可以帮助新手研究者和审稿人。


<details>
  <summary>Details</summary>
Motivation: 高级计算算法的兴起为计算密集型研究方法开辟了新途径，但这些算法的不透明性以及应用中缺乏透明度和严谨性带来了方法论上的挑战，可能会削弱对研究的信任。关于这种新研究类型中方法论严谨性的讨论仍在出现。

Method: 通过说明结构主题建模算法的应用并提出一套指南

Result: 讨论了如何确保主题建模研究的严谨性，并提出了一套指南

Conclusion: 这篇论文为确保主题建模研究的严谨性提供了一系列指南，这些指南也可以通过调整应用于其他算法。这些指南对新手研究者以及处理主题建模手稿的编辑和审阅者尤其有帮助。该研究为主题建模文献做出了贡献，并加入了关于计算密集型理论构建研究中方法论严谨性的新兴对话。

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [22] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: Proposes a multilingual hallucination identifier combining retrieval-based fact verification and a BERT-based system, achieving competitive results across multiple languages.


<details>
  <summary>Details</summary>
Motivation: Hallucinations are one of the major problems of LLMs, hindering their trustworthiness and deployment to wider use cases. However, most of the research on hallucinations focuses on English data, neglecting the multilingual nature of LLMs.

Method: a two-part pipeline that combines retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned to identify common hallucination patterns

Result: achieves competitive results across all languages, reaching top-10 results in eight languages, including English. Moreover, it supports multiple languages beyond the fourteen covered by the shared task

Conclusion: This multilingual hallucination identifier can help to improve LLM outputs and their usefulness in the future.

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [23] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: A unified framework combining knowledge transfer and parameter-efficient fine-tuning enhances LLM adaptation in low-resource languages, achieving superior performance and stability.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios.

Method: This paper proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies, introducing knowledge alignment loss and soft prompt tuning.

Result: The proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X, demonstrating particularly strong advantages under extremely data-scarce conditions.

Conclusion: The proposed method enhances task-specific adaptability while preserving the general capabilities of large language models, making it well-suited for complex semantic modeling and multilingual processing tasks.

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [24] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: MoR is a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering, which significantly enhances performance and eliminates the need for task-specific prompts.


<details>
  <summary>Details</summary>
Motivation: LLMs' reliance on manually crafted, task-specific prompts limits adaptability and efficiency.

Method: We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.

Result: MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines.

Conclusion: MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [25] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: This paper introduces SAFER, a framework using sparse autoencoders to interpret and improve reward models in RLHF, enhancing safety alignment in LLMs with minimal data modification.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque.

Method: We present sparse Autoencoder For Enhanced Reward model (SAFER), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. Using these feature-level signals, we design targeted data poisoning and denoising strategies.

Result: We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses.

Conclusion: SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks.

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [26] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: VLMs trained on different languages exhibit culturally grounded attentional patterns.


<details>
  <summary>Details</summary>
Motivation: investigate whether Vision-Language Models (VLMs) trained predominantly on different languages, specifically Japanese and English, exhibit similar culturally grounded attentional patterns.

Method: comparative analysis of image descriptions

Result: VLMs not only internalize the structural properties of language but also reproduce cultural behaviors embedded in the training data

Conclusion: VLMs reproduce cultural behaviors embedded in the training data, indicating that cultural cognition may implicitly shape model outputs.

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [27] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: LLMs can generate financial reports from time series data using prompt engineering and model selection.


<details>
  <summary>Details</summary>
Motivation: Explore the potential of large language models (LLMs) to generate financial reports from time series data.

Method: A framework encompassing prompt engineering, model selection, and evaluation with an automated highlighting system.

Result: Experiments utilizing both data from the real stock market indices and synthetic time series demonstrate the capability of LLMs to produce coherent and informative financial reports.

Conclusion: LLMs can produce coherent and informative financial reports from time series data.

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [28] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: LitBench是一个用于创造性写作评估的基准，结果表明，训练后的奖励模型比直接使用大型语言模型更好，并且与人类的判断更一致。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）生成的创造性写作仍然具有挑战性，因为开放式叙述缺乏基本事实。在没有高性能的自动评估方法的情况下，现成的（OTS）语言模型被用作zero-shot评判器，但它们在这种上下文中的可靠性尚不清楚。为了追求对创造性写作的稳健评估。

Method: 引入了LitBench，这是一个用于创造性写作验证的标准基准和配对数据集，包含来自Reddit的2,480个去偏、人工标记的故事比较的保留测试集和一个包含43,827对人类偏好标签的训练语料库。使用LitBench，我们对zero-shot LLM评判器进行基准测试，训练Bradley Terry和生成奖励模型，并进行在线人工研究，以验证新生成的LLM故事的奖励模型排名。

Result: Claude-3.7-Sonnet是最强大的现成评判器，与人类偏好达成73%的协议；在训练有素的奖励模型中，Bradley-Terry和生成奖励模型都达到了78%的准确率，优于所有现成的评判器。在线人工研究进一步证实，我们训练有素的奖励模型在新生成的LLM故事中始终与人类偏好保持一致。

Conclusion: 训练后的奖励模型在创造性写作评估方面优于现成的LLM评判器，并且与人类偏好一致。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [29] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: 本文研究了一种自然语言语义的功能编程方法，该方法允许我们提高更传统的外延风格的表现力。


<details>
  <summary>Details</summary>
Motivation: 研究自然语言语义的功能编程方法，允许我们提高更传统的外延风格的表现力

Method: 形式化了一个基于范畴的类型和效果系统，并构造了一个图解微积分来建模解析和处理效果

Result: 句子的外延被有效地计算出来

Conclusion: 模型用于有效地计算句子的外延。

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [30] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: This paper reviews the use of GenAI in scientometrics, finding promise in language generation tasks but limitations in tasks requiring stable semantics, and discusses the potential impact of GenAI on measuring science.


<details>
  <summary>Details</summary>
Motivation: review the use of GenAI in scientometrics, and to begin a debate on the broader implications for the field. inquire whether, by generating large amounts of scientific language, GenAI might have a fundamental impact on our field by affecting textual characteristics used to measure science, such as authors, words, and references.

Method: critical engagement with recent experiments using GenAI in scientometrics, including topic labelling, the analysis of citation contexts, predictive applications, scholars' profiling, and research assessment. systematically compare the performance of different GenAI models for specific tasks.

Result: GenAI shows promise in tasks where language generation dominates, such as labelling, but faces limitations in tasks that require stable semantics, pragmatic reasoning, or structured domain knowledge. However, these results might become quickly outdated.

Conclusion: GenAI shows promise in tasks where language generation dominates, but faces limitations in tasks that require stable semantics, pragmatic reasoning, or structured domain knowledge. Careful empirical work and theoretical reflection will be essential to remain capable of interpreting the evolving patterns of knowledge production.

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [31] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: LLM 群体在道德困境中表现出功利主义倾向，但其机制与人类不同。


<details>
  <summary>Details</summary>
Motivation: 理解 LLM 在协作过程中的集体功能至关重要，类似于人类道德判断中群体审议如何导致功利主义提升的现象。

Method: 通过在两种条件下测试六个模型来研究多智能体 LLM 系统中的道德判断：(1) 单独推理；(2) 成对或成三地进行多轮讨论。

Result: 在个人道德困境中，所有模型在群体中比单独行动时更容易接受道德违规行为。一些模型认可最大化整体福祉的行为，即使这些行为使陌生人受益超过了熟悉的人。其他模型则更愿意在群体中违反道德规范。与人类群体不同，LLM 群体表现出规范敏感性降低或公正性增强。

Conclusion: LLM 集体在道德困境中表现出功利主义倾向，但其机制与人类不同，这对于 AI 对齐、多智能体设计和人工道德推理具有重要意义。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [32] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: 我们设计了一个可扩展的人工评估协议和一个相应的自动近似，发现最佳的LLM代理在统计学上与人工注释员没有区别，因此可以在自动评估中可以作为合理的替代。


<details>
  <summary>Details</summary>
Motivation: 主题模型和文档聚类评估要么使用与人类偏好不符的自动指标，要么需要难以扩展的专家标签。

Method: 我们设计了一个可扩展的人工评估协议和一个相应的自动近似，反映了从业者对模型的实际使用。

Result: 我们收集了大量人群工作者对两个数据集上各种主题模型输出的注释。然后，我们使用这些注释来验证自动代理，

Conclusion: 最佳的LLM代理在统计学上与人工注释员没有区别，因此可以在自动评估中作为合理的替代。

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [33] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: This paper uses stylometry to distinguish between texts generated by LLMs and humans, achieving high accuracy in classification tasks.


<details>
  <summary>Details</summary>
Motivation: addressing issues of model attribution, intellectual property, and ethical AI use

Method: applying stylometry and tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based stylometric features

Result: cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1 in binary classification

Conclusion: it is possible to distinguish machine- from human-generated texts at least for a well-defined text type

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [34] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: TransLaw, a multi-agent framework, translates Hong Kong legal judgments with high accuracy and cost reduction, outperforming GPT-4o in some aspects.


<details>
  <summary>Details</summary>
Motivation: The potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures.

Method: A novel multi-agent framework called TransLaw, employing three specialized agents: Translator, Annotator, and Proofreader.

Result: The framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services.

Conclusion: The TransLaw framework surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, but trails human experts in contextualizing complex terminology and stylistic naturalness.

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [35] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: Culturally adapted variants of the GSM8K test set reveal a consistent performance gap.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like GSM8K are predominantly rooted in Western norms, including names, currencies, and everyday scenarios.

Method: We create culturally adapted variants of the GSM8K test set for five regions Africa, India, China, Korea, and Japan using prompt-based transformations followed by manual verification. We evaluate six large language models (LLMs), ranging from 8B to 72B parameters, across five prompting strategies

Result: reveal a consistent performance gap

Conclusion: LLMs perform best on the original US-centric dataset and comparatively worse on culturally adapted versions. Models with reasoning capabilities are more resilient to these shifts.

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [36] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 下游缩放规律并不总是有效，需要进一步研究其适用条件。


<details>
  <summary>Details</summary>
Motivation: 下游缩放规律旨在从小规模的预训练损失预测更大规模的任务表现。但这种预测是否可行尚不清楚，下游缩放规律面临涌现和逆缩放等根本性挑战。

Method: 对现有下游缩放规律数据进行荟萃分析

Result: 只有 39% 的情况下与线性缩放规律密切吻合，实验环境的微小变化会完全改变缩放趋势。

Conclusion: 线性缩放规律并不总是成立，实验环境的微小变化会完全改变缩放趋势。我们需要了解缩放规律成功的条件，并接受缩放行为偏离线性趋势的情况。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [37] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 提出了MemeCMD，一个自动生成的中文多轮对话数据集，其中包含上下文检索的meme。


<details>
  <summary>Details</summary>
Motivation: 现有的对话数据集主要局限于人工注释或纯文本对话，缺乏多模态交互所提供的表达性和上下文细微差别。

Method: 引入检索框架和自适应阈值，以确保上下文相关、自然间隔的meme使用。通过双重代理在不同的场景中自动生成对话，并结合大规模的、MLLM注释的meme库。

Result: 实验证明了该方法在生成上下文适当和多样化的、包含meme的对话方面的有效性。

Conclusion: 提出了一种生成上下文相关的、多样化的、包含meme的对话的方法，为推进多模态对话式AI提供可扩展和保护隐私的资源。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [38] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: Current computational phylogenetic methods needing large datasets can't be used on cognate data because automatically generated datasets are unreliable, questioning the applicability of these methods in historical linguistics.


<details>
  <summary>Details</summary>
Motivation: To leverage computational phylogenetic methods for cognate data, larger datasets are needed, but no feasible approach exists to automatically generate them.

Method: Automatically extracting datasets from BabelNet and performing phylogenetic inferences on the resulting character matrices.

Result: Phylogenetic inferences on character matrices extracted from BabelNet yield trees inconsistent with established ground truth trees.

Conclusion: Phylogenetic data analysis approaches requiring larger datasets cannot be applied to cognate data, leaving open the question of how computational approaches can be applied in historical linguistics.

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [39] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 道德自我纠正存在悖论，依赖于启发式捷径，共同增强自我纠正和自我诊断能力时会导致不一致。


<details>
  <summary>Details</summary>
Motivation: 道德自我纠正已成为一种有希望的方法，用于使大型语言模型 (LLM) 的输出与人类道德价值观保持一致。然而，道德自我纠正技术受到两个主要悖论的支配。首先，尽管有经验和理论证据支持自我纠正的有效性，但这种 LLM 能力仅在表面层面运作。其次，虽然 LLM 具有自我诊断其输出的不道德方面的能力，但它们在自我纠正过程中难以识别这种道德不一致的原因。

Method: 我们分析了旨在增强道德自我纠正的微调语料库中的话语结构，揭示了有效结构背后的启发式方法的存在。

Result: 我们证明了道德自我纠正依赖于反映启发式捷径的论述结构，并且在尝试共同增强自我纠正和自我诊断能力时，这些启发式捷径的存在会导致不一致。我们还强调了这种能力的泛化挑战，特别是在从情境背景和模型规模中学习方面。

Conclusion: 道德自我纠正依赖于反映启发式捷径的论述结构，并且在尝试共同增强自我纠正和自我诊断能力时，这些启发式捷径的存在会导致不一致。基于这些发现，我们提出了一种通过利用策展数据集的启发式方法来改进道德自我纠正的解决方案。

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [40] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 本文通过实验对比了 MLM 和 CLM 两种预训练方法在文本表示任务上的性能，发现 MLM 总体性能更好，CLM 数据效率更高，并提出了一种结合两者优势的双相训练策略。


<details>
  <summary>Details</summary>
Motivation: 学习高质量的文本表示是各种 NLP 任务的基础。虽然编码器预训练传统上依赖于 Masked Language Modeling (MLM)，但最近的证据表明，使用 Causal Language Modeling (CLM) 预训练的解码器模型可以有效地重新用作编码器，通常在文本表示基准测试中超过传统编码器。然而，目前尚不清楚这些收益是否反映了 CLM 目标的内在优势，或者是由模型和数据规模等混杂因素引起的。

Method: 通过一系列大规模、精心控制的预训练消融实验，训练了总共 30 个模型，参数范围从 2.1 亿到 10 亿，并进行了超过 15,000 次微调和评估运行。

Result: 虽然使用 MLM 进行训练通常会在文本表示任务中产生更好的性能，但经过 CLM 训练的模型具有更高的数据效率，并表现出改进的微调稳定性。

Conclusion: 在固定计算训练预算下，依次应用 CLM 和 MLM 的双相训练策略可实现最佳性能。此外，从现成的预训练 CLM 模型初始化时，该策略更具吸引力，从而减少了训练最佳编码器模型所需的计算负担。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [41] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: La Leaderboard是首个评估西班牙语生成LLM的开源排行榜，旨在为西班牙语社区建立评估标准。


<details>
  <summary>Details</summary>
Motivation: 为了促进代表西班牙语社区语言和文化多样性的LLM的开发。

Method: 结合了巴斯克语、加泰罗尼亚语、加利西亚语和不同西班牙语变体的66个数据集，并解释了为每个下游任务选择最合适评估设置的方法，使用了比文献中通常更少的few-shot示例。

Result: 展示了50个模型的评估结果。

Conclusion: La Leaderboard是首个评估西班牙语社区生成LLM的开源排行榜，展示了50个模型的评估结果。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [42] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena是一个用于评估科学文献任务的开放协作平台，它通过社区投票和元评估基准来促进模型评估研究。


<details>
  <summary>Details</summary>
Motivation: 传统科学文献理解和合成的基准测试不足以评估开放式的科学任务，因此需要一个社区驱动的评估方法。

Method: SciArena平台采用Chatbot Arena的评估方法，通过社区投票进行模型比较，并收集了来自不同科学领域的研究人员的超过13,000张选票。

Result: 分析表明，提交的问题是多样化的，与实际文献需求一致，并且参与的研究人员在评估中表现出很强的自我一致性和注释者间协议。实验强调了基准测试的挑战，并强调需要更可靠的自动化评估方法。

Conclusion: 该论文发布了一个基于社区投票的科学文献任务评估平台SciArena，并发布了基于收集到的偏好数据的元评估基准SciArena-Eval，以促进基于模型的自动化评估系统的研究。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 提出了一种新的帧采样方法，通过优先选择与问题相关的帧来提高视频大语言模型在长视频问答中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型方法在短视频上表现良好，但通常难以进行长视频中的远程推理。为了扩展视频大语言模型以适应更长的视频内容，通常采用帧子采样（以规则的时间间隔选择帧）。然而，这种方法并非最佳，通常会导致关键帧的丢失或包含来自多个相似帧的冗余信息。

Method: 提出了一种新颖的、与模型无关的方法，即“时刻采样”，使模型能够根据问题的上下文选择最相关的帧。具体来说，采用了一个轻量级的时刻检索模型来优先选择帧。

Result: 通过在四个长视频问答数据集上使用四个最先进的视频大语言模型进行的大量实验，证明了所提出方法的有效性。

Conclusion: 该方法通过选择与问题最相关的帧，增强了视频大语言模型在长视频问答方面的性能。在四个长视频问答数据集上，使用四个最先进的视频大语言模型进行了大量实验，证明了该方法的有效性。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [44] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: ER-EMU: An edge model update algorithm based on adaptive experience replay, to address the limitations of catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Continually adapting edge models in cloud-edge collaborative object detection for traffic monitoring suffers from catastrophic forgetting, where models lose previously learned knowledge when adapting to new data distributions. Existing approaches like experience replay and visual prompts offer some mitigation, but struggle to effectively prioritize and leverage historical data for optimal knowledge retention and adaptation. Specifically, simply storing and replaying all historical data can be inefficient, while treating all historical experiences as equally important overlooks their varying relevance to the current domain.

Method: an edge model update algorithm based on adaptive experience replay, utilizing a limited-size experience buffer managed using a First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target domains, prioritizing the selection of historical data that is most dissimilar to the current target domain. The experience buffer is also updated using a simple random sampling strategy

Result: Experiments on the Bellevue traffic video dataset, involving repeated day/night cycles, demonstrate that ER-EMU consistently improves the performance of several state-of-the-art cloud-edge collaborative object detection frameworks.

Conclusion: ER-EMU consistently improves the performance of several state-of-the-art cloud-edge collaborative object detection frameworks.

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [45] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP通过多模态对比学习，利用DICOM元数据学习对比度感知的MR图像表示，解决了临床应用中元数据不完整和缺乏标签的问题。


<details>
  <summary>Details</summary>
Motivation: 临床系统中对磁共振成像扫描的准确解释依赖于对图像对比度的精确理解。然而，DICOM元数据中的采集参数通常不完整、嘈杂或不一致，缺乏可靠和标准化的元数据会使图像解释、检索和集成到临床工作流程等任务复杂化。

Method: 提出了MR-CLIP，一个多模态对比学习框架，它将MR图像与其DICOM元数据对齐，以学习对比度感知表示，而无需依赖手动标签。

Result: MR-CLIP捕获了跨采集和扫描内的对比度变化，实现了与解剖结构无关的表示。

Conclusion: MR-CLIP在跨模态检索和对比分类中表现出色，展示了其可扩展性和在临床应用中的潜力。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [46] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: This paper proposes and compares three artifact detection approaches for WSIs, and the foundation model-based approach achieved the best performance.


<details>
  <summary>Details</summary>
Motivation: WSI is vulnerable to artifacts introduced during slide preparation and scanning, which can compromise downstream image analysis.

Method: Three artifact detection approaches were proposed and compared: a foundation model-based approach (FMA) using a fine-tuned Unified Neural Image (UNI) architecture, a deep learning approach (DLA) built on a ResNet50 backbone, and a knowledge-based approach (KBA) leveraging handcrafted features from texture, color, and frequency-based metrics.

Result: The FMA achieved the highest patch-wise AUROC of 0.995, outperforming the ResNet50-based method (AUROC: 0.977) and the KBA (AUROC: 0.940).

Conclusion: The foundation model-based approach (FMA) achieved the highest patch-wise AUROC of 0.995, outperforming the ResNet50-based method and the knowledge-based approach. A quality report scorecard was developed to quantify high-quality patches and visualize artifact distributions.

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [47] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces a new challenging task, CaughtCheating, to evaluate the detective perception and reasoning capabilities of MLLMs, finding that their performance drops significantly in this task.


<details>
  <summary>Details</summary>
Motivation: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives?

Method: We investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task.

Result: find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating.

Conclusion: CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [48] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: 本研究提出了一种基于进化计算的图像分割方法，用于分析增材摩擦搅拌沉积 (AFSD) 过程中的完整性。


<details>
  <summary>Details</summary>
Motivation: 本研究提出了一种基于进化计算的图像分割方法，用于分析增材摩擦搅拌沉积 (AFSD) 过程中的完整性。

Method: 采用粒子群优化 (PSO) 算法来确定最佳分割阈值，以检测多层 AFSD 构建中的缺陷和特征。该方法将梯度幅度分析与距离变换相结合，以创建突出关键界面区域的新型注意力加权可视化。

Result: PSO 算法自动识别了每个样本的最佳阈值（范围为 156-173），从而能够精确分割材料界面。多通道可视化技术有效地将边界信息（红色通道）、空间关系（绿色通道）和材料密度数据（蓝色通道）组合成量化界面质量的连贯表示。

Conclusion: 注意力分析成功识别了 AFSD 接头中不完全结合和不均匀的区域，为增材制造部件的工艺优化和质量评估提供了定量指标。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [49] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: AdaDeDup is a hybrid data pruning framework that combines density-based pruning with model-informed feedback to improve data efficiency for large-scale model training.


<details>
  <summary>Details</summary>
Motivation: The computational burden and inherent redundancy of large-scale datasets challenge the training of contemporary machine learning models. Data pruning offers a solution, yet existing methods struggle: density-based approaches can be task-agnostic, while model-based techniques may introduce redundancy or prove computationally prohibitive.

Method: Adaptive De-Duplication (AdaDeDup), a novel hybrid framework that synergistically integrates density-based pruning with model-informed feedback in a cluster-adaptive manner.

Result: It significantly outperforms prominent baselines, substantially reduces performance degradation (e.g., over 54% versus random sampling on Waymo), and achieves near-original model performance while pruning 20% of data, highlighting its efficacy in enhancing data efficiency for large-scale model training.

Conclusion: AdaDeDup significantly outperforms prominent baselines, substantially reduces performance degradation, and achieves near-original model performance while pruning 20% of data.

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [50] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: 提出了VSF-Med，一个用于评估医学VLM安全性的框架，发现Llama-3.2-11B-Vision-Instruct和GPT-4o存在安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 医学影像工作流程劳动密集，视觉语言模型（VLM）在简化这些流程方面具有巨大潜力，但临床环境中对VLM的系统安全评估仍然很少。

Method: 提出了一个名为VSF-Med的端到端漏洞评分框架，用于评估医学视觉语言模型（VLM）的安全性。该框架包括：(i)一个包含复杂文本提示攻击模板的丰富库，针对新兴威胁向量；(ii)通过结构相似性（SSIM）阈值校准的、不可察觉的视觉扰动，以保持临床真实感；(iii)一个八维评估标准，由两个独立的LLM评估者评估，其原始分数通过z-score标准化进行整合，以产生0-32的综合风险指标。

Result: 综合分析报告显示，对于最先进的VLM，攻击效果持久性的平均z-score变化为0.90σ，提示注入有效性的平均z-score变化为0.74σ，安全绕过成功率的平均z-score变化为0.63σ。

Conclusion: VSF-Med发现，Llama-3.2-11B-Vision-Instruct在攻击效果持久性方面表现出最高的漏洞增加（1.29σ），而GPT-4o在同一向量上显示出0.69σ的增加，在提示注入攻击方面显示出0.28σ的增加。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [51] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA 通过文本对齐统一多模态输入，显著提升了长视频问答的准确率，尤其在长时间视频和时间推理方面。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立处理多模态信息，导致表示和推理的不一致性。

Method: MANTA (Multi-modal Abstraction and Normalization via Textual Alignment) 框架，通过文本对齐统一视觉和听觉输入到结构化文本空间，并结合信息理论优化、自适应时间同步、分层内容表示和上下文感知检索等方法。

Result: MANTA 在长视频问答任务中，总体准确率提高了 22.6%，超过 30 分钟的视频提高了 27.3%，时间推理任务提高了 23.8%，跨模态理解提高了 25.1%。

Conclusion: MANTA在长视频问答任务中显著提升了现有模型的性能，尤其在处理超过30分钟的视频时，并在时间推理和跨模态理解方面表现出卓越性能。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [52] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: 本研究提出了一种使用迁移学习方法识别和监测植物病害的系统，实验结果表明YOLOv8具有卓越的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 植物病害对农民和整个农业部门构成了重大挑战。早期检测植物病害对于减轻其影响和防止广泛损害至关重要，因为疫情爆发会严重影响作物的生产力和质量。随着技术的进步，自动化监测和检测植物病害爆发的机会越来越多。

Method: 利用YOLOv7和YOLOv8，通过在植物叶片图像数据集上微调这些模型，该系统能够准确地检测细菌、真菌和病毒性疾病。

Result: 该模型使用平均精度均值（mAP）、F1-score、精确度和召回率等指标进行评估，得到的值分别为91.05、89.40、91.22和87.66。结果表明，与其他目标检测方法相比，YOLOv8具有卓越的有效性和效率。

Conclusion: YOLOv8在植物病害检测中表现出优越的性能，为现代农业实践提供了潜力，并为早期植物病害检测提供可扩展的自动化解决方案，有助于提高作物产量，减少对人工监测的依赖，并支持可持续农业实践。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [53] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: This paper proposes a diffusion-based image augmentation method to improve the performance of perception algorithms in underrepresented environments, specifically focusing on snow-filled environments for autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: The performance of leaning-based perception algorithms suffer when deployed in out-of-distribution and underrepresented environments. Outdoor robots are particularly susceptible to rapid changes in visual scene appearance due to dynamic lighting, seasonality and weather effects that lead to scenes underrepresented in the training data of the learning-based perception system.

Method: a novel method for diffusion-based image augmentation

Result: diffusion-based image augmentations allow us to take control over the semantic distribution of the ground surfaces in the training data and to fine-tune our model for its deployment environment. We employ open vocabulary semantic segmentation models to filter out augmentation candidates that contain hallucinations.

Conclusion: diffusion-based image augmentations can be extended to many other environments apart from snow surfaces, like sandy environments and volcanic terrains.

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [54] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: FreeLong++ 是一种免训练框架，通过平衡频率分布来改善长视频生成效果，解决了长视频生成中时间一致性和视觉保真度降低的问题。


<details>
  <summary>Details</summary>
Motivation: 将这些模型扩展到更长的视频仍然是一个重大挑战，这主要是由于时间一致性和视觉保真度的降低。初步观察表明，天真地将短视频生成模型应用于更长的序列会导致明显的质量下降。进一步的分析确定了一种系统趋势，即高频分量随着视频长度的增长而变得越来越失真，这个问题我们称之为高频失真。

Method: 提出 FreeLong，这是一个旨在平衡去噪过程中长视频特征的频率分布的免训练框架。FreeLong 通过混合全局低频特征（捕获整个视频的整体语义）与从短时窗提取的局部高频特征（以保留精细细节）来实现这一点。FreeLong++ 将 FreeLong 双分支设计扩展到具有多个注意分支的多分支架构，每个分支在不同的时间尺度上运行。通过排列从全局到局部的多个窗口大小，FreeLong++ 能够实现从低频到高频的多频带融合，从而确保更长视频序列中的语义连续性和精细运动动态。

Result: FreeLong++ 在更长的视频生成任务上优于以前的方法（例如，原始长度的 4 倍和 8 倍）。它还支持具有平滑场景过渡的连贯多提示视频生成，并支持使用长深度或姿势序列的可控视频生成。

Conclusion: FreeLong++可以插入到现有的视频生成模型中，以生成具有显着改善的时间一致性和视觉保真度的更长视频。该方法在更长的视频生成任务上优于以前的方法，并且支持具有平滑场景过渡的连贯多提示视频生成，并支持使用长深度或姿势序列的可控视频生成。

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [55] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

TL;DR: SelvaBox是最大的开放获取热带树冠检测数据集。


<details>
  <summary>Details</summary>
Motivation: 检测热带森林中的树冠对于研究受人类活动和气候变化影响的复杂生态系统至关重要。现有的带注释的数据集稀缺，阻碍了模型的开发。

Method: 使用高分辨率无人机图像和人工标注的树冠，创建了一个大型开放访问数据集SelvaBox，并在此基础上进行模型训练和评估。

Result: 更高分辨率的输入可以提高检测精度；仅在SelvaBox上训练的模型在未见过的热带树冠数据集上实现了有竞争力的零样本检测性能。

Conclusion: 联合训练SelvaBox和其他数据集可以在所有评估的数据集上获得最佳或次佳的检测器。

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [56] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络的深度学习架构，用于从3D点云数据中检测植物组件，实验结果表明该方法优于其他现有模型。


<details>
  <summary>Details</summary>
Motivation: 在精准农业中，探索作物生产最重要的任务之一是识别单个植物组件。目前已经有一些尝试通过使用传统的2D成像、3D重建和卷积神经网络(CNN)来实现这一任务。然而，在处理3D数据和识别单个植物组件时，它们存在一些缺点。

Method: 提出了一种新的深度学习架构，用于检测激光雷达(LiDAR)3D点云(PC)数据集上单个植物的组件。该架构基于图神经网络(GNN)的概念，并利用主成分分析(PCA)进行特征增强。每个点都被作为一个顶点，并通过使用k近邻(KNN)层来建立边，从而表示3D PC数据集。随后，使用Edge-Conv层来进一步增加每个点的特征。最后，应用图注意力网络(GAT)来对植物的可见表型成分(如叶、茎和土壤)进行分类。

Result: 基于图的深度学习方法提高了识别单个植物组件的分割精度，IoU平均值达到80%以上。

Conclusion: 该研究表明，基于图的深度学习方法提高了识别单个植物组件的分割精度，IoU平均值达到80%以上，优于其他基于点云的现有模型。

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [57] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: This paper introduces FiboSB, a new dataset for 6D pose estimation in collaborative settings, evaluates existing methods, and fine-tunes YOLO11-x to improve object detection performance.


<details>
  <summary>Details</summary>
Motivation: existing systems often lack the ability to accurately capture real-world interactions between students and physical objects. This issue could be addressed with automatic 6D pose estimation, i.e., estimation of an object's position and orientation in 3D space from RGB images or videos. For collaborative groups that interact with physical objects, 6D pose estimates allow AI systems to relate objects and entities.

Method: introduce FiboSB, a novel and challenging 6D pose video dataset featuring groups of three participants solving an interactive task featuring small hand-held cubes and a weight scale. evaluated four state-of-the-art 6D pose estimation methods on FiboSB, exposing the limitations of current algorithms on collaborative group work. address this by fine-tuning YOLO11-x for FiboSB, achieving an overall mAP_50 of 0.898.

Result: evaluated four state-of-the-art 6D pose estimation methods on FiboSB, exposing the limitations of current algorithms on collaborative group work. An error analysis of these methods reveals that the 6D pose methods' object detection modules fail. We address this by fine-tuning YOLO11-x for FiboSB, achieving an overall mAP_50 of 0.898.

Conclusion: The dataset, benchmark results, and analysis of YOLO11-x errors presented here lay the groundwork for leveraging the estimation of 6D poses in difficult collaborative contexts.

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [58] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: VOCAL是一种新的视觉里程计框架，它使用对比学习和贝叶斯推理来提高可解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 许多基于学习的VO技术依赖于刚性的几何假设，这通常缺乏可解释性，并且在完全数据驱动的框架内缺乏坚实的理论基础。

Method: VOCAL (Visual Odometry via ContrAstive Learning)：通过将贝叶斯推理与表示学习框架相结合，组织视觉特征以反映相机状态，并使用排序机制促使相似的相机状态在潜在空间中收敛成一致且空间连贯的表示。

Result: 在KITTI数据集上的大量评估突出了VOCAL的增强的可解释性和灵活性。

Conclusion: VOCAL通过对比学习，将视觉里程计重新定义为标签排序问题，从而增强了可解释性和灵活性，并推动VO朝着更通用和可解释的空间智能发展。

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [59] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: This paper presents a novel framework for real-time sign language recognition using lightweight DNNs trained on limited data, achieving 92% accuracy in isolated sign recognition with less than 10ms latency on edge devices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address key challenges in sign language recognition, including data scarcity, high computational costs, and discrepancies in frame rates between training and inference environments.

Method: The method involves encoding sign language specific parameters into vectorized inputs, leveraging MediaPipe for landmark extraction, and using a DNN architecture optimized for sub 10MB deployment.

Result: The model enables accurate classification of 343 signs with less than 10ms latency on edge devices.

Conclusion: The model achieves 92% accuracy in isolated sign recognition and has been integrated into the 'slait ai' web application, where it demonstrates stable inference.

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [60] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: GazeTarget360是一个新的系统，可以从图像中估计360度的视线目标，即使人们没有看着相机。


<details>
  <summary>Details</summary>
Motivation: 使机器人能够理解人类的视线目标是实现下游任务的关键步骤，例如，现实世界人机交互中的注意力估计和运动预测。以往的研究已经通过仔细移除框架外样本，利用数据驱动的方法解决了框架内目标定位问题。然而，基于视觉的视线估计方法，例如OpenFace，不能有效地吸收图像中的背景信息，并且不能在受试者看向远离相机的地方时预测视线目标。

Method: 该系统集成了眼神交流检测器、预训练的视觉编码器和多尺度融合解码器的条件推理引擎。

Result: 交叉验证结果表明，GazeTarget360系统能够在未见过的场景中产生准确且可靠的视线目标预测。

Conclusion: GazeTarget360系统能够在未见过的场景中产生准确且可靠的视线目标预测，使其成为首个能够从真实相机镜头中预测视线目标的高效且可部署的系统。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [61] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: VirtualFencer extracts 3D fencing motion and strategy from video to generate realistic fencing behavior, showcasing its capabilities through self-play, fencing against real fencers' motion, and interactive fencing against a professional.


<details>
  <summary>Details</summary>
Motivation: The combination of motion diversity with underlying two-player strategy motivates the application of data-driven modeling to fencing.

Method: A system capable of extracting 3D fencing motion and strategy from in-the-wild video without supervision, and then using that extracted knowledge to generate realistic fencing behavior.

Result: The system demonstrates versatile capabilities.

Conclusion: The system can fence against itself, a real fencer's motion, and interactively against a professional fencer.

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [62] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: This paper introduces a machine learning pipeline for room scene discovery, grouping, and bed type identification in vacation rentals, addressing the problem of unstructured property images and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Travelers face challenges in understanding the spatial layout of vacation rental properties due to the lack of structured categorization of property images, especially when multiple rooms of the same type are present.

Method: A computationally efficient machine learning pipeline integrating a supervised room-type detection model, a supervised overlap detection model, and a clustering algorithm, along with a Multi-modal Large Language Model (MLLM) for bed type identification.

Result: The models were evaluated individually and the pipeline was assessed in its entirety, showing strong performance.

Conclusion: The proposed pipeline demonstrates strong performance, significantly outperforming established approaches like contrastive learning and clustering with pretrained embeddings.

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [63] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: This paper presents a self-supervised pipeline to generate a many-to-many correspondence matrix between synthetic X-ray views, and demonstrates that learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data.


<details>
  <summary>Details</summary>
Motivation: current methods often struggle to establish robust correspondences between different X-ray views, an essential capability for precise clinical evaluations

Method: a novel self-supervised pipeline that eliminates the need for manual annotation by automatically generating a many-to-many correspondence matrix between synthetic X-ray views

Result: learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data

Conclusion: incorporating correspondences improves performance in multi-view fracture classification

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [64] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: Introduces a Multi-Fidelity, Model Fusion strategy for MIL methods to reduce performance variability and improve reproducibility in WSI classification tasks.


<details>
  <summary>Details</summary>
Motivation: High variability in performance across different runs of Multiple Instance Learning (MIL) methods, which can reach up to 10-15 AUC points on the test set, making it difficult to compare different MIL methods reliably. This variability mainly comes from three factors: i) weight initialization, ii) batch (shuffling) ordering, iii) and learning rate.

Method: The authors introduce a Multi-Fidelity, Model Fusion strategy for MIL methods, where they train multiple models for a few epochs and average the most stable and promising ones based on validation scores.

Result: The proposed approach reduces performance variability, simplifies hyperparameter tuning and improves reproducibility while maintaining computational efficiency. The approach is validated on WSI classification tasks.

Conclusion: The authors introduce a Multi-Fidelity, Model Fusion strategy for MIL methods. They train multiple models for a few epochs and average the most stable and promising ones based on validation scores. This approach can be applied to any existing MIL model to reduce performance variability, simplifies hyperparameter tuning and improves reproducibility while maintaining computational efficiency. The approach is validated on WSI classification tasks using 2 different datasets, 3 initialization strategies and 5 MIL methods, for a total of more than 2000 experiments.

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [65] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: SR-LoRA uses stable rank to improve LoRA's adaptability in domain gaps without extra search costs, outperforming other adaptive LoRA methods.


<details>
  <summary>Details</summary>
Motivation: Fixed low-rank structure restricts its adaptability in scenarios with substantial domain gaps, where higher ranks are often required to capture domain-specific complexities. Current adaptive LoRA methods attempt to overcome this limitation by dynamically expanding or selectively allocating ranks, but these approaches frequently depend on computationally intensive techniques such as iterative pruning, rank searches, or additional regularization.

Method: Stable Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the stable rank of pre-trained weight matrices as a natural prior for layer-wise rank allocation.

Result: principled and efficient redistribution of ranks across layers, enhancing adaptability without incurring additional search costs.

Conclusion: SR-LoRA consistently outperforms recent adaptive LoRA variants, achieving a superior trade-off between performance and efficiency.

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [66] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: MammoTracker, a mask-guided lesion tracking framework, automates lesion localization across consecutively exams and introduces a new large dataset for temporal lesion tracking in mammograms.


<details>
  <summary>Details</summary>
Motivation: Accurate lesion tracking in temporal mammograms is essential for monitoring breast cancer progression and facilitating early diagnosis. However, automated lesion correspondence across exams remains a challenges in computer-aided diagnosis (CAD) systems, limiting their effectiveness.

Method: a mask-guided lesion tracking framework that automates lesion localization across consecutively exams. Our approach follows a coarse-to-fine strategy incorporating three key modules: global search, local search, and score refinement.

Result: introduce a new dataset with curated prior-exam annotations for 730 mass and calcification cases from the public EMBED mammogram dataset, yielding over 20000 lesion pairs, making it the largest known resource for temporal lesion tracking in mammograms.

Conclusion: MammoTracker achieves 0.455 average overlap and 0.509 accuracy, surpassing baseline models by 8%, highlighting its potential to enhance CAD-based lesion progression analysis.

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [67] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: 本研究探索了文本到视频模型作为交互式世界模拟器的潜力，通过微调模型使其能够预测人与环境的互动，并揭示了预训练模型中内在的 affordance 感知能力。


<details>
  <summary>Details</summary>
Motivation: 探索文本到视频模型的 affordance 感知潜力，通过教导它们预测人与环境的互动。

Method: 通过微调文本到视频模型，使其能够预测人与环境的互动，从而将人插入场景中，同时确保连贯的行为、外观、协调和场景 affordance。

Result: 深入研究 cross-attention 热图表明，我们可以揭示预训练视频模型的内在 affordance 感知能力。

Conclusion: 预训练的视频模型具有内在的 affordance 感知能力，无需标注的 affordance 数据集。

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [68] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: Introduces MOVi-MC-AC, a large-scale multi-camera amodal segmentation and content completion dataset with ground-truth amodal content labels.


<details>
  <summary>Details</summary>
Motivation: Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene.

Method: introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene.  provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content.

Result: Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content.

Conclusion: MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content.

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [69] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: CGEarthEye是一个为吉林一号卫星设计的遥感视觉基础模型框架，它在多个遥感任务中取得了最先进的性能，并有望促进吉林一号数据的更广泛应用。


<details>
  <summary>Details</summary>
Motivation: 与中等分辨率数据的开放可访问性和高时空覆盖率相比，超高分辨率光学RS图像的有限获取渠道限制了高分辨率遥感视觉基础模型（RSVFM）的进展。作为世界上最大的亚米级商业RS卫星星座，吉林一号星座拥有丰富的亚米级图像资源。

Method: CGEarthEye，一个专门为吉林一号卫星特征设计的RSVFM框架，包含五个不同参数规模的骨干网络，总计21亿参数。通过多层次表征聚类和采样策略，开发了JLSSD，这是第一个1500万规模的多时态自监督学习（SSL）数据集，具有全球覆盖，并在一年内进行季度时间采样。该框架集成了季节对比、基于增强的对比和掩码补丁令牌对比策略进行预训练。

Result: 在涵盖四个典型RS任务的10个基准数据集上进行的综合评估表明，CGEarthEye始终如一地实现了最先进的（SOTA）性能。进一步的分析表明，CGEarthEye在特征可视化、模型收敛、参数效率和实际映射应用中具有卓越的特性。

Conclusion: CGEarthEye的卓越表征能力将促进吉林一号数据在传统地球观测应用中更广泛、更高效的应用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [70] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: Proposes a method to enhance 3D Gaussian Splatting (3DGS) by addressing challenges in initialization, optimization, and density control, achieving high-fidelity real-time rendering.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in initialization, optimization, and density control of 3D Gaussian Splatting (3DGS). 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far.

Method: A geometry-guided initialization to predict Gaussian parameters, a surface-aligned optimization strategy to refine Gaussian placement, and a dynamic adaptive density control mechanism.

Result: Enables high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Demonstrates comparable or superior results to state-of-the-art methods.

Conclusion: The method achieves high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes, demonstrating comparable or superior results to state-of-the-art methods.

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [71] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: 提出了一种基于改进U-net结构的签名手写体去噪模型，用于提高签名识别系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 手写签名作为一种重要的身份识别手段，因其法律效力和独特性而被广泛应用于金融交易、商业合同和个人事务等多个领域。在法医学鉴定中，离线手写签名的分析需要鉴定人提供一定数量的签名样本，这些样本通常来源于各种历史合同或档案材料。然而，所提供的笔迹样本往往混杂着大量的干扰信息，这给笔迹鉴定工作带来了严峻的挑战。

Method: 提出了一种基于改进U-net结构的签名手写体去噪模型，通过引入离散小波变换和PCA变换，增强了模型抑制噪声的能力。

Result: 该模型在去噪效果上明显优于传统方法，能够有效提高签名图像的清晰度和可读性。

Conclusion: 该模型在去噪效果上明显优于传统方法，能够有效提高签名图像的清晰度和可读性，并为签名分析和识别提供更可靠的技术支持。

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [72] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: 提出了一种新的OOD检测方法ATLI，该方法通过结合多个logits来提高OOD检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 神经网络经常对分布外（OOD）样本做出过度自信的预测。因此，检测OOD数据对于提高机器学习的安全性至关重要。

Method: 提出了一种名为ATLI（自适应Top-k Logits集成）的新方法，该方法自适应地确定每个模型特定的有效top-k logits，并将最大logit与其他top-k logits结合。

Result: 在ImageNet-1K基准测试中，与MaxLogit方法相比，假阳性率（FPR95）降低了6.73%，与其他最先进的方法相比，FPR95降低了2.67%。

Conclusion: 该研究提出了一种新的OOD检测方法ATLI，该方法通过自适应地确定有效的top-k logits并将最大logit与其他top-k logits结合，在ImageNet-1K基准测试中，与MaxLogit方法相比，假阳性率（FPR95）降低了6.73%，与其他最先进的方法相比，FPR95降低了2.67%。

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [73] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: PlantSegNeRF is a new method for high-precision plant organ segmentation from multi-view images, outperforming existing techniques in accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing plant point cloud organ segmentation techniques have limitations in resolution, accuracy, and generalizability across plant species.

Method: A novel approach called PlantSegNeRF is proposed, which generates high-precision instance point clouds from multi-view RGB image sequences. It uses 2D instance segmentation, instance matching, and instance NeRF to render and convert an implicit scene into high-precision plant instance point clouds.

Result: PlantSegNeRF outperforms existing methods in semantic segmentation, with improvements of 16.1% in precision, 18.3% in recall, 17.8% in F1-score, and 24.2% in IoU. It also shows significant advantages in plant point cloud instance segmentation, improving mPrec by 11.7%, mRec by 38.2%, mCov by 32.2% and mWCov by 25.3%.

Conclusion: PlantSegNeRF extends organ-level plant phenotyping and offers a high-throughput method for providing high-quality 3D data, beneficial for large-scale plant science models.

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [74] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: This paper proposes a method to train networks using synthetic data that generalizes to real-world images by modeling depth-dependent defocus and spatially varying optical aberrations, which is effective for fixed-focus cameras such as those used in smart glasses.


<details>
  <summary>Details</summary>
Motivation: Modern cameras with large apertures often suffer from a shallow depth of field, resulting in blurry images of objects outside the focal plane. This limitation is particularly problematic for fixed-focus cameras, such as those used in smart glasses, where adding autofocus mechanisms is challenging due to form factor and power constraints. Due to unmatched optical aberrations and defocus properties unique to each camera system, deep learning models trained on existing open-source datasets often face domain gaps and do not perform well in real-world settings.

Method: an efficient and scalable dataset synthesis approach that does not rely on fine-tuning with real-world data. Our method simultaneously models depth-dependent defocus and spatially varying optical aberrations

Result: a network trained on our low resolution synthetic images generalizes effectively to high resolution (12MP) real-world images across diverse scenes.

Conclusion: A network trained on our low resolution synthetic images generalizes effectively to high resolution (12MP) real-world images across diverse scenes.

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [75] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: Proposes a customizable ROI-based deep image compression paradigm with TMA, CVA, and LMA modules to address the needs of customization for ROI definition and reconstruction quality trade-off.


<details>
  <summary>Details</summary>
Motivation: Existing ROI-based image compression schemes predefine the ROI, making it unchangeable, and lack effective mechanisms to balance reconstruction quality between ROI and non-ROI. This work proposes a paradigm for customizable ROI-based deep image compression to support various preferences.

Method: develop a Text-controlled Mask Acquisition (TMA) module, design a Customizable Value Assign (CVA) mechanism, present a Latent Mask Attention (LMA) module

Result: Experimental results demonstrate that our proposed customizable ROI-based deep image compression paradigm effectively addresses the needs of customization for ROI definition and mask acquisition as well as the reconstruction quality trade-off management between the ROI and non-ROI.

Conclusion: This work proposes a customizable ROI-based deep image compression paradigm that effectively addresses the needs of customization for ROI definition and mask acquisition as well as the reconstruction quality trade-off management between the ROI and non-ROI.

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [76] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: MedDiff-FT是一种可控的医学图像生成方法，通过微调扩散模型并结合动态引导和质量评估，有效提升了医学图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割的深度学习进展受到高质量训练数据稀缺的限制。虽然扩散模型提供了一种通过生成合成图像的潜在解决方案，但由于它们依赖于大规模医学数据集以及需要更高的图像质量，因此它们在医学成像中的有效性仍然受到限制。

Method: 提出了一种可控的医学图像生成方法MedDiff-FT，该方法通过微调扩散基础模型以数据有效的方式生成具有结构依赖性和领域特异性的医学图像。在推理过程中，动态自适应引导mask强制空间约束以确保解剖学上连贯的合成，而轻量级随机mask生成器通过分层随机性注入来增强多样性。此外，自动质量评估协议使用特征空间度量来过滤次优输出，然后进行mask腐蚀以提高保真度。

Result: 在五个医学分割数据集上进行评估，MedDiff-FT的合成图像-mask对将SOTA方法的分割性能平均提高了1%的Dice分数。

Conclusion: MedDiff-FT通过生成高质量、多样化和计算高效的合成图像，有效平衡了生成质量、多样性和计算效率，为医学数据增强提供了一种实用的解决方案，并在五个医学分割数据集上，将SOTA方法的分割性能平均提高了1%的Dice分数。

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [77] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: 提出了一种名为 L2M 的两阶段框架，该框架利用大规模单视图图像将 2D 图像提升到 3D 空间，从而实现鲁棒的特征匹配和跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 特征匹配在许多计算机视觉任务中起着 фундаментальную роль，但现有的方法严重依赖于稀缺和干净的多视图图像集合，这限制了它们对各种具有挑战性的场景的泛化。此外，传统的特征编码器通常在单视图 2D 图像上进行训练，限制了它们捕获 3D 感知对应关系的能力。

Method: 我们提出了一个新颖的两阶段框架，将 2D 图像提升到 3D 空间，命名为 Lift to Match (L2M)，充分利用大规模和多样化的单视图图像。具体来说，在第一阶段，我们使用多视图图像合成和 3D 特征高斯表示的组合来学习 3D 感知特征编码器，从而将 3D 几何知识注入到编码器中。在第二阶段，采用一种新颖的视图渲染策略，结合单视图图像生成的大规模合成数据，学习一种用于鲁棒特征匹配的特征解码器，从而实现跨不同领域的泛化。

Result: 我们的方法在零样本评估基准上实现了卓越的泛化，突出了所提出的框架在鲁棒特征匹配方面的有效性。

Conclusion: 该方法在零样本评估基准上实现了卓越的泛化，突出了所提出的框架在鲁棒特征匹配方面的有效性。

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [78] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: 提出MIV-head用于跨域小样本学习，无需微调backbone，适应成本低，并在Meta-dataset上表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究了backbone微调不可行情况下的跨域小样本学习，这种情况在实际应用中越来越常见。处理frozen的“黑盒”backbone产生的低质量和静态嵌入，导致小样本分类的问题表示为一系列多实例验证(MIV)任务。

Method: 提出了一种名为“MIV-head”的新型小样本域适应方法，类似于与任何预训练backbone无关且计算效率高的分类head。

Result: MIV-head在各种设置下，在使用在ImageNet1K上预训练的代表性off-the-shelf卷积神经网络和视觉transformer backbone的跨域小样本图像分类Meta-dataset基准测试的扩展上进行实验，表明与应用于相同backbone的最先进的“adapter”方法相比，MIV-head实现了极具竞争力的准确性，同时适应成本大大降低。 还发现，在准确性方面，众所周知的“分类head”方法远远落后。

Conclusion: MIV-head在不微调backbone的情况下，在跨域小样本图像分类的Meta-dataset基准测试上实现了极具竞争力的准确性，与最先进的“adapter”方法相比，适应成本更低。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [79] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: DiGA3D is a 3D inpainting pipeline addressing robustness, appearance consistency, and geometry consistency using diffusion models and a coarse-to-fine approach.


<details>
  <summary>Details</summary>
Motivation: Developing a unified pipeline that enables users to remove, re-texture, or replace objects in a versatile manner is crucial for text-guided 3D inpainting. Challenges include single reference inpainting methods lacking robustness, appearance inconsistency when independently inpainting multi-view images, and geometry inconsistency limiting performance.

Method: DiGA3D, a novel and versatile 3D inpainting pipeline that leverages diffusion models to propagate consistent appearance and geometry in a coarse-to-fine manner. It includes a robust strategy for selecting multiple reference views, an Attention Feature Propagation (AFP) mechanism, and a Texture-Geometry Score Distillation Sampling (TG-SDS) loss.

Result: The method demonstrates effectiveness on multiple 3D inpainting tasks.

Conclusion: Extensive experiments on multiple 3D inpainting tasks demonstrate the effectiveness of our method.

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [80] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: This paper introduces a method that marries frequency domain with HMER (MFH), leveraging the discrete cosine transform (DCT) to enhance the structural analysis for recognizing mathematical formulas.


<details>
  <summary>Details</summary>
Motivation: Handwritten mathematical expression recognition (HMER) suffers from complex formula structures and character layouts in sequence prediction.

Method: incorporate frequency domain analysis into HMER, leveraging the discrete cosine transform (DCT)

Result: network exhibits a consistent performance enhancement, demonstrating the efficacy of frequency domain information

Conclusion: MFH-CoMER achieves noteworthy accuracy on CROHME 2014/2016/2019 test sets.

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [81] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: Latent-PMRF通过在VAE的潜在空间中重新构建PMRF，实现了更好的盲人面部修复效果和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: PMRF的像素空间建模方法限制了其与人类感知对齐的能力。

Method: Latent-PMRF在变分自编码器 (VAE) 的潜在空间中重新构建 PMRF。

Result: Latent-PMRF优于现有的盲人面部修复方法，在FID方面比PMRF提高了5.79倍。

Conclusion: Latent-PMRF在盲人面部修复方面表现出色，与现有方法相比，它提供了改进的PD权衡，并且具有显着的收敛效率，在FID方面比PMRF提高了5.79倍。

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [82] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: ATSTrack aligns temporal and spatial scales of visual and language inputs by decomposing language descriptions and introducing a Visual-Language token to improve visual-language tracking.


<details>
  <summary>Details</summary>
Motivation: The misalignment between visual inputs and language descriptions caused by target movement and the inherent differences in the temporal and spatial scale of information between visual and language inputs.

Method: A novel visual-language tracker that enhances the effect of feature modification by aligning temporal and spatial scale of different input components. It decomposes each language description into phrases and introduces a Visual-Language token.

Result: Achieves performance comparable to existing methods.

Conclusion: The proposed ATSTrack achieves performance comparable to existing methods.

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [83] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: MS-TTA is a training-free test-time adaptation method that enhances feature representations beyond CLIP's space using kNN Mean-Shift, improving feature compactness and class separability.


<details>
  <summary>Details</summary>
Motivation: Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones.

Method: a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift

Result: improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits.

Conclusion: MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training.

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [84] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: Bisecle是一种用于视频语言持续学习的新方法，它通过模仿人脑海马体的记忆形成和巩固机制，解决了灾难性遗忘和更新冲突等问题，实现了鲁棒而高效的持续学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界的视频通常以不断发展的数据流形式存在，需要模型不断适应变化的数据分布和新的场景。考虑到在新任务上微调模型的巨大计算成本，通常只更新一小部分参数，而模型的大部分保持冻结。这对大型多模态基础模型背景下现有的持续学习框架提出了新的挑战，即灾难性遗忘和更新冲突。

Method: 我们提出了用于视频语言持续学习的Bisecle，其中使用多向监督模块来捕获更多跨模态关系，并设计了一种对比提示学习方案来隔离特定于任务的知识，以促进高效的记忆存储。

Result: 我们对提出的Bisecle进行了彻底的评估，证明了它在缓解遗忘和增强跨任务泛化方面的能力。

Conclusion: Bisecle通过缓解遗忘和增强跨任务泛化能力，在多个VideoQA基准测试中表现出强大的能力，从而实现了视频理解任务中鲁棒而高效的持续学习。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI是一种无训练框架，通过动态视觉基础和模态感知优化，改进了GUI基础。


<details>
  <summary>Details</summary>
Motivation: 由于视觉元素的多样性、空间杂乱和语言的模糊性，在图形用户界面 (GUI) 中对自然语言查询进行基础化提出了独特的挑战。

Method: DiMo-GUI，一种用于GUI基础的无训练框架，它利用了两个核心策略：动态视觉基础和模态感知优化。将输入分成文本元素和图标元素，允许模型使用通用视觉语言模型独立地对每个模态进行推理。生成候选焦点区域，这些区域以模型的初始预测为中心，并逐步放大到子区域以优化基础结果。

Result: 通过结合模态分离和区域聚焦推理，在标准GUI基础基准上实现了相对于基线推理管道的一致改进。

Conclusion: DiMo-GUI在标准GUI基础基准上评估，并证明了相对于基线推理管道的一致改进，突出了将模态分离与区域聚焦推理相结合的有效性。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [86] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: TalentMine是一种新型LLM框架，它通过语义丰富表格表示来改进人才管理系统中表格信息的检索，并在问答任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型在处理人才管理系统中复杂的表格信息时面临检索挑战，现有的表格提取方法在语义理解方面存在不足，导致检索增强的聊天应用性能较差。

Method: 提出了一种名为TalentMine的LLM增强框架，该框架通过专门的多模态推理将提取的表格转换为语义丰富的表示，从而保留表格数据的结构和语义维度。

Result: TalentMine在员工福利文档集合上的实验评估中表现出色，在问答任务中达到了100%的准确率，而标准AWS Textract提取为0%，AWS Textract Visual Q&A为40%。

Conclusion: TalentMine在问答任务中达到了100%的准确率，显著优于AWS Textract和其他基线方法，证明了其在人才管理应用中的优越性能。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [87] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management.... Inspired by the concept of ``frugal twin", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color.


<details>
  <summary>Details</summary>
Motivation: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively.

Method: a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management

Result: New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments.

Conclusion: The tools introduced are generally applicable and can easily be extended to other optimization problems.

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [88] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: This paper introduces SEZ-HARN, a novel IMU-based ZS-HAR model that recognizes unseen activities and provides explanations via skeleton videos, achieving competitive accuracy and improved transparency compared to existing black-box models.


<details>
  <summary>Details</summary>
Motivation: lack of comprehensive IMU-based HAR datasets that cover a wide range of activities and the lack of transparency in existing HAR models. Zero-shot HAR (ZS-HAR) overcomes the data limitations, but current models struggle to explain their decisions, making them less transparent

Method: a novel IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN). It can recognize activities not encountered during training and provide skeleton videos to explain its decision-making process.

Result: The experiment results demonstrate that SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy.

Conclusion: SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3% of the best-performing black-box model on PAMAP2 while maintaining comparable performance on the other three datasets.

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [89] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: AdvDistill通过奖励机制改进数据集蒸馏，提升小模型在推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏技术能够将大型语言模型的能力转移到更小、更高效的小型语言模型中，但学生模型常常只是复制教师模型的分布内响应，限制了其泛化能力，尤其是在推理任务上，且计算成本高昂。

Method: 提出AdvDistill，一个奖励引导的数据集蒸馏框架。利用来自教师模型的多个生成（响应），并基于规则验证器分配奖励，这些奖励作为训练学生模型时的权重。

Result: AdvDistill在数学和复杂推理任务中显著提高了学生模型的性能，证明了在数据集蒸馏过程中引入奖励机制的有效性和益处。

Conclusion: AdvDistill通过在数据集蒸馏过程中引入奖励机制，显著提高了学生模型在数学和复杂推理任务中的性能。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [90] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: VoyagerVision, a multi-modal model, uses visual input to build structures in Minecraft, outperforming Voyager in open-ended task completion.


<details>
  <summary>Details</summary>
Motivation: The paper explores how visual inputs enhance a model's ability to interpret spatial environments, increasing the number of tasks it can perform and extending its open-ended potential in AGI.

Method: VoyagerVision, a multi-modal model building on Voyager, uses screenshots for visual feedback to construct structures in Minecraft.

Result: VoyagerVision created an average of 2.75 unique structures within 50 iterations, and achieved 50% success in building unit tests in flat worlds.

Conclusion: VoyagerVision, a multi-modal model using screenshots as visual feedback, can create structures in Minecraft, averaging 2.75 unique structures in 50 iterations, outperforming Voyager. It succeeds in 50% of building unit tests in flat worlds, with failures mainly in complex structures.

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [91] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-nano 采用逆向推理，提高了解释性和推理性能，为透明 AI 系统做出了贡献。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂的推理任务方面表现出了卓越的能力，但其决策过程仍然有些黑盒。

Method: SAGE-nano 采用了一种元认知结构，通过注意力过程反思，以识别主要的决策点并生成推理选择的解释。

Result: SAGE-nano 在 AQUA-RAT 上的推理准确率为 74.6%，解释质量的人类偏好得分为 92.1%，并且性能几乎与 Claude-3.5 Sonnet 或 GPT-4o 等模型相当。

Conclusion: SAGE-nano 通过逆向推理在推理准确性和解释质量方面都达到了最先进的水平，并且逆向推理提高了可解释性和推理性能，为透明 AI 系统开辟了新途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [92] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: This paper proposes a novel pipeline to automatically extract interpretable decision logic from legacy systems treated as black boxes.


<details>
  <summary>Details</summary>
Motivation: Modernizing legacy software systems is a critical but challenging task, often hampered by a lack of documentation and understanding of the original system's intricate decision logic. Traditional approaches like behavioral cloning merely replicate input-output behavior without capturing the underlying intent.

Method: The approach uses a Reinforcement Learning (RL) agent to explore the input space and identify critical decision boundaries by rewarding actions that cause meaningful changes in the system's output. These counterfactual state transitions, where the output changes, are collected and clustered using K-Means. Decision trees are then trained on these clusters to extract human-readable rules that approximate the system's decision logic near the identified boundaries.

Result: I demonstrated the pipeline's effectiveness on three dummy legacy systems with varying complexity, including threshold-based, combined-conditional, and non-linear range logic.

Conclusion: The RL agent successfully focuses exploration on relevant boundary regions, and the extracted rules accurately reflect the core logic of the underlying dummy systems, providing a promising foundation for generating specifications and test cases during legacy migration.

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [93] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: This study investigates the impact of ChatGPT on student cognitive engagement during academic writing, finding that AI assistance may lead to lower engagement.


<details>
  <summary>Details</summary>
Motivation: Concerns have emerged about the potential of large language models (LLMs) to reduce deep thinking and active learning.

Method: Experimental design with participants randomly assigned to either an AI-assisted (ChatGPT) or a non-assisted (control) condition. Participants completed a structured argumentative writing task followed by a cognitive engagement scale (CES).

Result: Significantly lower cognitive engagement scores in the ChatGPT group compared to the control group.

Conclusion: AI assistance may lead to cognitive offloading, potentially compromising self-regulated learning and deep cognitive involvement of students.

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [94] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: Introduces xHAIM, an explainable AI framework for medicine that improves prediction and explainability by leveraging Generative AI and linking predictions to patient-specific medical knowledge.


<details>
  <summary>Details</summary>
Motivation: HAIM uses data in a task-agnostic manner and lacks explainability.

Method: a novel framework leveraging Generative AI to enhance both prediction and explainability through four structured steps: (1) automatically identifying task-relevant patient data across modalities, (2) generating comprehensive patient summaries, (3) using these summaries for improved predictive modeling, and (4) providing clinical explanations by linking predictions to patient-specific medical knowledge.

Result: xHAIM improves average AUC from 79.9% to 90.3% across chest pathology and operative tasks.

Conclusion: xHAIM transforms AI from a black-box predictor into an explainable decision support system, enabling clinicians to interactively trace predictions back to relevant patient data, bridging AI advancements with clinical utility.

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [95] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: Using ML to solve routing problems like TSP and VRP.


<details>
  <summary>Details</summary>
Motivation: Exact algorithms for NP-hard combinatorial optimization problems often require excessive computational time, while heuristics can only provide approximate solutions without guaranteeing optimality. Machine learning models have shown recent success.

Method: We propose a taxonomy categorizing ML-based routing methods into construction-based and improvement-based approaches, highlighting their applicability to various problem characteristics.

Result: Reviews current progress in applying machine learning tools to solve NP-hard combinatorial optimization problems, focusing on routing problems like TSP and VRP.

Conclusion: This review integrates traditional OR methods with state-of-the-art ML techniques, providing a structured framework to guide future research and address emerging VRP variants.

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [96] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO teaches non-reasoner models to internalize structured search behavior through a synthetic dataset and improves performance via RL.


<details>
  <summary>Details</summary>
Motivation: It is yet unclear how to boost the reasoning capabilities of non-reasoner models including Llama 3.

Method: Training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs; a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories; finetuning models on search-derived traces and further improve performance via RL with verifiable rewards.

Result: Absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024.

Conclusion: Search-inspired training can instill robust reasoning capabilities into open LLMs.

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [97] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 数学能力强的模型不一定在其他领域表现出色。强化学习比监督微调更好。


<details>
  <summary>Details</summary>
Motivation: 数学推理已成为大型语言模型（LLM）进步的代表，但这些进步是否反映了更广泛的问题解决能力，还是仅仅反映了狭义的过度拟合？

Method: 对Qwen3-14B模型使用数学专用数据但不同的调整方法进行对照实验。对潜在空间表示和token空间分布偏移进行分析。

Result: 大多数在数学方面成功的模型未能将其成果推广到其他领域。强化学习（RL）调整的模型在各个领域都能很好地推广，而监督微调（SFT）调整的模型常常会忘记一般能力。

Conclusion: 标准监督微调（SFT）导致显著的表征和输出漂移，而强化学习（RL）保留了一般领域结构。结果表明需要重新思考标准后训练方法，特别是依赖于SFT提取的数据来推进推理模型。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [98] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: advance local search for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA for short) by introduce a two-dimensional cell-jump move, propose an extended local search framework, named $2d$-LS integrating the model constructing satisfiability calculus (MCSAT) framework to improve search efficiency, implement a recently proposed technique called sample-cell projection operator for MCSAT, design a hybrid framework for SMT-NRA combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through information exchange.


<details>
  <summary>Details</summary>
Motivation: advance local search for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA for short)

Method: introduce a two-dimensional cell-jump move, called $2d$-cell-jump, generalizing the key operation, cell-jump, of the local search method for SMT-NRA. Then, we propose an extended local search framework, named $2d$-LS integrating the model constructing satisfiability calculus (MCSAT) framework to improve search efficiency. To further improve the efficiency of MCSAT, we implement a recently proposed technique called sample-cell projection operator for MCSAT, which is well suited for CDCL-style search in the real domain and helps guide the search away from conflicting states. Finally, we design a hybrid framework for SMT-NRA combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through information exchange.

Result: improvements in local search performance

Conclusion: The experimental results demonstrate improvements in local search performance, highlighting the effectiveness of the proposed methods.

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [99] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: RL for LLMs in chess strategic reasoning shows promise but plateaus below expert levels due to limitations in the pretrained models' understanding of chess.


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLMs can develop strategic reasoning capabilities through RL in chess, an area that remains largely unexplored.

Method: Leverage a chess-pretrained action-value network to provide dense reward on the LLM's output move quality, which can be seen as a form of knowledge distillation.

Result: Distillation-based dense rewards often outperform sparse binary rewards.

Conclusion: LLMs plateau far below expert levels in chess strategic reasoning, suggesting a deficit in their internal understanding of chess that RL alone cannot fully overcome.

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [100] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: 提出了一种改进的数值算法来解决minimax问题，该算法具有收敛性证明，可广泛应用于各个领域


<details>
  <summary>Details</summary>
Motivation: 解决 minimax 问题

Method: 改进的数值算法，基于非光滑优化，二次规划和迭代过程

Result: 为该算法在梯度连续性和有界性等温和假设下提供了严格的收敛性证明

Conclusion: 该论文提出了一种改进的数值算法，用于解决基于非光滑优化、二次规划和迭代过程的 minimax 问题，并为该算法在梯度连续性和有界性等温和假设下提供了严格的收敛性证明。 该算法可广泛应用于鲁棒优化、不平衡学习等领域。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [101] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: 研究了移动多模态智能体的安全性问题，提出了一种结合行为序列信息的风险识别机制和基于大型语言模型的自动化辅助评估方案，初步验证表明可以提高风险识别并降低越狱概率。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型驱动的智能体系统面临越狱风险，攻击者可能诱导智能体绕过行为约束，触发风险操作。现有的安全措施在复杂交互中存在局限性，缺乏高效一致的自动化评估方法。

Method: 通过整合行为序列信息，构建风险判别机制，并设计了一种基于大型语言模型的自动化辅助评估方案。

Result: 在几个代表性的高风险任务中进行了初步验证，结果表明该方法可以提高对风险行为的识别。

Conclusion: 该方法可以在一定程度上提高对风险行为的识别，并有助于降低智能体被破解的可能性。希望这项研究能为多模态智能体系统的安全风险建模和保护提供一些有价值的参考。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [102] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: This paper discusses the limitations of current AI models and explores pathways towards AGI, emphasizing the importance of memory, reasoning, and modularity.


<details>
  <summary>Details</summary>
Motivation: Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency.

Method: cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior.

Result: VLMs are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior.

Conclusion: True intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior.

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [103] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: This paper presents CIP, a technique using causal influence diagrams to make LLM-powered agents safer, with positive results in code execution and mobile device control.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safe and reliable behavior of autonomous agents powered by large language models (LLMs) is crucial for preventing unintended consequences.

Method: The method consists of three key steps: (1) initializing a CID based on task specifications, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes.

Result: Experimental results demonstrate that the proposed method effectively enhances safety in both code execution and mobile device control tasks.

Conclusion: This paper introduces a novel technique called CIP that uses causal influence diagrams (CIDs) to improve the safety of autonomous agents powered by large language models (LLMs). Experimental results show that CIP enhances safety in code execution and mobile device control tasks.

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [104] [Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)](https://arxiv.org/abs/2507.00094)
*Jacobo Casas-Ramos,Sarah Winkler,Alessandro Gianola,Marco Montali,Manuel Mucientes,Manuel Lama*

Main category: cs.DB

TL;DR: This paper presents a new algorithm for data-aware conformance checking of declarative process models that is both efficient and expressive, using A* search and SMT solving.


<details>
  <summary>Details</summary>
Motivation: Alignment-based conformance checking for declarative process models has focused on pure control-flow specifications, or mild data-aware extensions limited to numerical data and variable-to-constant comparisons. Finding alignments is computationally hard, especially with data dependencies.

Method: The authors introduce a novel algorithmic technique that efficiently explores the search space, generating descendant states through the application of repair actions aiming at incrementally resolving constraint violations.

Result: The evaluation witnesses that our approach matches or surpasses the performance of the state of the art while also supporting significantly more expressive data dependencies.

Conclusion: This paper introduces an algorithm for computing data-aware optimal alignments for data-aware Declare models with general data types and data conditions, combining A* search and SMT solving.

Abstract: Despite growing interest in process analysis and mining for data-aware
specifications, alignment-based conformance checking for declarative process
models has focused on pure control-flow specifications, or mild data-aware
extensions limited to numerical data and variable-to-constant comparisons. This
is not surprising: finding alignments is computationally hard, even more so in
the presence of data dependencies. In this paper, we challenge this problem in
the case where the reference model is captured using data-aware Declare with
general data types and data conditions. We show that, unexpectedly, it is
possible to compute data-aware optimal alignments in this rich setting,
enjoying at once efficiency and expressiveness. This is achieved by carefully
combining the two best-known approaches to deal with control flow and data
dependencies when computing alignments, namely A* search and SMT solving.
Specifically, we introduce a novel algorithmic technique that efficiently
explores the search space, generating descendant states through the application
of repair actions aiming at incrementally resolving constraint violations. We
prove the correctness of our algorithm and experimentally show its efficiency.
The evaluation witnesses that our approach matches or surpasses the performance
of the state of the art while also supporting significantly more expressive
data dependencies, showcasing its potential to support real-world applications.

</details>


### [105] [LIMAO: A Framework for Lifelong Modular Learned Query Optimization](https://arxiv.org/abs/2507.00188)
*Qihan Zhang,Shaolin Xie,Ibrahim Sabek*

Main category: cs.DB

TL;DR: LIMAO是一种用于计划成本预测的终身学习框架，可以无缝集成到现有的LQO中，从而有效缓解灾难性遗忘，确保长期稳定的计划质量。


<details>
  <summary>Details</summary>
Motivation: 大多数学习型查询优化器(LQO)在静态查询环境下运行，无法有效处理实际场景中复杂的动态查询环境。广泛的再培训会导致灾难性遗忘问题，从而降低LQO随时间的推移的泛化能力。

Method: LIMAO利用模块化终身学习技术、基于注意力的神经网络组合架构和高效的训练范式，旨在保留先前的知识，同时不断适应新的环境。

Result: LIMAO显著提高了LQO的性能，在动态工作负载下，查询执行时间提高了40%，执行时间方差降低了60%。

Conclusion: LIMAO通过利用精确和自洽的设计，有效缓解了灾难性遗忘，确保了计划质量的稳定性和可靠性。与Postgres相比，LIMAO在选定的基准测试中实现了高达4倍的加速，突出了其在实际查询优化中的实际优势。

Abstract: Query optimizers are crucial for the performance of database systems.
Recently, many learned query optimizers (LQOs) have demonstrated significant
performance improvements over traditional optimizers. However, most of them
operate under a limited assumption: a static query environment. This limitation
prevents them from effectively handling complex, dynamic query environments in
real-world scenarios. Extensive retraining can lead to the well-known
catastrophic forgetting problem, which reduces the LQO generalizability over
time. In this paper, we address this limitation and introduce LIMAO (Lifelong
Modular Learned Query Optimizer), a framework for lifelong learning of plan
cost prediction that can be seamlessly integrated into existing LQOs. LIMAO
leverages a modular lifelong learning technique, an attention-based neural
network composition architecture, and an efficient training paradigm designed
to retain prior knowledge while continuously adapting to new environments. We
implement LIMAO in two LQOs, showing that our approach is agnostic to
underlying engines. Experimental results show that LIMAO significantly enhances
the performance of LQOs, achieving up to a 40% improvement in query execution
time and reducing the variance of execution time by up to 60% under dynamic
workloads. By leveraging a precise and self-consistent design, LIMAO
effectively mitigates catastrophic forgetting, ensuring stable and reliable
plan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup
on selected benchmarks, highlighting its practical advantages in real-world
query optimization.

</details>


### [106] [Meaningful Data Erasure in the Presence of Dependencies](https://arxiv.org/abs/2507.00343)
*Vishal Chakraborty,Youri Kaminsky,Sharad Mehrotra,Felix Naumann,Faisal Nawab,Primal Pappachan,Mohammad Sadoghi,Nalini Venkatasubramanian*

Main category: cs.DB

TL;DR: This paper defines a precise notion of data erasure for GDPR compliance in databases, designs efficient erasure mechanisms, and demonstrates their practicality and scalability.


<details>
  <summary>Details</summary>
Motivation: Data regulations like GDPR require systems to support data erasure but leave the definition of 'erasure' open to interpretation. This ambiguity makes compliance challenging, especially in databases where data dependencies can lead to erased data being inferred from remaining data.

Method: The paper designs erasure mechanisms that enforce the defined guarantee at minimal cost. It also explores strategies to balance cost and throughput, batch multiple erasures, and proactively compute data retention times.

Result: The paper formally defines a precise notion of data erasure that ensures any inference about deleted data, through dependencies, remains bounded to what could have been inferred before its insertion.

Conclusion: The paper demonstrates practical and scalable algorithms for data erasure using real and synthetic datasets.

Abstract: Data regulations like GDPR require systems to support data erasure but leave
the definition of "erasure" open to interpretation. This ambiguity makes
compliance challenging, especially in databases where data dependencies can
lead to erased data being inferred from remaining data. We formally define a
precise notion of data erasure that ensures any inference about deleted data,
through dependencies, remains bounded to what could have been inferred before
its insertion. We design erasure mechanisms that enforce this guarantee at
minimal cost. Additionally, we explore strategies to balance cost and
throughput, batch multiple erasures, and proactively compute data retention
times when possible. We demonstrate the practicality and scalability of our
algorithms using both real and synthetic datasets.

</details>


### [107] [Towards Robustness: A Critique of Current Vector Database Assessments](https://arxiv.org/abs/2507.00379)
*Zikai Wang,Qianxi Zhang,Baotong Lu,Qi Chen,Cheng Tan*

Main category: cs.DB

TL;DR: 平均召回率作为向量数据库评估指标存在问题，因为它掩盖了查询之间的差异性。我们提出了 Robustness-$\\delta$@K 指标来解决这个问题，该指标能更好反应向量数据库的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 平均召回率掩盖了查询之间的可变性，导致具有强大平均性能的系统在困难查询上的表现明显不佳。这些尾部用例会混淆用户，并可能导致下游应用程序（如 RAG）失败。

Method: 提出 Robustness-$\\delta$@K，一种新的指标，用于捕获召回率高于阈值 $\\delta$ 的查询比例。

Result: 将 Robustness-$\\delta$@K 集成到现有基准中，并评估主流向量索引，揭示了显着的鲁棒性差异。

Conclusion: 鲁棒性更强的向量索引能产生更好的应用性能，即使平均召回率相同。设计因素会影响鲁棒性，为提高实际性能提供了指导。

Abstract: Vector databases are critical infrastructure in AI systems, and average
recall is the dominant metric for their evaluation. Both users and researchers
rely on it to choose and optimize their systems. We show that relying on
average recall is problematic. It hides variability across queries, allowing
systems with strong mean performance to underperform significantly on hard
queries. These tail cases confuse users and can lead to failure in downstream
applications such as RAG. We argue that robustness consistently achieving
acceptable recall across queries is crucial to vector database evaluation. We
propose Robustness-$\delta$@K, a new metric that captures the fraction of
queries with recall above a threshold $\delta$. This metric offers a deeper
view of recall distribution, helps vector index selection regarding application
needs, and guides the optimization of tail performance. We integrate
Robustness-$\delta$@K into existing benchmarks and evaluate mainstream vector
indexes, revealing significant robustness differences. More robust vector
indexes yield better application performance, even with the same average
recall. We also identify design factors that influence robustness, providing
guidance for improving real-world performance.

</details>


### [108] [Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition](https://arxiv.org/abs/2507.00427)
*Hao Wu,Changzheng Wei,Yanhao Wang,Li Lin,Yilong Leng,Shiyu He,Minghao Zhao,Hanghang Wu,Ying Yan,Aoying Zhou*

Main category: cs.DB

TL;DR: 本文提出 ZKGraph，一个为图数据库提供可验证查询处理同时保护数据隐私的系统，通过将图查询分解为更小的算子并优化 ZKP 电路实现。


<details>
  <summary>Details</summary>
Motivation: 为图数据库实现零知识可验证性面临挑战，因为图数据库中查询的复杂性较高，直接将图查询转换为算术电路会导致电路规模过大，难以实际评估。

Method: 将图查询分解为更细粒度的、原始的算子，从而能够通过更小规模的电路进行逐步评估。设计了以扩展为中心的算子分解方法，并为扩展原语和各种属性设计了专门的 ZKP 电路，这些电路充分利用了 PLONKish 算术化。

Result: ZKGraph 在运行时间和内存消耗方面都取得了显著的改进，优于图算子的原生电路内实现。

Conclusion: ZKGraph系统通过集成优化的电路，实现了可验证的查询处理，同时保护了数据隐私。性能评估表明，ZKGraph 显著优于图算子的原生电路内实现，在运行时间和内存消耗方面都取得了显著的改进。

Abstract: This paper investigates the feasibility of achieving zero-knowledge
verifiability for graph databases, enabling database owners to
cryptographically prove the query execution correctness without disclosing the
underlying data. Although similar capabilities have been explored for
relational databases, their implementation for graph databases presents unique
challenges. This is mainly attributed to the relatively large complexity of
queries in graph databases. When translating graph queries into arithmetic
circuits, the circuit scale can be too large to be practically evaluated. To
address this issue, we propose to break down graph queries into more
fine-grained, primitive operators, enabling a step-by-step evaluation through
smaller-scale circuits. Accordingly, the verification with ZKP circuits of
complex graph queries can be decomposed into a series of composable
cryptographic primitives, each designed to verify a fundamental structural
property such as path ordering or edge directionality. Especially, having
noticed that the graph expansion (i.e., traversing from nodes to their
neighbors along edges) operation serves as the backbone of graph query
evaluation, we design the expansion centric operator decomposition. In addition
to constructing circuits for the expansion primitives, we also design
specialized ZKP circuits for the various attributes that augment this
traversal. The circuits are meticulously designed to take advantage of PLONKish
arithmetization. By integrating these optimized circuits, we implement ZKGraph,
a system that provides verifiable query processing while preserving data
privacy. Performance evaluation indicates that ZKGraph significantly
outperforms naive in circuit implementations of graph operators, achieving
substantial improvements in both runtime and memory consumption.

</details>


### [109] [Towards Efficient Random-Order Enumeration for Join Queries](https://arxiv.org/abs/2507.00489)
*Pengyu Chen,Zizheng Guo,Jianwei Yang,Dongjing Miao*

Main category: cs.DB

TL;DR: This paper introduces a near-optimal random-order enumeration algorithm for join queries that is faster than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack an efficient random-order enumeration algorithm with a worst-case runtime guarantee for (cyclic) join queries. The goal is to produce join results in uniformly random order for statistically meaningful representation.

Method: The authors develop an efficient random-order enumeration algorithm for join queries with specific time and delay complexities. They also devise two non-trivial techniques to speed up the enumeration.

Result: The proposed algorithm achieves specific time and delay complexities, is near-optimal, requires no query-specific preprocessing, and outperforms existing state-of-the-art methods in experiments.

Conclusion: The paper presents an efficient random-order enumeration algorithm for join queries, which is near-optimal and outperforms existing methods.

Abstract: In many data analysis pipelines, a basic and time-consuming process is to
produce join results and feed them into downstream tasks. Numerous enumeration
algorithms have been developed for this purpose. To be a statistically
meaningful representation of the whole join result, the result tuples are
required to be enumerated in uniformly random order. However, existing studies
lack an efficient random-order enumeration algorithm with a worst-case runtime
guarantee for (cyclic) join queries. In this paper, we study the problem of
enumerating the results of a join query in random order. We develop an
efficient random-order enumeration algorithm for join queries with no large
hidden constants in its complexity, achieving expected
$O(\frac{\mathrm{AGM}(Q)}{|Res(Q)|}\log^2|Q|)$ delay,
$O(\mathrm{AGM}(Q)\log|Q|)$ total running time after $O(|Q|\log|Q|)$-time index
construction, where $|Q|$ is the size of input, $\mathrm{AGM}(Q)$ is the AGM
bound, and $|Res(Q)|$ is the size of the join result. We prove that our
algorithm is near-optimal in the worst case, under the combinatorial $k$-clique
hypothesis. Our algorithm requires no query-specific preprocessing and can be
flexibly adapted to many common database indexes with only minor modifications.
We also devise two non-trivial techniques to speed up the enumeration, and
provide an experimental study on our enumeration algorithm along with the
speed-up techniques. The experimental results show that our algorithm, enhanced
with the proposed techniques, significantly outperforms existing
state-of-the-art methods.

</details>


### [110] [RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries](https://arxiv.org/abs/2507.00839)
*Chiyu Hao,Jixian Su,Shixuan Sun,Hao Zhang,Sen Gao,Jianwen Zhao,Chenyi Zhang,Jieru Zhao,Chen Chen,Minyi Guo*

Main category: cs.DB

TL;DR: RapidStore 是一种用于高效内存动态图存储的整体方法，专为读取密集型工作负载而设计，通过分离读写操作并解耦版本控制来优化并发性能。


<details>
  <summary>Details</summary>
Motivation: 现有的动态图存储系统在有效处理并发读写操作时面临重大挑战，因为写查询会干扰读取效率，并且由于每个边的版本控制而产生大量时间和空间开销，并且无法平衡性能，例如并发工作负载下的搜索速度较慢。

Method: RapidStore 采用了一种整体方法，通过分离读写查询的管理和将版本数据与图数据解耦的解耦系统设计，利用了图查询的特性，设计了一个高效的动态图存储，以配合图并发控制机制。

Result: 实验结果表明，RapidStore 能够实现快速且可扩展的并发图查询，有效平衡插入、搜索和扫描的性能，并显著提高动态图存储系统的效率。

Conclusion: RapidStore 通过分离读写查询管理和解耦版本数据与图数据，实现了快速且可扩展的并发图查询，有效平衡了插入、搜索和扫描的性能，并显著提高了动态图存储系统的效率。

Abstract: Dynamic graph storage systems are essential for real-time applications such
as social networks and recommendation, where graph data continuously evolves.
However, they face significant challenges in efficiently handling concurrent
read and write operations. We find that existing methods suffer from write
queries interfering with read efficiency, substantial time and space overhead
due to per-edge versioning, and an inability to balance performance, such as
slow searches under concurrent workloads. To address these issues, we propose
RapidStore, a holistic approach for efficient in-memory dynamic graph storage
designed for read-intensive workloads. Our key idea is to exploit the
characteristics of graph queries through a decoupled system design that
separates the management of read and write queries and decouples version data
from graph data. Particularly, we design an efficient dynamic graph store to
cooperate with the graph concurrency control mechanism. Experimental results
demonstrate that RapidStore enables fast and scalable concurrent graph queries,
effectively balancing the performance of inserts, searches, and scans, and
significantly improving efficiency in dynamic graph storage systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [111] [Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via Continual Pre-training](https://arxiv.org/abs/2507.00477)
*Qi Wang,Yixuan Cao,Yifan Liu,Jiangtao Zhao,Ping Luo*

Main category: cs.IR

TL;DR: R&R通过在专业文档上进行持续预训练，解决了RAG在专业领域由于领域知识有限而导致的重写模型困难问题，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 基于检索增强生成（RAG）的问答（QA）系统通过检索基于用户查询的相关文档来增强大型语言模型的知识。用户查询和文档措辞之间的差异通常需要重写查询。但是，在专业领域中，由于领域知识有限，重写模型可能会遇到困难。

Method: 我们提出了R&R（重写前阅读文档）重写器，它涉及在专业文档上进行持续的预训练，类似于学生通过复习教科书来准备开卷考试。

Result: R&R擅长于多个领域的专业QA，有效地弥合了查询-文档之间的差距，同时在一般场景中保持了良好的性能。

Conclusion: R&R在多个数据集上的实验表明，R&R擅长于多个领域的专业QA，有效地弥合了查询-文档之间的差距，同时在一般场景中保持了良好的性能，从而推进了基于RAG的QA系统在专业领域的应用。

Abstract: A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system
enhances a large language model's knowledge by retrieving relevant documents
based on user queries. Discrepancies between user queries and document
phrasings often necessitate query rewriting. However, in specialized domains,
the rewriter model may struggle due to limited domain-specific knowledge. To
resolve this, we propose the R\&R (Read the doc before Rewriting) rewriter,
which involves continual pre-training on professional documents, akin to how
students prepare for open-book exams by reviewing textbooks. Additionally, it
can be combined with supervised fine-tuning for improved results. Experiments
on multiple datasets demonstrate that R\&R excels in professional QA across
multiple domains, effectively bridging the query-document gap, while
maintaining good performance in general scenarios, thus advancing the
application of RAG-based QA systems in specialized fields.

</details>


### [112] [On Mitigating Data Sparsity in Conversational Recommender Systems](https://arxiv.org/abs/2507.00479)
*Sixiao Zhang,Mingrui Liu,Cheng Long,Wei Yuan,Hongxu Chen,Xiangyu Zhao,Hongzhi Yin*

Main category: cs.IR

TL;DR: This paper proposes DACRS, a conversational recommender system model that addresses data sparsity issues in dialogue and item spaces through dialogue augmentation, knowledge-guided entity modeling, and dialogue-entity matching. DACRS achieves state-of-the-art results on two public datasets.


<details>
  <summary>Details</summary>
Motivation: Conversational recommender systems (CRSs) capture user preference through textual information in dialogues. However, they suffer from data sparsity on two fronts: the dialogue space is vast and linguistically diverse, while the item space exhibits long-tail and sparse distributions. Existing methods struggle with (1) generalizing to varied dialogue expressions due to underutilization of rich textual cues, and (2) learning informative item representations under severe sparsity.

Method: a CRS model named DACRS. It consists of three modules, namely Dialogue Augmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching. In the Dialogue Augmentation module, we apply a two-stage augmentation pipeline to augment the dialogue context to enrich the data and improve generalizability. In the Knowledge-Guided Entity Modeling, we propose a knowledge graph (KG) based entity substitution and an entity similarity constraint to enhance the expressiveness of entity embeddings. In the Dialogue-Entity Matching module, we fuse the dialogue embedding with the mentioned entity embeddings through a dialogue-guided attention aggregation to acquire user embeddings that contain both the explicit and implicit user preferences.

Result: DACRS achieves state-of-the-art performance on two public datasets.

Conclusion: Extensive experiments on two public datasets demonstrate the state-of-the-art performance of DACRS.

Abstract: Conversational recommender systems (CRSs) capture user preference through
textual information in dialogues. However, they suffer from data sparsity on
two fronts: the dialogue space is vast and linguistically diverse, while the
item space exhibits long-tail and sparse distributions. Existing methods
struggle with (1) generalizing to varied dialogue expressions due to
underutilization of rich textual cues, and (2) learning informative item
representations under severe sparsity. To address these problems, we propose a
CRS model named DACRS. It consists of three modules, namely Dialogue
Augmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching.
In the Dialogue Augmentation module, we apply a two-stage augmentation pipeline
to augment the dialogue context to enrich the data and improve
generalizability. In the Knowledge-Guided Entity Modeling, we propose a
knowledge graph (KG) based entity substitution and an entity similarity
constraint to enhance the expressiveness of entity embeddings. In the
Dialogue-Entity Matching module, we fuse the dialogue embedding with the
mentioned entity embeddings through a dialogue-guided attention aggregation to
acquire user embeddings that contain both the explicit and implicit user
preferences. Extensive experiments on two public datasets demonstrate the
state-of-the-art performance of DACRS.

</details>


### [113] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: MassTool is a multi-task framework that enhances query representation and tool retrieval accuracy for LLMs by using a two-tower architecture and search-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches often neglect the importance of precise query comprehension in tool retrieval for large language models (LLMs).

Method: MassTool uses a two-tower architecture with a query-centric graph convolution network (QC-GCN) and search-based user intent modeling (SUIM).

Result: Extensive experiments demonstrate MassTool's effectiveness in improving retrieval accuracy.

Conclusion: MassTool improves retrieval accuracy through a multi-task search-based framework.

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


### [114] [\texttt{WebANNS}: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers](https://arxiv.org/abs/2507.00521)
*Mugeng Liu,Siqi Zhong,Qi Yang,Yudong Han,Xuanzhe Liu,Yun Ma*

Main category: cs.IR

TL;DR: WebANNS是一个专为浏览器设计的高效ANNS引擎，它解决了现有方案的局限性，显著提升了查询速度和内存效率，使得浏览器内ANNS更实用。


<details>
  <summary>Details</summary>
Motivation: 现有的近似最近邻搜索（ANNS）引擎未能全面解决Web浏览器在计算限制、外部存储访问问题和内存利用约束方面带来的独特挑战，而ANNS对于检索增强生成（RAG）应用至关重要。

Method: WebANNS：一种专为Web浏览器设计的新型ANNS引擎，它利用WebAssembly、延迟加载策略和启发式方法。

Result: WebANNS在第99百分位的查询延迟方面比SOTA引擎提高了高达743.8倍，同时将内存使用率降低了高达39％。WebANNS将查询时间从10秒减少到10毫秒范围，使得浏览器中的ANNS在用户可接受的延迟下变得实用。

Conclusion: WebANNS通过利用WebAssembly克服计算瓶颈，设计延迟加载策略以优化从外部存储的数据检索，并应用启发式方法来减少内存使用，从而在浏览器中实现了快速且内存高效的近似最近邻搜索。

Abstract: Approximate nearest neighbor search (ANNS) has become vital to modern AI
infrastructure, particularly in retrieval-augmented generation (RAG)
applications. Numerous in-browser ANNS engines have emerged to seamlessly
integrate with popular LLM-based web applications, while addressing privacy
protection and challenges of heterogeneous device deployments. However, web
browsers present unique challenges for ANNS, including computational
limitations, external storage access issues, and memory utilization
constraints, which state-of-the-art (SOTA) solutions fail to address
comprehensively.
  We propose \texttt{WebANNS}, a novel ANNS engine specifically designed for
web browsers. \texttt{WebANNS} leverages WebAssembly to overcome computational
bottlenecks, designs a lazy loading strategy to optimize data retrieval from
external storage, and applies a heuristic approach to reduce memory usage.
Experiments show that \texttt{WebANNS} is fast and memory efficient, achieving
up to $743.8\times$ improvement in 99th percentile query latency over the SOTA
engine, while reducing memory usage by up to 39\%. Note that \texttt{WebANNS}
decreases query time from 10 seconds to the 10-millisecond range in browsers,
making in-browser ANNS practical with user-acceptable latency.

</details>


### [115] [Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support](https://arxiv.org/abs/2507.00535)
*Dietmar Jannach,Amra Delić,Francesco Ricci,Markus Zanker*

Main category: cs.IR

TL;DR: Group recommender systems need a revamp! Let's use AI assistants in chat interfaces to make them more user-friendly and actually useful.


<details>
  <summary>Details</summary>
Motivation: The lack of real-world group recommender systems despite extensive research suggests that common assumptions in academic research may not align with user needs and expectations.

Method: The paper proposes integrating AI-based group recommendation agents into chat-based interactions to assist the decision-making process.

Result: The paper envisions a future where group recommender systems involve human group members interacting in a chat, with an AI agent assisting in decision-making, leading to a more natural environment and wider adoption.

Conclusion: This paper advocates for a reorientation in group recommender systems research, leveraging Generative AI assistants to create more natural group decision-making environments and promote wider adoption.

Abstract: More than twenty-five years ago, first ideas were developed on how to design
a system that can provide recommendations to groups of users instead of
individual users. Since then, a rich variety of algorithmic proposals were
published, e.g., on how to acquire individual preferences, how to aggregate
them, and how to generate recommendations for groups of users. However, despite
the rich literature on the topic, barely any examples of real-world group
recommender systems can be found. This lets us question common assumptions in
academic research, in particular regarding communication processes in a group
and how recommendation-supported decisions are made. In this essay, we argue
that these common assumptions and corresponding system designs often may not
match the needs or expectations of users. We thus call for a reorientation in
this research area, leveraging the capabilities of modern Generative AI
assistants like ChatGPT. Specifically, as one promising future direction, we
envision group recommender systems to be systems where human group members
interact in a chat and an AI-based group recommendation agent assists the
decision-making process in an agentic way. Ultimately, this shall lead to a
more natural group decision-making environment and finally to wider adoption of
group recommendation systems in practice.

</details>


### [116] [Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications](https://arxiv.org/abs/2507.00543)
*Leila Tavakoli,Hamed Zamani*

Main category: cs.IR

TL;DR: LLMs在细粒度标注任务中表现不佳，但人机协作可以提高性能并减少人工。


<details>
  <summary>Details</summary>
Motivation: 尽管人们越来越有兴趣使用大型语言模型(llm)来自动化注释，但它们在复杂、细致和多维标记任务中的有效性仍有待探索。

Method: 系统评估

Result: LLM预测通常不一致，校准不良，并且对提示变化高度敏感。轻量级干预显著提高了注释可靠性，同时减少了高达45%的人工工作量。

Conclusion: LLMs在主观或细粒度评估任务中难以达到人类水平，但通过结合置信度阈值和模型间差异的HITL工作流程，可以在显著提高注释可靠性的同时，减少高达45%的人工工作量。

Abstract: Despite growing interest in using large language models (LLMs) to automate
annotation, their effectiveness in complex, nuanced, and multi-dimensional
labelling tasks remains relatively underexplored. This study focuses on
annotation for the search clarification task, leveraging a high-quality,
multi-dimensional dataset that includes five distinct fine-grained annotation
subtasks. Although LLMs have shown impressive capabilities in general settings,
our study reveals that even state-of-the-art models struggle to replicate
human-level performance in subjective or fine-grained evaluation tasks. Through
a systematic assessment, we demonstrate that LLM predictions are often
inconsistent, poorly calibrated, and highly sensitive to prompt variations. To
address these limitations, we propose a simple yet effective human-in-the-loop
(HITL) workflow that uses confidence thresholds and inter-model disagreement to
selectively involve human review. Our findings show that this lightweight
intervention significantly improves annotation reliability while reducing human
effort by up to 45%, offering a relatively scalable and cost-effective yet
accurate path forward for deploying LLMs in real-world evaluation settings.

</details>


### [117] [EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens](https://arxiv.org/abs/2507.00715)
*Chaoqun Yang,Xinyu Lin,Wenjie Wang,Yongqi Li,Teng Sun,Xianjing Han,Tat-Seng Chua*

Main category: cs.IR

TL;DR: EARN是一种用于加速LLMRec推理的新框架，通过压缩早期层的信息到寄存器令牌并在后续层中关注这些令牌来实现。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型的生成式推荐（LLMRec）取得了显著成功，但由于大量的计算开销和KV缓存的内存压力，它存在高推理延迟的问题。现有的KV缓存减少方法面临着关键的限制：考虑到推荐任务的短解码步骤，缓存压缩提供的加速效果微乎其微，而提示压缩则有丢弃重要交互历史的风险。

Method: EARN，一种高效的推理框架，它利用早期层将信息压缩到位于输入序列边界的寄存器令牌中，然后在后续层中仅关注这些令牌。

Result: 在三个数据集、两种LLMRec方法和两种LLM架构上进行的大量实验表明，EARN具有优越性，与通用微调方法相比，实现了高达3.79倍的加速和80.8%的KV缓存减少，同时具有更高的准确性。

Conclusion: EARN通过利用早期层将信息压缩到位于输入序列边界的寄存器令牌中，然后在后续层中仅关注这些令牌，从而在LLMRec中实现了效率和效果的平衡，为工业场景提供了实际的部署优势。

Abstract: Large Language Model-based generative recommendation (LLMRec) has achieved
notable success, but it suffers from high inference latency due to massive
computational overhead and memory pressure of KV Cache. Existing KV Cache
reduction methods face critical limitations: cache compression offers marginal
acceleration given recommendation tasks' short decoding steps, while prompt
compression risks discarding vital interaction history. Through systematic
analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)
layer-wise attention sparsity inversion where early layers retain dense
informative patterns while later layers exhibit high redundancy, and 2) dual
attention sinks phenomenon where attention scores concentrate on both head and
tail tokens of input sequences. Motivated by these insights, we propose EARN,
an efficient inference framework that leverages the early layers to compress
information into register tokens placed at the input sequence boundaries, then
focuses solely on these tokens in the subsequent layers. Extensive experiments
on three datasets, two LLMRec methods and two LLM architectures demonstrate
EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction
with better accuracy than the general finetuning approach. Our work bridges the
efficiency-effectiveness gap in LLMRec, offering practical deployment
advantages for industrial scenarios.

</details>


### [118] [WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks](https://arxiv.org/abs/2507.00938)
*Zihao Sun,Meng Fang,Ling Chen*

Main category: cs.IR

TL;DR: 提出了 WebArXiv，这是一个静态且时不变的基准，包含 275 个基于 web 的任务，这些任务基于 arXiv 平台。为了解决这个问题，我们提出了一种轻量级的动态反射机制，允许 agent 在决策过程中选择性地检索相关的过去步骤。


<details>
  <summary>Details</summary>
Motivation: 现有基准的不稳定性和不一致性使得评估 web agent 具有挑战性，这些基准通常依赖于动态内容或过于简化的模拟。

Method: 提出了一种轻量级的动态反射机制，允许 agent 在决策过程中选择性地检索相关的过去步骤。

Result: WebArXiv 确保了可重复和可靠的评估，因为它将任务锚定在具有确定性 ground truth 和标准化动作轨迹的固定 web 快照中。确定了一种常见的失败模式，即 Rigid History Reflection，在这种模式下，agent 过度依赖于固定的交互历史。

Conclusion: 通过在 WebArXiv 上的评估，证明了不同 Web agent 之间的性能差异，并验证了所提出的反射策略的有效性。

Abstract: Recent progress in large language models (LLMs) has enabled the development
of autonomous web agents capable of navigating and interacting with real
websites. However, evaluating such agents remains challenging due to the
instability and inconsistency of existing benchmarks, which often rely on
dynamic content or oversimplified simulations. In this work, we introduce
WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks
grounded in the arXiv platform. WebArXiv ensures reproducible and reliable
evaluation by anchoring tasks in fixed web snapshots with deterministic ground
truths and standardized action trajectories. Through behavioral analysis, we
identify a common failure mode, Rigid History Reflection, where agents
over-rely on fixed interaction histories. To address this, we propose a
lightweight dynamic reflection mechanism that allows agents to selectively
retrieve relevant past steps during decision-making. We evaluate ten
state-of-the-art web agents on WebArXiv. Results demonstrate clear performance
differences across agents and validate the effectiveness of our proposed
reflection strategy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [119] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: 本文提出了一种新的存储框架HDRAM，通过结合经典、全息和量子启发式原理，提高了LLM的关联检索能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)表现出卓越的能力，但存在明显的精度损失，这里将其重新定义为信息传播。这种重构将问题从计算精度转移到信息论通信问题。

Method: 引入了HDRAM (全息定义随机存取存储器)，一个将Transformer潜在空间视为扩频信道的符号存储框架。HDRAM建立在超tokens之上，这是一种集成了经典纠错码(ECC)、全息计算和量子启发式搜索的结构化符号代码，通过有原则的解扩来恢复分布式信息。

Result: HDRAM通过相干相位存储地址实现了潜在空间中高效的键-值操作和Grover式搜索，在没有架构变化的情况下，显著提高了关联检索能力。

Conclusion: HDRAM通过结合ECC语法、压缩感知和Krylov子空间对齐，显著提高了关联检索能力，展示了经典-全息-量子启发式(CHQ)原理如何加强Transformer架构。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [120] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: NeutroSENSE, a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT environments, achieved 97% accuracy.


<details>
  <summary>Details</summary>
Motivation: interpretable intrusion detection in IoT environments

Method: Integrating Random Forest, XGBoost, and Logistic Regression with neutrosophic logic

Result: NeutroSENSE achieved 97% accuracy, while demonstrating that misclassified samples exhibit significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24).

Conclusion: Neutrosophic logic enhances both accuracy and explainability, providing a practical foundation for trust-aware AI in edge and fog-based IoT security systems.

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [121] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: 论文提出了一个名为定向随机技能搜索（DS3）的通用框架，用于优化大型语言模型的推理过程，通过对训练-推理的相互依赖性进行显式表征，加深理论理解并支持算法设计和资源分配。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在训练和部署期间需要大量的计算、能源和财务资源。推理成本现在代表了总体资源负担的一个重要且不断增长的组成部分，特别是对于以推理为中心的模型。

Method: 我们引入了定向随机技能搜索（DS3），这是一个通用框架，它将推理表示为在学习的技能图上的随机遍历。

Result: 我们从理论上恢复了经验观察到的模式，包括：精度随对数计算线性缩放；首选推理策略随任务难度和模型能力的变化而变化；即使在参数缩放下性能趋于稳定时，推理也会引发涌现行为；以及在统一的分析框架内捕获的 Best-of-N (BoN) 和多数投票行为。

Conclusion: 该框架加深了理论理解，并支持有原则的算法设计和资源分配。

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [122] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的电梯群控系统，该系统优于传统的基于规则的算法。


<details>
  <summary>Details</summary>
Motivation: 大型建筑中高效的电梯交通管理对于最大限度地减少乘客的旅行时间和能源消耗至关重要。由于基于启发式或模式检测的控制器难以处理调度的随机性和组合性。

Method: 将阿姆斯特丹自由大学的六部电梯、十五层楼系统建模为马尔可夫决策过程，并训练一个端到端的强化学习(RL)电梯群控系统(EGCS)。

Result: 该研究的关键创新包括一种新颖的动作空间编码，用于处理电梯调度的组合复杂性，引入了infra-step来模拟连续的乘客到达，以及定制的奖励信号来提高学习效率。此外，我们还探索了各种方法来调整折扣因子以适应infra-step公式。我们研究了基于Dueling Double Deep Q-learning的RL架构。

Conclusion: 提出的基于强化学习的电梯群控系统(EGCS)能够适应波动的交通模式，从高度随机的环境中学习，并因此优于传统的基于规则的算法。

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [123] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: 提出了一种新的训练方法CMIM，可以构建无法提炼的DNN，并且CMIM模型在预测精度方面也优于单独使用CE损失训练的模型。


<details>
  <summary>Details</summary>
Motivation: 为了保护DNN的知识产权，构建无法提炼的DNN是可取的。为此，首先观察到无法提炼的DNN可能具有这样的特性：其响应于具有相同标签的所有样本实例的输出概率分布的每个集群应该高度集中，以至于对应于每个标签的每个集群应该理想地崩溃为一个概率分布。

Method: 提出了一种新的训练方法，称为CMI最小化（CMIM）方法，该方法通过共同最小化传统交叉熵（CE）损失和整个温度范围内所有温度缩放集群的CMI值来训练DNN。

Result: CMIM模型无法通过所有测试的KD方法进行提炼，并且CMIM模型在预测精度方面优于单独使用CE损失训练的模型。

Conclusion: 通过大量实验表明，CMIM模型无法通过文献中已有的所有KD方法进行提炼。也就是说，由这些KD方法从CMIM模型中提炼出的仿冒学生不如各自的LS学生。此外，CMIM模型在预测精度方面也优于单独使用CE损失训练的模型。

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [124] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: ST-MTM is proposed to address the problem of forecasting complex time series. It uses seasonal-trend decomposition and a novel masking method to capture distinct temporal semantics. It achieves better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Simply masking a raw time series ignores the inherent semantic structure, which may cause MTM to learn spurious temporal patterns present in the raw data. To capture distinct temporal semantics, we show that masked modeling techniques should address entangled patterns through a decomposition approach.

Method: ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition, which includes a novel masking method for the seasonal-trend components that incorporates different temporal variations from each component. ST-MTM uses a period masking strategy for seasonal components and a sub-series masking strategy for trend components. Additionally, ST-MTM introduces a contrastive learning task to support masked modeling by enhancing contextual consistency among multiple masked seasonal representations.

Result: achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.

Conclusion: ST-MTM achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [125] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: Introduces SWE-Bench-CL, a continual learning benchmark for evaluating AI agents in software engineering, featuring chronologically ordered GitHub issues, an evaluation framework, and specialized metrics.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs perform well on static code-generation benchmarks, but real-world software development is a continuous stream of evolving issues, fixes, and feature requests. The paper aims to address the gap in evaluating an agent's ability to accumulate experience, transfer knowledge, and resist catastrophic forgetting in such dynamic environments.

Method: The paper introduces SWE-Bench-CL, which organizes GitHub issues into chronologically ordered sequences. It uses a LangGraph-based evaluation framework with a FAISS-backed semantic memory module. The paper also introduces specialized continual learning metrics, including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and composite continual learning scores.

Result: The paper provides a preliminary analysis of inter-task structural similarity and contextual sensitivity. It compares memory-enabled and memory-disabled agents across diverse Python repositories, and shares the code and data publicly.

Conclusion: The paper introduces SWE-Bench-CL, a continual learning benchmark for evaluating the ability of AI agents to adapt and learn in software engineering scenarios. The benchmark is built on chronologically ordered GitHub issues and includes an evaluation framework, specialized continual learning metrics, and a rigorous experimental protocol. The results of comparing memory-enabled and memory-disabled agents are shared to provide a reproducible platform.

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [126] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Théo Bontempelli,Thomas Bouabça,Tristan Cazenave*

Main category: cs.LG

TL;DR: 本文提出了一种可扩展的探索方法vMF-exp，用于解决强化学习中大型动作集的问题，该方法在推荐系统中取得了成功。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习中大型动作集的问题，传统Boltzmann探索方法存在可扩展性问题。

Method: 提出了一种名为von Mises-Fisher exploration (vMF-exp) 的方法，该方法通过 von Mises-Fisher 分布采样状态嵌入表示，然后探索该表示的最近邻。

Result: vMF-exp在理论上与Boltzmann探索具有相同的动作探索概率，并通过模拟数据、真实世界公共数据以及全球音乐流媒体服务的推荐系统上的大规模部署进行了验证。

Conclusion: vMF-exp是一种可扩展的探索方法，适用于具有超球面嵌入向量的大型动作集强化学习问题，并在推荐系统中成功大规模部署，验证了该方法的关键特性。

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [127] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: 提出了一种新的视觉Transformer (ViT)架构，通过引入一种称为对抗指标(AdvI)令牌的新概念来检测对抗性攻击，以解决基于transformer的无线电信号分类容易受到细微但复杂的对抗性攻击问题。


<details>
  <summary>Details</summary>
Motivation: 基于transformer的无线电信号分类容易受到细微但复杂的对抗性攻击。

Method: 提出了一种新的视觉Transformer (ViT)架构，通过引入一种称为对抗指标(AdvI)令牌的新概念来检测对抗性攻击。

Result: 提出的AdvI令牌作为ViT中的一个关键元素，影响注意力权重，从而突出输入数据中潜在的可疑或异常区域或特征。

Conclusion: 通过实验结果表明，该方法在处理白盒攻击场景（包括快速梯度法、投影梯度下降攻击和基本迭代法）方面优于几种竞争方法。

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [128] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: This paper introduces GRFT, a highly efficient fine-tuning method for large pre-trained models. It updates only a small subset of parameters, achieving state-of-the-art performance with reduced storage and computational costs.


<details>
  <summary>Details</summary>
Motivation: fine-tuning these models for specific downstream tasks demands significant computational resources and storage. One fine-tuning method, gradient-based parameter selection (GPS), focuses on fine-tuning only the parameters with high gradients in each neuron, thereby reducing the number of training parameters. Nevertheless, this approach increases computational resource requirements and storage demands.

Method: an efficient gradient-based and regularized fine-tuning method (GRFT) that updates the rows or columns of the weight matrix

Result: the rows or columns with the highest sum of squared gradients are optimal for updating. This strategy effectively reduces storage overhead and improves the efficiency of parameter selection. Additionally, we incorporate regularization to enhance knowledge transfer from the pre-trained model.

Conclusion: GRFT achieves state-of-the-art performance, surpassing existing methods such as GPS, Adapter Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the total parameters on FGVC and VTAB datasets, respectively, demonstrating its high efficiency and effectiveness.

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [129] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: This paper presents a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. The paper reveals a critical limitation in conventional SFT and introduces a learning rate reduction approach that yields significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: learning from demonstrations or preference signals play a crucial role in this adaptation for grounding pre-trained language models to real-world tasks

Method: a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. rigorous mathematical derivation, learning rate reduction approach, alternative SFT objectives from various f-divergence functions

Result: SFT representing a special case of implicit reward learning. a simple yet effective learning rate reduction approach that yields significant performance improvements (up to 25% relative gain and 6% absolute win rate increase in instruction following tasks

Conclusion: conventional SFT has a critical limitation: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. a simple yet effective learning rate reduction approach that yields significant performance improvements (up to 25% relative gain and 6% absolute win rate increase in instruction following tasks. Additionally, alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, the theoretical relationship between LLM logits and Q-functions from preference learning is extended to the SFT context, providing mathematical derivations and experimental validation.

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [130] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: 本研究提出了三种量子启发式数据编码策略，旨在减少编码时间，同时保持或提高经典机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 主要目标是在确保正确的编码值的同时，减少高编码时间，并分析它们对分类性能的影响。

Method: 我们提出、评估和比较了三种量子启发的数据编码策略：实例级别策略（ILS）、全局离散策略（GDS）和类条件值策略（CCVS）。

Result: 通过分析编码时间、精度和预测性能之间的权衡，本研究深入了解了如何优化经典机器学习工作流程的量子启发数据转换。

Conclusion: 本研究分析了三种量子启发的数据编码策略在经典机器学习模型中的应用，并评估了它们在编码效率、正确性、模型准确性和计算成本方面的影响。

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [131] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

TL;DR: 本研究利用变分自编码器方法，通过生成更广谱的先验建议，提高马尔可夫链蒙特卡罗(McMC)方法的效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法，如Karhunen-Loève展开(KLE)，需要预先了解协方差函数，这在实际应用中通常是不可用的。

Method: 使用变分自动编码器(VAE)

Result: VAE参数化方法在相关长度已知时，其精度与KLE相当;当假设的相关长度偏离真实值时，VAE参数化方法的性能优于KLE。此外，VAE方法显著降低了随机维数，提高了计算效率。

Conclusion: 利用深度生成模型改进McMC方法可以提高高维问题中贝叶斯推断的适应性和效率。

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [132] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: Introduces GLU Attention, a novel attention mechanism that improves model performance and convergence speed with zero additional parameters and negligible computational costs.


<details>
  <summary>Details</summary>
Motivation: Gated Linear Units (GLU) have shown great potential in enhancing neural network performance

Method: introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention

Result: GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs

Conclusion: GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [133] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: 本文提出了一种名为AIMatDesign的强化学习框架，用于解决材料逆向设计中高维空间和有限数据的问题。实验表明，该框架在材料发现效率和性能方面显著优于传统方法，并成功合成了一种高性能Zr基合金。


<details>
  <summary>Details</summary>
Motivation: 随着对新型材料的需求不断增长，机器学习驱动的逆向设计方法在协调高维材料成分空间与有限的实验数据方面面临着严峻的挑战。现有的方法有两个主要的局限性：(I) 机器学习模型在高维空间中通常缺乏可靠性，导致设计过程中的预测偏差；(II) 这些模型未能有效整合领域专家的知识，限制了它们支持知识引导的逆向设计的能力。

Method: 提出了一种强化学习框架AIMatDesign，该框架通过使用基于差异的算法增强实验数据来构建可信的经验池，从而解决了这些限制，从而加速了模型收敛。为了提高模型可靠性，一种由大型语言模型 (LLM) 引导的自动细化策略动态地纠正了预测不一致性，从而加强了奖励信号和状态值函数之间的一致性。此外，基于知识的奖励函数利用专家领域规则来提高训练期间的稳定性和效率。

Result: 实验表明，AIMatDesign在发现效率、收敛速度和成功率方面显著超过了传统的机器学习和强化学习方法。在AIMatDesign提出的众多候选方案中，代表性Zr基合金的实验合成产生了一种高性能BMG，屈服强度为1.7GPa，延伸率为10.2%，与预测结果非常吻合。

Conclusion: AIMatDesign显著超过了传统的机器学习和强化学习方法，在Zr基合金的实验合成中，屈服强度达到1.7GPa，延伸率达到10.2%，与预测结果非常接近，并且准确地捕捉到了屈服强度随成分变化的趋势。

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [134] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: FNSDA：一种傅里叶神经模拟器，通过在傅里叶空间中进行自适应，可以轻松地推广到新的动态，并在动态系统泛化方面表现出色，参数成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特定领域做出可靠预测的能力有限，并且难以推广到受相同一般动力学支配但在环境特征上不同的未见系统。

Method: 提出了一种参数高效的方法，即用于动态自适应的傅里叶神经模拟器 (FNSDA)，它可以通过在傅里叶空间中进行自适应来轻松地推广到新的动态。

Result: FNSDA在四个具有代表性的动态系统家族中进行了评估，结果表明，与现有方法相比，FNSDA可以实现卓越或具有竞争力的泛化性能，并且参数成本显着降低。

Conclusion: FNSDA在动态系统泛化方面表现出色，参数成本显著降低。

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [135] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: This paper introduces ROSE, a new framework using reinforcement learning to generate diverse and realistic adversarial prompts for evaluating LLM safety, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing manual safety benchmarks are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. Current automated methods often suffer from insufficient adversarial topic coverage and weak alignment with real-world contexts.

Method: a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts.

Result: ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics.

Conclusion: ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with improvements in evaluation metrics. ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs.

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [136] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: HiT-JEPA 是一种用于学习跨语义抽象级别的多尺度城市轨迹表示的统一框架。


<details>
  <summary>Details</summary>
Motivation: 设计能够捕获多样化和互补信息的轨迹表示仍然是一个开放的研究问题。现有方法难以在单个模型中结合轨迹细粒度细节和高级摘要，限制了它们在保持局部细微差别的同时关注长期依赖性的能力。

Method: HiT-JEPA（通过联合嵌入预测架构的轨迹语义分层交互）

Result: 在用于轨迹相似性计算的多个真实世界数据集上的大量实验表明

Conclusion: HiT-JEPA 的分层设计产生了更丰富、多尺度的表示。

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [137] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: LoRA-Mixer is a parameter-efficient MoE framework that integrates LoRA experts for adapting LLMs, achieving strong performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Combining LoRA with MoE for adapting LLMs has limitations in parameter efficiency and task fidelity.

Method: The LoRA-Mixer replaces projection matrices with dynamically routed, task-specific LoRA experts and introduces an adaptive Specialization Balance Loss (SBL) for router training.

Result: LoRA-Mixer achieves improvements of 7.61%, 4.88%, and 3.08% on GSM8K, HumanEval, and MedQA, respectively, and outperforms state-of-the-art methods with only 48% of the parameters.

Conclusion: LoRA-Mixer achieves significant improvements over base models and state-of-the-art methods with fewer parameters, demonstrating its efficiency and strong performance.

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [138] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: DRL with contextual bandits selects action durations, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: temporal scale of action execution is a critical yet underexplored aspect of DRL

Method: integrates contextual bandits with DRL to adaptively select action durations

Result: significant performance improvements over static duration baselines on Atari 2600 games

Conclusion: adaptive temporal abstractions in DRL improve performance on Atari 2600 games

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [139] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

TL;DR: This paper proposes a spatial neighborhood fusion technique to improve the forecasting of mobility flows across urban regions using a large-scale spatio-temporal dataset collected from Peru's national Digital Contact Tracing (DCT) application during the COVID-19 pandemic. The technique addresses the spatial sparsity of hourly mobility counts and improves forecasting performance.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of human mobility is critical for understanding epidemic spread and deploying timely interventions. A key challenge lies in the spatial sparsity of hourly mobility counts across hexagonal grid cells, which limits the predictive power of conventional time series models.

Method: A lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN) technique that augments each cell's features with aggregated signals from its immediate H3 neighbors.

Result: SPN consistently improves forecasting performance, achieving up to 9.85 percent reduction in test MSE.

Conclusion: Spatial smoothing of sparse mobility signals provides a simple yet effective path toward robust spatio-temporal forecasting during public health crises.

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [140] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

TL;DR: This study compiles a CHF dataset for both uniform and non-uniform heating conditions to support the OECD/NEA AI/ML CHF benchmark. It highlights the limitations of existing models and sets the stage for advanced modeling techniques.


<details>
  <summary>Details</summary>
Motivation: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. The OECD/NEA AI/ML CHF benchmark introduces spatially varying power profiles.

Method: This work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats.

Result: The curated datasets and baseline modeling results lay the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.

Conclusion: Classical CHF correlations exhibit substantial errors and degrade markedly when applied to non-uniform profiles, while a neural network trained solely on uniform data fails to generalize to spatially varying scenarios.

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [141] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

TL;DR: 提出了一种混合的、物理驱动的深度学习模型IDRIFTNET，用于更准确地预测冰山轨迹。


<details>
  <summary>Details</summary>
Motivation: 准确预测冰山轨迹仍然是一个巨大的挑战，主要是由于时空数据的稀缺以及冰山运动的复杂非线性性质，冰山运动受到多种动态环境因素的影响。

Method: 提出了一种混合IDRIFTNET模型，该模型结合了冰山漂移物理的解析公式和一个增强的残差学习模型，结合旋转增强的频谱神经网络，从数据中捕获全局和局部模式，以预测未来的冰山漂移位置。

Result: IDRIFTNET模型在两个南极冰山A23A和B22A上的性能优于其他模型，在各种时间点上实现了更低的最终位移误差（FDE）和平均位移误差（ADE）。

Conclusion: IDRIFTNET模型在预测冰山轨迹方面优于其他模型，能够有效捕捉冰山在有限数据和动态环境条件下的复杂非线性漂移。

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [142] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: This paper introduces neuron-centric model fusion algorithms that effectively integrate multiple neural networks into a single network, outperforming previous methods, especially in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Model fusion aims to combine the knowledge of multiple models, but differences in internal representations due to permutation invariance, random initialization, or differently distributed training data make this process non-trivial.

Method: A novel, neuron-centric family of model fusion algorithms is presented to integrate multiple trained neural networks into a single network. The algorithms group intermediate neurons of parent models to create target representations that the fused model approximates with its corresponding sub-network. Neuron attribution scores are incorporated into the fusion process, and the algorithms can generalize to arbitrary layer types.

Result: The proposed algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID fusion scenarios, on various benchmark datasets.

Conclusion: The proposed neuron-centric model fusion algorithms outperform previous techniques, especially in zero-shot and non-IID scenarios, as demonstrated on various benchmark datasets.

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [143] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于PVI的数据缩减策略，该策略通过过滤低难度实例并使用渐进式学习来提高模型性能和训练效率，已成功应用于英语和中文数据集。


<details>
  <summary>Details</summary>
Motivation: 在以数据为中心的人工智能中，数据缩减通过识别大规模数据集中信息量最大的实例来提高模型训练效率。核心挑战在于如何选择最佳实例（而不是整个数据集）以提高数据质量和训练效率。

Method: 基于Pointwise V-information(PVI)的有效数据缩减策略

Result: 删除10%-30%的数据可保持分类器性能，而准确率仅损失0.0001%至0.76%。在按升序PVI排序的实例上训练分类器，加速收敛，并且比传统训练方法提高了0.8%的准确率。

Conclusion: 通过有效的数据缩减策略，在选择的最佳子集上训练分类器可以提高模型性能并提高训练效率。将PVI框架从仅适用于英语数据集转移到各种中文NLP任务和基础模型，为跨语言数据缩减和更快的训练带来了宝贵的见解。

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [144] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: 我们对图分类中的 38 种质量度量进行了比较分析，发现一些流行的度量标准表现不佳，并提出了一种基于聚类的预处理步骤来提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 在用于分类的模式可以直接解释的情况下，依赖于模式（即子图）的方法提供了良好的可解释性。为了识别有意义的模式，一种标准方法是使用质量度量，即评估每个模式的区分能力的函数。然而，文献提供了数十种这样的度量，因此很难为给定的应用选择最合适的度量。只有少数调查试图通过比较这些度量来提供一些见解，而且没有一个调查专门关注图。

Method: 对来自文献的38个质量指标进行了比较分析。我们基于四个数学属性对它们进行了理论上的表征。我们利用公开可用的数据集来构成基准，并提出一种方法来详细说明模式的黄金标准排序。我们利用这些资源对这些措施进行经验比较，包括模式排序和分类性能。

Result: 我们提出了一个基于聚类的预处理步骤，该步骤对出现在同一图中的模式进行分组，以提高分类性能。我们的实验结果证明了这一步骤的有效性，减少了要处理的模式数量，同时实现了可比的性能。此外，我们表明，文献中广泛使用的一些流行措施与最佳结果无关。

Conclusion: 通过实验结果表明，一些在文献中广泛使用的流行措施与最佳结果无关。

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [145] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: LiSER 利用未标记的音频-视觉数据进行 SER，使用基于高级语音和面部表示模型的大型教师模型，将关于语音情绪和面部表情的知识从教师模型转移到轻量级学生模型。


<details>
  <summary>Details</summary>
Motivation: 语音界面中的语音情感识别 (SER) 可以根据用户的情绪自定义响应，从而使人机交互系统受益。使用多模态音频-视觉线索开发 SER 系统是有益的。然而，收集大量用于开发的数据非常昂贵。

Method: 知识蒸馏框架 LightweightSER (LiSER)

Result: 在两个基准数据集 RAVDESS 和 CREMA-D 上进行的实验表明，

Conclusion: LiSER 可以在 SER 任务中减少对大量标记数据集的依赖。

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [146] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: Smooth-Distill: a self-distillation framework for HAR and sensor placement detection using wearable sensor data, reducing computational overhead and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional distillation methods require separate teacher and student models, leading to training computational overhead. Frequent model updates or training on resource-constrained platforms.

Method: A unified CNN-based architecture, MTL-net, which processes accelerometer data and branches into two outputs for each respective task. A smoothed, historical version of the model itself as the teacher.

Result: Notable improvements in both human activity recognition and device placement detection tasks. Enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines.

Conclusion: Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving improvements in both human activity recognition and device placement detection tasks. It reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms.

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [147] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

TL;DR: 提出了Fractional Policy Gradients (FPG)，利用分数阶导数改进强化学习中的策略优化，实现了样本效率和方差的显著提升。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法面临马尔可夫假设的限制，表现出高方差和低效采样。

Method: 使用Caputo fractional derivatives重新构建梯度，建立了状态转移之间的幂律时间相关性，并开发了一种有效的递归计算方法。

Result: FPG实现了O(t^(-alpha))阶的渐近方差降低，样本效率提高了35-68%，方差降低了24-52%。

Conclusion: Fractional Policy Gradients (FPG)框架通过利用长期依赖性，实现了显著的样本效率提升和方差降低，且没有增加计算开销。

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [148] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: 本文对 LLM 自我完善过程中的训练动态进行了理论建模，并提出了预测自我完善能力的方案。


<details>
  <summary>Details</summary>
Motivation: 旨在增强 LLM 性能，而无需依赖外部数据。一般来说，LLM 在自我完善过程中的性能如何演变仍未得到充分探索。

Method: 通过求解器-验证器差距的概念，对自我改进的训练动态进行理论建模。

Result: 我们进一步介绍了如何仅使用前几个训练 epoch 的信息来预测自我改进的最终能力。我们从经验上验证了理论模型在各种 LLM 和数据集上的有效性。

Conclusion: 在有限的外部数据情况下，这些外部数据可以在任何阶段使用，而不会显着影响最终性能，这与经验观察一致。

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [149] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: 本文研究了时间序列基础模型的表征学习机制和泛化能力，解释了其卓越性能的原因，并为理解、评估和提高其安全性与可靠性提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型表现出卓越的表达能力、泛化能力和跨领域迁移能力，但时间序列数据反映了不同的动态系统，使得跨领域迁移在直觉上似乎是不合理的，然而这与模型的实证成功相矛盾。为了解决这个悖论，

Method: 从理论和实验的角度，研究了基于patch的时间序列基础模型的表征学习机制和泛化能力。

Result: 连续时间序列patch可以忠实地量化为离散词汇表，其关键统计特性与自然语言的统计特性高度一致。这种泛化使得时间序列模型能够继承大型语言模型的鲁棒表示和迁移能力，从而解释了它们在时间任务中的卓越性能。

Conclusion: 这项工作为理解、评估和提高大规模时间序列基础模型的安全性与可靠性提供了严谨的理论基石。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [150] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

TL;DR: Dynamical modes from CGM data are used to detect meal events, improving accuracy and interpretability compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: By leveraging the inherent properties of underlying dynamics, these modes capture key aspects of glucose variability, enabling the identification of patterns and anomalies associated with meal consumption.

Method: We utilize dynamical modes as features derived from Continuous Glucose Monitoring (CGM) data to detect meal events.

Result: This approach not only improves the accuracy of meal detection but also enhances the interpretability of the underlying glucose dynamics.

Conclusion: The proposed technique offers significant advantages over traditional approaches, improving detection accuracy.

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [151] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedHLM的通信高效HLM框架，该框架集成了不确定性感知推理与联邦学习，以减少LLM传输。


<details>
  <summary>Details</summary>
Motivation: 混合语言模型（HLM）结合了边缘设备上小型语言模型（SLM）的低延迟效率与集中式服务器上大型语言模型（LLM）的高精度。但是，含糊不清或低置信度的预测仍然需要频繁卸载到LLM，从而导致带宽受限环境中产生大量的通信开销。

Method: 该论文提出了一种名为FedHLM的通信高效HLM框架，该框架集成了不确定性感知推理与联邦学习（FL）。FedHLM的关键创新在于协同学习token级别的不确定性阈值，该阈值控制何时需要LLM协助。此外，它还利用基于嵌入的token表示进行对等（P2P）解析，使客户端能够重用语义相似对等方推断的token，而无需与LLM交互。此外，它还引入了分层模型聚合：边缘服务器通过客户端更新来改进本地路由策略，而跨集群协调则对齐全局决策边界。

Result: FedHLM减少了超过95%的LLM传输，且精度损失可忽略不计。

Conclusion: FedHLM在大型新闻分类任务中减少了超过95%的LLM传输，且精度损失可忽略不计，非常适合可扩展和高效的边缘AI应用。

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [152] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: 本研究提出了一种新的 IA-STGNN 框架，用于解决战略模拟中战术打击行为和战略延迟之间缺乏结构化因果建模的问题。


<details>
  <summary>Details</summary>
Motivation: 当前战略级模拟中缺乏战术打击行为和战略延迟之间的结构化因果建模，特别是在捕获“弹性-节点抑制-谈判窗口”链中的中间变量时存在结构性瓶颈。

Method: 提出了干预感知时空图神经网络 (IA-STGNN)，该框架集成了图注意力机制、反事实模拟单元和空间干预节点重构，以实现对打击配置和同步策略的动态模拟。

Result: 实验结果表明，IA-STGNN 显著优于基线模型 (ST-GNN、GCN-LSTM、XGBoost)，MAE 降低了 12.8%，Top-5% 准确率提高了 18.4%，同时提高了因果路径一致性和干预稳定性。

Conclusion: IA-STGNN 实现了对战略延迟的可解释预测，并支持核威慑模拟、外交窗口评估和多策略优化等应用，为高级策略建模提供了一个结构化和透明的 AI 决策支持机制。

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [153] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: 提出了一种新的网络级交通速度预测框架，名为图融合增强网络(GFEN)，该框架优于当前方法。


<details>
  <summary>Details</summary>
Motivation: 当前的方法难以处理交通动态的内在复杂性和非线性，难以整合空间和时间特征。此外，现有方法使用静态技术来处理非平稳和异常的历史数据，这限制了适应性并破坏了数据平滑。

Method: 提出了一种图融合增强网络(GFEN)，GFEN 采用了一种混合方法，将基于k阶差分的数学框架与基于注意力的深度学习结构相结合，以自适应地平滑历史观测数据，并动态地缓解数据异常和非平稳性。

Result: GFEN在预测精度上超过了最先进的方法约6.3%，并且收敛速度几乎是最近混合模型的两倍。

Conclusion: GFEN在预测精度上超过了最先进的方法约6.3%，并且收敛速度几乎是最近混合模型的两倍，证明了其卓越的性能和显著提高交通预测系统效率的潜力。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [154] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: pUniFind, a large-scale multimodal pre-trained model in proteomics, integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing, outperforming traditional methods and improving sensitivity, modification coverage, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Deep learning has advanced mass spectrometry data interpretation, yet most models remain feature extractors rather than unified scoring frameworks.

Method: a large-scale multimodal pre-trained model that integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing

Result: pUniFind outperforms traditional engines across diverse datasets, particularly achieving a 42.6 percent increase in the number of identified peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind identifies 60 percent more PSMs than existing de novo methods despite a 300-fold larger search space. A deep learning based quality control module further recovers 38.5 percent additional peptides including 1,891 mapped to the genome but absent from reference proteomes while preserving full fragment ion coverage.

Conclusion: pUniFind is a unified, scalable deep learning framework for proteomic analysis, offering improved sensitivity, modification coverage, and interpretability.

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [155] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

TL;DR: 我们提出了一个短期职业事故预测的通用框架，该框架利用安全检查并将事故发生建模为二元时间序列。


<details>
  <summary>Details</summary>
Motivation: 为了确保预测的可靠性和可操作性，我们应用了专门为时间序列数据设计的滑动窗口交叉验证程序，并结合基于聚合周期级别指标的评估。

Method: 利用安全检查并建模事故发生作为二元时间序列的通用框架，生成每日预测，然后汇总成每周安全评估。

Result: 该方法将常规安全检查数据转换为清晰的每周风险评分，检测事故最可能发生的时期。

Conclusion: LSTM网络优于其他方法，能够预测即将到来的高风险期，平衡准确率达到0.86，证明了该方法的稳健性，并表明二元时间序列模型可以根据安全检查预测这些关键时期。

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [156] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

TL;DR: 本研究比较了不同的数据生成方法在消防员干预中的应用，并使用领域特定和标准指标评估了合成数据的质量。


<details>
  <summary>Details</summary>
Motivation: 在消防员干预的情况下，人员和物资资源的分配非常重要。这种分配依赖于模拟来试验各种场景。这种分配的主要目标是全局优化消防员的响应。因此，数据生成对于研究各种场景是强制性的。

Method: 研究考察了随机抽样、表格变分自动编码器、标准生成对抗网络、条件表格生成对抗网络和扩散概率模型等方法。

Result: 使用领域特定的指标（如响应时间分布、干预的时空分布和事故表示）以及标准度量（如 Wasserstein 距离）相结合来评估合成数据的质量。这些指标旨在评估数据可变性，保留精细和复杂的相关性和异常（例如发生率非常低的事件），符合初始统计分布以及合成数据的操作相关性。

Conclusion: 本研究比较了不同的数据生成方法，以确定它们在捕捉消防员干预的复杂性方面的有效性。

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [157] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

TL;DR: DFReg: a physics-inspired regularization method based on Density Functional Theory (DFT) for deep neural networks.


<details>
  <summary>Details</summary>
Motivation: We introduce DFReg, a physics-inspired regularization method for deep neural networks that operates on the global distribution of weights. Drawing from Density Functional Theory (DFT).

Method: DFReg applies a functional penalty to encourage smooth, diverse, and well-distributed weight configurations.

Result: DFReg encourages smooth, diverse, and well-distributed weight configurations.

Conclusion: DFReg imposes global structural regularity without architectural changes or stochastic perturbations.

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [158] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

TL;DR: A data-driven and transparent method for industrial fault detection is introduced, integrating machine learning, Shapley explanations, and domain-specific visualization. It achieves high accuracy and interpretability in a crimping process application.


<details>
  <summary>Details</summary>
Motivation: Conventional quality control approaches lack adaptability and data-driven methods function as black-box models.

Method: integrates a supervised machine learning model for multi-class fault classification, Shapley Additive Explanations for post-hoc interpretability, and a domain-specific visualisation technique

Result: achieves a fault detection accuracy of 95.9 %, and both quantitative selectivity analysis and qualitative expert evaluations confirmed the relevance and interpretability of the generated explanations.

Conclusion: The human-centric approach enhances trust and interpretability in data-driven fault detection, contributing to applied system design in industrial quality control.

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [159] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

TL;DR: 图神经网络在风能预测中表现良好。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络在风能预测中的适用性。

Method: 使用图神经网络进行风能预测。

Result: 在三个风电设施上，使用五年历史数据进行实验，图神经网络达到了与基于CNN的基准模型相当的性能。使用数值天气预报（NWP）变量作为预测因子，并在24至36小时的测试范围内评估模型。

Conclusion: 某些图神经网络架构在风能预测问题上达到了与最佳CNN基准模型相当的性能。

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [160] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

TL;DR: This paper explores text-to-level generation using diffusion models, presenting strategies for captioning levels, training models, and evaluating results. It finds that simple transformer models can outperform complex text encoders and introduces a GUI for level design.


<details>
  <summary>Details</summary>
Motivation: Use of diffusion models for text-to-level generation is underexplored, and there are practical considerations for creating a usable model, such as caption/level pairs, a text embedding model, and generating entire playable levels.

Method: Strategies to automatically assign descriptive captions to an existing level dataset, and train diffusion models using both pretrained text encoders and simple transformer models trained from scratch.

Result: The diversity and playability of the resulting levels are assessed and compared with an unconditional diffusion model, a generative adversarial network, and other text-to-level approaches. Captions are automatically assigned to generated levels to compare the overlap between input and output captions.

Conclusion: The best diffusion model uses a simple transformer model for text embedding and trains faster than models using complex text encoders, suggesting that large language models are unnecessary. A GUI is presented for constructing long levels from generated scenes.

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [161] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

TL;DR: 本文构建了一个基于可穿戴设备行为数据的健康预测基础模型，经验证，该模型在多种健康任务中表现出色，尤其是在睡眠预测等行为驱动的任务中。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备记录的生理和行为信号可以改善健康预测。尽管基础模型越来越多地用于此类预测，但它们主要应用于低级传感器数据，而行为数据通常更具信息性，因为它们与生理相关的时间尺度和数量对齐。

Method: 利用来自16.2万人的超过25亿小时的可穿戴设备数据，系统地优化架构和tokenization策略，开发行为信号的基础模型。

Result: 在57项与健康相关的任务中，该模型在各种真实世界的应用中表现出强大的性能，包括个体水平的分类和随时间变化的健康状态预测。该模型擅长于行为驱动的任务，如睡眠预测，并且在与原始传感器数据的表示相结合时，性能得到进一步提高。

Conclusion: 针对可穿戴设备定制的专用行为信号基础模型可以提升健康预测的准确性，并为新的健康应用提供潜力。

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>
