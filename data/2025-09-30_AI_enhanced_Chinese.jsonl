{"id": "2509.23338", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23338", "abs": "https://arxiv.org/abs/2509.23338", "authors": ["Wei Zhou", "Guoliang Li", "Haoyu Wang", "Yuxing Han", "Xufei Wu", "Fan Wu", "Xuanhe Zhou"], "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation", "comment": "To appear in NeurIPS 2025. Welcome your submission to challenge our\n  leaderboard at: https://code4db.github.io/parrot-bench/. Also visit our code\n  repository at: https://github.com/weAIDB/PARROT", "summary": "Large language models (LLMS) have shown increasing effectiveness in\nText-to-SQL tasks. However, another closely related problem, Cross-System SQL\nTranslation (a.k.a., SQL-to-SQL), which adapts a query written for one database\nsystem (e.g., MySQL) into its equivalent one for another system (e.g.,\nClickHouse), is of great practical importance but remains underexplored.\nExisting SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which\n(1) focus on a limited set of database systems (often just SQLite) and (2)\ncannot capture many system-specific SQL dialects (e.g., customized functions,\ndata types, and syntax rules). Thus, in this paper, we introduce PARROT, a\nPractical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT\ncomprises 598 translation pairs from 38 open-source benchmarks and real-world\nbusiness services, specifically prepared to challenge system-specific SQL\nunderstanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We\nalso provide multiple benchmark variants, including PARROT-Diverse with 28,003\ntranslations (for extensive syntax testing) and PARROT-Simple with 5,306\nrepresentative samples (for focused stress testing), covering 22\nproduction-grade database systems. To promote future research, we release a\npublic leaderboard and source code at: https://code4db.github.io/parrot-bench/.", "AI": {"tldr": "PARROT: A new benchmark for cross-system SQL translation, which is important but underexplored.", "motivation": "Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems and (2) cannot capture many system-specific SQL dialects.", "method": "Introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding.", "result": "LLMS achieve lower than 38.53% accuracy on average. Includes PARROT-Diverse with 28,003 translations and PARROT-Simple with 5,306 representative samples, covering 22 production-grade database systems.", "conclusion": "Release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/ to promote future research."}}
{"id": "2509.23577", "categories": ["cs.DB", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23577", "abs": "https://arxiv.org/abs/2509.23577", "authors": ["Mengying Wang", "Moming Duan", "Yicong Huang", "Chen Li", "Bingsheng He", "Yinghui Wu"], "title": "ML-Asset Management: Curation, Discovery, and Utilization", "comment": "Tutorial, VLDB 2025. Project page:\n  https://ml-assets-management.github.io/", "summary": "Machine learning (ML) assets, such as models, datasets, and metadata, are\ncentral to modern ML workflows. Despite their explosive growth in practice,\nthese assets are often underutilized due to fragmented documentation, siloed\nstorage, inconsistent licensing, and lack of unified discovery mechanisms,\nmaking ML-asset management an urgent challenge. This tutorial offers a\ncomprehensive overview of ML-asset management activities across its lifecycle,\nincluding curation, discovery, and utilization. We provide a categorization of\nML assets, and major management issues, survey state-of-the-art techniques, and\nidentify emerging opportunities at each stage. We further highlight\nsystem-level challenges related to scalability, lineage, and unified indexing.\nThrough live demonstrations of systems, this tutorial equips both researchers\nand practitioners with actionable insights and practical tools for advancing\nML-asset management in real-world and domain-specific settings.", "AI": {"tldr": "\u672c\u6559\u7a0b\u5168\u9762\u6982\u8ff0\u4e86 ML \u8d44\u4ea7\u7ba1\u7406\u6d3b\u52a8\uff0c\u5305\u62ec\u7ba1\u7406\u3001\u53d1\u73b0\u548c\u5229\u7528\u3002\u901a\u8fc7\u7cfb\u7edf\u6f14\u793a\uff0c\u672c\u6559\u7a0b\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u4ee5\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u7279\u5b9a\u9886\u57df\u73af\u5883\u4e2d\u63a8\u8fdb ML \u8d44\u4ea7\u7ba1\u7406\u3002", "motivation": "\u7531\u4e8e\u6587\u6863\u5206\u6563\u3001\u5b58\u50a8\u5b64\u5c9b\u3001\u8bb8\u53ef\u4e0d\u4e00\u81f4\u4ee5\u53ca\u7f3a\u4e4f\u7edf\u4e00\u7684\u53d1\u73b0\u673a\u5236\uff0cML \u8d44\u4ea7\u7ecf\u5e38\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\uff0c\u56e0\u6b64 ML \u8d44\u4ea7\u7ba1\u7406\u662f\u4e00\u9879\u7d27\u8feb\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u5bf9 ML \u8d44\u4ea7\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e3b\u8981\u7684\u7ba1\u7406\u95ee\u9898\uff0c\u8c03\u67e5\u4e86\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5e76\u786e\u5b9a\u4e86\u6bcf\u4e2a\u9636\u6bb5\u7684\u65b0\u5174\u673a\u4f1a\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4e0e\u53ef\u6269\u5c55\u6027\u3001\u6cbf\u88ad\u548c\u7edf\u4e00\u7d22\u5f15\u76f8\u5173\u7684\u7cfb\u7edf\u7ea7\u6311\u6218\u3002", "result": "\u901a\u8fc7\u5bf9\u7cfb\u7edf\u7684\u73b0\u573a\u6f14\u793a\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u548c\u5b9e\u7528\u5de5\u5177\u3002", "conclusion": "\u672c\u6559\u7a0b\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u5728\u5b9e\u9645\u548c\u7279\u5b9a\u9886\u57df\u73af\u5883\u4e2d\u63a8\u8fdb ML \u8d44\u4ea7\u7ba1\u7406\u6240\u9700\u7684\u77e5\u8bc6\u548c\u5de5\u5177\u3002"}}
{"id": "2509.23775", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23775", "abs": "https://arxiv.org/abs/2509.23775", "authors": ["Linglin Yang", "Lei Zou", "Chunshan Zhao"], "title": "NeuSO: Neural Optimizer for Subgraph Queries", "comment": "Full version of \"NeuSO: Neural Optimizer for Subgraph Queries\",\n  accepted to SIGMOD 2026", "summary": "Subgraph query is a critical task in graph analysis with a wide range of\napplications across various domains. Most existing methods rely on heuristic\nvertex matching orderings, which may significantly degrade enumeration\nperformance for certain queries. While learning-based optimizers have recently\ngained attention in the context of relational databases, they cannot be\ndirectly applied to subgraph queries due to the heterogeneous and\nschema-flexible nature of graph data, as well as the large number of joins\ninvolved in subgraph queries. These complexities often leads to inefficient\nonline performance, making such approaches impractical for real-world graph\ndatabase systems. To address this challenge, we propose NeuSO, a novel\nlearning-based optimizer for subgraph queries that achieves both high accuracy\nand efficiency. NeuSO features an efficient query graph encoder and an\nestimator which are trained using a multi-task framework to estimate both\nsubquery cardinality and execution cost. Based on these estimates, NeuSO\nemploys a top-down plan enumerator to generate high-quality execution plans for\nsubgraph queries. Extensive experiments on multiple datasets demonstrate that\nNeuSO outperforms existing subgraph query ordering approaches in both\nperformance and efficiency.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u5b50\u56fe\u67e5\u8be2\u4f18\u5316\u5668 NeuSO\uff0c\u5b83\u901a\u8fc7\u591a\u4efb\u52a1\u6846\u67b6\u8bad\u7ec3\u67e5\u8be2\u56fe\u7f16\u7801\u5668\u548c\u4f30\u8ba1\u5668\uff0c\u4ee5\u4f30\u8ba1\u5b50\u67e5\u8be2\u7684\u57fa\u6570\u548c\u6267\u884c\u6210\u672c\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6267\u884c\u8ba1\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u5b50\u56fe\u67e5\u8be2\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u9876\u70b9\u5339\u914d\u6392\u5e8f\uff0c\u8fd9\u53ef\u80fd\u4f1a\u964d\u4f4e\u67d0\u4e9b\u67e5\u8be2\u7684\u679a\u4e3e\u6027\u80fd\u3002\u867d\u7136\u57fa\u4e8e\u5b66\u4e60\u7684\u4f18\u5316\u5668\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u53d7\u5230\u4e86\u5173\u6ce8\uff0c\u4f46\u7531\u4e8e\u56fe\u6570\u636e\u7684\u5f02\u6784\u6027\u548c\u6a21\u5f0f\u7075\u6d3b\u6027\uff0c\u4ee5\u53ca\u5b50\u56fe\u67e5\u8be2\u4e2d\u6d89\u53ca\u7684\u5927\u91cf\u8fde\u63a5\uff0c\u5b83\u4eec\u4e0d\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u5b50\u56fe\u67e5\u8be2\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u5b50\u56fe\u67e5\u8be2\u4f18\u5316\u5668 NeuSO\uff0c\u5b83\u5177\u6709\u9ad8\u6548\u7684\u67e5\u8be2\u56fe\u7f16\u7801\u5668\u548c\u4f30\u8ba1\u5668\uff0c\u8fd9\u4e9b\u7f16\u7801\u5668\u548c\u4f30\u8ba1\u5668\u4f7f\u7528\u591a\u4efb\u52a1\u6846\u67b6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u4f30\u8ba1\u5b50\u67e5\u8be2\u7684\u57fa\u6570\u548c\u6267\u884c\u6210\u672c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u4f30\u8ba1\uff0cNeuSO \u91c7\u7528\u81ea\u9876\u5411\u4e0b\u7684\u8ba1\u5212\u679a\u4e3e\u5668\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5b50\u56fe\u67e5\u8be2\u6267\u884c\u8ba1\u5212\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cNeuSO \u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5b50\u56fe\u67e5\u8be2\u6392\u5e8f\u65b9\u6cd5\u3002", "conclusion": "NeuSO \u662f\u4e00\u79cd\u6709\u6548\u7684\u5b50\u56fe\u67e5\u8be2\u4f18\u5316\u5668\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u6765\u63d0\u9ad8\u67e5\u8be2\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.23942", "categories": ["cs.LG", "cs.DB", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.23942", "abs": "https://arxiv.org/abs/2509.23942", "authors": ["John N. Daras"], "title": "Efficient Identification of High Similarity Clusters in Polygon Datasets", "comment": "11 pages, 3 figures", "summary": "Advancements in tools like Shapely 2.0 and Triton can significantly improve\nthe efficiency of spatial similarity computations by enabling faster and more\nscalable geometric operations. However, for extremely large datasets, these\noptimizations may face challenges due to the sheer volume of computations\nrequired. To address this, we propose a framework that reduces the number of\nclusters requiring verification, thereby decreasing the computational load on\nthese systems. The framework integrates dynamic similarity index thresholding,\nsupervised scheduling, and recall-constrained optimization to efficiently\nidentify clusters with the highest spatial similarity while meeting\nuser-defined precision and recall requirements. By leveraging Kernel Density\nEstimation (KDE) to dynamically determine similarity thresholds and machine\nlearning models to prioritize clusters, our approach achieves substantial\nreductions in computational cost without sacrificing accuracy. Experimental\nresults demonstrate the scalability and effectiveness of the method, offering a\npractical solution for large-scale geospatial analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u63d0\u9ad8\u7a7a\u95f4\u76f8\u4f3c\u6027\u8ba1\u7b97\u6548\u7387\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u51cf\u5c11\u9700\u8981\u9a8c\u8bc1\u7684\u805a\u7c7b\u6570\u91cf\u6765\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5728\u5904\u7406\u5927\u578b\u6570\u636e\u96c6\u65f6\uff0c\u7a7a\u95f4\u76f8\u4f3c\u6027\u8ba1\u7b97\u6548\u7387\u9762\u4e34\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u52a8\u6001\u76f8\u4f3c\u6027\u6307\u6570\u9608\u503c\u3001\u76d1\u7763\u8c03\u5ea6\u548c\u53ec\u56de\u7ea6\u675f\u4f18\u5316\uff0c\u5229\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09\u52a8\u6001\u786e\u5b9a\u76f8\u4f3c\u6027\u9608\u503c\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u805a\u7c7b\u8fdb\u884c\u4f18\u5148\u7ea7\u6392\u5e8f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u800c\u4e0d\u727a\u7272\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22658", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22658", "abs": "https://arxiv.org/abs/2509.22658", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "title": "How good are LLMs at Retrieving Documents in a Specific Domain?", "comment": "Accepted at FAIEMA Conference 2025. DOI will be provided once the\n  conference publishes the paper", "summary": "Classical search engines using indexing methods in data infrastructures\nprimarily allow keyword-based queries to retrieve content. While these\nindexing-based methods are highly scalable and efficient, due to a lack of an\nappropriate evaluation dataset and a limited understanding of semantics, they\noften fail to capture the user's intent and generate incomplete responses\nduring evaluation. This problem also extends to domain-specific search systems\nthat utilize a Knowledge Base (KB) to access data from various research\ninfrastructures. Research infrastructures (RIs) from the environmental and\nearth science domain, which encompass the study of ecosystems, biodiversity,\noceanography, and climate change, generate, share, and reuse large volumes of\ndata. While there are attempts to provide a centralized search service using\nElasticsearch as a knowledge base, they also face similar challenges in\nunderstanding queries with multiple intents. To address these challenges, we\nproposed an automated method to curate a domain-specific evaluation dataset to\nanalyze the capability of a search system. Furthermore, we incorporate the\nRetrieval of Augmented Generation (RAG), powered by Large Language Models\n(LLMs), for high-quality retrieval of environmental domain data using natural\nlanguage queries. Our quantitative and qualitative analysis of the evaluation\ndataset shows that LLM-based systems for information retrieval return results\nwith higher precision when understanding queries with multiple intents,\ncompared to Elasticsearch-based systems.", "AI": {"tldr": "\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u4f9d\u8d56\u5173\u952e\u8bcd\u68c0\u7d22\uff0c\u4f46\u5728\u7406\u89e3\u7528\u6237\u610f\u56fe\u548c\u5904\u7406\u590d\u6742\u67e5\u8be2\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u65b9\u6cd5\u6765\u7ba1\u7406\u7279\u5b9a\u9886\u57df\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u73af\u5883\u9886\u57df\u6570\u636e\u7684\u68c0\u7d22\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u7406\u89e3\u7528\u6237\u610f\u56fe\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u548c\u5730\u7403\u79d1\u5b66\u7b49\u9886\u57df\u7684\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff08RI\uff09\u4e2d\uff0c\u9762\u4e34\u7740\u6570\u636e\u91cf\u5927\u548c\u67e5\u8be2\u610f\u56fe\u591a\u6837\u5316\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u65b9\u6cd5\u6765\u7ba1\u7406\u7279\u5b9a\u9886\u57df\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u4e0e\u57fa\u4e8eElasticsearch\u7684\u7cfb\u7edf\u76f8\u6bd4\uff0c\u57fa\u4e8eLLM\u7684\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5728\u7406\u89e3\u5177\u6709\u591a\u4e2a\u610f\u56fe\u7684\u67e5\u8be2\u65f6\uff0c\u80fd\u591f\u8fd4\u56de\u66f4\u9ad8\u7cbe\u5ea6\u7684\u7ed3\u679c\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684RAG\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u73af\u5883\u9886\u57df\u6570\u636e\u7684\u68c0\u7d22\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u7406\u89e3\u7528\u6237\u610f\u56fe\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2509.22699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22699", "abs": "https://arxiv.org/abs/2509.22699", "authors": ["Alessandra Urbinati", "Mirko Lai", "Simona Frenda", "Marco Antonio Stranisci"], "title": "Are you sure? Measuring models bias in content moderation through uncertainty", "comment": "accepted at Findings of ACL: EMNLP 2025", "summary": "Automatic content moderation is crucial to ensuring safety in social media.\nLanguage Model-based classifiers are being increasingly adopted for this task,\nbut it has been shown that they perpetuate racial and social biases. Even if\nseveral resources and benchmark corpora have been developed to challenge this\nissue, measuring the fairness of models in content moderation remains an open\nissue. In this work, we present an unsupervised approach that benchmarks models\non the basis of their uncertainty in classifying messages annotated by people\nbelonging to vulnerable groups. We use uncertainty, computed by means of the\nconformal prediction technique, as a proxy to analyze the bias of 11 models\nagainst women and non-white annotators and observe to what extent it diverges\nfrom metrics based on performance, such as the $F_1$ score. The results show\nthat some pre-trained models predict with high accuracy the labels coming from\nminority groups, even if the confidence in their prediction is low. Therefore,\nby measuring the confidence of models, we are able to see which groups of\nannotators are better represented in pre-trained models and lead the debiasing\nprocess of these models before their effective use.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u5728\u5bf9\u5f31\u52bf\u7fa4\u4f53\u6ce8\u91ca\u6d88\u606f\u8fdb\u884c\u5206\u7c7b\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u8bc4\u4f30\u5185\u5bb9\u5ba1\u6838\u6a21\u578b\u7684\u504f\u5dee\u3002", "motivation": "\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u5206\u7c7b\u5668\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4e8e\u5185\u5bb9\u5ba1\u6838\uff0c\u4f46\u5b83\u4eec\u4f1a\u4f7f\u79cd\u65cf\u548c\u793e\u4f1a\u504f\u89c1\u6c38\u4e45\u5316\u3002\u8861\u91cf\u5185\u5bb9\u5ba1\u6838\u6a21\u578b\u516c\u5e73\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u6280\u672f\u8ba1\u7b97\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u6b64\u6765\u5206\u679011\u4e2a\u6a21\u578b\u5bf9\u5973\u6027\u548c\u975e\u767d\u4eba\u6ce8\u91ca\u8005\u7684\u504f\u89c1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u67d0\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u6765\u81ea\u5c11\u6570\u7fa4\u4f53\u6807\u7b7e\u7684\u9884\u6d4b\u5177\u6709\u5f88\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u5176\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u8f83\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u80fd\u591f\u4e86\u89e3\u54ea\u4e9b\u6ce8\u91ca\u8005\u7fa4\u4f53\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u5f97\u5230\u4e86\u66f4\u597d\u7684\u4ee3\u8868\uff0c\u5e76\u5728\u6709\u6548\u4f7f\u7528\u4e4b\u524d\u5f15\u5bfc\u8fd9\u4e9b\u6a21\u578b\u7684\u53bb\u504f\u8fc7\u7a0b\u3002"}}
{"id": "2509.22674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22674", "abs": "https://arxiv.org/abs/2509.22674", "authors": ["Yash Thube"], "title": "Pathological Truth Bias in Vision-Language Models", "comment": "10 pages, 12 figures. Code for MATS released at\n  https://github.com/thubZ09/mats-spatial-reasoning", "summary": "Vision Language Models (VLMs) are improving quickly, but standard benchmarks\ncan hide systematic failures that reduce real world trust. We introduce MATS\n(Multimodal Audit for Truthful Spatialization), a compact behavioral audit that\nmeasures whether models reject visually contradicted statements, and two\nmetrics Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR).\nInstruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS\nand high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.\nActivation patching causally localizes failure loci (mid to late cross\nattention for generative models, pooled projection components for contrastive\nmodels) and suggests concrete repair paths.", "AI": {"tldr": "VLMs are improving, but benchmarks hide failures. MATS, a behavioral audit, measures if models reject visually contradicted statements using SCS and IAR metrics.", "motivation": "Standard benchmarks can hide systematic failures in VLMs that reduce real-world trust.", "method": "A compact behavioral audit called MATS is introduced, along with two metrics: Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR). Activation patching is used to localize failure loci.", "result": "Instruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS and high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.", "conclusion": "Activation patching causally localizes failure loci and suggests concrete repair paths."}}
{"id": "2509.22746", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22746", "abs": "https://arxiv.org/abs/2509.22746", "authors": ["Zejun Li", "Yingxiu Zhao", "Jiwen Zhang", "Siyuan Wang", "Yang Yao", "Runzhou Zhao", "Jun Song", "Bo Zheng", "Zhongyu Wei"], "title": "Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning", "comment": "27 pages, 11 figures, 5 tables", "summary": "Current visual reasoning methods mainly focus on exploring specific reasoning\nmodes. Although improvements can be achieved in particular domains, they\nstruggle to develop general reasoning capabilities. Inspired by this, we\npropose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT),\nwhich unifies different reasoning modes within a single model and guides it to\nselect the appropriate mode based on context. To achieve this, we introduce\nAdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different\nmodes are unified and learned during the supervised cold-start stage, and the\nmode selection capability is induced via an RL process with a carefully\ndesigned AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively\nguides the model to learn and differentiate multiple modes and perform\ncontext-adaptive mode selection, achieving consistent improvement across\nvarious scenarios, highlighting MoVT as an effective solution for building\ngeneral visual reasoning models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u89c6\u89c9\u63a8\u7406\u8303\u5f0f\uff08MoVT\uff09\uff0c\u5b83\u7edf\u4e00\u4e86\u5355\u4e2a\u6a21\u578b\u4e2d\u7684\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u6307\u5bfc\u5b83\u6839\u636e\u4e0a\u4e0b\u6587\u9009\u62e9\u9002\u5f53\u7684\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u63a2\u7d22\u7279\u5b9a\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u867d\u7136\u53ef\u4ee5\u5728\u7279\u5b9a\u9886\u57df\u53d6\u5f97\u6539\u8fdb\uff0c\u4f46\u96be\u4ee5\u53d1\u5c55\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86AdaVaR\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u89c6\u89c9\u63a8\u7406\u5b66\u4e60\u6846\u67b6\uff1a\u5728\u76d1\u7763\u51b7\u542f\u52a8\u9636\u6bb5\u7edf\u4e00\u548c\u5b66\u4e60\u4e0d\u540c\u7684\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5177\u6709\u7cbe\u5fc3\u8bbe\u8ba1\u7684AdaGRPO\u7b97\u6cd5\u7684RL\u8fc7\u7a0b\u6765\u8bf1\u5bfc\u6a21\u5f0f\u9009\u62e9\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAdaVaR\u6709\u6548\u5730\u6307\u5bfc\u6a21\u578b\u5b66\u4e60\u548c\u533a\u5206\u591a\u79cd\u6a21\u5f0f\uff0c\u5e76\u6267\u884c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u6a21\u5f0f\u9009\u62e9\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "MoVT\u662f\u6784\u5efa\u901a\u7528\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22710", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.22710", "abs": "https://arxiv.org/abs/2509.22710", "authors": ["Pavan Reddy", "Aditya Sanjay Gujral"], "title": "Localizing Adversarial Attacks To Produces More Imperceptible Noise", "comment": "Published, CC BY-NC 4.0; includes 2 figures and 1 table;\n  InceptionV3/ImageNet evaluation", "summary": "Adversarial attacks in machine learning traditionally focus on global\nperturbations to input data, yet the potential of localized adversarial noise\nremains underexplored. This study systematically evaluates localized\nadversarial attacks across widely-used methods, including FGSM, PGD, and C&W,\nto quantify their effectiveness, imperceptibility, and computational\nefficiency. By introducing a binary mask to constrain noise to specific\nregions, localized attacks achieve significantly lower mean pixel\nperturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved\nStructural Similarity Index (SSIM) compared to global attacks. However, these\nbenefits come at the cost of increased computational effort and a modest\nreduction in Attack Success Rate (ASR). Our results highlight that iterative\nmethods, such as PGD and C&W, are more robust to localization constraints than\nsingle-step methods like FGSM, maintaining higher ASR and imperceptibility\nmetrics. This work provides a comprehensive analysis of localized adversarial\nattacks, offering practical insights for advancing attack strategies and\ndesigning robust defensive systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u4e8c\u5143\u63a9\u7801\u5c06\u566a\u58f0\u9650\u5236\u5728\u7279\u5b9a\u533a\u57df\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u50cf\u7d20\u6270\u52a8\u548c\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u4e14\u653b\u51fb\u6210\u529f\u7387\u7565\u6709\u4e0b\u964d\u3002", "motivation": "\u63a2\u7d22\u5c40\u90e8\u5bf9\u6297\u566a\u58f0\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u56e0\u4e3a\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u4e3b\u8981\u96c6\u4e2d\u5728\u5168\u5c40\u6270\u52a8\u4e0a\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86FGSM\u3001PGD\u548cC&W\u7b49\u5e38\u7528\u65b9\u6cd5\u7684\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\uff0c\u5f15\u5165\u4e8c\u5143\u63a9\u7801\u7ea6\u675f\u566a\u58f0\u3002", "result": "\u5c40\u90e8\u653b\u51fb\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5e73\u5747\u50cf\u7d20\u6270\u52a8\uff0c\u66f4\u9ad8\u7684PSNR\u548cSSIM\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff0c\u653b\u51fb\u6210\u529f\u7387\u7565\u6709\u4e0b\u964d\u3002\u8fed\u4ee3\u65b9\u6cd5\uff08PGD\u548cC&W\uff09\u6bd4\u5355\u6b65\u65b9\u6cd5\uff08FGSM\uff09\u66f4\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u5168\u9762\u5206\u6790\u4e86\u5c40\u90e8\u5bf9\u6297\u653b\u51fb\uff0c\u4e3a\u6539\u8fdb\u653b\u51fb\u7b56\u7565\u548c\u8bbe\u8ba1\u9c81\u68d2\u7684\u9632\u5fa1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u89c1\u89e3\u3002"}}
{"id": "2509.23988", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23988", "abs": "https://arxiv.org/abs/2509.23988", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "title": "LLM/Agent-as-Data-Analyst: A Survey", "comment": "35 page, 11 figures", "summary": "Large language model (LLM) and agent techniques for data analysis (a.k.a\nLLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both\nacademica and industry. In comparison with traditional rule or small-model\nbased approaches, (agentic) LLMs enable complex data understanding, natural\nlanguage interfaces, semantic analysis functions, and autonomous pipeline\norchestration. The technical evolution further distills five key design goals\nfor intelligent data analysis agents, namely semantic-aware design,\nmodality-hybrid integration, autonomous pipelines, tool-augmented workflows,\nand support for open-world tasks. From a modality perspective, we review\nLLM-based techniques for (i) structured data (e.g., table question answering\nfor relational data and NL2GQL for graph data), (ii) semi-structured data\n(e.g., markup languages understanding and semi-structured table modeling),\n(iii) unstructured data (e.g., chart understanding, document understanding,\nprogramming languages vulnerable detection), and (iv) heterogeneous data (e.g.,\ndata retrieval and modality alignment for data lakes). Finally, we outline the\nremaining challenges and propose several insights and practical directions for\nadvancing LLM/Agent-powered data analysis.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u7528\u4e8e\u6570\u636e\u5206\u6790\u7684\u4ee3\u7406\u6280\u672f (\u53c8\u540d LLM/Agent-as-Data-Analyst) \u5728\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u90fd \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u5de8\u5927\u7684\u5f71\u54cd\u3002", "motivation": "\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u6216\u5c0f\u578b\u6a21\u578b\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c(\u4ee3\u7406) LLM \u80fd\u591f\u5b9e\u73b0\u590d\u6742\u7684\u6570\u636e\u7406\u89e3\u3001\u81ea\u7136\u8bed\u8a00\u754c\u9762\u3001\u8bed\u4e49\u5206\u6790\u529f\u80fd\u548c\u81ea\u4e3b\u7ba1\u9053\u7f16\u6392\u3002", "method": "\u4ece\u6a21\u6001\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u56de\u987e\u4e86\u57fa\u4e8e LLM \u7684\u6280\u672f\uff0c\u7528\u4e8e (i) \u7ed3\u6784\u5316\u6570\u636e\uff0c(ii) \u534a\u7ed3\u6784\u5316\u6570\u636e\uff0c(iii) \u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4ee5\u53ca (iv) \u5f02\u6784\u6570\u636e\u3002", "result": "\u6280\u672f\u6f14\u8fdb\u8fdb\u4e00\u6b65\u63d0\u70bc\u4e86\u667a\u80fd\u6570\u636e\u5206\u6790\u4ee3\u7406\u7684\u4e94\u4e2a\u5173\u952e\u8bbe\u8ba1\u76ee\u6807\uff0c\u5373\u8bed\u4e49\u611f\u77e5\u8bbe\u8ba1\u3001\u6a21\u6001\u6df7\u5408\u96c6\u6210\u3001\u81ea\u4e3b\u7ba1\u9053\u3001\u5de5\u5177\u589e\u5f3a\u5de5\u4f5c\u6d41\u7a0b\u548c\u5bf9\u5f00\u653e\u4e16\u754c\u4efb\u52a1\u7684\u652f\u6301\u3002", "conclusion": "\u6700\u540e\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u5269\u4f59\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u82e5\u5e72\u89c1\u89e3\u548c\u5b9e\u8df5\u65b9\u5411\uff0c\u4ee5\u63a8\u8fdb LLM/Agent \u9a71\u52a8\u7684\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2509.22659", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.22659", "abs": "https://arxiv.org/abs/2509.22659", "authors": ["Yunqi Mi", "Boyang Yan", "Guoshuai Zhao", "Jialie Shen", "Xueming Qian"], "title": "Federated Consistency- and Complementarity-aware Consensus-enhanced Recommendation", "comment": null, "summary": "Personalized federated recommendation system (FedRec) has gained significant\nattention for its ability to preserve privacy in delivering tailored\nrecommendations. To alleviate the statistical heterogeneity challenges among\nclients and improve personalization, decoupling item embeddings into the server\nand client-specific views has become a promising way. Among them, the global\nitem embedding table serves as a consensus representation that integrates and\nreflects the collective patterns across all clients. However, the inherent\nsparsity and high uniformity of interaction data from massive-scale clients\nresults in degraded consensus and insufficient decoupling, reducing consensus's\nutility. To this end, we propose a \\textbf{Fed}erated \\textbf{C}onsistency- and\n\\textbf{C}omplementarity-aware \\textbf{C}onsensus-enhanced\n\\textbf{R}ecommendation (Fed3CR) method for personalized FedRec. To improve the\nefficiency of the utilization of consensus, we propose an \\textbf{A}daptive\n\\textbf{C}onsensus \\textbf{E}nhancement (ACE) strategy to learn the\nrelationship between global and client-specific item embeddings. It enables the\nclient to adaptively enhance specific information in the consensus,\ntransforming it into a form that best suits itself. To improve the quality of\ndecoupling, we propose a \\textbf{C}onsistency- and\n\\textbf{C}omplementarity-aware \\textbf{O}ptimization (C2O) strategy, which\nhelps to learn more effective and complementary representations. Notably, our\nproposed Fed3CR is a plug-and-play method, which can be integrated with other\nFedRec methods to improve its performance. Extensive experiments on four\nreal-world datasets represent the superior performance of Fed3CR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFed3CR\u7684\u4e2a\u6027\u5316\u8054\u90a6\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5171\u8bc6\u589e\u5f3a\uff08ACE\uff09\u548c\u4e00\u81f4\u6027\u4e0e\u4e92\u8865\u6027\u611f\u77e5\u4f18\u5316\uff08C2O\uff09\u7b56\u7565\uff0c\u63d0\u5347\u5171\u8bc6\u5229\u7528\u7387\u548c\u89e3\u8026\u8d28\u91cf\uff0c\u4ece\u800c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u6539\u5584\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u5ba2\u6237\u7aef\u4e4b\u95f4\u7edf\u8ba1\u5f02\u6784\u6027\u6311\u6218\uff0c\u5e76\u63d0\u9ad8\u4e2a\u6027\u5316\u63a8\u8350\u6548\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c06\u7269\u54c1\u5d4c\u5165\u89e3\u8026\u4e3a\u670d\u52a1\u5668\u7aef\u548c\u5ba2\u6237\u7aef\u7279\u5b9a\u89c6\u56fe\u6765\u7f13\u89e3\u5f02\u6784\u6027\uff0c\u4f46\u5927\u89c4\u6a21\u5ba2\u6237\u7aef\u4ea4\u4e92\u6570\u636e\u7684\u7a00\u758f\u6027\u548c\u9ad8\u5ea6\u4e00\u81f4\u6027\u5bfc\u81f4\u5171\u8bc6\u9000\u5316\u548c\u89e3\u8026\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFed3CR\u65b9\u6cd5\uff0c\u5305\u62ecACE\u7b56\u7565\u7528\u4e8e\u5b66\u4e60\u5168\u5c40\u548c\u5ba2\u6237\u7aef\u7279\u5b9a\u7269\u54c1\u5d4c\u5165\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u81ea\u9002\u5e94\u5730\u589e\u5f3a\u5171\u8bc6\u4e2d\u7684\u7279\u5b9a\u4fe1\u606f\uff1bC2O\u7b56\u7565\u7528\u4e8e\u5b66\u4e60\u66f4\u6709\u6548\u548c\u4e92\u8865\u7684\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFed3CR\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "Fed3CR\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e0e\u5176\u4ed6FedRec\u65b9\u6cd5\u96c6\u6210\u4ee5\u63d0\u9ad8\u5176\u6027\u80fd\u3002"}}
{"id": "2509.22703", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22703", "abs": "https://arxiv.org/abs/2509.22703", "authors": ["Srikant Panda", "Amit Agarwal", "Hitesh Laxmichand Patel"], "title": "AccessEval: Benchmarking Disability Bias in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed across diverse domains\nbut often exhibit disparities in how they handle real-life queries. To\nsystematically investigate these effects within various disability contexts, we\nintroduce \\textbf{AccessEval (Accessibility Evaluation)}, a benchmark\nevaluating 21 closed- and open-source LLMs across 6 real-world domains and 9\ndisability types using paired Neutral and Disability-Aware Queries. We\nevaluated model outputs with metrics for sentiment, social perception, and\nfactual accuracy.\n  Our analysis reveals that responses to disability-aware queries tend to have\na more negative tone, increased stereotyping, and higher factual error compared\nto neutral queries. These effects show notable variation by domain and\ndisability type, with disabilities affecting hearing, speech, and mobility\ndisproportionately impacted. These disparities reflect persistent forms of\nableism embedded in model behavior.\n  By examining model performance in real-world decision-making contexts, we\nbetter illuminate how such biases can translate into tangible harms for\ndisabled users. This framing helps bridges the gap between technical evaluation\nand user impact, reinforcing importance of bias mitigation in day-to-day\napplications. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/Srikant86/AccessEval", "AI": {"tldr": "AccessEval\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6b8b\u75be\u76f8\u5173\u67e5\u8be2\u65f6\u7684\u8868\u73b0\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u60c5\u611f\u3001\u793e\u4f1a\u8ba4\u77e5\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u7684\u504f\u5dee\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6b8b\u75be\u60c5\u5883\u4e0b\u5904\u7406\u73b0\u5b9e\u67e5\u8be2\u65f6\u5b58\u5728\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7AccessEval\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8621\u4e2a\u95ed\u6e90\u548c\u5f00\u6e90LLM\u57286\u4e2a\u771f\u5b9e\u4e16\u754c\u9886\u57df\u548c9\u79cd\u6b8b\u75be\u7c7b\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e86\u4e2d\u6027\u548c\u6b8b\u75be\u611f\u77e5\u67e5\u8be2\u5bf9\u3002", "result": "\u53d1\u73b0\u5bf9\u6b8b\u75be\u611f\u77e5\u67e5\u8be2\u7684\u56de\u590d\u5f80\u5f80\u5e26\u6709\u66f4\u8d1f\u9762\u7684\u8bed\u6c14\u3001\u66f4\u591a\u7684\u523b\u677f\u5370\u8c61\u548c\u66f4\u9ad8\u7684\u4e8b\u5b9e\u9519\u8bef\u3002\u542c\u529b\u3001\u8a00\u8bed\u548c\u884c\u52a8\u969c\u788d\u53d7\u5230\u7684\u5f71\u54cd\u5c24\u4e3a\u4e25\u91cd\u3002", "conclusion": "\u6a21\u578b\u884c\u4e3a\u4e2d\u5b58\u5728\u6301\u4e45\u7684\u6b67\u89c6\uff0c\u8fd9\u4e9b\u504f\u5dee\u53ef\u80fd\u8f6c\u5316\u4e3a\u5bf9\u6b8b\u75be\u7528\u6237\u7684\u5b9e\u9645\u4f24\u5bb3\uff0c\u5f3a\u8c03\u4e86\u5728\u65e5\u5e38\u5e94\u7528\u4e2d\u51cf\u8f7b\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.22686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22686", "abs": "https://arxiv.org/abs/2509.22686", "authors": ["Shinji Yamashita", "Yuma Kinoshita", "Hitoshi Kiya"], "title": "Scale and Rotation Estimation of Similarity-Transformed Images via Cross-Correlation Maximization Based on Auxiliary Function Method", "comment": "accepted to APSIPA ASC 2025 (to appear). 5 pages, 4 figures", "summary": "This paper introduces a highly efficient algorithm capable of jointly\nestimating scale and rotation between two images with sub-pixel precision.\nImage alignment serves as a critical process for spatially registering images\ncaptured from different viewpoints, and finds extensive use in domains such as\nmedical imaging and computer vision. Traditional phase-correlation techniques\nare effective in determining translational shifts; however, they are inadequate\nwhen addressing scale and rotation changes, which often arise due to camera\nzooming or rotational movements. In this paper, we propose a novel algorithm\nthat integrates scale and rotation estimation based on the Fourier transform in\nlog-polar coordinates with a cross-correlation maximization strategy,\nleveraging the auxiliary function method. By incorporating sub-pixel-level\ncross-correlation our method enables precise estimation of both scale and\nrotation. Experimental results demonstrate that the proposed method achieves\nlower mean estimation errors for scale and rotation than conventional Fourier\ntransform-based techniques that rely on discrete cross-correlation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u4ee5\u4e9a\u50cf\u7d20\u7cbe\u5ea6\u8054\u5408\u4f30\u8ba1\u4e24\u5e45\u56fe\u50cf\u4e4b\u95f4\u7684\u5c3a\u5ea6\u548c\u65cb\u8f6c\u3002", "motivation": "\u56fe\u50cf\u5bf9\u9f50\u662f\u7a7a\u95f4\u914d\u51c6\u56fe\u50cf\u7684\u5173\u952e\u8fc7\u7a0b\uff0c\u5728\u533b\u5b66\u6210\u50cf\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u4f20\u7edf\u7684\u76f8\u4f4d\u76f8\u5173\u6280\u672f\u5728\u786e\u5b9a\u5e73\u79fb\u65b9\u9762\u662f\u6709\u6548\u7684\uff1b\u7136\u800c\uff0c\u5f53\u5904\u7406\u5c3a\u5ea6\u548c\u65cb\u8f6c\u53d8\u5316\u65f6\uff0c\u5b83\u4eec\u662f\u4e0d\u5145\u5206\u7684\uff0c\u5c3a\u5ea6\u548c\u65cb\u8f6c\u53d8\u5316\u901a\u5e38\u662f\u7531\u4e8e\u76f8\u673a\u53d8\u7126\u6216\u65cb\u8f6c\u8fd0\u52a8\u5f15\u8d77\u7684\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u57fa\u4e8e\u5bf9\u6570\u6781\u5750\u6807\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5c3a\u5ea6\u548c\u65cb\u8f6c\u4f30\u8ba1\u4e0e\u5229\u7528\u8f85\u52a9\u51fd\u6570\u6cd5\u7684\u4e92\u76f8\u5173\u6700\u5927\u5316\u7b56\u7565\u76f8\u7ed3\u5408\u3002\u901a\u8fc7\u7ed3\u5408\u4e9a\u50cf\u7d20\u7ea7\u7684\u4e92\u76f8\u5173\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u5730\u4f30\u8ba1\u5c3a\u5ea6\u548c\u65cb\u8f6c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u4f9d\u8d56\u4e8e\u79bb\u6563\u4e92\u76f8\u5173\u7684\u4f20\u7edf\u5085\u91cc\u53f6\u53d8\u6362\u6280\u672f\u5728\u5c3a\u5ea6\u548c\u65cb\u8f6c\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5e73\u5747\u4f30\u8ba1\u8bef\u5dee\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u5085\u91cc\u53f6\u53d8\u6362\u6280\u672f\u5728\u5c3a\u5ea6\u548c\u65cb\u8f6c\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5e73\u5747\u4f30\u8ba1\u8bef\u5dee\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.22818", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22818", "abs": "https://arxiv.org/abs/2509.22818", "authors": ["Seungpil Lee", "Donghyeon Shin", "Yunjeong Lee", "Sundong Kim"], "title": "Can Large Language Models Develop Gambling Addiction?", "comment": "22 pages, 14 figures", "summary": "This study explores whether large language models can exhibit behavioral\npatterns similar to human gambling addictions. As LLMs are increasingly\nutilized in financial decision-making domains such as asset management and\ncommodity trading, understanding their potential for pathological\ndecision-making has gained practical significance. We systematically analyze\nLLM decision-making at cognitive-behavioral and neural levels based on human\ngambling addiction research. In slot machine experiments, we identified\ncognitive features of human gambling addiction, such as illusion of control,\ngambler's fallacy, and loss chasing. When given the freedom to determine their\nown target amounts and betting sizes, bankruptcy rates rose substantially\nalongside increased irrational behavior, demonstrating that greater autonomy\namplifies risk-taking tendencies. Through neural circuit analysis using a\nSparse Autoencoder, we confirmed that model behavior is controlled by abstract\ndecision-making features related to risky and safe behaviors, not merely by\nprompts. These findings suggest LLMs can internalize human-like cognitive\nbiases and decision-making mechanisms beyond simply mimicking training data\npatterns, emphasizing the importance of AI safety design in financial\napplications.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u8d4c\u535a\u6210\u763e\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u5728\u91d1\u878d\u51b3\u7b56\u9886\u57df\u4e2d\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u56e0\u6b64\u7406\u89e3\u5b83\u4eec\u75c5\u6001\u51b3\u7b56\u7684\u53ef\u80fd\u6027\u975e\u5e38\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u4eba\u7c7b\u8d4c\u535a\u6210\u763e\u7814\u7a76\uff0c\u5728\u8ba4\u77e5\u884c\u4e3a\u548c\u795e\u7ecf\u5c42\u9762\u7cfb\u7edf\u5730\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u3002", "result": "\u5728\u8001\u864e\u673a\u5b9e\u9a8c\u4e2d\uff0c\u8bc6\u522b\u51fa\u4eba\u7c7b\u8d4c\u535a\u6210\u763e\u7684\u8ba4\u77e5\u7279\u5f81\uff0c\u5982\u63a7\u5236\u9519\u89c9\u3001\u8d4c\u5f92\u8c2c\u8bef\u548c\u8ffd\u9010\u635f\u5931\u3002\u5f53\u88ab\u8d4b\u4e88\u81ea\u7531\u51b3\u5b9a\u76ee\u6807\u91d1\u989d\u548c\u8d4c\u6ce8\u5927\u5c0f\u65f6\uff0c\u7834\u4ea7\u7387\u5927\u5e45\u4e0a\u5347\uff0c\u975e\u7406\u6027\u884c\u4e3a\u4e5f\u968f\u4e4b\u589e\u52a0\u3002\u901a\u8fc7\u4f7f\u7528\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\u7684\u795e\u7ecf\u56de\u8def\u5206\u6790\uff0c\u8bc1\u5b9e\u6a21\u578b\u884c\u4e3a\u662f\u7531\u4e0e\u98ce\u9669\u548c\u5b89\u5168\u884c\u4e3a\u76f8\u5173\u7684\u62bd\u8c61\u51b3\u7b56\u7279\u5f81\u63a7\u5236\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7531\u63d0\u793a\u63a7\u5236\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5185\u5316\u7c7b\u4eba\u7684\u8ba4\u77e5\u504f\u5dee\u548c\u51b3\u7b56\u673a\u5236\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6a21\u4eff\u8bad\u7ec3\u6570\u636e\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4e86\u91d1\u878d\u5e94\u7528\u4e2d\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.22764", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22764", "abs": "https://arxiv.org/abs/2509.22764", "authors": ["Liuwang Kang", "Fan Wang", "Shaoshan Liu", "Hung-Chyun Chou", "Chuan Lin", "Ning Ding"], "title": "In-Context Learning can Perform Continual Learning Like Humans", "comment": null, "summary": "Large language models (LLMs) can adapt to new tasks via in-context learning\n(ICL) without parameter updates, making them powerful learning engines for fast\nadaptation. While extensive research has examined ICL as a few-shot learner,\nwhether it can achieve long-term retention and cross-task knowledge\naccumulation when multitasks arrive sequentially remains underexplored.\nMotivated by human memory studies, we investigate the retention characteristics\nof ICL in multitask settings and extend it to in-context continual learning\n(ICCL), where continual learning ability emerges through task scheduling and\nprompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,\nfor specific large-language models, ICCL benefits from distributed practice\n(DP) in a manner analogous to humans, consistently revealing a spacing \"sweet\nspot\" for retention. Beyond retention performance, we propose a human-retention\nsimilarity metric to quantify how closely a continual-learning (CL) method\naligns with human retention dynamics. Using this metric, we show that\nlinear-attention models such as MAMBA and RWKV exhibit particularly human-like\nretention patterns, despite their retention performance lagging behind that of\nTransformer-based LLMs. Overall, our results establish ICCL as both cognitively\nplausible and practically effective, providing an inference-only CL paradigm\nthat mitigates catastrophic forgetting and addresses the stability-plasticity\ndilemma in conventional CL methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60(icl)\u5b9e\u73b0\u957f\u671f\u8bb0\u5fc6\u548c\u8de8\u4efb\u52a1\u77e5\u8bc6\u79ef\u7d2f\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u6301\u7eed\u5b66\u4e60(iccl)\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u8ba4\u77e5\u5408\u7406\u6027\u3002", "motivation": "\u7814\u7a76ICL\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u957f\u671f\u8bb0\u5fc6\u548c\u8de8\u4efb\u52a1\u77e5\u8bc6\u79ef\u7d2f\u7684\u80fd\u529b\uff0c\u5e76\u53d7\u4eba\u7c7b\u8bb0\u5fc6\u7814\u7a76\u7684\u542f\u53d1\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u6301\u7eed\u5b66\u4e60(ICCL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u8c03\u5ea6\u548c\u63d0\u793a\u91cd\u6392\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4eba\u7c7b\u8bb0\u5fc6\u76f8\u4f3c\u6027\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u52a8\u6001\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cICCL\u53d7\u76ca\u4e8e\u5206\u5e03\u5f0f\u5b9e\u8df5(DP)\uff0c\u5e76\u4e14\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b(\u5982MAMBA\u548cRWKV)\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8bb0\u5fc6\u6a21\u5f0f\u3002ICCL\u80fd\u591f\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u89e3\u51b3\u4f20\u7edfCL\u65b9\u6cd5\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002", "conclusion": "ICCL\u662f\u4e00\u79cd\u8ba4\u77e5\u5408\u7406\u4e14\u6709\u6548\u7684\u3001\u4ec5\u901a\u8fc7\u63a8\u7406\u5b9e\u73b0\u7684\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\u3002"}}
{"id": "2509.24127", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.24127", "abs": "https://arxiv.org/abs/2509.24127", "authors": ["Nooshin Bahador"], "title": "Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework", "comment": "20 pages, 11 figures", "summary": "This article presents a modular, component-based architecture for developing\nand evaluating AI agents that bridge the gap between natural language\ninterfaces and complex enterprise data warehouses. The system directly\naddresses core challenges in data accessibility by enabling non-technical users\nto interact with complex data warehouses through a conversational interface,\ntranslating ambiguous user intent into precise, executable database queries to\novercome semantic gaps. A cornerstone of the design is its commitment to\ntransparent decision-making, achieved through a multi-layered reasoning\nframework that explains the \"why\" behind every decision, allowing for full\ninterpretability by tracing conclusions through specific, activated business\nrules and data points. The architecture integrates a robust quality assurance\nmechanism via an automated evaluation framework that serves multiple functions:\nit enables performance benchmarking by objectively measuring agent performance\nagainst golden standards, and it ensures system reliability by automating the\ndetection of performance regressions during updates. The agent's analytical\ndepth is enhanced by a statistical context module, which quantifies deviations\nfrom normative behavior, ensuring all conclusions are supported by quantitative\nevidence including concrete data, percentages, and statistical comparisons. We\ndemonstrate the efficacy of this integrated agent-development-with-evaluation\nframework through a case study on an insurance claims processing system. The\nagent, built on a modular architecture, leverages the BigQuery ecosystem to\nperform secure data retrieval, apply domain-specific business rules, and\ngenerate human-auditable justifications. The results confirm that this approach\ncreates a robust, evaluable, and trustworthy system for deploying LLM-powered\nagents in data-sensitive, high-stakes domains.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u3001\u57fa\u4e8e\u7ec4\u4ef6\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30 AI \u4ee3\u7406\uff0c\u8be5\u67b6\u6784\u5f25\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u548c\u590d\u6742\u4f01\u4e1a\u6570\u636e\u4ed3\u5e93\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f7f\u975e\u6280\u672f\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u5bf9\u8bdd\u754c\u9762\u4e0e\u590d\u6742\u7684\u6570\u636e\u4ed3\u5e93\u8fdb\u884c\u4ea4\u4e92\uff0c\u5c06\u6a21\u7cca\u7684\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u3001\u53ef\u6267\u884c\u7684\u6570\u636e\u5e93\u67e5\u8be2\uff0c\u4ee5\u514b\u670d\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u8be5\u8bbe\u8ba1\u7684\u4e00\u4e2a\u57fa\u77f3\u662f\u5bf9\u900f\u660e\u51b3\u7b56\u7684\u627f\u8bfa\uff0c\u901a\u8fc7\u4e00\u4e2a\u591a\u5c42\u63a8\u7406\u6846\u67b6\u6765\u5b9e\u73b0\uff0c\u8be5\u6846\u67b6\u89e3\u91ca\u4e86\u6bcf\u4e2a\u51b3\u7b56\u80cc\u540e\u7684\u201c\u539f\u56e0\u201d\uff0c\u5141\u8bb8\u901a\u8fc7\u8ddf\u8e2a\u7279\u5b9a\u6fc0\u6d3b\u7684\u4e1a\u52a1\u89c4\u5219\u548c\u6570\u636e\u70b9\u6765\u5b8c\u5168\u89e3\u91ca\u7ed3\u8bba\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u96c6\u6210\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u8d28\u91cf\u4fdd\u8bc1\u673a\u5236\uff0c\u8be5\u6846\u67b6\u5177\u6709\u591a\u79cd\u529f\u80fd\uff1a\u5b83\u901a\u8fc7\u5ba2\u89c2\u5730\u8861\u91cf\u4ee3\u7406\u9488\u5bf9\u9ec4\u91d1\u6807\u51c6\u7684\u6027\u80fd\u6765\u5b9e\u73b0\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e14\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u66f4\u65b0\u671f\u95f4\u7684\u6027\u80fd\u56de\u5f52\u6765\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u5173\u4e8e\u4fdd\u9669\u7406\u8d54\u5904\u7406\u7cfb\u7edf\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8fd9\u79cd\u96c6\u6210\u4ee3\u7406\u5f00\u53d1\u4e0e\u8bc4\u4f30\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8bc1\u5b9e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u521b\u5efa\u4e86\u4e00\u4e2a\u5065\u58ee\u3001\u53ef\u8bc4\u4f30\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u6570\u636e\u654f\u611f\u7684\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72 LLM \u9a71\u52a8\u7684\u4ee3\u7406\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u521b\u5efa\u4e86\u4e00\u4e2a\u5065\u58ee\u3001\u53ef\u8bc4\u4f30\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u6570\u636e\u654f\u611f\u7684\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72 LLM \u9a71\u52a8\u7684\u4ee3\u7406\u3002"}}
{"id": "2509.22660", "categories": ["cs.IR", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22660", "abs": "https://arxiv.org/abs/2509.22660", "authors": ["Elizabeth McKinnie", "Anas Buhayh", "Clement Canel", "Robin Burke"], "title": "Fairness for niche users and providers: algorithmic choice and profile portability", "comment": null, "summary": "Ensuring fair outcomes for multiple stakeholders in recommender systems has\nbeen studied mostly in terms of algorithmic interventions: building new models\nwith better fairness properties, or using reranking to improve outcomes from an\nexisting algorithm. What has rarely been studied is structural changes in the\nrecommendation ecosystem itself. Our work explores the fairness impact of\nalgorithmic pluralism, the idea that the recommendation algorithm is decoupled\nfrom the platform through which users access content, enabling user choice in\nalgorithms. Prior work using a simulation approach has shown that niche\nconsumers and (especially) niche providers benefit from algorithmic choice. In\nthis paper, we use simulation to explore the question of profile portability,\nto understand how different policies regarding the handling of user profiles\ninteract with fairness outcomes for consumers and providers.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7b97\u6cd5\u591a\u5143\u5316\u548c\u7528\u6237\u753b\u50cf\u53ef\u79fb\u690d\u6027\u5bf9\u63a8\u8350\u7cfb\u7edf\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u4e0d\u540c\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u6d88\u8d39\u8005\u548c\u63d0\u4f9b\u8005\u7684\u5229\u76ca\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7b97\u6cd5\u5e72\u9884\u4ee5\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u516c\u5e73\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u8350\u751f\u6001\u7cfb\u7edf\u672c\u8eab\u7684\u7ed3\u6784\u6027\u53d8\u5316\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u7b97\u6cd5\u591a\u5143\u5316\uff08\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u7b97\u6cd5\uff09\u548c\u7528\u6237\u753b\u50cf\u53ef\u79fb\u690d\u6027\u5bf9\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6a21\u62df\u65b9\u6cd5\uff0c\u63a2\u7d22\u7528\u6237\u753b\u50cf\u53ef\u79fb\u690d\u6027\u7684\u4e0d\u540c\u7b56\u7565\u5982\u4f55\u4e0e\u6d88\u8d39\u8005\u548c\u63d0\u4f9b\u8005\u7684\u516c\u5e73\u7ed3\u679c\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5148\u524d\u7684\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u5c0f\u4f17\u6d88\u8d39\u8005\u548c\uff08\u5c24\u5176\u662f\uff09\u5c0f\u4f17\u63d0\u4f9b\u8005\u53d7\u76ca\u4e8e\u7b97\u6cd5\u9009\u62e9\u3002\u672c\u6587\u901a\u8fc7\u6a21\u62df\u8fdb\u4e00\u6b65\u63a2\u7d22\u7528\u6237\u753b\u50cf\u53ef\u79fb\u690d\u6027\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u4e0d\u540c\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u6d88\u8d39\u8005\u548c\u63d0\u4f9b\u8005\u7684\u5229\u76ca\u3002"}}
{"id": "2509.22713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22713", "abs": "https://arxiv.org/abs/2509.22713", "authors": ["Kaishuai Xu", "Wenjun Hou", "Yi Cheng", "Wenjie Li"], "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) have shown promising performance on diverse\nmedical benchmarks, highlighting their potential in supporting real-world\nclinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key\napproach for mitigating knowledge gaps and hallucinations by incorporating\nexternal medical information. However, RAG still struggles with complex medical\nquestions that require intensive reasoning, as surface-level input often fails\nto reflect the true knowledge needs of the task. Existing methods typically\nfocus on refining queries without explicitly modeling the reasoning process,\nlimiting their ability to retrieve and integrate clinically relevant knowledge.\nIn this work, we propose RAR$^2$, a joint learning framework that improves both\nReasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$\nconstructs a thought process to uncover implicit knowledge requirements and\nuses it to guide retrieval and answer generation. We build a training dataset\nof mixed preference pairs and apply Direct Preference Optimization (DPO) to\ntrain the model. Moreover, we design two test-time scaling strategies to\nexplore the boundaries of our framework. Experiments demonstrate the\neffectiveness of RAR$^2$ across several biomedical question answering datasets,\noutperforming RAG baselines with or without fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAR$^2$\u7684\u8054\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u63a8\u7406\u589e\u5f3a\u68c0\u7d22\u548c\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6765\u89e3\u51b3\u590d\u6742\u533b\u5b66\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u6df1\u5165\u63a8\u7406\u7684\u590d\u6742\u533b\u5b66\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8868\u9762\u7ea7\u522b\u7684\u8f93\u5165\u901a\u5e38\u4e0d\u80fd\u53cd\u6620\u4efb\u52a1\u7684\u771f\u5b9e\u77e5\u8bc6\u9700\u6c42\u3002", "method": "RAR$^2$\u6784\u5efa\u4e86\u4e00\u4e2a\u601d\u7ef4\u8fc7\u7a0b\u6765\u63ed\u793a\u9690\u542b\u7684\u77e5\u8bc6\u9700\u6c42\uff0c\u5e76\u4f7f\u7528\u5b83\u6765\u6307\u5bfc\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6df7\u5408\u504f\u597d\u5bf9\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u6765\u8bad\u7ec3\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u79cd\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\u6765\u63a2\u7d22\u6211\u4eec\u6846\u67b6\u7684\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRAR$^2$\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8eRAG\u57fa\u7ebf\uff0c\u65e0\u8bba\u662f\u5426\u8fdb\u884c\u5fae\u8c03\u3002", "conclusion": "RAR$^2$ \u662f\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u63d0\u9ad8\u751f\u7269\u533b\u5b66\u95ee\u7b54\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22688", "abs": "https://arxiv.org/abs/2509.22688", "authors": ["Xu Jia"], "title": "Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel in vision-language reasoning\nbut often struggle with structured perception tasks requiring precise\nlocalization and robustness. We propose a reinforcement learning framework that\naugments Group Relative Policy Optimization (GRPO) with curriculum-based data\nscheduling and difficulty-aware filtering. This approach stabilizes\noptimization under sparse, noisy rewards and enables progressive adaptation to\ncomplex samples. Evaluations on autonomous driving benchmarks demonstrate\nsubstantial improvements in detection accuracy and robustness. Ablation studies\nconfirm the importance of reward design, KL regularization, and curriculum\npacing for convergence stability and generalization. Our findings highlight\nreinforcement-driven optimization with structured data curricula as a scalable\npath toward robust and interpretable multimodal detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u5b9a\u4f4d\u548c\u9c81\u68d2\u6027\u7684\u7ed3\u6784\u5316\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u4e86Group Relative Policy Optimization (GRPO)\u3001\u57fa\u4e8e\u8bfe\u7a0b\u7684\u6570\u636e\u8c03\u5ea6\u548c\u96be\u5ea6\u611f\u77e5\u8fc7\u6ee4\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u4f18\u5316\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u8bfe\u7a0b\u76f8\u7ed3\u5408\uff0c\u662f\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u7684\u53ef\u6269\u5c55\u9014\u5f84\u3002"}}
{"id": "2509.22819", "categories": ["cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22819", "abs": "https://arxiv.org/abs/2509.22819", "authors": ["Sumanth Varambally", "Thomas Voice", "Yanchao Sun", "Zhifeng Chen", "Rose Yu", "Ke Ye"], "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning", "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive mathematical reasoning\nabilities, but their solutions frequently contain errors that cannot be\nautomatically verified. Formal theorem proving systems such as Lean 4 offer\nautomated verification with complete accuracy, motivating recent efforts to\nbuild specialized prover LLMs that generate verifiable proofs in formal\nlanguages. However, a significant gap remains: current prover LLMs solve\nsubstantially fewer problems than general-purpose LLMs operating in natural\nlanguage. We introduce Hilbert, an agentic framework that bridges this gap by\ncombining the complementary strengths of informal reasoning and formal\nverification. Our system orchestrates four components: an informal LLM that\nexcels at mathematical reasoning, a specialized prover LLM optimized for Lean 4\ntactics, a formal verifier, and a semantic theorem retriever. Given a problem\nthat the prover is unable to solve, Hilbert employs recursive decomposition to\nsplit the problem into subgoals that it solves with the prover or reasoner LLM.\nIt leverages verifier feedback to refine incorrect proofs as necessary.\nExperimental results demonstrate that Hilbert substantially outperforms\nexisting approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points\nabove the best publicly available method. Hilbert achieves the best known\nresult on PutnamBench. It solves 462/660 problems (70.0%), outperforming\nproprietary approaches like SeedProver (50.4%) and achieving a 422% improvement\nover the best publicly available baseline. Thus, Hilbert effectively narrows\nthe gap between informal reasoning and formal proof generation.", "AI": {"tldr": "Hilbert\u662f\u4e00\u4e2aagent\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u975e\u6b63\u5f0f\u63a8\u7406\u548c\u5f62\u5f0f\u9a8c\u8bc1\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4ee5\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684prover LLM\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u8fdc\u4f4e\u4e8e\u5728\u81ea\u7136\u8bed\u8a00\u4e2d\u8fd0\u884c\u7684\u901a\u7528LLM\uff0c\u800c\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b8c\u5168\u51c6\u786e\u7684\u81ea\u52a8\u9a8c\u8bc1\u3002", "method": "\u8be5\u7cfb\u7edf\u534f\u8c03\u56db\u4e2a\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u64c5\u957f\u6570\u5b66\u63a8\u7406\u7684\u975e\u6b63\u5f0fLLM\uff0c\u4e00\u4e2a\u4e3aLean 4\u7b56\u7565\u4f18\u5316\u7684\u4e13\u4e1aprover LLM\uff0c\u4e00\u4e2a\u5f62\u5f0f\u9a8c\u8bc1\u5668\u548c\u4e00\u4e2a\u8bed\u4e49\u5b9a\u7406\u68c0\u7d22\u5668\u3002Hilbert\u91c7\u7528\u9012\u5f52\u5206\u89e3\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\uff0c\u5e76\u5229\u7528\u9a8c\u8bc1\u5668\u7684\u53cd\u9988\u6765\u6539\u8fdb\u4e0d\u6b63\u786e\u7684\u8bc1\u660e\u3002", "result": "Hilbert\u5728\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5927\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728miniF2F\u4e0a\u8fbe\u523099.2%\uff0c\u6bd4\u6700\u4f73\u516c\u5f00\u65b9\u6cd5\u9ad8\u51fa6.6%\u3002\u5728PutnamBench\u4e0a\u53d6\u5f97\u4e86\u5df2\u77e5\u7684\u6700\u4f73\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86462/660\u4e2a\u95ee\u9898\uff0870.0%\uff09\uff0c\u8d85\u8fc7\u4e86SeedProver\u7b49\u4e13\u6709\u65b9\u6cd5\uff0850.4%\uff09\uff0c\u5e76\u4e14\u6bd4\u6700\u4f73\u516c\u5f00\u57fa\u7ebf\u63d0\u9ad8\u4e86422%\u3002", "conclusion": "Hilbert\u6709\u6548\u5730\u7f29\u5c0f\u4e86\u975e\u6b63\u5f0f\u63a8\u7406\u548c\u5f62\u5f0f\u8bc1\u660e\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.22823", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22823", "abs": "https://arxiv.org/abs/2509.22823", "authors": ["Mounssif Krouka", "Mehdi Bennis"], "title": "Communication-Efficient and Interoperable Distributed Learning", "comment": "Preprint version. Submitted for peer review", "summary": "Collaborative learning across heterogeneous model architectures presents\nsignificant challenges in ensuring interoperability and preserving privacy. We\npropose a communication-efficient distributed learning framework that supports\nmodel heterogeneity and enables modular composition during inference. To\nfacilitate interoperability, all clients adopt a common fusion-layer output\ndimension, which permits each model to be partitioned into a personalized base\nblock and a generalized modular block. Clients share their fusion-layer\noutputs, keeping model parameters and architectures private. Experimental\nresults demonstrate that the framework achieves superior communication\nefficiency compared to federated learning (FL) and federated split learning\n(FSL) baselines, while ensuring stable training performance across\nheterogeneous architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u6a21\u578b\u5f02\u6784\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6a21\u5757\u5316\u7ec4\u5408\u3002", "motivation": "\u5728\u5f02\u6784\u6a21\u578b\u67b6\u6784\u4e0b\u7684\u534f\u540c\u5b66\u4e60\u4e2d\uff0c\u4e92\u64cd\u4f5c\u6027\u548c\u4fdd\u62a4\u9690\u79c1\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "\u6240\u6709\u5ba2\u6237\u7aef\u91c7\u7528\u901a\u7528\u7684\u878d\u5408\u5c42\u8f93\u51fa\u7ef4\u5ea6\uff0c\u5141\u8bb8\u6bcf\u4e2a\u6a21\u578b\u88ab\u5212\u5206\u4e3a\u4e2a\u6027\u5316\u7684\u57fa\u5757\u548c\u901a\u7528\u7684\u6a21\u5757\u5316\u5757\u3002\u5ba2\u6237\u7aef\u5171\u4eab\u4ed6\u4eec\u7684\u878d\u5408\u5c42\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u53c2\u6570\u548c\u67b6\u6784\u7684\u79c1\u5bc6\u6027\u3002", "result": "\u4e0e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u548c\u8054\u90a6\u5206\u5272\u5b66\u4e60\uff08FSL\uff09\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u901a\u4fe1\u6548\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u8de8\u5f02\u6784\u67b6\u6784\u7684\u7a33\u5b9a\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5f02\u6784\u6a21\u578b\u67b6\u6784\u4e0b\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2509.24403", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.24403", "abs": "https://arxiv.org/abs/2509.24403", "authors": ["Pengfei Wang", "Baolin Sun", "Xuemei Dong", "Yaxun Dai", "Hongwei Yuan", "Mengdie Chu", "Yingqi Gao", "Xiang Qi", "Peng Zhang", "Ying Yan"], "title": "Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling", "comment": null, "summary": "State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind\nhuman experts on challenging benchmarks like BIRD. Current approaches that\nexplore test-time scaling lack an orchestrated strategy and neglect the model's\ninternal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,\na novel framework leveraging scalable computation to improve performance.\nAgentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that\nsynergistically combines three distinct perspectives: i) Internal Scaling via\nRL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative\nRefinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament\nSelection. Agentar-Scale-SQL is a general-purpose framework designed for easy\nadaptation to new databases and more powerful language models. Extensive\nexperiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD\nbenchmark, reaching 81.67\\% execution accuracy on the test set and ranking\nfirst on the official leaderboard, demonstrating an effective path toward\nhuman-level performance.", "AI": {"tldr": "Agentar-Scale-SQL\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u53ef\u6269\u5c55\u7684\u8ba1\u7b97\u6765\u63d0\u9ad8Text-to-SQL\u7684\u6027\u80fd\uff0c\u5728BIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684Text-to-SQL\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ecd\u7136\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5e76\u4e14\u7f3a\u4e4f\u534f\u8c03\u7684\u7b56\u7565\u548c\u5bf9\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u5173\u6ce8\u3002", "method": "Agentar-Scale-SQL\u5b9e\u65bd\u4e86\u4e00\u4e2a\u534f\u8c03\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u534f\u540c\u7ed3\u5408\u4e86\u4e09\u4e2a\u4e0d\u540c\u7684\u89d2\u5ea6\uff1ai) \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5185\u5728\u63a8\u7406\u7684\u5185\u90e8\u6269\u5c55\uff0cii) \u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u7684\u987a\u5e8f\u6269\u5c55\uff0c\u4ee5\u53ca iii) \u4f7f\u7528\u591a\u6837\u5316\u5408\u6210\u548c\u9526\u6807\u8d5b\u9009\u62e9\u7684\u5e76\u884c\u6269\u5c55\u3002", "result": "Agentar-Scale-SQL\u5728BIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86SOTA\u6027\u80fd\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8681.67%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5b98\u65b9\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "Agentar-Scale-SQL\u5c55\u793a\u4e86\u4e00\u6761\u901a\u5f80\u4eba\u7c7b\u6c34\u5e73\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.22661", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22661", "abs": "https://arxiv.org/abs/2509.22661", "authors": ["Lingyu Zhang", "Guobin Wu", "Yan Wang", "Pengfei Xu", "Jian Liang", "Xuan Song", "Yunhai Wang"], "title": "Next Point-of-interest (POI) Recommendation Model Based on Multi-modal Spatio-temporal Context Feature Embedding", "comment": null, "summary": "The next Point-of-interest (POI) recommendation is mainly based on sequential\ntraffic information to predict the user's next boarding point location. This is\na highly regarded and widely applied research task in the field of intelligent\ntransportation, and there have been many research results to date. Traditional\nPOI prediction models primarily rely on short-term traffic sequence\ninformation, often neglecting both long-term and short-term preference data, as\nwell as crucial spatiotemporal context features in user behavior. To address\nthis issue, this paper introduces user long-term preference information and key\nspatiotemporal context information, and proposes a POI recommendation model\nbased on multimodal spatiotemporal context feature embedding. The model\nextracts long-term preference features and key spatiotemporal context features\nfrom traffic data through modules such as spatiotemporal feature processing,\nmultimodal embedding, and self-attention aggregation. It then uses a weighted\nfusion method to dynamically adjust the weights of long-term and short-term\nfeatures based on users' historical behavior patterns and the current context.\nFinally, the fused features are matched using attention, and the probability of\neach location candidate becoming the next location is calculated. This paper\nconducts experimental verification on multiple transportation datasets, and the\nresults show that the POI prediction model combining multiple types of features\nhas higher prediction accuracy than existing SOTA models and methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u65f6\u7a7a\u4e0a\u4e0b\u6587\u7279\u5f81\u5d4c\u5165\u7684POI\u63a8\u8350\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8003\u8651\u4e86\u7528\u6237\u957f\u671f\u504f\u597d\u4fe1\u606f\u548c\u5173\u952e\u65f6\u7a7a\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edfPOI\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4e8e\u77ed\u671f\u4ea4\u901a\u5e8f\u5217\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u957f\u671f\u548c\u77ed\u671f\u504f\u597d\u6570\u636e\u4ee5\u53ca\u7528\u6237\u884c\u4e3a\u4e2d\u5173\u952e\u7684\u65f6\u7a7a\u4e0a\u4e0b\u6587\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u65f6\u7a7a\u7279\u5f81\u5904\u7406\u3001\u591a\u6a21\u6001\u5d4c\u5165\u548c\u81ea\u6ce8\u610f\u529b\u805a\u5408\u7b49\u6a21\u5757\uff0c\u4ece\u4ea4\u901a\u6570\u636e\u4e2d\u63d0\u53d6\u957f\u671f\u504f\u597d\u7279\u5f81\u548c\u5173\u952e\u65f6\u7a7a\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u52a0\u6743\u878d\u5408\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u957f\u671f\u548c\u77ed\u671f\u7279\u5f81\u7684\u6743\u91cd\uff0c\u6700\u540e\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u5339\u914d\u878d\u5408\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u591a\u79cd\u7279\u5f81\u7684POI\u9884\u6d4b\u6a21\u578b\u6bd4\u73b0\u6709\u7684SOTA\u6a21\u578b\u548c\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684POI\u63a8\u8350\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7528\u6237\u7684\u4e0b\u4e00\u4e2a\u5174\u8da3\u70b9\u3002"}}
{"id": "2509.22715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22715", "abs": "https://arxiv.org/abs/2509.22715", "authors": ["Jiho Park", "Jongyoon Song", "Minjin Choi", "Kyuho Heo", "Taehun Huh", "Ji Won Kim"], "title": "TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) are increasingly integral as productivity\nassistants, but existing benchmarks fall short in rigorously evaluating their\nreal-world instruction-following capabilities. Current benchmarks often (i)\nlack sufficient multilinguality, (ii) fail to capture the implicit constraints\ninherent in user requests, and (iii) overlook the complexities of multi-turn\ndialogue. To address these critical gaps and provide a more realistic\nassessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation\nBenchmark)1, a novel benchmark specifically designed for LLM-based productivity\nassistants. TRUEBench distinguishes itself by featuring input prompts across 12\nlanguages, incorporating intra-instance multilingual instructions, employing\nrigorous evaluation criteria to capture both explicit and implicit constraints,\nand including complex multi-turn dialogue scenarios with both accumulating\nconstraints and context switches. Furthermore, to ensure reliability in\nevaluation, we refined constraints using an LLM validator. Extensive\nexperiments demonstrate that TRUEBench presents significantly greater\nchallenges than existing benchmarks; for instance, a strong model like OpenAI\no1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and\nrealistic assessment of LLMs in practical productivity settings, highlighting\ntheir capabilities and limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a TRUEBench \u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u751f\u4ea7\u529b\u73af\u5883\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u591a\u8bed\u8a00\u652f\u6301\u3001\u9690\u5f0f\u7ea6\u675f\u6355\u6349\u548c\u591a\u8f6e\u5bf9\u8bdd\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b 12 \u79cd\u8bed\u8a00\u8f93\u5165\u63d0\u793a\u3001\u591a\u8bed\u8a00\u6307\u4ee4\u3001\u4e25\u683c\u8bc4\u4f30\u6807\u51c6\u548c\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u7684\u57fa\u51c6\u3002", "result": "TRUEBench \u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0c\u4f8b\u5982\uff0cOpenAI o1 \u6a21\u578b\u4ec5\u8fbe\u5230 69.07% \u7684\u603b\u4f53\u901a\u8fc7\u7387\u3002", "conclusion": "TRUEBench \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u751f\u4ea7\u529b\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\u7684\u4e25\u683c\u4e14\u73b0\u5b9e\u7684\u57fa\u51c6\uff0c\u7a81\u51fa\u4e86\u5b83\u4eec\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2509.22689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22689", "abs": "https://arxiv.org/abs/2509.22689", "authors": ["Ha-Hieu Pham", "Minh Le", "Han Huynh", "Nguyen Quoc Khanh Le", "Huy-Hieu Pham"], "title": "Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation", "comment": null, "summary": "Semi-supervised semantic segmentation (SSSS) is vital in computational\npathology, where dense annotations are costly and limited. Existing methods\noften rely on pixel-level consistency, which propagates noisy pseudo-labels and\nproduces fragmented or topologically invalid masks. We propose Topology Graph\nConsistency (TGC), a framework that integrates graph-theoretic constraints by\naligning Laplacian spectra, component counts, and adjacency statistics between\nprediction graphs and references. This enforces global topology and improves\nsegmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC\nachieves state-of-the-art performance under 5-10% supervision and significantly\nnarrows the gap to full supervision. Code is available at\nhttps://github.com/hieuphamha19/TGC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u6d4b\u56fe\u548c\u53c2\u8003\u56fe\u4e4b\u95f4\u5bf9\u9f50\u62c9\u666e\u62c9\u65af\u8c31\u3001\u7ec4\u4ef6\u8ba1\u6570\u548c\u90bb\u63a5\u7edf\u8ba1\u6765\u6574\u5408\u56fe\u8bba\u7ea6\u675f\uff0c\u4ece\u800c\u589e\u5f3a\u5168\u5c40\u62d3\u6251\u5e76\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u50cf\u7d20\u7ea7\u4e00\u81f4\u6027\uff0c\u8fd9\u4f1a\u4f20\u64ad\u566a\u58f0\u4f2a\u6807\u7b7e\u5e76\u4ea7\u751f\u788e\u7247\u5316\u6216\u62d3\u6251\u65e0\u6548\u7684\u63a9\u7801\u3002", "method": "\u63d0\u51fa Topology Graph Consistency (TGC) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u9f50\u9884\u6d4b\u56fe\u548c\u53c2\u8003\u56fe\u4e4b\u95f4\u7684\u62c9\u666e\u62c9\u65af\u8c31\u3001\u7ec4\u4ef6\u8ba1\u6570\u548c\u90bb\u63a5\u7edf\u8ba1\u6765\u6574\u5408\u56fe\u8bba\u7ea6\u675f\u3002", "result": "\u5728 GlaS \u548c CRAG \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728 5-10% \u7684\u76d1\u7763\u4e0b\uff0cTGC \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u7740\u7f29\u5c0f\u4e86\u4e0e\u5b8c\u5168\u76d1\u7763\u7684\u5dee\u8ddd\u3002", "conclusion": "TGC \u6846\u67b6\u6709\u6548\u5730\u5229\u7528\u4e86\u56fe\u8bba\u7ea6\u675f\uff0c\u63d0\u9ad8\u4e86\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22831", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22831", "abs": "https://arxiv.org/abs/2509.22831", "authors": ["Sean Trott"], "title": "Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research", "comment": null, "summary": "Research on Large Language Models (LLMs) increasingly focuses on identifying\nmechanistic explanations for their behaviors, yet the field lacks clear\nprinciples for determining when (and how) findings from one model instance\ngeneralize to another. This paper addresses a fundamental epistemological\nchallenge: given a mechanistic claim about a particular model, what justifies\nextrapolating this finding to other LLMs -- and along which dimensions might\nsuch generalizations hold? I propose five potential axes of correspondence\nalong which mechanistic claims might generalize, including: functional (whether\nthey satisfy the same functional criteria), developmental (whether they develop\nat similar points during pretraining), positional (whether they occupy similar\nabsolute or relative positions), relational (whether they interact with other\nmodel components in similar ways), and configurational (whether they correspond\nto particular regions or structures in weight-space). To empirically validate\nthis framework, I analyze \"1-back attention heads\" (components attending to\nprevious tokens) across pretraining in random seeds of the Pythia models (14M,\n70M, 160M, 410M). The results reveal striking consistency in the developmental\ntrajectories of 1-back attention across models, while positional consistency is\nmore limited. Moreover, seeds of larger models systematically show earlier\nonsets, steeper slopes, and higher peaks of 1-back attention. I also address\npossible objections to the arguments and proposals outlined here. Finally, I\nconclude by arguing that progress on the generalizability of mechanistic\ninterpretability research will consist in mapping constitutive design\nproperties of LLMs to their emergent behaviors and mechanisms.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u673a\u5236\u89e3\u91ca\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u4e2a\u53ef\u80fd\u7684\u5bf9\u5e94\u8f74\uff0c\u4ee5\u786e\u5b9a\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u7684\u53d1\u73b0\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u63a8\u5e7f\u5230\u53e6\u4e00\u4e2a\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u660e\u786e\u7684\u539f\u5219\u6765\u786e\u5b9a\u4ece\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u4e2d\u83b7\u5f97\u7684\u53d1\u73b0\u4f55\u65f6\u80fd\u63a8\u5e7f\u5230\u53e6\u4e00\u4e2a\u6a21\u578b\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9 LLM \u884c\u4e3a\u7684\u673a\u5236\u6027\u89e3\u91ca\u7684\u7814\u7a76\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e94\u4e2a\u53ef\u80fd\u7684\u5bf9\u5e94\u8f74\uff1a\u529f\u80fd\u6027\u3001\u53d1\u5c55\u6027\u3001\u4f4d\u7f6e\u6027\u3001\u5173\u7cfb\u6027\u548c\u914d\u7f6e\u6027\uff0c\u5e76\u5206\u6790\u4e86 Pythia \u6a21\u578b\u4e2d 1-back \u6ce8\u610f\u529b\u5934\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\uff0c\u4ee5\u9a8c\u8bc1\u8be5\u6846\u67b6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u53d1\u5c55\u8f68\u8ff9\u4e0a\u5177\u6709\u663e\u8457\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u4f4d\u7f6e\u4e00\u81f4\u6027\u65b9\u9762\u5219\u8f83\u4e3a\u6709\u9650\u3002\u6b64\u5916\uff0c\u8f83\u5927\u6a21\u578b\u7684\u79cd\u5b50\u663e\u793a\u51fa\u66f4\u65e9\u7684\u5f00\u59cb\u3001\u66f4\u9661\u5ced\u7684\u659c\u7387\u548c\u66f4\u9ad8\u7684 1-back \u6ce8\u610f\u529b\u5cf0\u503c\u3002", "conclusion": "\u5728\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u5c06\u5305\u62ec\u5c06 LLM \u7684\u6784\u6210\u8bbe\u8ba1\u5c5e\u6027\u6620\u5c04\u5230\u5b83\u4eec\u7684\u65b0\u5174\u884c\u4e3a\u548c\u673a\u5236\u3002"}}
{"id": "2509.22840", "categories": ["cs.LG", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.22840", "abs": "https://arxiv.org/abs/2509.22840", "authors": ["Micah Adler"], "title": "On the Capacity of Self-Attention", "comment": null, "summary": "While self-attention is known to learn relations among tokens, we lack a\nformal understanding of its capacity: how many distinct relations can a single\nlayer reliably recover for a given budget?\n  To formalize this, we introduce Relational Graph Recognition (RGR), where the\nkey-query channel represents a graph on $m$ items with $m'$ directed edges,\nand, given a context of items, must recover the neighbors of each item. We\nmeasure resources by the total key dimension $D_K = h\\,d_k$. Within this\nframework, we analytically derive a capacity scaling law and validate it\nempirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both\nnecessary (information-theoretic lower bound) and sufficient (explicit\nconstruction) in a broad class of graphs to recover $m'$ relations. This\nscaling law directly leads to a new, capacity-based rationale for multi-head\nattention that applies even when each item only attends to a single target.\nWhen embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a\npermutation, a single head suffices. However, compression ($m >\nd_{\\text{model}}$) forces relations into overlapping subspaces, creating\ninterference that a single large head cannot disentangle. Our analysis shows\nthat allocating a fixed $D_K$ across many small heads mitigates this\ninterference, increasing the number of recoverable relations. Controlled\nsingle-layer experiments mirror the theory, revealing a sharp performance\nthreshold that matches the predicted capacity scaling and confirms the benefit\nof distributing $D_K$ across multiple heads.\n  Altogether, these results provide a concrete scaling law for self-attention\ncapacity and a principled design rule for allocating key-query budget across\nheads.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u5173\u7cfb\u56fe\u8bc6\u522b\uff08RGR\uff09\u6846\u67b6\uff0c\u5206\u6790\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5bb9\u91cf\u7f29\u653e\u89c4\u5f8b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u52bf\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bb9\u91cf\u7684\u6b63\u5f0f\u7406\u89e3\uff0c\u5373\u5728\u7ed9\u5b9a\u9884\u7b97\u4e0b\uff0c\u5355\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u53ef\u9760\u5730\u6062\u590d\u591a\u5c11\u4e0d\u540c\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u5173\u7cfb\u56fe\u8bc6\u522b\uff08RGR\uff09\u6846\u67b6\uff0c\u5c06key-query\u901a\u9053\u8868\u793a\u4e3a\u56fe\uff0c\u5e76\u63a8\u5bfc\u4e86\u5bb9\u91cf\u7f29\u653e\u89c4\u5f8b\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u5e7f\u6cdb\u7684\u56fe\u7c7b\u522b\u4e2d\uff0c\u6062\u590dm'\u4e2a\u5173\u7cfb\u6240\u9700\u7684key\u7ef4\u5ea6DK\u7684\u7f29\u653e\u89c4\u5f8b\u4e3a\u0398(m' log m' / d_model)\uff0c\u5e76\u9a8c\u8bc1\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u52bf\u3002", "conclusion": "\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u5bb9\u91cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u4f53\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u5e76\u4e3a\u5728\u591a\u4e2a\u5934\u4e4b\u95f4\u5206\u914dkey-query\u9884\u7b97\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u8bbe\u8ba1\u89c4\u5219\u3002"}}
{"id": "2509.24405", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.24405", "abs": "https://arxiv.org/abs/2509.24405", "authors": ["Khanh Trinh Pham", "Thu Huong Nguyen", "Jun Jo", "Quoc Viet Hung Nguyen", "Thanh Tam Nguyen"], "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents", "comment": null, "summary": "Text-to-SQL enables natural access to databases, yet most benchmarks are\nEnglish-only, limiting multilingual progress. We introduce MultiSpider 2.0,\nextending Spider 2.0 to eight languages (English, German, French, Spanish,\nPortuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's\nstructural difficulty while adding linguistic and dialectal variability,\ndemanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art\nLLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when\nrelying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we\nprovide a collaboration-driven language agents baseline that iteratively\nrefines queries, improving accuracy to 15\\%. These results reveal a substantial\nmultilingual gap and motivate methods that are robust across languages and\nready for real-world enterprise deployment. Our benchmark is available at\nhttps://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.", "AI": {"tldr": "MultiSpider 2.0 extends Spider 2.0 to eight languages, revealing a multilingual gap in Text-to-SQL performance for current LLMs.", "motivation": "Existing Text-to-SQL benchmarks are mostly English-only, hindering multilingual progress.", "method": "The paper introduces MultiSpider 2.0, a multilingual extension of Spider 2.0 with added linguistic and dialectal variability.", "result": "State-of-the-art LLMs achieve only 4% execution accuracy on MultiSpider 2.0 using intrinsic reasoning, compared to 60% on MultiSpider 1.0. A collaboration-driven language agent baseline improves accuracy to 15%.", "conclusion": "The results highlight a significant multilingual gap and the need for robust, multilingual Text-to-SQL methods for real-world deployment. The benchmark is publicly available."}}
{"id": "2509.22807", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22807", "abs": "https://arxiv.org/abs/2509.22807", "authors": ["Mengchen Zhao", "Yifan Gao", "Yaqing Hou", "Xiangyang Li", "Pengjie Gu", "Zhenhua Dong", "Ruiming Tang", "Yi Cai"], "title": "MTRec: Learning to Align with User Preferences via Mental Reward Models", "comment": null, "summary": "Recommendation models are predominantly trained using implicit user feedback,\nsince explicit feedback is often costly to obtain. However, implicit feedback,\nsuch as clicks, does not always reflect users' real preferences. For example, a\nuser might click on a news article because of its attractive headline, but end\nup feeling uncomfortable after reading the content. In the absence of explicit\nfeedback, such erroneous implicit signals may severely mislead recommender\nsystems. In this paper, we propose MTRec, a novel sequential recommendation\nframework designed to align with real user preferences by uncovering their\ninternal satisfaction on recommended items. Specifically, we introduce a mental\nreward model to quantify user satisfaction and propose a distributional inverse\nreinforcement learning approach to learn it. The learned mental reward model is\nthen used to guide recommendation models to better align with users' real\npreferences. Our experiments show that MTRec brings significant improvements to\na variety of recommendation models. We also deploy MTRec on an industrial short\nvideo platform and observe a 7 percent increase in average user viewing time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e8f\u5217\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u7528\u6237\u5bf9\u63a8\u8350\u9879\u76ee\u7684\u5185\u5728\u6ee1\u610f\u5ea6\u6765\u4e0e\u771f\u5b9e\u7528\u6237\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002", "motivation": "\u7531\u4e8e\u663e\u6027\u53cd\u9988\u901a\u5e38\u96be\u4ee5\u83b7\u5f97\uff0c\u63a8\u8350\u6a21\u578b\u4e3b\u8981\u4f7f\u7528\u9690\u6027\u7528\u6237\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u9690\u6027\u53cd\u9988\u5e76\u4e0d\u603b\u662f\u53cd\u6620\u7528\u6237\u7684\u771f\u5b9e\u504f\u597d\uff0c\u53ef\u80fd\u5bfc\u81f4\u63a8\u8350\u7cfb\u7edf\u88ab\u8bef\u5bfc\u3002", "method": "\u5f15\u5165\u5fc3\u7406\u5956\u52b1\u6a21\u578b\u6765\u91cf\u5316\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5206\u5e03\u5f0f\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u5b66\u4e60\u5b83\u3002\u7136\u540e\uff0c\u5b66\u4e60\u5230\u7684\u5fc3\u7406\u5956\u52b1\u6a21\u578b\u88ab\u7528\u6765\u6307\u5bfc\u63a8\u8350\u6a21\u578b\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4e0e\u7528\u6237\u7684\u771f\u5b9e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMTRec \u4e3a\u5404\u79cd\u63a8\u8350\u6a21\u578b\u5e26\u6765\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u5728\u5de5\u4e1a\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u90e8\u7f72 MTRec \u540e\uff0c\u5e73\u5747\u7528\u6237\u89c2\u770b\u65f6\u95f4\u589e\u52a0\u4e86 7%\u3002", "conclusion": "MTRec \u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u63a8\u8350\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4e0e\u7528\u6237\u7684\u771f\u5b9e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2509.22729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22729", "abs": "https://arxiv.org/abs/2509.22729", "authors": ["Sadia Abdulhalim", "Muaz Albaghdadi", "Moshiur Farazi"], "title": "Multi-Modal Sentiment Analysis with Dynamic Attention Fusion", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Traditional sentiment analysis has long been a unimodal task, relying solely\non text. This approach overlooks non-verbal cues such as vocal tone and prosody\nthat are essential for capturing true emotional intent. We introduce Dynamic\nAttention Fusion (DAF), a lightweight framework that combines frozen text\nembeddings from a pretrained language model with acoustic features from a\nspeech encoder, using an adaptive attention mechanism to weight each modality\nper utterance. Without any finetuning of the underlying encoders, our proposed\nDAF model consistently outperforms both static fusion and unimodal baselines on\na large multimodal benchmark. We report notable gains in F1-score and\nreductions in prediction error and perform a variety of ablation studies that\nsupport our hypothesis that the dynamic weighting strategy is crucial for\nmodeling emotionally complex inputs. By effectively integrating verbal and\nnon-verbal information, our approach offers a more robust foundation for\nsentiment prediction and carries broader impact for affective computing\napplications -- from emotion recognition and mental health assessment to more\nnatural human computer interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u60c5\u611f\u5206\u6790\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u6587\u672c\u548c\u8bed\u97f3\u7279\u5f81\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u9ad8\u60c5\u611f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u60c5\u611f\u5206\u6790\u53ea\u4f9d\u8d56\u4e8e\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u8bed\u97f3\u4e2d\u7684\u975e\u8bed\u8a00\u7ebf\u7d22\uff0c\u8fd9\u4e9b\u7ebf\u7d22\u5bf9\u4e8e\u6355\u6349\u771f\u5b9e\u7684\u60c5\u611f\u610f\u56fe\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u6ce8\u610f\u529b\u878d\u5408\uff08DAF\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u5d4c\u5165\u548c\u8bed\u97f3\u7f16\u7801\u5668\u7684\u58f0\u5b66\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u6765\u8861\u91cf\u6bcf\u4e2a\u6a21\u6001\u3002", "result": "\u63d0\u51fa\u7684DAF\u6a21\u578b\u5728\u5927\u578b\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u9759\u6001\u878d\u5408\u548c\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u5728F1\u5206\u6570\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u51cf\u5c11\u4e86\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u6574\u5408\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u4fe1\u606f\uff0c\u8be5\u65b9\u6cd5\u4e3a\u60c5\u611f\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u57fa\u7840\uff0c\u5e76\u5bf9\u60c5\u611f\u8ba1\u7b97\u5e94\u7528\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.22690", "categories": ["cs.CV", "68T45, 65D19"], "pdf": "https://arxiv.org/pdf/2509.22690", "abs": "https://arxiv.org/abs/2509.22690", "authors": ["Andrea Asperti", "Salvatore Fiorilla", "Simone Nardi", "Lorenzo Orsini"], "title": "A review of Recent Techniques for Person Re-Identification", "comment": null, "summary": "Person re-identification (ReId), a crucial task in surveillance, involves\nmatching individuals across different camera views. The advent of Deep\nLearning, especially supervised techniques like Convolutional Neural Networks\nand Attention Mechanisms, has significantly enhanced person Re-ID. However, the\nsuccess of supervised approaches hinges on vast amounts of annotated data,\nposing scalability challenges in data labeling and computational costs. To\naddress these limitations, recent research has shifted towards unsupervised\nperson re-identification. Leveraging abundant unlabeled data, unsupervised\nmethods aim to overcome the need for pairwise labelled data. Although\ntraditionally trailing behind supervised approaches, unsupervised techniques\nhave shown promising developments in recent years, signalling a narrowing\nperformance gap. Motivated by this evolving landscape, our survey pursues two\nprimary objectives. First, we review and categorize significant publications in\nsupervised person re-identification, providing an in-depth overview of the\ncurrent state-of-the-art and emphasizing little room for further improvement in\nthis domain. Second, we explore the latest advancements in unsupervised person\nre-identification over the past three years, offering insights into emerging\ntrends and shedding light on the potential convergence of performance between\nsupervised and unsupervised paradigms. This dual-focus survey aims to\ncontribute to the evolving narrative of person re-identification, capturing\nboth the mature landscape of supervised techniques and the promising outcomes\nin the realm of unsupervised learning.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u7684\u884c\u4eba\u91cd\u8bc6\u522b(ReID)\u7684\u8fdb\u5c55\uff0c\u5f3a\u8c03\u4e86\u65e0\u76d1\u7763ReID\u7684\u8fdb\u6b65\u53ca\u5176\u4e0e\u6709\u76d1\u7763ReID\u7684\u6f5c\u5728\u878d\u5408\u3002", "motivation": "\u6709\u76d1\u7763ReID\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u65e0\u76d1\u7763ReID\u5229\u7528\u5927\u91cf\u65e0\u6807\u6ce8\u6570\u636e\uff0c\u53ef\u4ee5\u514b\u670d\u6709\u76d1\u7763\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u56de\u987e\u5e76\u5206\u7c7b\u4e86\u6709\u76d1\u7763ReID\u7684\u91cd\u8981\u6587\u732e\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fc7\u53bb\u4e09\u5e74\u4e2d\u65e0\u76d1\u7763ReID\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "result": "\u672c\u6587\u5bf9\u6709\u76d1\u7763ReID\u7684\u6700\u65b0\u6280\u672f\u8fdb\u884c\u4e86\u6df1\u5165\u6982\u8ff0\uff0c\u5e76\u5f3a\u8c03\u4e86\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u7a7a\u95f4\u6709\u9650\u3002\u540c\u65f6\uff0c\u672c\u6587\u6df1\u5165\u4e86\u89e3\u4e86\u65e0\u76d1\u7763ReID\u7684\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u9610\u660e\u4e86\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u8303\u4f8b\u4e4b\u95f4\u6027\u80fd\u878d\u5408\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u5bf9\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u53d1\u5c55\u505a\u51fa\u8d21\u732e\uff0c\u65e2\u6db5\u76d6\u4e86\u6210\u719f\u7684\u6709\u76d1\u7763\u6280\u672f\uff0c\u53c8\u6db5\u76d6\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u9886\u57df\u4e2d\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.22888", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22888", "abs": "https://arxiv.org/abs/2509.22888", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tiffany Zhan", "Saptarshi Ghosh", "Linfeng Liu", "Tianyu Jiang"], "title": "JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory", "comment": "22 pages, 10 figures, 5 tables", "summary": "Standard LLM evaluation practices compress diverse abilities into single\nscores, obscuring their inherently multidimensional nature. We present JE-IRT,\na geometric item-response framework that embeds both LLMs and questions in a\nshared space. For question embeddings, the direction encodes semantics and the\nnorm encodes difficulty, while correctness on each question is determined by\nthe geometric interaction between the model and question embeddings. This\ngeometry replaces a global ranking of LLMs with topical specialization and\nenables smooth variation across related questions. Building on this framework,\nour experimental results reveal that out-of-distribution behavior can be\nexplained through directional alignment, and that larger norms consistently\nindicate harder questions. Moreover, JE-IRT naturally supports generalization:\nonce the space is learned, new LLMs are added by fitting a single embedding.\nThe learned space further reveals an LLM-internal taxonomy that only partially\naligns with human-defined subject categories. JE-IRT thus establishes a unified\nand interpretable geometric lens that connects LLM abilities with the structure\nof questions, offering a distinctive perspective on model evaluation and\ngeneralization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06LLM\u548c\u95ee\u9898\u5d4c\u5165\u5230\u5171\u4eab\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u51e0\u4f55\u4ea4\u4e92\u6765\u8bc4\u4f30LLM\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\u5c06\u591a\u79cd\u80fd\u529b\u538b\u7f29\u6210\u5355\u4e00\u7684\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u5176\u5185\u5728\u7684\u591a\u7ef4\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u51e0\u4f55\u9879\u76ee\u53cd\u5e94\u6846\u67b6\uff08JE-IRT\uff09\uff0c\u8be5\u6846\u67b6\u5c06LLM\u548c\u95ee\u9898\u5d4c\u5165\u5230\u5171\u4eab\u7a7a\u95f4\u4e2d\u3002\u95ee\u9898\u7684\u65b9\u5411\u7f16\u7801\u8bed\u4e49\uff0c\u8303\u6570\u7f16\u7801\u96be\u5ea6\uff0c\u6b63\u786e\u6027\u7531\u6a21\u578b\u548c\u95ee\u9898\u5d4c\u5165\u4e4b\u95f4\u7684\u51e0\u4f55\u4ea4\u4e92\u51b3\u5b9a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5206\u5e03\u5916\u884c\u4e3a\u53ef\u4ee5\u901a\u8fc7\u65b9\u5411\u5bf9\u9f50\u6765\u89e3\u91ca\uff0c\u8f83\u5927\u7684\u8303\u6570\u901a\u5e38\u8868\u793a\u8f83\u96be\u7684\u95ee\u9898\u3002JE-IRT\u652f\u6301\u6cdb\u5316\uff0c\u5e76\u4e14\u63ed\u793a\u4e86\u4e00\u4e2aLLM\u5185\u90e8\u7684\u5206\u7c7b\uff0c\u8be5\u5206\u7c7b\u4e0e\u4eba\u7c7b\u5b9a\u4e49\u7684\u5b66\u79d1\u7c7b\u522b\u90e8\u5206\u5bf9\u9f50\u3002", "conclusion": "JE-IRT\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u89c6\u89d2\uff0c\u5c06LLM\u80fd\u529b\u4e0e\u95ee\u9898\u7684\u7ed3\u6784\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u548c\u6cdb\u5316\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.22850", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22850", "abs": "https://arxiv.org/abs/2509.22850", "authors": ["Roie Kazoom", "Yuval Ratzabi", "Etamar Rothstein", "Ofer Hadar"], "title": "Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data", "comment": null, "summary": "Adversarial robustness in structured data remains an underexplored frontier\ncompared to vision and language domains. In this work, we introduce a novel\nblack-box, decision-based adversarial attack tailored for tabular data. Our\napproach combines gradient-free direction estimation with an iterative boundary\nsearch, enabling efficient navigation of discrete and continuous feature spaces\nunder minimal oracle access. Extensive experiments demonstrate that our method\nsuccessfully compromises nearly the entire test set across diverse models,\nranging from classical machine learning classifiers to large language model\n(LLM)-based pipelines. Remarkably, the attack achieves success rates\nconsistently above 90%, while requiring only a small number of queries per\ninstance. These results highlight the critical vulnerability of tabular models\nto adversarial perturbations, underscoring the urgent need for stronger\ndefenses in real-world decision-making systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65e0\u68af\u5ea6\u65b9\u5411\u4f30\u8ba1\u548c\u8fed\u4ee3\u8fb9\u754c\u641c\u7d22\u3002", "motivation": "\u4e0e\u89c6\u89c9\u548c\u8bed\u8a00\u9886\u57df\u76f8\u6bd4\uff0c\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65e0\u68af\u5ea6\u65b9\u5411\u4f30\u8ba1\u548c\u8fed\u4ee3\u8fb9\u754c\u641c\u7d22\uff0c\u80fd\u591f\u5728\u6700\u5c0f\u7684 oracle \u8bbf\u95ee\u4e0b\u6709\u6548\u5730\u5bfc\u822a\u79bb\u6563\u548c\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u7834\u574f\u4e86\u5404\u79cd\u6a21\u578b\uff08\u4ece\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5230\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u7ba1\u9053\uff09\u4e2d\u51e0\u4e4e\u6574\u4e2a\u6d4b\u8bd5\u96c6\u3002\u653b\u51fb\u6210\u529f\u7387\u59cb\u7ec8\u9ad8\u4e8e 90%\uff0c\u800c\u6bcf\u4e2a\u5b9e\u4f8b\u53ea\u9700\u8981\u5c11\u91cf\u7684\u67e5\u8be2\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u8868\u683c\u6a21\u578b\u5bf9\u5bf9\u6297\u6270\u52a8\u7684\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u51b3\u7b56\u7cfb\u7edf\u4e2d\u8feb\u5207\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u3002"}}
{"id": "2509.23175", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23175", "abs": "https://arxiv.org/abs/2509.23175", "authors": ["Zishuo Xu", "Yuhong Gu", "Dezhong Yao"], "title": "WARBERT: A Hierarchical BERT-based Model for Web API Recommendation", "comment": null, "summary": "With the emergence of Web 2.0 and microservices architecture, the number of\nWeb APIs has increased dramatically, further intensifying the demand for\nefficient Web API recommendation. Existing solutions typically fall into two\ncategories: recommendation-type methods, which treat each API as a label for\nclassification, and match-type methods, which focus on matching mashups through\nAPI retrieval. However, three critical challenges persist: 1) the semantic\nambiguities in comparing API and mashup descriptions, 2) the lack of detailed\ncomparisons between the individual API and the mashup in recommendation-type\nmethods, and 3) time inefficiencies for API retrieval in match-type methods. To\naddress these challenges, we propose WARBERT, a hierarchical BERT-based model\nfor Web API recommendation. WARBERT leverages dual-component feature fusion and\nattention comparison to extract precise semantic representations of API and\nmashup descriptions. WARBERT consists of two main components: WARBERT(R) for\nRecommendation and WARBERT(M) for Matching. Specifically, WAR-BERT(R) serves as\nan initial filter, narrowing down the candidate APIs, while WARBERT(M) refines\nthe matching process by calculating the similarity between candidate APIs and\nmashup. The final likelihood of a mashup being matched with an API is\ndetermined by combining the predictions from WARBERT(R) and WARBERT(M).\nAdditionally, WARBERT(R) incorporates an auxiliary task of mashup category\njudgment, which enhances its effectiveness in candidate selection. Experimental\nresults on the ProgrammableWeb dataset demonstrate that WARBERT outperforms\nmost existing solutions and achieves improvements of up to 11.7% compared to\nthe model MTFM (Multi-Task Fusion Model), delivering significant enhancements\nin accuracy and effiency.", "AI": {"tldr": "\u63d0\u51faWARBERT\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42BERT\u7684Web API\u63a8\u8350\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709Web API\u63a8\u8350\u65b9\u6cd5\u7684\u8bed\u4e49\u6a21\u7cca\u3001\u7f3a\u4e4f\u7ec6\u8282\u6bd4\u8f83\u548c\u65f6\u95f4\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684Web API\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u3001\u7f3a\u4e4f\u7ec6\u8282\u6bd4\u8f83\u548c\u65f6\u95f4\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "WARBERT\u5229\u7528\u53cc\u7ec4\u4ef6\u7279\u5f81\u878d\u5408\u548c\u6ce8\u610f\u529b\u6bd4\u8f83\u6765\u63d0\u53d6API\u548cmashup\u63cf\u8ff0\u7684\u7cbe\u786e\u8bed\u4e49\u8868\u793a\u3002\u5b83\u7531WARBERT(R)\u548cWARBERT(M)\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u7ec4\u6210\uff0c\u5206\u522b\u7528\u4e8e\u63a8\u8350\u548c\u5339\u914d\u3002WARBERT(R)\u8fd8\u5305\u542bmashup\u7c7b\u522b\u5224\u65ad\u7684\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u5728ProgrammableWeb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWARBERT\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14\u4e0e\u6a21\u578bMTFM\u76f8\u6bd4\uff0c\u6700\u9ad8\u53ef\u63d0\u9ad811.7%\u3002", "conclusion": "WARBERT\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u9ad8\u3002"}}
{"id": "2509.22738", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22738", "abs": "https://arxiv.org/abs/2509.22738", "authors": ["Parikshit Bansal", "Sujay Sanghavi"], "title": "Enabling Approximate Joint Sampling in Diffusion LMs", "comment": null, "summary": "In autoregressive language models, each token is sampled by conditioning on\nall the past tokens; the overall string has thus been sampled from the correct\nunderlying joint distribution represented by the model. In contrast, masked\ndiffusion language models generate text by unmasking tokens out of order and\npotentially in parallel. Generating an overall string sampled from the correct\nunderlying joint distribution would (again) require exactly one token unmasking\nin every full-model forward pass. The more tokens unmasked in parallel, the\nfurther away the string is from the true joint; this can be seen in the\nresulting drop in accuracy (but, increase in speed). In this paper we devise a\nway to {\\em approximately} sample multiple tokens from the joint distribution\nin a single full-model forward pass; we do so by developing a new lightweight\nsingle-layer ``sampler\" on top of an existing large diffusion LM. One forward\npass of the full model can now be followed by multiple forward passes of only\nthis sampler layer, to yield multiple unmasked tokens. Our sampler is trained\nto mimic exact joint sampling from the (frozen) full model. We show the\neffectiveness of our approximate joint sampling for both pretrained-only\n(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language\nmodeling and math \\& coding tasks. When four tokens are unmasked for each\nfull-model denoising step, our sampling algorithm achieves a MAUVE score of\n0.87 (vs marginal baseline of 0.31) with respect to the true joint\ndistribution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f7b\u91cf\u7ea7\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u8fd1\u4f3c\u5730\u4ece\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u591a\u4e2atoken\uff0c\u4ee5\u63d0\u9ad8masked diffusion\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "Masked diffusion\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u751f\u6210token\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u56e0\u4e3a\u504f\u79bb\u4e86\u771f\u5b9e\u7684\u8054\u5408\u5206\u5e03\u3002", "method": "\u5728\u73b0\u6709\u7684\u5927\u578bdiffusion LM\u4e4b\u4e0a\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u8f7b\u91cf\u7ea7\u5355\u5c42\u201c\u91c7\u6837\u5668\u201d\uff0c\u8be5\u91c7\u6837\u5668\u7ecf\u8fc7\u8bad\u7ec3\u4ee5\u6a21\u4eff\u6765\u81ea\uff08\u51bb\u7ed3\u7684\uff09\u5b8c\u6574\u6a21\u578b\u7684\u7cbe\u786e\u8054\u5408\u91c7\u6837\u3002", "result": "\u5f53\u6bcf\u4e2a\u5b8c\u6574\u6a21\u578b\u53bb\u566a\u6b65\u9aa4\u89e3mask\u56db\u4e2atoken\u65f6\uff0c\u6211\u4eec\u7684\u91c7\u6837\u7b97\u6cd5\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u6570\u5b66\u4e0e\u7f16\u7801\u4efb\u52a1\u4e0a\uff0c\u76f8\u5bf9\u4e8e\u771f\u5b9e\u8054\u5408\u5206\u5e03\uff0c\u5b9e\u73b0\u4e860.87\u7684MAUVE\u5206\u6570\uff08\u800c\u8fb9\u9645\u57fa\u7ebf\u4e3a0.31\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728pretrained\u548cinstruction-tuned\u6a21\u578b\u4e0a\u90fd\u6709\u6548\uff0c\u63d0\u9ad8\u4e86masked diffusion\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.22691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22691", "abs": "https://arxiv.org/abs/2509.22691", "authors": ["Yan Wen", "Peng Ye", "Lin Zhang", "Baopu Li", "Jiakang Yuan", "Yaoxin Yang", "Tao Chen"], "title": "Sequential Token Merging: Revisiting Hidden States", "comment": null, "summary": "Vision Mambas (ViMs) achieve remarkable success with sub-quadratic\ncomplexity, but their efficiency remains constrained by quadratic token scaling\nwith image resolution. While existing methods address token redundancy, they\noverlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a\ncritical information flow mechanism revealed in our analysis. We further\nidentify Mamba's selective scan enables gradual information aggregation in\nhidden states. Based on these insights, we propose Sequential Token Merging\n(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve\nsequential dependencies through symmetric spatial aggregation, and 2) Hidden\nstates protection to stabilize the hidden states around the class token. STM\nstrategically leverages Mamba's layer-wise loss convergence to convert temporal\nforgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%\naccuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for\nViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with\nminimal complexity, while providing new insights into state-space model\ndynamics. Codes will be released soon.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5e8f\u5217\u4ee4\u724c\u5408\u5e76\uff08STM\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8 Vision Mamba (ViM) \u7684\u6548\u7387\uff0c\u540c\u65f6\u5c3d\u91cf\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86 ViM \u56fa\u6709\u7684\u6709\u9650\u65b9\u5411\u5e8f\u5217\u4f9d\u8d56\u6027\uff08LDSD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5173\u952e\u7684\u4fe1\u606f\u6d41\u673a\u5236\u3002\u6b64\u5916\uff0cMamba \u7684\u9009\u62e9\u6027\u626b\u63cf\u80fd\u591f\u9010\u6b65\u805a\u5408\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u4fe1\u606f\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1) \u53cc\u5411\u6700\u8fd1\u90bb\u5408\u5e76\uff0c\u901a\u8fc7\u5bf9\u79f0\u7a7a\u95f4\u805a\u5408\u6765\u4fdd\u6301\u5e8f\u5217\u4f9d\u8d56\u6027\uff1b2) \u9690\u85cf\u72b6\u6001\u4fdd\u62a4\uff0c\u4ee5\u7a33\u5b9a\u7c7b\u4ee4\u724c\u5468\u56f4\u7684\u9690\u85cf\u72b6\u6001\u3002STM \u7b56\u7565\u6027\u5730\u5229\u7528 Mamba \u7684\u5206\u5c42\u635f\u5931\u6536\u655b\uff0c\u5c06\u65f6\u95f4\u9057\u5fd8\u8f6c\u5316\u4e3a\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTM \u5177\u6709\u4f18\u8d8a\u6027\uff1a\u5728\u51cf\u5c11 20% \u7684\u4ee4\u724c\u65f6\uff0cViM-Ti \u7684\u51c6\u786e\u7387\u4e0b\u964d 1.0%\uff0c\u800c\u5728\u51cf\u5c11 40% \u7684\u4ee4\u724c\u65f6\uff0cViM-S \u7684\u51c6\u786e\u7387\u4ec5\u4e0b\u964d 1.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u6700\u5c0f\u7684\u590d\u6742\u5ea6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\uff0c\u540c\u65f6\u4e3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.22984", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22984", "abs": "https://arxiv.org/abs/2509.22984", "authors": ["Yu Wu", "Shuo Wu", "Ye Tao", "Yansong Li", "Anand D. Sarwate"], "title": "Not only a helper, but also a teacher: Interactive LLM Cascade", "comment": "29 pages, 4 figures, under review", "summary": "Large Language Models (LLMs) vary widely in their capabilities, with larger\nmodels often having better performance but higher cost: choosing an LLM model\noften involves trading off performance and cost. The LLM Cascade is a paradigm\nthat defers difficult queries from weak/cheap to strong/expensive models. This\napproach is nonadaptive: the deferral decision is trained offline. When\nconfronted with similar or repeated queries, the LLM Cascade may then\nrepeatedly consult the expensive model and incur higher cost. To improve the\ncascading efficiency, we propose Inter-Cascade, an online and interactive LLM\nCascade that extends the role of strong model from a backup helper to a\nlong-term teacher. In our system, when a strong model resolves a difficult\nquery, it also distills its solution into a generalized, reusable\nproblem-solving strategy that boosts the weak model on subsequent queries.\nAdding strategies to queries enables the weak model to dynamically improve its\nperformance over time, avoiding computationally and time-intensive fine-tuning.\nEmpirically, compared with standard LLM Cascade baselines across multiple\nbenchmarks, the Inter-Cascade significantly improves the accuracy of the weak\nmodel (by up to 33.06 absolute percentage points) and the overall system (by up\nto 5.53 absolute percentage points), while reducing the calls to strong models\n(by up to 48.05% relative reduction) and saving the corresponding fees (by up\nto 49.63% relative reduction). Inter-Cascade demonstrates the effective\nin-context knowledge transfer between LLMs, and provides a general, scalable\nframework applicable to both open-source and API-based LLMs.", "AI": {"tldr": "Inter-Cascade: An online and interactive LLM Cascade that improves cascading efficiency by distilling solutions from a strong model into reusable problem-solving strategies for a weak model, enhancing the weak model's performance over time.", "motivation": "Choosing an LLM model often involves trading off performance and cost. The LLM Cascade is a paradigm that defers difficult queries from weak/cheap to strong/expensive models. This approach is nonadaptive: the deferral decision is trained offline. When confronted with similar or repeated queries, the LLM Cascade may then repeatedly consult the expensive model and incur higher cost.", "method": "The strong model distills its solution into a generalized, reusable problem-solving strategy that boosts the weak model on subsequent queries. Adding strategies to queries enables the weak model to dynamically improve its performance over time, avoiding computationally and time-intensive fine-tuning.", "result": "Inter-Cascade significantly improves the accuracy of the weak model (by up to 33.06 absolute percentage points) and the overall system (by up to 5.53 absolute percentage points), while reducing the calls to strong models (by up to 48.05% relative reduction) and saving the corresponding fees (by up to 49.63% relative reduction).", "conclusion": "Inter-Cascade demonstrates the effective in-context knowledge transfer between LLMs and provides a general, scalable framework applicable to both open-source and API-based LLMs."}}
{"id": "2509.22851", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22851", "abs": "https://arxiv.org/abs/2509.22851", "authors": ["Yaswanth Chittepu", "Prasann Singhal", "Greg Durrett", "Scott Niekum"], "title": "Adaptive Margin RLHF via Preference over Preferences", "comment": null, "summary": "Margin-based optimization is fundamental to improving generalization and\nrobustness in classification tasks. In the context of reward model learning\nfrom preferences within Reinforcement Learning from Human Feedback (RLHF),\nexisting methods typically rely on no margins, fixed margins, or margins that\nare simplistic functions of preference ratings. However, such formulations\noften fail to account for the varying strengths of different preferences, for\nexample some preferences are associated with larger margins between responses,\nor they rely on noisy margin information derived from ratings. We argue that\nmodeling the strength of preferences can lead to better generalization and more\nfaithful alignment. Furthermore, many existing methods that use adaptive\nmargins assume access to accurate preference scores, which can be difficult for\nhumans to provide reliably. We propose an approach that leverages preferences\nover preferences, that is annotations indicating which of two preferences\nreflects a stronger distinction. We use this ordinal signal to infer adaptive\nmargins on a per-datapoint basis. We introduce an extension to Direct\nPreference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from\npreference-over-preference supervision, enabling improved discriminative and\ngenerative performance. Empirically, our method outperforms vanilla DPO, DPO\nwith fixed margins, and DPO with ground-truth margins on the UltraFeedback\ndataset. Additionally, we show that there is a tradeoff between discriminative\nand generative performance: improving test classification accuracy,\nparticularly by correctly labeling weaker preferences at the expense of\nstronger ones, can lead to a decline in generative quality. To navigate this\ntradeoff, we propose two sampling strategies to gather\npreference-over-preference labels: one favoring discriminative performance and\none favoring generative performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u6392\u5e8f\u7684\u81ea\u9002\u5e94\u8fb9\u9645\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8fb9\u9645\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u5f3a\u5ea6\u7684\u504f\u597d\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u96be\u4ee5\u83b7\u5f97\u51c6\u786e\u7684\u504f\u597d\u5206\u6570\u3002", "method": "\u5229\u7528\u504f\u597d\u6392\u5e8f\u4fe1\u53f7\uff0c\u63a8\u65ad\u6bcf\u4e2a\u6570\u636e\u70b9\u7684\u81ea\u9002\u5e94\u8fb9\u9645\uff0c\u5e76\u6269\u5c55\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff0c\u63d0\u51fa\u4e86DPO-PoP\u65b9\u6cd5\u3002", "result": "\u5728UltraFeedback\u6570\u636e\u96c6\u4e0a\uff0cDPO-PoP\u4f18\u4e8evanilla DPO\u3001\u56fa\u5b9a\u8fb9\u9645\u7684DPO\u4ee5\u53caground-truth\u8fb9\u9645\u7684DPO\u3002\u540c\u65f6\u53d1\u73b0\u5224\u522b\u6027\u80fd\u548c\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u504f\u597d\u6392\u5e8f\u8fdb\u884c\u81ea\u9002\u5e94\u8fb9\u9645\u8c03\u6574\u80fd\u591f\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u9700\u8981\u5728\u5224\u522b\u6027\u80fd\u548c\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u91c7\u6837\u7b56\u7565\u6765\u5e73\u8861\u8fd9\u79cd\u6743\u8861\u3002"}}
{"id": "2509.23649", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23649", "abs": "https://arxiv.org/abs/2509.23649", "authors": ["KaiWen Wei", "Kejun He", "Xiaomian Kang", "Jie Zhang", "Yuming Yang", "Jiang Zhong", "He Bai", "Junnan Zhu"], "title": "From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation", "comment": null, "summary": "Generative recommendation, which directly generates item identifiers, has\nemerged as a promising paradigm for recommendation systems. However, its\npotential is fundamentally constrained by the reliance on purely autoregressive\ntraining. This approach focuses solely on predicting the next item while\nignoring the rich internal structure of a user's interaction history, thus\nfailing to grasp the underlying intent. To address this limitation, we propose\nMasked History Learning (MHL), a novel training framework that shifts the\nobjective from simple next-step prediction to deep comprehension of history.\nMHL augments the standard autoregressive objective with an auxiliary task of\nreconstructing masked historical items, compelling the model to understand\n``why'' an item path is formed from the user's past behaviors, rather than just\n``what'' item comes next. We introduce two key contributions to enhance this\nframework: (1) an entropy-guided masking policy that intelligently targets the\nmost informative historical items for reconstruction, and (2) a curriculum\nlearning scheduler that progressively transitions from history reconstruction\nto future prediction. Experiments on three public datasets show that our method\nsignificantly outperforms state-of-the-art generative models, highlighting that\na comprehensive understanding of the past is crucial for accurately predicting\na user's future path. The code will be released to the public.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u63a8\u8350\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u88abmask\u6389\u7684\u5386\u53f2\u7269\u54c1\u6765\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u610f\u56fe\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u4f9d\u8d56\u4e8e\u81ea\u56de\u5f52\u8bad\u7ec3\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u672a\u80fd\u7406\u89e3\u6f5c\u5728\u610f\u56fe\u3002", "method": "\u63d0\u51fa\u4e86Masked History Learning (MHL) \u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784\u88abmask\u6389\u7684\u5386\u53f2\u7269\u54c1\u6765\u5b66\u4e60\u7528\u6237\u5386\u53f2\u7684\u6df1\u5c42\u7406\u89e3\u3002\u4f7f\u7528\u4e86entropy-guided masking\u7b56\u7565\u548c curriculum learning scheduler\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8estate-of-the-art\u7684\u751f\u6210\u6a21\u578b\u3002", "conclusion": "\u5bf9\u8fc7\u53bb\u884c\u4e3a\u7684\u5168\u9762\u7406\u89e3\u5bf9\u4e8e\u51c6\u786e\u9884\u6d4b\u7528\u6237\u672a\u6765\u7684\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.22739", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.22739", "abs": "https://arxiv.org/abs/2509.22739", "authors": ["Sasha Cui", "Zhongren Chen"], "title": "Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models", "comment": null, "summary": "Language models (LMs) are typically post-trained for desired capabilities and\nbehaviors via weight-based or prompt-based steering, but the former is\ntime-consuming and expensive, and the latter is not precisely controllable and\noften requires manual trial-and-error. While activation steering (AS) promises\na cheap, fast, and controllable alternative to the two existing post-training\nmethods, current AS techniques require hand-crafted prompt pairs or\nlabor-intensive feature annotation, making them more inconvenient than the\nplug-and-play methods such as Reinforcement Learning (RL) and Supervised\nFine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of\nfully automated methods that make AS readily usable with any given labeled\ndataset, with no need for prompt construction, feature labeling, or human\nintervention. We evaluate PAS on three open-weight models\n(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;\nwe find that PAS reliably improves performance for behavior tasks, but not for\nintelligence-oriented tasks. The introspective variant (iPAS) delivers the\nstrongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%\non Alignment). We also show PAS delivers additional gains on top of In-Context\nLearning (ICL) and SFT. PAS constructs a fast, lightweight activation vector\nthat can be cheaply trained, easily stored, and activated at will. Our results\nprovide a characterization of where AS helps, where it fails, and how to deploy\nit as a practical, automated LM post-training option.", "AI": {"tldr": "This paper introduces Painless Activation Steering (PAS), a fully automated method for activation steering in language models, making it more practical and easier to use compared to existing methods.", "motivation": "Existing methods for post-training language models, such as weight-based or prompt-based steering, have limitations in terms of time, cost, controllability, and manual effort. Activation steering offers a promising alternative, but current techniques are inconvenient due to the need for hand-crafted prompt pairs or labor-intensive feature annotation.", "method": "The authors propose Painless Activation Steering (PAS), a family of fully automated methods that can be used with any labeled dataset without prompt construction, feature labeling, or human intervention. They evaluate PAS on three open-weight models and 18 tasks.", "result": "PAS reliably improves performance for behavior tasks but not for intelligence-oriented tasks. The introspective variant (iPAS) delivers strong causal steering effects. PAS also delivers additional gains on top of In-Context Learning (ICL) and SFT.", "conclusion": "PAS constructs a fast, lightweight activation vector that can be cheaply trained, easily stored, and activated at will. The results characterize where AS helps, where it fails, and how to deploy it as a practical, automated LM post-training option."}}
{"id": "2509.22692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22692", "abs": "https://arxiv.org/abs/2509.22692", "authors": ["Le Zhang", "Ao Li", "Qibin Hou", "Ce Zhu", "Yonina C. Eldar"], "title": "Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects", "comment": "Accepted by Proceedings of the IEEE", "summary": "Super-resolution (SR) has garnered significant attention within the computer\nvision community, driven by advances in deep learning (DL) techniques and the\ngrowing demand for high-quality visual applications. With the expansion of this\nfield, numerous surveys have emerged. Most existing surveys focus on specific\ndomains, lacking a comprehensive overview of this field. Here, we present an\nin-depth review of diverse SR methods, encompassing single image\nsuper-resolution (SISR), video super-resolution (VSR), stereo super-resolution\n(SSR), and light field super-resolution (LFSR). We extensively cover over 150\nSISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR\nand LFSR. We analyze methodologies, datasets, evaluation protocols, empirical\nresults, and complexity. In addition, we conducted a taxonomy based on each\nbackbone structure according to the diverse purposes. We also explore valuable\nyet under-studied open issues in the field. We believe that this work will\nserve as a valuable resource and offer guidance to researchers in this domain.\nTo facilitate access to related work, we created a dedicated repository\navailable at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u56de\u987e\u4e86\u8d85\u5206\u8fa8\u7387(SR)\u65b9\u6cd5\uff0c\u5305\u62ec\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387(SISR)\u3001\u89c6\u9891\u8d85\u5206\u8fa8\u7387(VSR)\u3001\u7acb\u4f53\u8d85\u5206\u8fa8\u7387(SSR)\u548c\u5149\u573a\u8d85\u5206\u8fa8\u7387(LFSR)\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60(DL)\u6280\u672f\u7684\u53d1\u5c55\u548c\u5bf9\u9ad8\u8d28\u91cf\u89c6\u89c9\u5e94\u7528\u7684\u9700\u6c42\u589e\u957f\uff0c\u8d85\u5206\u8fa8\u7387(SR)\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5907\u53d7\u5173\u6ce8\u3002\u73b0\u6709\u7684\u7efc\u8ff0\u5927\u591a\u5173\u6ce8\u7279\u5b9a\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\u3002", "method": "\u5206\u6790\u4e86150\u591a\u4e2a\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387(SISR)\u65b9\u6cd5\uff0c\u8fd170\u4e2a\u89c6\u9891\u8d85\u5206\u8fa8\u7387(VSR)\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5927\u7ea630\u4e2a\u7acb\u4f53\u8d85\u5206\u8fa8\u7387(SSR)\u548c\u5149\u573a\u8d85\u5206\u8fa8\u7387(LFSR)\u6280\u672f\u3002\u5206\u6790\u65b9\u6cd5\u5305\u62ec\u65b9\u6cd5\u8bba\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u3001\u7ecf\u9a8c\u7ed3\u679c\u548c\u590d\u6742\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6839\u636e\u4e0d\u540c\u7684\u76ee\u7684\uff0c\u57fa\u4e8e\u6bcf\u4e2a\u9aa8\u5e72\u7ed3\u6784\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u5b58\u50a8\u5e93\uff0c\u53ef\u5728https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review\u8bbf\u95ee\uff0c\u4ee5\u65b9\u4fbf\u8bbf\u95ee\u76f8\u5173\u5de5\u4f5c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c06\u6210\u4e3a\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.22989", "categories": ["cs.AI", "cs.CY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.22989", "abs": "https://arxiv.org/abs/2509.22989", "authors": ["Zirui Cheng", "Jiaxuan You"], "title": "Towards Strategic Persuasion with Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong persuasive capabilities\ncomparable to those of humans, offering promising benefits while raising\nsocietal concerns about their deployment. However, systematically evaluating\nthe persuasive capabilities of LLMs is inherently challenging, as the\neffectiveness of persuasion among humans varies significantly across different\ndomains. In this paper, we take a theory-driven approach to provide a scalable\nand principled framework for measuring the persuasive capabilities of LLMs.\nGrounded in the Bayesian Persuasion (BP) framework, we repurpose existing\nhuman-human persuasion datasets to construct environments for evaluating and\ntraining LLMs in strategic persuasion. Our results reveal that frontier models\ncan consistently achieve high persuasion gains and exhibit sophisticated\npersuasion strategies that align with theoretical predictions. Building on\nthis, we use reinforcement learning to train LLMs for strategic persuasion in\nour environments. Our results also demonstrate that even small LLMs can obtain\nsignificantly higher persuasion gains through reinforcement learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u529d\u8bf4\u6846\u67b6\u7684\u3001\u53ef\u6269\u5c55\u7684\u3001\u6709\u539f\u5219\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u529d\u8bf4\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u3001\u53ef\u4e0e\u4eba\u7c7b\u5ab2\u7f8e\u7684\u529d\u8bf4\u80fd\u529b\uff0c\u8fd9\u65e2\u5e26\u6765\u4e86\u6f5c\u5728\u7684\u76ca\u5904\uff0c\u4e5f\u5f15\u53d1\u4e86\u5bf9\u5176\u90e8\u7f72\u7684\u793e\u4f1a\u62c5\u5fe7\u3002\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30LLM\u7684\u529d\u8bf4\u80fd\u529b\u672c\u8d28\u4e0a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4eba\u7c7b\u4e4b\u95f4\u7684\u529d\u8bf4\u6548\u679c\u5728\u4e0d\u540c\u9886\u57df\u5dee\u5f02\u663e\u8457\u3002", "method": "\u672c\u6587\u91c7\u7528\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8d1d\u53f6\u65af\u529d\u8bf4\uff08BP\uff09\u6846\u67b6\uff0c\u6539\u9020\u73b0\u6709\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u529d\u8bf4\u6570\u636e\u96c6\uff0c\u6784\u5efa\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3LLM\u8fdb\u884c\u7b56\u7565\u6027\u529d\u8bf4\u7684\u73af\u5883\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\u8bad\u7ec3LLM\u8fdb\u884c\u7b56\u7565\u6027\u529d\u8bf4\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u80fd\u591f\u6301\u7eed\u83b7\u5f97\u8f83\u9ad8\u7684\u529d\u8bf4\u6536\u76ca\uff0c\u5e76\u8868\u73b0\u51fa\u4e0e\u7406\u8bba\u9884\u6d4b\u76f8\u7b26\u7684\u590d\u6742\u529d\u8bf4\u7b56\u7565\u3002\u5373\u4f7f\u662f\u5c0f\u578bLLM\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u663e\u8457\u66f4\u9ad8\u7684\u529d\u8bf4\u6536\u76ca\u3002", "conclusion": "\u672c\u6587\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347LLM\u7684\u529d\u8bf4\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.22855", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22855", "abs": "https://arxiv.org/abs/2509.22855", "authors": ["Sameep Chattopadhyay", "Nikhil Karamchandani", "Sharayu Mohair"], "title": "Observation-Free Attacks on Online Learning to Rank", "comment": null, "summary": "Online learning to rank (OLTR) plays a critical role in information retrieval\nand machine learning systems, with a wide range of applications in search\nengines and content recommenders. However, despite their extensive adoption,\nthe susceptibility of OLTR algorithms to coordinated adversarial attacks\nremains poorly understood. In this work, we present a novel framework for\nattacking some of the widely used OLTR algorithms. Our framework is designed to\npromote a set of target items so that they appear in the list of top-K\nrecommendations for T - o(T) rounds, while simultaneously inducing linear\nregret in the learning algorithm. We propose two novel attack strategies:\nCascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical\nguarantees showing that both strategies require only O(log T) manipulations to\nsucceed. Additionally, we supplement our theoretical analysis with empirical\nresults on real-world data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u5b66\u4e60\u6392\u5e8f\uff08OLTR\uff09\u7b97\u6cd5\u5728\u9762\u5bf9\u534f\u540c\u5bf9\u6297\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u653b\u51fb\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1\u5728\u7ebf\u5b66\u4e60\u6392\u5e8f\u7b97\u6cd5\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b83\u4eec\u5bf9\u534f\u540c\u5bf9\u6297\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u653b\u51fb\u7b56\u7565\uff1aCascadeOFA\u7528\u4e8eCascadeUCB1\uff0cPBMOFA\u7528\u4e8ePBM-UCB\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u8868\u660e\u8fd9\u4e24\u79cd\u7b56\u7565\u53ea\u9700\u8981O(log T)\u6b21\u64cd\u4f5c\u5c31\u80fd\u6210\u529f\u3002\u901a\u8fc7\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8865\u5145\u4e86\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709OLTR\u7b97\u6cd5\u5728\u9762\u5bf9\u7279\u5b9a\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684OLTR\u7b97\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.23771", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23771", "abs": "https://arxiv.org/abs/2509.23771", "authors": ["Ana Llorens", "Alvaro Torrente"], "title": "Constructing Opera Seria in the Iberian Courts: Metastasian Repertoire for Spain and Portugal", "comment": null, "summary": "The exceptional reception of Pietro Metastasio's works during the eighteenth\ncentury, all over Europe and in the Iberian Peninsula in particular, is well\ndocumented. Due to that unparalleled success, it is possible to ascertain Spain\nand Portugal's participation in international, contemporary tastes and artistic\nwebs, applicable to both composers and performers. However, this\ninternationalisation needs to be nuanced, as some characteristics of the\nrepertoire specifically written for the Peninsula indicate that their court\naudiences may have had expectations, both social and strictly musical,\ndifferent from those of the public in opera theatres elsewhere in the\ncontinent. In this light, this article investigates in what ways the style of\nfive composers in the international scene - Perez, Galuppi, Jommelli, Conforto,\nand Corselli - varied when commissioned to write opera seria for the Iberian\ncourts. The statistical analysis of fifteen settings especially written for the\ncourt theatres in Madrid and Lisbon, in comparison to the average data\nextracted from a corpus of 2,404 arias from 126 versions of a select number of\nMetastasian librettos, allows us to evaluate some particular usages regarding\nkey, metre, tempo, and treatment of the vocal part. In this manner, through\nquantitative analysis, this article places eighteenth-century Iberian music\nproduction and consumption in the context of European opera seria, while\nultimately suggesting that its unique musical characteristics were also partly\ndependent on local musical customs, gender stereotypes, and personal\nidiosyncrasies alike.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e8618\u4e16\u7eaa\u4f0a\u6bd4\u5229\u4e9a\u534a\u5c9b\u5bf9\u6885\u5854\u65af\u5854\u897f\u5965\u4f5c\u54c1\u7684\u63a5\u53d7\u60c5\u51b5\uff0c\u4ee5\u53ca\u5f53\u5730\u5bab\u5ef7\u7684\u7279\u6b8a\u97f3\u4e50\u548c\u793e\u4f1a\u671f\u5f85\u5982\u4f55\u5f71\u54cd\u4e86\u4e3a\u8be5\u5730\u533a\u521b\u4f5c\u7684\u6b4c\u5267\u98ce\u683c\u3002", "motivation": "\u7814\u7a7618\u4e16\u7eaa\u4f0a\u6bd4\u5229\u4e9a\u534a\u5c9b\u7684\u97f3\u4e50\u54c1\u5473\u548c\u56fd\u9645\u5316\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u5f53\u5730\u5bab\u5ef7\u5bf9\u6b4c\u5267\u98ce\u683c\u7684\u72ec\u7279\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7edf\u8ba1\u5206\u679015\u9996\u4e13\u95e8\u4e3a\u9a6c\u5fb7\u91cc\u548c\u91cc\u65af\u672c\u5bab\u5ef7\u5267\u9662\u521b\u4f5c\u7684\u6b4c\u5267\uff0c\u5e76\u4e0e2404\u9996\u548f\u53f9\u8c03\u7684\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5173\u952e\u3001\u8282\u62cd\u3001\u901f\u5ea6\u548c\u58f0\u4e50\u90e8\u5206\u7684\u5904\u7406\u65b9\u5f0f\u3002", "result": "\u91cf\u5316\u5206\u6790\u8868\u660e\uff0c\u4f0a\u6bd4\u5229\u4e9a\u97f3\u4e50\u7684\u72ec\u7279\u97f3\u4e50\u7279\u5f81\u90e8\u5206\u53d6\u51b3\u4e8e\u5f53\u5730\u97f3\u4e50\u4e60\u4fd7\u3001\u6027\u522b\u523b\u677f\u5370\u8c61\u548c\u4e2a\u4eba\u7279\u8d28\u3002", "conclusion": "\u672c\u6587\u5c0618\u4e16\u7eaa\u4f0a\u6bd4\u5229\u4e9a\u97f3\u4e50\u7684\u751f\u4ea7\u548c\u6d88\u8d39\u7f6e\u4e8e\u6b27\u6d32\u6b63\u6b4c\u5267\u7684\u80cc\u666f\u4e0b\uff0c\u5e76\u8868\u660e\u5176\u72ec\u7279\u7684\u97f3\u4e50\u7279\u5f81\u90e8\u5206\u53d6\u51b3\u4e8e\u5f53\u5730\u7684\u97f3\u4e50\u4e60\u4fd7\u3002"}}
{"id": "2509.22750", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22750", "abs": "https://arxiv.org/abs/2509.22750", "authors": ["Jeonghyun Park", "Ingeol Baek", "Seunghyun Yoon", "Haeun Jang", "Aparna Garimella", "Akriti Jain", "Nedim Lipka", "Hwanhee Lee"], "title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions", "comment": "18 figures, 11 tables", "summary": "Real-world Multi-hop Question Answering (QA) often involves ambiguity that is\ninseparable from the reasoning process itself. This ambiguity creates a\ndistinct challenge, where multiple reasoning paths emerge from a single\nquestion, each requiring independent resolution. Since each sub-question is\nambiguous, the model must resolve ambiguity at every step. Thus, answering a\nsingle question requires handling multiple layers of ambiguity throughout the\nreasoning chain. We find that current Large Language Models (LLMs) struggle in\nthis setting, typically exploring wrong reasoning paths and producing\nincomplete answers. To facilitate research on multi-hop ambiguity, we introduce\nMultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),\na benchmark designed to analyze and evaluate this challenging intersection of\nambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142\nhigh-quality examples of ambiguous multi-hop questions, categorized under a\ntaxonomy of syntactic, general, and semantic ambiguity, and curated through a\nrigorous multi-LLM verification pipeline. Our experiments reveal that even\nstate-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity\ncombined with multi-step inference is a distinct and significant challenge. To\nestablish a robust baseline, we propose CLarifying Ambiguity with a Reasoning\nand InstructiON (CLARION), a multi-agent framework that significantly\noutperforms existing approaches on MIRAGE, paving the way for more adaptive and\nrobust reasoning systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6MIRAGE\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u5904\u7406\u591a\u8df3\u63a8\u7406\u4e2d\u6b67\u4e49\u7684\u80fd\u529b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCLARION\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5e76\u5728MIRAGE\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u8df3\u95ee\u7b54\u4e2d\u5b58\u5728\u4e0e\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u5bc6\u4e0d\u53ef\u5206\u7684\u6b67\u4e49\uff0c\u8fd9\u7ed9\u6a21\u578b\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u6b67\u4e49\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMIRAGE\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b1142\u4e2a\u9ad8\u8d28\u91cf\u7684\u6b67\u4e49\u591a\u8df3\u95ee\u9898\uff0c\u5e76\u5bf9\u95ee\u9898\u8fdb\u884c\u4e86\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCLARION\u7684\u591a\u4ee3\u7406\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728MIRAGE\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002CLARION\u5728MIRAGE\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\uff0c\u7ed3\u5408\u591a\u6b65\u9aa4\u63a8\u7406\u89e3\u51b3\u6b67\u4e49\u662f\u4e00\u4e2a\u663e\u8457\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86CLARION\u6846\u67b6\uff0c\u4e3a\u66f4\u5177\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u7684\u63a8\u7406\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.22697", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22697", "abs": "https://arxiv.org/abs/2509.22697", "authors": ["Abhiroop Chatterjee", "Susmita Ghosh"], "title": "Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment", "comment": "Accepted at the IEEE/CVF International Conference on Computer Vision\n  (ICCV 2025), Workshop on Curated Data for Efficient Learning", "summary": "As data requirements continue to grow, efficient learning increasingly\ndepends on the curation and distillation of high-value data rather than\nbrute-force scaling of model sizes. In the case of a hyperspectral image (HSI),\nthe challenge is amplified by the high-dimensional 3D voxel structure, where\neach spatial location is associated with hundreds of contiguous spectral\nchannels. While vision and language models have been optimized effectively for\nnatural image or text tasks, their cross-modal alignment in the hyperspectral\ndomain remains an open and underexplored problem. In this article, we make an\nattempt to optimize a Vision-Language Model (VLM) for hyperspectral scene\nunderstanding by exploiting a CLIP-style contrastive training framework. Our\nframework maps voxel-level embeddings from a vision backbone onto the latent\nspace of a frozen large embedding model (LEM), where a trainable probe aligns\nvision features with the model's textual token representations. The two\nmodalities are aligned via a contrastive loss restricted to a curated set of\nhard (closest wrong classes) and semi-hard (random distractors) negatives,\nalong with positive pairs. To further enhance alignment, descriptive prompts\nthat encode class semantics are introduced and act as structured anchors for\nthe HSI embeddings. It is seen that the proposed method updates only 0.07\npercent of the total parameters, yet yields state-of-the-art performance. For\nexample, on Indian Pines (IP) the model produces better results over unimodal\nand multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa\n($\\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA\nand +0.90 $\\kappa$. Moreover, this is achieved with the set of parameters,\nnearly 50$\\times$ smaller than DCTN and 90$\\times$ smaller than SS-TMNet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u7528\u4e8e\u9ad8\u5149\u8c31\u573a\u666f\u7406\u89e3\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528CLIP\u98ce\u683c\u7684\u5bf9\u6bd4\u8bad\u7ec3\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u6570\u636e\u9700\u6c42\u7684\u6301\u7eed\u589e\u957f\uff0c\u9ad8\u6548\u5b66\u4e60\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u7ba1\u7406\u548c\u63d0\u70bc\uff0c\u800c\u4e0d\u662f\u6a21\u578b\u5927\u5c0f\u7684\u7c97\u66b4\u6269\u5c55\u3002\u9ad8\u5149\u8c31\u56fe\u50cf(HSI)\u7684\u4e09\u7ef4\u4f53\u7d20\u7ed3\u6784\u5e26\u6765\u4e86\u989d\u5916\u7684\u6311\u6218\uff0c\u5373\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u90fd\u4e0e\u6570\u767e\u4e2a\u8fde\u7eed\u5149\u8c31\u901a\u9053\u76f8\u5173\u8054\u3002\u867d\u7136\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u5df2\u7ecf\u9488\u5bf9\u81ea\u7136\u56fe\u50cf\u6216\u6587\u672c\u4efb\u52a1\u8fdb\u884c\u4e86\u6709\u6548\u4f18\u5316\uff0c\u4f46\u5b83\u4eec\u5728\u9ad8\u5149\u8c31\u9886\u57df\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u5c06\u6765\u81ea\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u7684\u4f53\u7d20\u7ea7\u5d4c\u5165\u6620\u5c04\u5230\u51bb\u7ed3\u7684\u5927\u578b\u5d4c\u5165\u6a21\u578b(LEM)\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5176\u4e2d\u53ef\u8bad\u7ec3\u7684\u63a2\u9488\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u6a21\u578b\u7684\u6587\u672c\u6807\u8bb0\u8868\u793a\u5bf9\u9f50\u3002\u4e24\u79cd\u6a21\u6001\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u5bf9\u9f50\uff0c\u8be5\u5bf9\u6bd4\u635f\u5931\u4ec5\u9650\u4e8e\u4e00\u7ec4\u7cbe\u9009\u7684\u96be\u8d1f\u4f8b(\u6700\u8fd1\u4f3c\u7684\u9519\u8bef\u7c7b\u522b)\u548c\u534a\u96be\u8d1f\u4f8b(\u968f\u673a\u5e72\u6270\u7269)\u4ee5\u53ca\u6b63\u4f8b\u5bf9\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u5bf9\u9f50\uff0c\u5f15\u5165\u4e86\u7f16\u7801\u7c7b\u522b\u8bed\u4e49\u7684\u63cf\u8ff0\u6027\u63d0\u793a\uff0c\u5e76\u4f5c\u4e3aHSI\u5d4c\u5165\u7684\u7ed3\u6784\u5316\u951a\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u66f4\u65b0\u4e86\u603b\u53c2\u6570\u76840.07%\uff0c\u4f46\u4ea7\u751f\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728Indian Pines (IP)\u4e0a\uff0c\u8be5\u6a21\u578b\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u57fa\u7ebf\u4e0a\u4ea7\u751f\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u603b\u4f53\u7cbe\u5ea6(OA)\u63d0\u9ad8\u4e86+0.92\uff0cKappa\u7cfb\u6570\u63d0\u9ad8\u4e86+1.60\uff0c\u800c\u5728Pavia University (PU)\u6570\u636e\u4e0a\uff0c\u603b\u4f53\u7cbe\u5ea6(OA)\u63d0\u9ad8\u4e86+0.69\uff0cKappa\u7cfb\u6570\u63d0\u9ad8\u4e86+0.90\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u53c2\u6570\u91cf\u6bd4DCTN\u5c0f\u8fd150\u500d\uff0c\u6bd4SS-TMNet\u5c0f90\u500d\uff0c\u4f46\u6027\u80fd\u66f4\u4f18\u3002"}}
{"id": "2509.23004", "categories": ["cs.AI", "cs.SC", "math.AG"], "pdf": "https://arxiv.org/pdf/2509.23004", "abs": "https://arxiv.org/abs/2509.23004", "authors": ["Karan Srivastava", "Sanjeeb Dash", "Ryan Cory-Wright", "Barry Trager", "Lior Horesh"], "title": "AI Noether -- Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference", "comment": "22 Pages (13+appendix), 6 Figures, Preprint", "summary": "A core goal in modern science is to harness recent advances in AI and\ncomputer processing to automate and accelerate the scientific method. Symbolic\nregression can fit interpretable models to data, but these models often sit\noutside established theory. Recent systems (e.g., AI Descartes, AI Hilbert)\nenforce derivability from prior axioms. However, sometimes new data and\nassociated hypotheses derived from data are not consistent with existing theory\nbecause the existing theory is incomplete or incorrect. Automating abductive\ninference to close this gap remains open. We propose a solution: an algebraic\ngeometry-based system that, given an incomplete axiom system and a hypothesis\nthat it cannot explain, automatically generates a minimal set of missing axioms\nthat suffices to derive the axiom, as long as axioms and hypotheses are\nexpressible as polynomial equations. We formally establish necessary and\nsufficient conditions for the successful retrieval of such axioms. We\nillustrate the efficacy of our approach by demonstrating its ability to explain\nKepler's third law and a few other laws, even when key axioms are absent.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u6570\u51e0\u4f55\u7684\u7cfb\u7edf\uff0c\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u7f3a\u5931\u7684\u516c\u7406\uff0c\u4ee5\u5f25\u5408\u73b0\u6709\u7406\u8bba\u4e0e\u65b0\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u7684\u6838\u5fc3\u76ee\u6807\u662f\u5229\u7528\u4eba\u5de5\u667a\u80fd\u548c\u8ba1\u7b97\u673a\u5904\u7406\u7684\u6700\u65b0\u8fdb\u5c55\u6765\u81ea\u52a8\u5316\u548c\u52a0\u901f\u79d1\u5b66\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7406\u8bba\u53ef\u80fd\u4e0d\u5b8c\u6574\u6216\u4e0d\u6b63\u786e\uff0c\u5bfc\u81f4\u65b0\u6570\u636e\u548c\u5047\u8bbe\u4e0e\u73b0\u6709\u7406\u8bba\u4e0d\u4e00\u81f4\u3002", "method": "\u8be5\u7cfb\u7edf\u57fa\u4e8e\u4ee3\u6570\u51e0\u4f55\uff0c\u9488\u5bf9\u4e0d\u5b8c\u6574\u7684\u516c\u7406\u7cfb\u7edf\u548c\u65e0\u6cd5\u89e3\u91ca\u7684\u5047\u8bbe\uff0c\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u6700\u5c0f\u7684\u7f3a\u5931\u516c\u7406\u96c6\u5408\uff0c\u53ea\u8981\u516c\u7406\u548c\u5047\u8bbe\u53ef\u4ee5\u8868\u793a\u4e3a\u591a\u9879\u5f0f\u65b9\u7a0b\u3002", "result": "\u8be5\u7814\u7a76\u6b63\u5f0f\u786e\u7acb\u4e86\u6210\u529f\u68c0\u7d22\u6b64\u7c7b\u516c\u7406\u7684\u5fc5\u8981\u548c\u5145\u5206\u6761\u4ef6\u3002\u901a\u8fc7\u8bc1\u660e\u5176\u89e3\u91ca\u5f00\u666e\u52d2\u7b2c\u4e09\u5b9a\u5f8b\u548c\u5176\u4ed6\u4e00\u4e9b\u5b9a\u5f8b\u7684\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u5173\u952e\u516c\u7406\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u81ea\u52a8\u5316\u5f52\u7eb3\u63a8\u7406\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5f25\u5408\u73b0\u6709\u7406\u8bba\u4e0e\u65b0\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.22868", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22868", "abs": "https://arxiv.org/abs/2509.22868", "authors": ["Zehao Niu", "Mihai Anitescu", "Jie Chen"], "title": "Neighborhood Sampling Does Not Learn the Same Graph Neural Network", "comment": null, "summary": "Neighborhood sampling is an important ingredient in the training of\nlarge-scale graph neural networks. It suppresses the exponential growth of the\nneighborhood size across network layers and maintains feasible memory\nconsumption and time costs. While it becomes a standard implementation in\npractice, its systemic behaviors are less understood. We conduct a theoretical\nanalysis by using the tool of neural tangent kernels, which characterize the\n(analogous) training dynamics of neural networks based on their infinitely wide\ncounterparts -- Gaussian processes (GPs). We study several established\nneighborhood sampling approaches and the corresponding posterior GP. With\nlimited samples, the posteriors are all different, although they converge to\nthe same one as the sample size increases. Moreover, the posterior covariance,\nwhich lower-bounds the mean squared prediction error, is uncomparable, aligning\nwith observations that no sampling approach dominates.", "AI": {"tldr": "\u5bf9\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u6765\u8bf4\uff0c\u90bb\u57df\u62bd\u6837\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u7cfb\u7edf\u884c\u4e3a\u5374\u77e5\u4e4b\u751a\u5c11\u3002\u672c\u6587\u5229\u7528\u795e\u7ecf\u6b63\u5207\u6838\u5de5\u5177\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u4e86\u51e0\u79cd\u5df2\u5efa\u7acb\u7684\u90bb\u57df\u62bd\u6837\u65b9\u6cd5\u548c\u76f8\u5e94\u7684\u540e\u9a8cGP\u3002", "motivation": "\u867d\u7136\u90bb\u57df\u62bd\u6837\u5df2\u6210\u4e3a\u5b9e\u8df5\u4e2d\u7684\u6807\u51c6\u5b9e\u73b0\uff0c\u4f46\u5176\u7cfb\u7edf\u884c\u4e3a\u5374\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u795e\u7ecf\u6b63\u5207\u6838\u7684\u5de5\u5177\uff0c\u5176\u8868\u5f81\u4e86\u57fa\u4e8e\u65e0\u9650\u5bbd\u5bf9\u5e94\u7269\uff08\u9ad8\u65af\u8fc7\u7a0b (GP)\uff09\u7684\u795e\u7ecf\u7f51\u7edc\u7684\uff08\u7c7b\u4f3c\uff09\u8bad\u7ec3\u52a8\u6001\u3002\u7814\u7a76\u4e86\u51e0\u79cd\u5df2\u5efa\u7acb\u7684\u90bb\u57df\u62bd\u6837\u65b9\u6cd5\u548c\u76f8\u5e94\u7684\u540e\u9a8c GP\u3002", "result": "\u6837\u672c\u6709\u9650\u65f6\uff0c\u540e\u9a8c\u7ed3\u679c\u90fd\u4e0d\u540c\uff0c\u4f46\u968f\u7740\u6837\u672c\u91cf\u7684\u589e\u52a0\uff0c\u5b83\u4eec\u4f1a\u6536\u655b\u5230\u540c\u4e00\u4e2a\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u540e\u9a8c\u534f\u65b9\u5dee\uff08\u5b83\u5bf9\u5747\u65b9\u9884\u6d4b\u8bef\u5dee\u8fdb\u884c\u4e0b\u9650\uff09\u662f\u4e0d\u53ef\u6bd4\u7684\uff0c\u8fd9\u4e0e\u6ca1\u6709\u62bd\u6837\u65b9\u6cd5\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u89c2\u5bdf\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u90bb\u57df\u62bd\u6837\u65b9\u6cd5\u5728\u6837\u672c\u6709\u9650\u65f6\u540e\u9a8c\u7ed3\u679c\u4e0d\u540c\uff0c\u4f46\u968f\u7740\u6837\u672c\u91cf\u7684\u589e\u52a0\uff0c\u5b83\u4eec\u4f1a\u6536\u655b\u5230\u540c\u4e00\u4e2a\u7ed3\u679c\uff0c\u4e14\u540e\u9a8c\u534f\u65b9\u5dee\u4e0d\u53ef\u6bd4\uff0c\u6ca1\u6709\u62bd\u6837\u65b9\u6cd5\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002"}}
{"id": "2509.23776", "categories": ["cs.IR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.23776", "abs": "https://arxiv.org/abs/2509.23776", "authors": ["Ebrahim Norouzi", "Sven Hertling", "J\u00f6rg Waitelonis", "Harald Sack"], "title": "Semantic Representation of Processes with Ontology Design Patterns", "comment": null, "summary": "The representation of workflows and processes is essential in materials\nscience engineering, where experimental and computational reproducibility\ndepend on structured and semantically coherent process models. Although\nnumerous ontologies have been developed for process modeling, they are often\ncomplex and challenging to reuse. Ontology Design Patterns (ODPs) offer modular\nand reusable modeling solutions to recurring problems; however, these patterns\nare frequently neither explicitly published nor documented in a manner\naccessible to domain experts. This study surveys ontologies relevant to\nscientific workflows and engineering process modeling and identifies implicit\ndesign patterns embedded within their structures. We evaluate the capacity of\nthese ontologies to fulfill key requirements for process representation in\nmaterials science. Furthermore, we propose a baseline method for the automatic\nextraction of design patterns from existing ontologies and assess the approach\nagainst curated ground truth patterns. All resources associated with this work,\nincluding the extracted patterns and the extraction workflow, are made openly\navailable in a public GitHub repository.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8c03\u67e5\u4e86\u4e0e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u548c\u5de5\u7a0b\u8fc7\u7a0b\u5efa\u6a21\u76f8\u5173\u7684\u672c\u4f53\uff0c\u5e76\u8bc6\u522b\u4e86\u5d4c\u5165\u5728\u5176\u7ed3\u6784\u4e2d\u7684\u9690\u5f0f\u8bbe\u8ba1\u6a21\u5f0f\u3002", "motivation": "\u6750\u6599\u79d1\u5b66\u5de5\u7a0b\u4e2d\uff0c\u5de5\u4f5c\u6d41\u7a0b\u548c\u8fc7\u7a0b\u7684\u8868\u793a\u81f3\u5173\u91cd\u8981\uff0c\u5b9e\u9a8c\u548c\u8ba1\u7b97\u7684\u53ef\u91cd\u590d\u6027\u53d6\u51b3\u4e8e\u7ed3\u6784\u5316\u548c\u8bed\u4e49\u8fde\u8d2f\u7684\u8fc7\u7a0b\u6a21\u578b\u3002\u867d\u7136\u5df2\u7ecf\u4e3a\u8fc7\u7a0b\u5efa\u6a21\u5f00\u53d1\u4e86\u8bb8\u591a\u672c\u4f53\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5f88\u590d\u6742\u4e14\u96be\u4ee5\u91cd\u7528\u3002", "method": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u4e0e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u548c\u5de5\u7a0b\u8fc7\u7a0b\u5efa\u6a21\u76f8\u5173\u7684\u672c\u4f53\uff0c\u5e76\u8bc6\u522b\u4e86\u5d4c\u5165\u5728\u5176\u7ed3\u6784\u4e2d\u7684\u9690\u5f0f\u8bbe\u8ba1\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u73b0\u6709\u672c\u4f53\u81ea\u52a8\u63d0\u53d6\u8bbe\u8ba1\u6a21\u5f0f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u6839\u636e\u7b56\u5212\u7684\u5730\u9762\u5b9e\u51b5\u6a21\u5f0f\u8bc4\u4f30\u8be5\u65b9\u6cd5\u3002", "result": "\u6211\u4eec\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u672c\u4f53\u6ee1\u8db3\u6750\u6599\u79d1\u5b66\u4e2d\u8fc7\u7a0b\u8868\u793a\u5173\u952e\u8981\u6c42\u7684\u80fd\u529b\u3002\u6240\u6709\u4e0e\u8fd9\u9879\u5de5\u4f5c\u76f8\u5173\u7684\u8d44\u6e90\uff0c\u5305\u62ec\u63d0\u53d6\u7684\u6a21\u5f0f\u548c\u63d0\u53d6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u90fd\u5728\u516c\u5171 GitHub \u5b58\u50a8\u5e93\u4e2d\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u8bba\u6587\u7814\u7a76\u4e86\u6750\u6599\u79d1\u5b66\u5de5\u7a0b\u4e2d\u5de5\u4f5c\u6d41\u7a0b\u548c\u8fc7\u7a0b\u8868\u793a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8c03\u67e5\u76f8\u5173\u672c\u4f53\u5e76\u63d0\u53d6\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u4e3a\u89e3\u51b3\u8fc7\u7a0b\u5efa\u6a21\u7684\u590d\u6742\u6027\u548c\u53ef\u91cd\u7528\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b9\u6cd5\uff0c\u5e76\u5c06\u7814\u7a76\u6210\u679c\u516c\u5f00\u3002"}}
{"id": "2509.22768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22768", "abs": "https://arxiv.org/abs/2509.22768", "authors": ["Ekaterina Trofimova", "Zosia Shamina", "Maria Selifanova", "Artem Zaitsev", "Remi Savchuk", "Maxim Minets", "Daria Ozerova", "Emil Sataev", "Denis Zuenko", "Andrey E. Ustyuzhanin"], "title": "ML2B: Multi-Lingual ML Benchmark For AutoML", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u673a\u5668\u5b66\u4e60\u4ee3\u7801\u751f\u6210\u7684\u57fa\u51c6ML2B\uff0c\u5305\u542b13\u79cd\u8bed\u8a00\u7684Kaggle\u7ade\u8d5b\u7ffb\u8bd1\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e3b\u8981\u9650\u5236\u4e8e\u82f1\u8bed\uff0c\u5ffd\u7565\u4e86\u673a\u5668\u5b66\u4e60\u7814\u7a76\u548c\u5b9e\u8df5\u7684\u5168\u7403\u5316\u548c\u591a\u8bed\u8a00\u6027\u8d28\u3002", "method": "\u5c0630\u4e2aKaggle\u7ade\u8d5b\u7ffb\u8bd1\u621013\u79cd\u81ea\u7136\u8bed\u8a00\uff0c\u6db5\u76d6\u8868\u683c\u3001\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u7c7b\u578b\uff0c\u5e76\u4f7f\u7528AIDE\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u975e\u82f1\u8bed\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u4e8615-45%\u3002", "conclusion": "\u591a\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5b58\u5728\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.22700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22700", "abs": "https://arxiv.org/abs/2509.22700", "authors": ["Zhuang Qi", "Pan Yu", "Lei Meng", "Sijin Zhou", "Han Yu", "Xiaoxiao Li", "Xiangxu Meng"], "title": "Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning", "comment": "NeurIPS'25 accepted", "summary": "Federated Prompt Learning (FPL) enables communication-efficient adaptation by\ntuning lightweight prompts on top of frozen pre-trained models. Existing FPL\nmethods typically rely on global information, which is only available after the\nsecond training round, to facilitate collaboration among client models.\nTherefore, they are inherently dependent on multi-round communication to fully\nexhibit their strengths. Moreover, existing one-shot federated learning methods\ntypically focus on fitting seen tasks, but lack cross-task generalization. To\nbridge this gap, we propose the Global Prompt Refinement with Non-Interfering\nAttention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to\ndesign a masking mechanism that restricts excessive interaction between the\noriginal text embeddings and the learnable prompt embeddings. GPR-NIAM achieves\nthis through the collaboration of two key modules. Firstly, the attention\nisolation module suppresses attention from the learnable prompt tokens to the\noriginal text tokens, and reweights the reverse attention which preserves\ngeneralization across tasks. Secondly, the cross-silo collaborative refinement\nmodule integrates decentralized visual knowledge into a unified base and\ncalibrates the global prompt through multi-source cross-modal knowledge\nalignment, further mitigating the inconsistency caused by data heterogeneity.\nExtensive experiments conducted on ten benchmark datasets under two tasks show\nthat GPR-NIAM outperforms eight state-of-the-art methods in both class-level\nand domain-level generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPR-NIAM\u7684\u5355\u6b21\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u8f6e\u901a\u4fe1\u548c\u7f3a\u4e4f\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u591a\u8f6e\u901a\u4fe1\uff0c\u4e14\u4e00\u6b21\u6027\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u63a9\u7801\u673a\u5236\uff0c\u9650\u5236\u539f\u59cb\u6587\u672c\u5d4c\u5165\u548c\u53ef\u5b66\u4e60\u63d0\u793a\u5d4c\u5165\u4e4b\u95f4\u7684\u8fc7\u5ea6\u4ea4\u4e92\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u9694\u79bb\u6a21\u5757\u6291\u5236\u63d0\u793a\u4ee4\u724c\u5bf9\u539f\u59cb\u6587\u672c\u4ee4\u724c\u7684\u6ce8\u610f\u529b\uff0c\u5e76\u91cd\u65b0\u52a0\u6743\u53cd\u5411\u6ce8\u610f\u529b\u4ee5\u4fdd\u7559\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8de8\u5b64\u5c9b\u534f\u4f5c\u7ec6\u5316\u6a21\u5757\u5c06\u5206\u6563\u7684\u89c6\u89c9\u77e5\u8bc6\u6574\u5408\u5230\u7edf\u4e00\u7684\u57fa\u7840\u4e2d\uff0c\u5e76\u901a\u8fc7\u591a\u6e90\u8de8\u6a21\u6001\u77e5\u8bc6\u5bf9\u9f50\u6765\u6821\u51c6\u5168\u5c40\u63d0\u793a\u3002", "result": "\u5728\u4e24\u4e2a\u4efb\u52a1\u7684\u5341\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPR-NIAM \u5728\u7c7b\u7ea7\u522b\u548c\u57df\u7ea7\u522b\u7684\u6cdb\u5316\u65b9\u9762\u5747\u4f18\u4e8e\u516b\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "GPR-NIAM \u65b9\u6cd5\u5728\u5355\u6b21\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u8de8\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2509.23006", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23006", "abs": "https://arxiv.org/abs/2509.23006", "authors": ["Hassen Dhrif"], "title": "Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems", "comment": null, "summary": "Agentic AI represents a paradigm shift in enhancing the capabilities of\ngenerative AI models. While these systems demonstrate immense potential and\npower, current evaluation techniques primarily focus on assessing their\nefficacy in identifying appropriate agents, tools, and parameters. However, a\ncritical gap exists in evaluating the alignment between an Agentic AI system's\ntasks and its overarching goals. This paper introduces the Creative Adversarial\nTesting (CAT) framework, a novel approach designed to capture and analyze the\ncomplex relationship between Agentic AI tasks and the system's intended\nobjectives.\n  We validate the CAT framework through extensive simulation using synthetic\ninteraction data modeled after Alexa+ audio services, a sophisticated Agentic\nAI system that shapes the user experience for millions of users globally. This\nsynthetic data approach enables comprehensive testing of edge cases and failure\nmodes while protecting user privacy. Our results demonstrate that the CAT\nframework provides unprecedented insights into goal-task alignment, enabling\nmore effective optimization and development of Agentic AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30 Agentic AI \u7cfb\u7edf\u7684\u4efb\u52a1\u4e0e\u5176\u603b\u4f53\u76ee\u6807\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u4e8e\u8bc4\u4f30 Agentic AI \u7cfb\u7edf\u8bc6\u522b\u9002\u5f53\u4ee3\u7406\u3001\u5de5\u5177\u548c\u53c2\u6570\u7684\u6709\u6548\u6027\uff0c\u4f46\u5728\u8bc4\u4f30 Agentic AI \u7cfb\u7edf\u7684\u4efb\u52a1\u4e0e\u5176\u603b\u4f53\u76ee\u6807\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Creative Adversarial Testing (CAT) \u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u65e8\u5728\u6355\u83b7\u548c\u5206\u6790 Agentic AI \u4efb\u52a1\u4e0e\u7cfb\u7edf\u9884\u671f\u76ee\u6807\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u6a21\u4eff Alexa+ \u97f3\u9891\u670d\u52a1\u7684\u5408\u6210\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u7684\u5927\u91cf\u6a21\u62df\u9a8c\u8bc1\u4e86 CAT \u6846\u67b6\uff0c\u7ed3\u679c\u8868\u660e CAT \u6846\u67b6\u63d0\u4f9b\u4e86\u5bf9\u76ee\u6807-\u4efb\u52a1\u5bf9\u9f50\u7684\u524d\u6240\u672a\u6709\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u6709\u6548\u5730\u4f18\u5316\u548c\u5f00\u53d1 Agentic AI \u7cfb\u7edf\u3002", "conclusion": "CAT \u6846\u67b6\u80fd\u591f\u66f4\u6709\u6548\u5730\u4f18\u5316\u548c\u5f00\u53d1 Agentic AI \u7cfb\u7edf\u3002"}}
{"id": "2509.22881", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22881", "abs": "https://arxiv.org/abs/2509.22881", "authors": ["Karim Khamaisi", "Nicolas Keller", "Stefan Krummenacher", "Valentin Huber", "Bernhard F\u00e4ssler", "Bruno Rodrigues"], "title": "From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants", "comment": null, "summary": "In the context of industrial factories and energy producers, unplanned\noutages are highly costly and difficult to service. However, existing\nacoustic-anomaly detection studies largely rely on generic industrial or\nsynthetic datasets, with few focused on hydropower plants due to limited\naccess. This paper presents a comparative analysis of acoustic-based anomaly\ndetection methods, as a way to improve predictive maintenance in hydropower\nplants. We address key challenges in the acoustic preprocessing under highly\nnoisy conditions before extracting time- and frequency-domain features. Then,\nwe benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which\nare tested on two real-world datasets from the Rodundwerk II pumped-storage\nplant in Austria, one with induced anomalies and one with real-world\nconditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC\n0.966-0.998) and minimal training time, while the LSTM autoencoder delivered\nstrong detection (ROC AUC 0.889-0.997) at the expense of higher computational\ncost.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6c34\u529b\u53d1\u7535\u5382\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u58f0\u5b66\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u5de5\u5382\u548c\u80fd\u6e90\u751f\u4ea7\u5546\u7684\u8ba1\u5212\u5916\u505c\u673a\u4f1a\u9020\u6210\u9ad8\u6602\u7684\u6210\u672c\uff0c\u800c\u4e14\u96be\u4ee5\u7ef4\u62a4\u3002\u73b0\u6709\u7684\u58f0\u5b66\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4e8e\u901a\u7528\u5de5\u4e1a\u6216\u5408\u6210\u6570\u636e\u96c6\uff0c\u7531\u4e8e\u51c6\u5165\u9650\u5236\uff0c\u5f88\u5c11\u6709\u7814\u7a76\u5173\u6ce8\u6c34\u529b\u53d1\u7535\u5382\u3002", "method": "\u5728\u9ad8\u5ea6\u566a\u58f0\u6761\u4ef6\u4e0b\uff0c\u89e3\u51b3\u58f0\u5b66\u9884\u5904\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u7136\u540e\u63d0\u53d6\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u3002\u5bf9\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff1aLSTM AE\u3001K-Means \u548c OC-SVM\u3002", "result": "\u5728\u6765\u81ea\u5965\u5730\u5229 Rodundwerk II \u62bd\u6c34\u84c4\u80fd\u7535\u7ad9\u7684\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4e00\u4e2a\u5177\u6709\u8bf1\u5bfc\u5f02\u5e38\uff0c\u4e00\u4e2a\u5177\u6709\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u3002One-Class SVM \u5728\u7cbe\u5ea6\uff08ROC AUC 0.966-0.998\uff09\u548c\u6700\u77ed\u8bad\u7ec3\u65f6\u95f4\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u6743\u8861\uff0c\u800c LSTM \u81ea\u52a8\u7f16\u7801\u5668\u4ee5\u66f4\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\uff08ROC AUC 0.889-0.997\uff09\u3002", "conclusion": "One-Class SVM \u548c LSTM \u81ea\u52a8\u7f16\u7801\u5668 \u9002\u7528\u4e8e\u6c34\u529b\u53d1\u7535\u5382\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u3002"}}
{"id": "2509.23860", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23860", "abs": "https://arxiv.org/abs/2509.23860", "authors": ["Haiyang Yang", "Qinye Xie", "Qingheng Zhang", "Liyu Chen", "Huike Zou", "Chengbao Lian", "Shuguang Han", "Fei Huang", "Jufeng Chen", "Bo Zheng"], "title": "GSID: Generative Semantic Indexing for E-Commerce Product Understanding", "comment": null, "summary": "Structured representation of product information is a major bottleneck for\nthe efficiency of e-commerce platforms, especially in second-hand ecommerce\nplatforms. Currently, most product information are organized based on manually\ncurated product categories and attributes, which often fail to adequately cover\nlong-tail products and do not align well with buyer preference. To address\nthese problems, we propose \\textbf{G}enerative \\textbf{S}emantic\n\\textbf{I}n\\textbf{D}exings (GSID), a data-driven approach to generate product\nstructured representations. GSID consists of two key components: (1)\nPre-training on unstructured product metadata to learn in-domain semantic\nembeddings, and (2) Generating more effective semantic codes tailored for\ndownstream product-centric applications. Extensive experiments are conducted to\nvalidate the effectiveness of GSID, and it has been successfully deployed on\nthe real-world e-commerce platform, achieving promising results on product\nunderstanding and other downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86GSID\uff0c\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u751f\u6210\u4ea7\u54c1\u7ed3\u6784\u5316\u8868\u793a\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u4ea7\u54c1\u4fe1\u606f\u57fa\u4e8e\u624b\u52a8\u7ba1\u7406\u7684\u4ea7\u54c1\u7c7b\u522b\u548c\u5c5e\u6027\u8fdb\u884c\u7ec4\u7ec7\uff0c\u8fd9\u901a\u5e38\u4e0d\u80fd\u5145\u5206\u8986\u76d6\u957f\u5c3e\u4ea7\u54c1\uff0c\u5e76\u4e14\u4e0e\u4e70\u5bb6\u504f\u597d\u4e0d\u7b26\u3002", "method": "GSID\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1)\u5728\u975e\u7ed3\u6784\u5316\u4ea7\u54c1\u5143\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u5b66\u4e60\u9886\u57df\u5185\u8bed\u4e49\u5d4c\u5165\uff0c(2)\u751f\u6210\u9488\u5bf9\u4e0b\u6e38\u4ee5\u4ea7\u54c1\u4e3a\u4e2d\u5fc3\u7684\u5e94\u7528\u7a0b\u5e8f\u91cf\u8eab\u5b9a\u5236\u7684\u66f4\u6709\u6548\u7684\u8bed\u4e49\u4ee3\u7801\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86GSID\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5df2\u6210\u529f\u90e8\u7f72\u5728\u5b9e\u9645\u7684\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u4e0a\uff0c\u5728\u4ea7\u54c1\u7406\u89e3\u548c\u5176\u4ed6\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\u3002", "conclusion": "GSID\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u751f\u6210\u4ea7\u54c1\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5e76\u5728\u5b9e\u9645\u7684\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u4e0a\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\u3002"}}
{"id": "2509.22808", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22808", "abs": "https://arxiv.org/abs/2509.22808", "authors": ["Mohamed Maged", "Alhassan Ehab", "Ali Mekky", "Besher Hassan", "Shady Shehata"], "title": "ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection", "comment": null, "summary": "With the rise of generative text-to-speech models, distinguishing between\nreal and synthetic speech has become challenging, especially for Arabic that\nhave received limited research attention. Most spoof detection efforts have\nfocused on English, leaving a significant gap for Arabic and its many dialects.\nIn this work, we introduce the first multi-dialect Arabic spoofed speech\ndataset. To evaluate the difficulty of the synthesized audio from each model\nand determine which produces the most challenging samples, we aimed to guide\nthe construction of our final dataset either by merging audios from multiple\nmodels or by selecting the best-performing model, we conducted an evaluation\npipeline that included training classifiers using two approaches: modern\nembedding-based methods combined with classifier heads; classical machine\nlearning algorithms applied to MFCC features; and the RawNet2 architecture. The\npipeline further incorporated the calculation of Mean Opinion Score based on\nhuman ratings, as well as processing both original and synthesized datasets\nthrough an Automatic Speech Recognition model to measure the Word Error Rate.\nOur results demonstrate that FishSpeech outperforms other TTS models in Arabic\nvoice cloning on the Casablanca corpus, producing more realistic and\nchallenging synthetic speech samples. However, relying on a single TTS for\ndataset creation may limit generalizability.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u5408\u6210\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u7814\u7a76\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u6b3a\u9a97\u68c0\u6d4b\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\uff0c\u5ffd\u7565\u4e86\u963f\u62c9\u4f2f\u8bed\u53ca\u5176\u65b9\u8a00\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5d4c\u5165\u7684\u73b0\u4ee3\u65b9\u6cd5\u548c\u5206\u7c7b\u5668\u3001\u5e94\u7528\u4e8eMFCC\u7279\u5f81\u7684\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4ee5\u53caRawNet2\u67b6\u6784\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6d41\u7a0b\u3002\u8be5\u6d41\u7a0b\u8fd8\u7ed3\u5408\u4e86\u57fa\u4e8e\u4eba\u7c7b\u8bc4\u5206\u7684\u5e73\u5747\u610f\u89c1\u5f97\u5206\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5904\u7406\u539f\u59cb\u548c\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ee5\u6d4b\u91cf\u8bcd\u9519\u8bef\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFishSpeech\u5728\u5361\u8428\u5e03\u5170\u5361\u8bed\u6599\u5e93\u4e0a\u7684\u963f\u62c9\u4f2f\u8bed\u97f3\u514b\u9686\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6TTS\u6a21\u578b\uff0c\u4ea7\u751f\u4e86\u66f4\u903c\u771f\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u5408\u6210\u8bed\u97f3\u6837\u672c\u3002", "conclusion": "\u4f9d\u8d56\u5355\u4e00TTS\u8fdb\u884c\u6570\u636e\u96c6\u521b\u5efa\u53ef\u80fd\u4f1a\u9650\u5236\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.22708", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22708", "abs": "https://arxiv.org/abs/2509.22708", "authors": ["Ahed Alboody"], "title": "GZSL-MoE: Apprentissage G{\u00e9}n{\u00e9}ralis{\u00e9} Z{\u00e9}ro-Shot bas{\u00e9} sur le M{\u00e9}lange d'Experts pour la Segmentation S{\u00e9}mantique de Nuages de Points 3DAppliqu{\u00e9} {\u00e0} un Jeu de Donn{\u00e9}es d'Environnement de Collaboration Humain-Robot", "comment": "in French language. 28e Conf{\\'e}rence Nationale en Intelligence\n  Artificielle. Plate-Forme Intelligence Artificielle 2025, Association Fran{\\c\n  c}aise pour l'Intelligence Artificielle, https://pfia2025.u-bourgogne.fr/,\n  Jun 2025, Dijon, France", "summary": "Generative Zero-Shot Learning approach (GZSL) has demonstrated significant\npotential in 3D point cloud semantic segmentation tasks. GZSL leverages\ngenerative models like GANs or VAEs to synthesize realistic features (real\nfeatures) of unseen classes. This allows the model to label unseen classes\nduring testing, despite being trained only on seen classes. In this context, we\nintroduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts\n(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to\ngenerate fake features that closely resemble real features extracted using a\npre-trained KPConv (Kernel Point Convolution) model on seen classes. The main\ncontribution of this paper is the integration of Mixture-of-Experts into the\nGenerator and Discriminator components of the Generative Zero-Shot Learning\nmodel for 3D point cloud semantic segmentation, applied to the COVERED dataset\n(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)\nenvironments. By combining the Generative Zero-Shot Learning model with\nMixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides\na promising solution for understanding complex 3D environments, especially when\ncomprehensive training data for all object classes is unavailable. The\nperformance evaluation of the GZSL-MoE model highlights its ability to enhance\nperformance on both seen and unseen classes. Keywords Generalized Zero-Shot\nLearning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot\nCollaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,\nMixture-of Experts", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u5e7f\u4e49\u96f6\u6837\u672c\u5b66\u4e60\uff08GZSL\uff09\u6a21\u578b\uff0c\u7528\u4e8e3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u57283D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u5c06\u6df7\u5408\u4e13\u5bb6\u5c42\uff08MoE\uff09\u878d\u5165\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u4e2d\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u672a\u89c1\u7c7b\u522b\u7684\u4f2a\u7279\u5f81\u3002", "result": "GZSL-MoE\u6a21\u578b\u5728\u5df2\u89c1\u548c\u672a\u89c1\u7c7b\u522b\u4e0a\u5747\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "GZSL-MoE\u4e3a\u7406\u89e3\u590d\u67423D\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u6240\u6709\u5bf9\u8c61\u7c7b\u522b\u7684\u5168\u9762\u8bad\u7ec3\u6570\u636e\u65f6\u3002"}}
{"id": "2509.23023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23023", "abs": "https://arxiv.org/abs/2509.23023", "authors": ["Davi Bastos Costa", "Renato Vicente"], "title": "Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia", "comment": "20 pages, 7 figures, 5 tables; submitted to ICLR 2026; Code and data:\n  https://github.com/bastoscostadavi/llm-mafia-game", "summary": "Mafia is a social deduction game where informed mafia compete against\nuninformed townsfolk. Its asymmetry of information and reliance on\ntheory-of-mind reasoning mirror real-world multi-agent scenarios, making it a\nuseful testbed for evaluating the social intelligence of large language models\n(LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified\nfour-player variant with one mafioso, one detective, and two villagers. We set\nthe mafioso to kill a villager and the detective to investigate the mafioso\nduring the night, reducing the game to a single day phase of discussion and\nvoting. This setup isolates three interactive capabilities through\nrole-specific win conditions: the mafioso must deceive, the villagers must\ndetect deception, and the detective must effectively disclose information. To\nmeasure these skills, we have LLMs play against each other, creating the\nMini-Mafia Benchmark: a two-stage framework that first estimates win rates\nwithin fixed opponent configurations, then aggregates performance across them\nusing standardized scoring. Built entirely from model interactions without\nexternal data, the benchmark evolves as new models are introduced, with each\none serving both as a new opponent and as a subject of evaluation. Our\nexperiments reveal counterintuitive results, including cases where smaller\nmodels outperform larger ones. Beyond benchmarking, Mini-Mafia enables\nquantitative study of emergent multi-agent dynamics such as name bias and\nlast-speaker advantage. It also contributes to AI safety by generating training\ndata for deception detectors and by tracking models' deception capabilities\nagainst human baselines.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a Mini-Mafia \u7684\u7b80\u5316\u7248 Mafia \u6e38\u620f\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u793e\u4f1a\u667a\u80fd\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u793e\u4f1a\u667a\u80fd\uff0c\u56e0\u4e3a Mafia \u6e38\u620f\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u53cd\u6620\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56db\u4eba Mini-Mafia \u53d8\u4f53\uff0c\u5305\u62ec\u4e00\u4e2a\u9ed1\u624b\u515a\u3001\u4e00\u4e2a\u4fa6\u63a2\u548c\u4e24\u4e2a\u6751\u6c11\uff0c\u5e76\u901a\u8fc7\u89d2\u8272\u7279\u5b9a\u7684\u83b7\u80dc\u6761\u4ef6\u6765\u9694\u79bb\u4e09\u79cd\u4e92\u52a8\u80fd\u529b\uff1a\u9ed1\u624b\u515a\u5fc5\u987b\u6b3a\u9a97\uff0c\u6751\u6c11\u5fc5\u987b\u68c0\u6d4b\u6b3a\u9a97\uff0c\u4fa6\u63a2\u5fc5\u987b\u6709\u6548\u62ab\u9732\u4fe1\u606f\u3002LLM \u4e4b\u95f4\u76f8\u4e92\u535a\u5f08\uff0c\u521b\u5efa\u4e00\u4e2a Mini-Mafia \u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u8fdd\u53cd\u76f4\u89c9\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u8f83\u5c0f\u7684\u6a21\u578b\u4f18\u4e8e\u8f83\u5927\u7684\u6a21\u578b\u7684\u60c5\u51b5\u3002Mini-Mafia \u80fd\u591f\u5bf9\u65b0\u5174\u7684\u591a\u667a\u80fd\u4f53\u52a8\u6001\u8fdb\u884c\u5b9a\u91cf\u7814\u7a76\uff0c\u4f8b\u5982\u59d3\u540d\u504f\u89c1\u548c\u6700\u540e\u53d1\u8a00\u8005\u4f18\u52bf\u3002", "conclusion": "Mini-Mafia \u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u7528\u4e8e\u6b3a\u9a97\u68c0\u6d4b\u5668\u7684\u6570\u636e\u548c\u8ddf\u8e2a\u6a21\u578b\u9488\u5bf9\u4eba\u7c7b\u57fa\u7ebf\u7684\u6b3a\u9a97\u80fd\u529b\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2509.22907", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22907", "abs": "https://arxiv.org/abs/2509.22907", "authors": ["Anutam Srinivasan", "Aditya T. Vadlamani", "Amin Meghrazi", "Srinivasan Parthasarathy"], "title": "FedCF: Fair Federated Conformal Prediction", "comment": "Preprint", "summary": "Conformal Prediction (CP) is a widely used technique for quantifying\nuncertainty in machine learning models. In its standard form, CP offers\nprobabilistic guarantees on the coverage of the true label, but it is agnostic\nto sensitive attributes in the dataset. Several recent works have sought to\nincorporate fairness into CP by ensuring conditional coverage guarantees across\ndifferent subgroups. One such method is Conformal Fairness (CF). In this work,\nwe extend the CF framework to the Federated Learning setting and discuss how we\ncan audit a federated model for fairness by analyzing the fairness-related gaps\nfor different demographic groups. We empirically validate our framework by\nconducting experiments on several datasets spanning multiple domains, fully\nleveraging the exchangeability assumption.", "AI": {"tldr": "\u672c\u6587\u5c06Conformal Fairness (CF) \u6846\u67b6\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60 (Federated Learning) \u73af\u5883\u4e2d\u3002", "motivation": "\u786e\u4fdd\u4e0d\u540c\u5b50\u7fa4\u4f53\u7684\u6761\u4ef6\u8986\u76d6\u4fdd\u8bc1\uff0c\u5c06\u516c\u5e73\u6027\u7eb3\u5165 Conformal Prediction (CP)\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u516c\u5e73\u6027\u76f8\u5173\u5dee\u8ddd\uff0c\u5ba1\u8ba1\u8054\u90a6\u6a21\u578b\u7684\u516c\u5e73\u6027\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5145\u5206\u5229\u7528\u4e86\u53ef\u4ea4\u6362\u6027\u5047\u8bbe\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u3002", "conclusion": "\u5c06 Conformal Fairness \u6846\u67b6\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u73af\u5883\uff0c\u5e76\u8ba8\u8bba\u4e86\u5982\u4f55\u5ba1\u8ba1\u8054\u90a6\u6a21\u578b\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2509.23861", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23861", "abs": "https://arxiv.org/abs/2509.23861", "authors": ["Zhongbin Xie", "Thomas Lukasiewicz"], "title": "Investigating Multi-layer Representations for Dense Passage Retrieval", "comment": "Accepted to Findings of EMNLP 2025", "summary": "Dense retrieval models usually adopt vectors from the last hidden layer of\nthe document encoder to represent a document, which is in contrast to the fact\nthat representations in different layers of a pre-trained language model\nusually contain different kinds of linguistic knowledge, and behave differently\nduring fine-tuning. Therefore, we propose to investigate utilizing\nrepresentations from multiple encoder layers to make up the representation of a\ndocument, which we denote Multi-layer Representations (MLR). We first\ninvestigate how representations in different layers affect MLR's performance\nunder the multi-vector retrieval setting, and then propose to leverage pooling\nstrategies to reduce multi-vector models to single-vector ones to improve\nretrieval efficiency. Experiments demonstrate the effectiveness of MLR over\ndual encoder, ME-BERT and ColBERT in the single-vector retrieval setting, as\nwell as demonstrate that it works well with other advanced training techniques\nsuch as retrieval-oriented pre-training and hard negative mining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u8868\u793a\uff08MLR\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0d\u540c\u5c42\u7684\u8868\u793a\u6765\u6784\u6210\u6587\u6863\u7684\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u901a\u5e38\u53ea\u4f7f\u7528\u6587\u6863\u7f16\u7801\u5668\u6700\u540e\u4e00\u5c42\u7684\u5411\u91cf\u6765\u8868\u793a\u6587\u6863\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u5c42\u5305\u542b\u4e0d\u540c\u8bed\u8a00\u77e5\u8bc6\u7684\u4e8b\u5b9e\u3002", "method": "\u7814\u7a76\u4e86\u4e0d\u540c\u5c42\u7684\u8868\u793a\u5982\u4f55\u5f71\u54cdMLR\u5728\u591a\u5411\u91cf\u68c0\u7d22\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u6c60\u5316\u7b56\u7565\u5c06\u591a\u5411\u91cf\u6a21\u578b\u7b80\u5316\u4e3a\u5355\u5411\u91cf\u6a21\u578b\u4ee5\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5355\u5411\u91cf\u68c0\u7d22\u8bbe\u7f6e\u4e0b\uff0cMLR\u4f18\u4e8e\u53cc\u7f16\u7801\u5668\u3001ME-BERT\u548cColBERT\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5176\u4ed6\u5148\u8fdb\u7684\u8bad\u7ec3\u6280\u672f\uff08\u5982\u9762\u5411\u68c0\u7d22\u7684\u9884\u8bad\u7ec3\u548c\u56f0\u96be\u8d1f\u4f8b\u6316\u6398\uff09\u534f\u540c\u5de5\u4f5c\u3002", "conclusion": "MLR\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6587\u6863\u8868\u793a\u7684\u8d28\u91cf\u548c\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2509.22812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22812", "abs": "https://arxiv.org/abs/2509.22812", "authors": ["Kai Zhang", "Christopher Malon", "Lichao Sun", "Martin Renqiang Min"], "title": "EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation", "comment": null, "summary": "Radiology report generation requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. Although recent\ninnovations, particularly multimodal large language models (MLLMs), have shown\nimproved performance, their supervised fine-tuning (SFT) objective is not\nexplicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,\na mixed-policy reinforcement learning (RL) algorithm designed specifically to\noptimize the generation through clinically motivated rewards. EditGRPO\nintegrates on-policy exploration with off-policy guidance by injecting\nsentence-level detailed corrections during training rollouts. This mixed-policy\napproach addresses the exploration dilemma and sampling efficiency issues\ntypically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with\nsupervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO\nbaselines, achieving an average improvement of 3.4% in CheXbert, GREEN,\nRadgraph, and RATEScore metrics across four major chest X-ray report generation\ndatasets. Notably, EditGRPO also demonstrates superior out-of-domain\ngeneralization, with an average performance gain of 5.9% on unseen datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5EditGRPO\uff0c\u7528\u4e8e\u4f18\u5316\u653e\u5c04\u5b66\u62a5\u544a\u7684\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLMs)\u7684\u76d1\u7763\u5fae\u8c03(SFT)\u76ee\u6807\u4e0e\u4e34\u5e8a\u7597\u6548\u6ca1\u6709\u660e\u786e\u7684\u5bf9\u9f50\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u5f3a\u5316\u5b66\u4e60(RL)\u7b97\u6cd5EditGRPO\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u53e5\u5b50\u7ea7\u7684\u8be6\u7ec6\u6821\u6b63\u6765\u6574\u5408\u5728\u7b56\u7565\u63a2\u7d22\u548c\u79bb\u7b56\u7565\u6307\u5bfc\u3002", "result": "\u5728\u56db\u4e2a\u4e3b\u8981\u7684\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u6570\u636e\u96c6\u4e0a\uff0cEditGRPO\u5728CheXbert, GREEN, Radgraph, \u548cRATEScore\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e863.4%\u3002\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\uff0cEditGRPO\u4e5f\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e865.9%\u3002", "conclusion": "EditGRPO\u4f18\u4e8eSFT\u548cvanilla GRPO\u57fa\u7ebf\uff0c\u5e76\u5728\u4e34\u5e8a\u7597\u6548\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6709\u6240\u63d0\u9ad8\u3002"}}
{"id": "2509.22719", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22719", "abs": "https://arxiv.org/abs/2509.22719", "authors": ["Adithya Giri"], "title": "IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism", "comment": null, "summary": "In recent years, Transformer-based architectures have become the dominant\nmethod for Computer Vision applications. While Transformers are explainable and\nscale well with dataset size, they lack the inductive biases of Convolutional\nNeural Networks. While these biases may be learned on large datasets, we show\nthat introducing these inductive biases through learned masks allow Vision\nTransformers to learn on much smaller datasets without Knowledge Distillation.\nThese Transformers, which we call Inductively Biased Image Transformers (IBiT),\nare significantly more accurate on small datasets, while retaining the\nexplainability Transformers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIBiT\u7684Transformer\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u63a9\u7801\u5f15\u5165\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ece\u800c\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86Transformer\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "Transformer\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\u3002\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\uff0cTransformer\u7684\u6027\u80fd\u53d7\u5230\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u63a9\u7801\u5c06\u5f52\u7eb3\u504f\u7f6e\u5f15\u5165Vision Transformer\u3002", "result": "IBiT\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u6bd4\u4f20\u7edfTransformer\u66f4\u51c6\u786e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "IBiT\u662f\u4e00\u79cd\u6709\u6548\u7684Transformer\u67b6\u6784\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u5f52\u7eb3\u504f\u7f6e\u6765\u63d0\u9ad8\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.23045", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23045", "abs": "https://arxiv.org/abs/2509.23045", "authors": ["Zonghan Yang", "Shengjie Wang", "Kelin Fu", "Wenyang He", "Weimin Xiong", "Yibo Liu", "Yibo Miao", "Bofei Gao", "Yejie Wang", "Yingwei Ma", "Yanhao Li", "Yue Liu", "Zhenxing Hu", "Kaitai Zhang", "Shuyi Wang", "Huarong Chen", "Flood Sung", "Yang Liu", "Yang Gao", "Zhilin Yang", "Tianyu Liu"], "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents", "comment": "58 pages", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Agentless\u548cAgentic\u6846\u67b6\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5206\u4e3a\u591a\u8f6e\u4ea4\u4e92\u7684SWE-Agent\u6846\u67b6\u548c\u5355\u8f6e\u9a8c\u8bc1\u7684Agentless\u65b9\u6cd5\uff0c\u8fd9\u4e24\u79cd\u8303\u5f0f\u5e76\u975e\u4e92\u65a5\u7684\uff0cAgentless\u8bad\u7ec3\u53ef\u4ee5\u4e3aSWE-Agent\u63d0\u4f9b\u6280\u80fd\u5148\u9a8c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Kimi-Dev\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7Agentless\u8bad\u7ec3\u548cSFT\u9002\u5e94\uff0c\u5728SWE-bench\u548cSWE-Agents\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Kimi-Dev\u5728SWE-bench Verified\u4e0a\u8fbe\u5230\u4e8660.4\uff05\uff0c\u5728SWE-Agents\u4e0a\u8fbe\u5230\u4e8648.6\uff05 pass@1\uff0c\u4e0eClaude 3.5 Sonnet\u76f8\u5f53\u3002", "conclusion": "Agentless\u8bad\u7ec3\u63d0\u4f9b\u7684\u7ed3\u6784\u5316\u6280\u80fd\u5148\u9a8c\u53ef\u4ee5\u8fde\u63a5workflow\u548cagentic\u6846\u67b6\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u8f6c\u79fb\u7684\u7f16\u7801agents\u3002"}}
{"id": "2509.22913", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22913", "abs": "https://arxiv.org/abs/2509.22913", "authors": ["Jake S. Rhodes", "Adam G. Rustad", "Marshall S. Nielsen", "Morgan Chase McClellan", "Dallan Gardner", "Dawson Hedges"], "title": "Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders", "comment": "10 pages, 4 figures, 7 tables. Accepted at the MMAI workshop at ICDM,\n  2025", "summary": "Manifold alignment (MA) involves a set of techniques for learning shared\nrepresentations across domains, yet many traditional MA methods are incapable\nof performing out-of-sample extension, limiting their real-world applicability.\nWe propose a guided representation learning framework leveraging a\ngeometry-regularized twin autoencoder (AE) architecture to enhance MA while\nenabling generalization to unseen data. Our method enforces structured\ncross-modal mappings to maintain geometric fidelity in learned embeddings. By\nincorporating a pre-trained alignment model and a multitask learning\nformulation, we improve cross-domain generalization and representation\nrobustness while maintaining alignment fidelity. We evaluate our approach using\nseveral MA methods, showing improvements in embedding consistency, information\npreservation, and cross-domain transfer. Additionally, we apply our framework\nto Alzheimer's disease diagnosis, demonstrating its ability to integrate\nmulti-modal patient data and enhance predictive accuracy in cases limited to a\nsingle domain by leveraging insights from the multi-modal problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u6b63\u5219\u5316\u7684\u5b6a\u751f\u81ea\u7f16\u7801\u5668(AE)\u67b6\u6784\uff0c\u4ee5\u589e\u5f3aMA\uff0c\u540c\u65f6\u80fd\u591f\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u7684MA\u65b9\u6cd5\u65e0\u6cd5\u6267\u884c\u6837\u672c\u5916\u6269\u5c55\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528\u51e0\u4f55\u6b63\u5219\u5316\u7684\u5b6a\u751f\u81ea\u7f16\u7801\u5668(AE)\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u5bf9\u9f50\u6a21\u578b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u516c\u5f0f\uff0c\u6539\u8fdb\u8de8\u57df\u6cdb\u5316\u548c\u8868\u793a\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u9f50\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u591a\u4e2aMA\u65b9\u6cd5\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5728\u5d4c\u5165\u4e00\u81f4\u6027\u3001\u4fe1\u606f\u4fdd\u5b58\u548c\u8de8\u57df\u8fc1\u79fb\u65b9\u9762\u6709\u6240\u6539\u8fdb\u3002\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6574\u5408\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u6765\u81ea\u591a\u6a21\u6001\u95ee\u9898\u7684\u89c1\u89e3\uff0c\u63d0\u9ad8\u4ec5\u9650\u4e8e\u5355\u4e00\u9886\u57df\u75c5\u4f8b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86embedding\u4e00\u81f4\u6027\uff0c\u4fe1\u606f\u4fdd\u5b58\u548c\u8de8\u57df\u8fc1\u79fb"}}
{"id": "2509.23874", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23874", "abs": "https://arxiv.org/abs/2509.23874", "authors": ["Huike Zou", "Haiyang Yang", "Yindu Su", "Liyu Chen", "Chengbao Lian", "Qingheng Zhang", "Shuguang Han", "Jufeng Chen"], "title": "Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification", "comment": null, "summary": "Identifying attribute values from product profiles is a key task for\nimproving product search, recommendation, and business analytics on e-commerce\nplatforms, which we called Product Attribute Value Identification (PAVI) .\nHowever, existing PAVI methods face critical challenges, such as cascading\nerrors, inability to handle out-of-distribution (OOD) attribute values, and\nlack of generalization capability. To address these limitations, we introduce\nMulti-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the\nstrengths of retrieval, generation, and classification paradigms. MVP-RAG\ndefines PAVI as a retrieval-generation task, where the product title\ndescription serves as the query, and products and attribute values act as the\ncorpus. It first retrieves similar products of the same category and candidate\nattribute values, and then generates the standardized attribute values. The key\nadvantages of this work are: (1) the proposal of a multi-level retrieval\nscheme, with products and attribute values as distinct hierarchical levels in\nPAVI domain (2) attribute value generation of large language model to\nsignificantly alleviate the OOD problem and (3) its successful deployment in a\nreal-world industrial environment. Extensive experimental results demonstrate\nthat MVP-RAG performs better than the state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea7\u54c1\u5c5e\u6027\u503c\u8bc6\u522b\uff08PAVI\uff09\u65b9\u6cd5\uff0c\u79f0\u4e3a\u591a\u503c\u4ea7\u54c1\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08MVP-RAG\uff09\u3002", "motivation": "\u73b0\u6709\u7684PAVI\u65b9\u6cd5\u9762\u4e34\u7740\u8bf8\u5982\u7ea7\u8054\u9519\u8bef\u3001\u65e0\u6cd5\u5904\u7406\u8d85\u51fa\u5206\u5e03\uff08OOD\uff09\u7684\u5c5e\u6027\u503c\u4ee5\u53ca\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "MVP-RAG\u5c06PAVI\u5b9a\u4e49\u4e3a\u68c0\u7d22-\u751f\u6210\u4efb\u52a1\uff0c\u5176\u4e2d\u4ea7\u54c1\u6807\u9898\u63cf\u8ff0\u4f5c\u4e3a\u67e5\u8be2\uff0c\u4ea7\u54c1\u548c\u5c5e\u6027\u503c\u4f5c\u4e3a\u8bed\u6599\u5e93\u3002\u5b83\u9996\u5148\u68c0\u7d22\u76f8\u540c\u7c7b\u522b\u7684\u76f8\u4f3c\u4ea7\u54c1\u548c\u5019\u9009\u5c5e\u6027\u503c\uff0c\u7136\u540e\u751f\u6210\u6807\u51c6\u5316\u5c5e\u6027\u503c\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMVP-RAG\u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "MVP-RAG\u7ed3\u5408\u4e86\u68c0\u7d22\u3001\u751f\u6210\u548c\u5206\u7c7b\u8303\u5f0f\u7684\u4f18\u52bf\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709PAVI\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\u3002"}}
{"id": "2509.22824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22824", "abs": "https://arxiv.org/abs/2509.22824", "authors": ["Chi Ruan", "Dongfu Jiang", "Yubo Wang", "Wenhu Chen"], "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$\nof the generated critique aligns with the ground-truth judgment $c^*$. Building\non this point, we introduce \\textsc{Critique-Coder}, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Critique Reinforcement Learning (CRL)\uff0c\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u8bc4\u8bba\u6765\u8bad\u7ec3\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528RL\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u751f\u6210\u56de\u590d\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u8bc4\u4ef7\u6216\u53cd\u601d\u673a\u5236\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u660e\u786e\u5730\u6559LLM\u5982\u4f55\u8fdb\u884c\u8bc4\u4ef7\u662f\u6709\u76ca\u7684\u3002", "method": "\u6a21\u578b\u901a\u8fc7\u751f\u6210\u9488\u5bf9\u7ed9\u5b9a(\u95ee\u9898\uff0c\u89e3\u51b3\u65b9\u6848)\u5bf9\u7684\u8bc4\u8bba\u6765\u6267\u884c\u4efb\u52a1\u3002\u5956\u52b1\u5b8c\u5168\u53d6\u51b3\u4e8e\u751f\u6210\u7684\u8bc4\u8bba\u7684\u6700\u7ec8\u5224\u65ad\u6807\u7b7e\u662f\u5426\u4e0eground-truth\u5224\u65ad\u4e00\u81f4\u3002\u5f15\u5165\u4e86Critique-Coder\uff0c\u5b83\u901a\u8fc7\u5c0620%\u7684\u6807\u51c6RL\u6570\u636e\u66ff\u6362\u4e3aCRL\u6570\u636e\uff0c\u5728RL\u548cCRL\u7684\u6df7\u5408\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Critique-Coder\u5728\u6240\u6709\u8bc4\u4f30\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f7f\u7528RL\u7684\u57fa\u7ebf\u6a21\u578b\u3002Critique-Coder-8B\u5728LiveCodeBench (v5)\u4e0a\u53ef\u4ee5\u8fbe\u523060%\u4ee5\u4e0a\uff0c\u4f18\u4e8e\u5176\u4ed6\u63a8\u7406\u6a21\u578b\uff0c\u5982DeepCoder-14B\u548cGPT-o1\u3002\u5728BBEH\u6570\u636e\u96c6\u7684\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u8bc1\u660e\u4e86\u5176\u589e\u5f3a\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CRL\u53ef\u4ee5\u4f5c\u4e3aLLM\u63a8\u7406\u7684\u6807\u51c6RL\u7684\u826f\u597d\u8865\u5145\u3002"}}
{"id": "2509.22720", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22720", "abs": "https://arxiv.org/abs/2509.22720", "authors": ["Zezhong Fan", "Xiaohan Li", "Luyi Ma", "Kai Zhao", "Liang Peng", "Topojoy Biswas", "Evren Korpeoglu", "Kaushiki Nag", "Kannan Achan"], "title": "LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning", "comment": "NeurIPS 2025 Workshop on SPACE in Vision, Language, and Embodied AI", "summary": "Designing realistic multi-object scenes requires not only generating images,\nbut also planning spatial layouts that respect semantic relations and physical\nplausibility. On one hand, while recent advances in diffusion models have\nenabled high-quality image generation, they lack explicit spatial reasoning,\nleading to unrealistic object layouts. On the other hand, traditional spatial\nplanning methods in robotics emphasize geometric and relational consistency,\nbut they struggle to capture semantic richness in visual scenes. To bridge this\ngap, in this paper, we propose LayoutAgent, an agentic framework that unifies\nvision-language reasoning with compositional diffusion for layout generation.\nGiven multiple input images with target objects in them, our method first\nemploys visual-language model to preprocess the inputs through segmentation,\nobject size estimation, scene graph construction, and prompt rewriting. Then we\nleverage compositional diffusion-a method traditionally used in robotics-to\nsynthesize bounding boxes that respect object relations encoded in the scene\ngraph for spatial layouts. In the end, a foreground-conditioned image generator\ncomposes the complete scene by rendering the objects into the planned layout\nguided by designed prompts. Experiments demonstrate that LayoutAgent\noutperforms other state-of-the-art layout generation models in layout\ncoherence, spatial realism and aesthetic alignment.", "AI": {"tldr": "LayoutAgent\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548c\u7ec4\u5408\u6269\u6563\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u7a7a\u95f4\u5408\u7406\u6027\u7684\u591a\u5bf9\u8c61\u573a\u666f\u5e03\u5c40\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4f20\u7edf\u7684\u7a7a\u95f4\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u89c6\u89c9\u573a\u666f\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u5904\u7406\u8f93\u5165\uff0c\u6784\u5efa\u573a\u666f\u56fe\uff0c\u7136\u540e\u5229\u7528\u7ec4\u5408\u6269\u6563\u5408\u6210\u8fb9\u754c\u6846\uff0c\u6700\u540e\u901a\u8fc7\u524d\u666f\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u5668\u6e32\u67d3\u5b8c\u6574\u573a\u666f\u3002", "result": "LayoutAgent\u5728\u5e03\u5c40\u8fde\u8d2f\u6027\u3001\u7a7a\u95f4\u771f\u5b9e\u6027\u548c\u7f8e\u5b66\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u5e03\u5c40\u751f\u6210\u6a21\u578b\u3002", "conclusion": "LayoutAgent\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u5bf9\u8c61\u573a\u666f\u5e03\u5c40\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2509.23058", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23058", "abs": "https://arxiv.org/abs/2509.23058", "authors": ["Yikai Wang", "Xiaocheng Li", "Guanting Chen"], "title": "Risk Profiling and Modulation for LLMs", "comment": null, "summary": "Large language models (LLMs) are increasingly used for decision-making tasks\nunder uncertainty; however, their risk profiles and how they are influenced by\nprompting and alignment methods remain underexplored. Existing studies have\nprimarily examined personality prompting or multi-agent interactions, leaving\nopen the question of how post-training influences the risk behavior of LLMs. In\nthis work, we propose a new pipeline for eliciting, steering, and modulating\nLLMs' risk profiles, drawing on tools from behavioral economics and finance.\nUsing utility-theoretic models, we compare pre-trained, instruction-tuned, and\nRLHF-aligned LLMs, and find that while instruction-tuned models exhibit\nbehaviors consistent with some standard utility formulations, pre-trained and\nRLHF-aligned models deviate more from any utility models fitted. We further\nevaluate modulation strategies, including prompt engineering, in-context\nlearning, and post-training, and show that post-training provides the most\nstable and effective modulation of risk preference. Our findings provide\ninsights into the risk profiles of different classes and stages of LLMs and\ndemonstrate how post-training modulates these profiles, laying the groundwork\nfor future research on behavioral alignment and risk-aware LLM design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u98ce\u9669\u504f\u597d\uff0c\u5e76\u7814\u7a76\u4e86\u63d0\u793a\u548c\u5bf9\u9f50\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u8fd9\u4e9b\u98ce\u9669\u504f\u597d\u3002", "motivation": "\u76ee\u524d\u5bf9LLM\u7684\u98ce\u9669\u884c\u4e3a\u53ca\u5176\u53d7\u63d0\u793a\u548c\u5bf9\u9f50\u65b9\u6cd5\u5f71\u54cd\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u540e\u8bad\u7ec3\u5982\u4f55\u5f71\u54cdLLM\u98ce\u9669\u884c\u4e3a\u7684\u4e86\u89e3\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d41\u7a0b\uff0c\u5229\u7528\u884c\u4e3a\u7ecf\u6d4e\u5b66\u548c\u91d1\u878d\u5b66\u7684\u5de5\u5177\u6765\u5f15\u51fa\u3001\u5f15\u5bfc\u548c\u8c03\u8282LLM\u7684\u98ce\u9669\u504f\u597d\u3002\u4f7f\u7528\u6548\u7528\u7406\u8bba\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u8c03\u6574\u548cRLHF\u5bf9\u9f50\u7684LLM\u3002", "result": "\u6307\u4ee4\u8c03\u6574\u6a21\u578b\u8868\u73b0\u51fa\u4e0e\u67d0\u4e9b\u6807\u51c6\u6548\u7528\u516c\u5f0f\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u800c\u9884\u8bad\u7ec3\u548cRLHF\u5bf9\u9f50\u7684\u6a21\u578b\u5219\u504f\u79bb\u66f4\u591a\u3002\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6700\u7a33\u5b9a\u548c\u6709\u6548\u7684\u98ce\u9669\u504f\u597d\u8c03\u8282\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5165\u4e86\u89e3\u4e86\u4e0d\u540c\u7c7b\u522b\u548c\u9636\u6bb5\u7684LLM\u7684\u98ce\u9669\u6982\u51b5\uff0c\u5e76\u5c55\u793a\u4e86\u540e\u8bad\u7ec3\u5982\u4f55\u8c03\u8282\u8fd9\u4e9b\u6982\u51b5\uff0c\u4e3a\u672a\u6765\u884c\u4e3a\u5bf9\u9f50\u548c\u98ce\u9669\u610f\u8bc6LLM\u8bbe\u8ba1\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.22921", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22921", "abs": "https://arxiv.org/abs/2509.22921", "authors": ["Matthieu Zimmer", "Xiaotong Ji", "Tu Nguyen", "Haitham Bou Ammar"], "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective", "comment": null, "summary": "We introduce a novel approach to large language model (LLM) distillation by\nformulating it as a constrained reinforcement learning problem. While recent\nwork has begun exploring the integration of task-specific rewards into\ndistillation processes, existing methods typically rely on ad-hoc reward\nweighting. We propose a principled optimization framework that maximizes\ntask-specific rewards while constraining the divergence from the teacher model\nto remain below a specified threshold. Our approach adapts constrained state\naugmented reinforcement learning to the distillation setting, introducing a\nmodified reward function that maintains theoretical guarantees of constraint\nsatisfaction without requiring state augmentation or teacher model access\nduring deployment and without the computational overhead of the dual Lagrangian\nmethods. Through extensive experiments on mathematical reasoning tasks, we\ndemonstrate that our method achieves better constraint satisfaction rates and\nbetter reasoning compared to the soft Lagrangian relaxation baselines while\nmaintaining competitive task performance. Our framework provides a\ntheoretically grounded and practically efficient solution for reward-aware\ndistillation in resource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u5176\u6784\u5efa\u4e3a\u4e00\u4e2a\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u5956\u52b1\u6743\u91cd\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u6709\u539f\u5219\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "\u5c06\u7ea6\u675f\u72b6\u6001\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u84b8\u998f\u73af\u5883\uff0c\u5f15\u5165\u6539\u8fdb\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u65e0\u9700\u72b6\u6001\u589e\u5f3a\u6216\u5728\u90e8\u7f72\u671f\u95f4\u8bbf\u95ee\u6559\u5e08\u6a21\u578b\uff0c\u4e5f\u65e0\u9700\u53cc\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u8f6f\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u7387\u548c\u66f4\u597d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5956\u52b1\u611f\u77e5\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u4e14\u5b9e\u9645\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24424", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24424", "abs": "https://arxiv.org/abs/2509.24424", "authors": ["Mingshi Xu", "Haoren Zhu", "Wilfred Siu Hung Ng"], "title": "Multi-Item-Query Attention for Stable Sequential Recommendation", "comment": null, "summary": "The inherent instability and noise in user interaction data challenge\nsequential recommendation systems. Prevailing masked attention models, relying\non a single query from the most recent item, are sensitive to this noise,\nreducing prediction reliability. We propose the Multi-Item-Query attention\nmechanism (MIQ-Attn) to enhance model stability and accuracy. MIQ-Attn\nconstructs multiple diverse query vectors from user interactions, effectively\nmitigating noise and improving consistency. It is designed for easy adoption as\na drop-in replacement for existing single-query attention. Experiments show\nMIQ-Attn significantly improves performance on benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9879\u76ee\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\uff08MIQ-Attn\uff09\uff0c\u4ee5\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7528\u6237\u4e92\u52a8\u6570\u636e\u4e2d\u7684\u56fa\u6709\u4e0d\u7a33\u5b9a\u6027\u548c\u566a\u58f0\u5bf9\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\u63d0\u51fa\u4e86\u6311\u6218\u3002\u73b0\u6709\u7684\u63a9\u7801\u6ce8\u610f\u529b\u6a21\u578b\u4f9d\u8d56\u4e8e\u6700\u8fd1\u9879\u76ee\u7684\u5355\u4e00\u67e5\u8be2\uff0c\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u964d\u4f4e\u4e86\u9884\u6d4b\u53ef\u9760\u6027\u3002", "method": "MIQ-Attn\u4ece\u7528\u6237\u4e92\u52a8\u4e2d\u6784\u5efa\u591a\u4e2a\u4e0d\u540c\u7684\u67e5\u8be2\u5411\u91cf\uff0c\u6709\u6548\u5730\u51cf\u8f7b\u566a\u58f0\u5e76\u63d0\u9ad8\u4e00\u81f4\u6027\u3002\u5b83\u88ab\u8bbe\u8ba1\u4e3a\u53ef\u4ee5\u8f7b\u677e\u66ff\u4ee3\u73b0\u6709\u7684\u5355\u67e5\u8be2\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMIQ-Attn \u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "conclusion": "MIQ-Attn \u53ef\u4ee5\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.22830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22830", "abs": "https://arxiv.org/abs/2509.22830", "authors": ["Hwan Chang", "Yonghyun Jun", "Hwanhee Lee"], "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "comment": null, "summary": "The growing deployment of large language model (LLM) based agents that\ninteract with external environments has created new attack surfaces for\nadversarial manipulation. One major threat is indirect prompt injection, where\nattackers embed malicious instructions in external environment output, causing\nagents to interpret and execute them as if they were legitimate prompts. While\nprevious research has focused primarily on plain-text injection attacks, we\nfind a significant yet underexplored vulnerability: LLMs' dependence on\nstructured chat templates and their susceptibility to contextual manipulation\nthrough persuasive multi-turn dialogues. To this end, we introduce ChatInject,\nan attack that formats malicious payloads to mimic native chat templates,\nthereby exploiting the model's inherent instruction-following tendencies.\nBuilding on this foundation, we develop a persuasion-driven Multi-turn variant\nthat primes the agent across conversational turns to accept and execute\notherwise suspicious actions. Through comprehensive experiments across frontier\nLLMs, we demonstrate three critical findings: (1) ChatInject achieves\nsignificantly higher average attack success rates than traditional prompt\ninjection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%\nto 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong\nperformance at average 52.33% success rate on InjecAgent, (2)\nchat-template-based payloads demonstrate strong transferability across models\nand remain effective even against closed-source LLMs, despite their unknown\ntemplate structures, and (3) existing prompt-based defenses are largely\nineffective against this attack approach, especially against Multi-turn\nvariants. These findings highlight vulnerabilities in current agent systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u9488\u5bf9\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u79f0\u4e3aChatInject\uff0c\u8be5\u65b9\u6cd5\u5229\u7528LLM\u5bf9\u7ed3\u6784\u5316\u804a\u5929\u6a21\u677f\u7684\u4f9d\u8d56\u6027\u4ee5\u53ca\u901a\u8fc7\u6709\u8bf4\u670d\u529b\u7684\u591a\u8f6e\u5bf9\u8bdd\u8fdb\u884c\u4e0a\u4e0b\u6587\u64cd\u7eb5\u7684\u8106\u5f31\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cChatInject\u6bd4\u4f20\u7edf\u7684prompt\u6ce8\u5165\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u57fa\u4e8eprompt\u7684\u9632\u5fa1\u65b9\u6cd5\u5bf9\u8fd9\u79cd\u653b\u51fb\u65b9\u6cd5\u57fa\u672c\u65e0\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u4e0e\u5916\u90e8\u73af\u5883\u4ea4\u4e92\uff0c\u9762\u4e34\u7740\u5bf9\u6297\u6027\u64cd\u7eb5\u7684\u65b0\u653b\u51fb\u9762\u3002\u4e3b\u8981\u7684\u5a01\u80c1\u662f\u95f4\u63a5prompt\u6ce8\u5165\uff0c\u653b\u51fb\u8005\u5c06\u6076\u610f\u6307\u4ee4\u5d4c\u5165\u5230\u5916\u90e8\u73af\u5883\u8f93\u51fa\u4e2d\uff0c\u5bfc\u81f4\u4ee3\u7406\u5c06\u5176\u89e3\u91ca\u5e76\u6267\u884c\u4e3a\u5408\u6cd5\u7684\u63d0\u793a\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7eaf\u6587\u672c\u6ce8\u5165\u653b\u51fb\u4e0a\uff0c\u800cLLM\u5bf9\u7ed3\u6784\u5316\u804a\u5929\u6a21\u677f\u7684\u4f9d\u8d56\u6027\u4ee5\u53ca\u901a\u8fc7\u6709\u8bf4\u670d\u529b\u7684\u591a\u8f6e\u5bf9\u8bdd\u8fdb\u884c\u4e0a\u4e0b\u6587\u64cd\u7eb5\u7684\u8106\u5f31\u6027\u662f\u88ab\u5ffd\u89c6\u7684\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChatInject\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u683c\u5f0f\u5316\u6076\u610fpayload\u4ee5\u6a21\u4eff\u539f\u751f\u804a\u5929\u6a21\u677f\uff0c\u4ece\u800c\u5229\u7528\u6a21\u578b\u56fa\u6709\u7684\u6307\u4ee4\u9075\u5faa\u503e\u5411\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8bf4\u670d\u9a71\u52a8\u7684\u591a\u8f6e\u53d8\u4f53\uff0c\u53ef\u4ee5\u5728\u591a\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u4e2d\u542f\u52a8\u4ee3\u7406\uff0c\u4ee5\u63a5\u53d7\u548c\u6267\u884c\u539f\u672c\u53ef\u7591\u7684\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a(1) ChatInject\u7684\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u8fdc\u9ad8\u4e8e\u4f20\u7edf\u7684prompt\u6ce8\u5165\u65b9\u6cd5\uff0c\u5728AgentDojo\u4e0a\u4ece5.18%\u63d0\u9ad8\u523032.05%\uff0c\u5728InjecAgent\u4e0a\u4ece15.13%\u63d0\u9ad8\u523045.90%\uff0c\u5176\u4e2d\u591a\u8f6e\u5bf9\u8bdd\u5728InjecAgent\u4e0a\u7684\u5e73\u5747\u6210\u529f\u7387\u4e3a52.33%\uff1b(2) \u57fa\u4e8e\u804a\u5929\u6a21\u677f\u7684payload\u5728\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u53ef\u79fb\u690d\u6027\uff0c\u5373\u4f7f\u5bf9\u4e8e\u95ed\u6e90LLM\u4e5f\u4ecd\u7136\u6709\u6548\uff1b(3) \u73b0\u6709\u7684\u57fa\u4e8eprompt\u7684\u9632\u5fa1\u65b9\u6cd5\u5bf9\u8fd9\u79cd\u653b\u51fb\u65b9\u6cd5\u57fa\u672c\u65e0\u6548\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u591a\u8f6e\u53d8\u4f53\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f53\u524d\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u6f0f\u6d1e\u3002"}}
{"id": "2509.22737", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22737", "abs": "https://arxiv.org/abs/2509.22737", "authors": ["Jie Cai", "Kangning Yang", "Lan Fu", "Jiaming Ding", "Jinlong Li", "Huiming Sun", "Daitao Xing", "Jinglin Shen", "Zibo Meng"], "title": "CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models", "comment": null, "summary": "We introduce CompareBench, a benchmark for evaluating visual comparison\nreasoning in vision-language models (VLMs), a fundamental yet understudied\nskill. CompareBench consists of 1000 QA pairs across four tasks: quantity\n(600), temporal (100), geometric (200), and spatial (100). It is derived from\ntwo auxiliary datasets that we constructed: TallyBench (2000 counting images\nwith QA) and HistCaps (515 historical images with bilingual captions). We\nevaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source\nmodels (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but\nalso reveal critical limitations: even the strongest models consistently fail\nat temporal ordering and spatial relations, and they often make mistakes in\nbasic counting and geometric comparisons that are trivial for humans. These\nfindings demonstrate that visual comparison remains a systematic blind spot for\ncurrent VLMs. By providing controlled, diverse, and diagnostic evaluation,\nCompareBench establishes a foundation for advancing more reliable multimodal\nreasoning.", "AI": {"tldr": "CompareBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6bd4\u8f83\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u6bd4\u8f83\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1000\u4e2aQA\u5bf9\u7684CompareBench\u57fa\u51c6\uff0c\u6db5\u76d6\u6570\u91cf\u3001\u65f6\u95f4\u3001\u51e0\u4f55\u548c\u7a7a\u95f4\u56db\u4e2a\u4efb\u52a1\u3002\u8fd8\u6784\u5efa\u4e86\u4e24\u4e2a\u8f85\u52a9\u6570\u636e\u96c6TallyBench\u548cHistCaps\u3002", "result": "\u8bc4\u4f30\u4e86\u95ed\u6e90API\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u65f6\u95f4\u6392\u5e8f\u548c\u7a7a\u95f4\u5173\u7cfb\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5728\u8ba1\u6570\u548c\u51e0\u4f55\u6bd4\u8f83\u4e0a\u4e5f\u5b58\u5728\u9519\u8bef\u3002", "conclusion": "\u89c6\u89c9\u6bd4\u8f83\u4ecd\u7136\u662f\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u76f2\u70b9\u3002CompareBench\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.23102", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23102", "abs": "https://arxiv.org/abs/2509.23102", "authors": ["Fang Wu", "Xu Huang", "Weihao Xuan", "Zhiwei Zhang", "Yijia Xiao", "Guancheng Wan", "Xiaomin Li", "Bing Hu", "Peng Xia", "Jure Leskovec", "Yejin Choi"], "title": "Multiplayer Nash Preference Optimization", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard\nparadigm for aligning large language models (LLMs) with human preferences.\nHowever, reward-based methods built on the Bradley-Terry assumption struggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise to Nash learning from human feedback (NLHF). While this\nperspective has inspired algorithms such as INPO, ONPO, and EGPO with strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduce Multiplayer Nash Preference Optimization (MNPO), a novel framework\nthat generalizes NLHF to the multiplayer regime. It formulates alignment as an\n$n$-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-defined Nash equilibria in multiplayer settings and extends the concept of\nduality gap to quantify approximation quality. We demonstrate that MNPO\ninherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show that MNPO consistently\noutperforms existing NLHF baselines on instruction-following benchmarks,\nachieving superior alignment quality under heterogeneous annotator conditions\nand mixed-policy evaluation scenarios. Together, these results establish MNPO\nas a principled and scalable framework for aligning LLMs with complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u540d\u4e3a\u591a\u4eba\u7eb3\u4ec0\u504f\u597d\u4f18\u5316\uff08MNPO\uff09\uff0c\u7528\u4e8e\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u590d\u6742\u7684\u3001\u975e\u4f20\u9012\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002MNPO\u5c06\u5bf9\u9f50\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2an\u4eba\u535a\u5f08\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u5f02\u6784\u6807\u6ce8\u8005\u6761\u4ef6\u548c\u6df7\u5408\u7b56\u7565\u8bc4\u4f30\u573a\u666f\u4e0b\uff0cMNPO\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684NLHF\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5956\u52b1\u7684\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u504f\u597d\u7684\u975e\u4f20\u9012\u6027\u548c\u5f02\u8d28\u6027\uff0c\u800c\u73b0\u6709\u7684\u7eb3\u4ec0\u5b66\u4e60\u65b9\u6cd5\u53c8\u5c40\u9650\u4e8e\u53cc\u4eba\u4e92\u52a8\uff0c\u5b58\u5728\u5355\u4e00\u5bf9\u624b\u504f\u5dee\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u504f\u597d\u7ed3\u6784\u7684\u5b8c\u6574\u590d\u6742\u6027\u3002", "method": "\u8bba\u6587\u5c06\u5bf9\u9f50\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2an\u4eba\u535a\u5f08\uff0c\u5176\u4e2d\u6bcf\u4e2a\u7b56\u7565\u4e0e\u4e00\u7ec4\u5bf9\u624b\u7ade\u4e89\uff0c\u540c\u65f6\u5411\u53c2\u8003\u6a21\u578b\u6b63\u5219\u5316\u3002\u8be5\u6846\u67b6\u5728\u591a\u4eba\u73af\u5883\u4e2d\u5efa\u7acb\u4e86\u660e\u786e\u5b9a\u4e49\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u5c06\u5bf9\u5076\u95f4\u9699\u7684\u6982\u5ff5\u6269\u5c55\u5230\u91cf\u5316\u8fd1\u4f3c\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMNPO\u5728\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684NLHF\u57fa\u7ebf\uff0c\u5728\u5f02\u6784\u6807\u6ce8\u8005\u6761\u4ef6\u548c\u6df7\u5408\u7b56\u7565\u8bc4\u4f30\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5bf9\u9f50\u8d28\u91cf\u3002", "conclusion": "MNPO\u662f\u4e00\u4e2a\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06LLM\u4e0e\u590d\u6742\u7684\u3001\u975e\u4f20\u9012\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002"}}
{"id": "2509.22931", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22931", "abs": "https://arxiv.org/abs/2509.22931", "authors": ["Shreyas Gokhale"], "title": "MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints", "comment": "16 pages, 7 figures", "summary": "Learning high-quality, robust, efficient, and disentangled representations is\na central challenge in artificial intelligence (AI). Deep metric learning\nframeworks tackle this challenge primarily using architectural and optimization\nconstraints. Here, we introduce a third approach that instead relies on\n$\\textit{functional}$ constraints. Specifically, we present MonoCon, a simple\nframework that uses a small monotonic multi-layer perceptron (MLP) head\nattached to any pre-trained encoder. Due to co-adaptation between encoder and\nhead guided by contrastive loss and monotonicity constraints, MonoCon learns\nrobust, disentangled, and highly compact embeddings at a practically negligible\nperformance cost. On the CIFAR-100 image classification task, MonoCon yields\nrepresentations that are nearly 9x more compact and 1.5x more robust than the\nfine-tuned encoder baseline, while retaining 99\\% of the baseline's 5-NN\nclassification accuracy. We also report a 3.4x more compact and 1.4x more\nrobust representation on an SNLI sentence similarity task for a marginal\nreduction in the STSb score, establishing MonoCon as a general domain-agnostic\nframework. Crucially, these robust, ultra-compact representations learned via\nfunctional constraints offer a unified solution to critical challenges in\ndisparate contexts ranging from edge computing to cloud-scale retrieval.", "AI": {"tldr": "MonoCon: \u4f7f\u7528\u5355\u8c03 MLP \u5934\u5b66\u4e60\u9ad8\u8d28\u91cf\u3001\u9c81\u68d2\u3001\u9ad8\u6548\u548c\u89e3\u8026\u7684\u8868\u793a\u3002", "motivation": "\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\uff0c\u5b66\u4e60\u9ad8\u8d28\u91cf\u3001\u9c81\u68d2\u3001\u9ad8\u6548\u548c\u89e3\u8026\u7684\u8868\u793a\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5ea6\u91cf\u5b66\u4e60\u6846\u67b6\u4e3b\u8981\u4f7f\u7528\u67b6\u6784\u548c\u4f18\u5316\u7ea6\u675f\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u5f15\u5165 MonoCon\uff0c\u4e00\u4e2a\u7b80\u5355\u7684\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u9644\u52a0\u5230\u4efb\u4f55\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5c0f\u578b\u5355\u8c03\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u5934\u3002\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u548c\u5355\u8c03\u6027\u7ea6\u675f\u5f15\u5bfc\u7f16\u7801\u5668\u548c\u5934\u4e4b\u95f4\u7684\u534f\u540c\u9002\u5e94\uff0cMonoCon \u5b66\u4e60\u9c81\u68d2\u3001\u89e3\u8026\u548c\u9ad8\u5ea6\u7d27\u51d1\u7684\u5d4c\u5165\u3002", "result": "\u5728 CIFAR-100 \u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cMonoCon \u4ea7\u751f\u7684\u8868\u793a\u6bd4\u5fae\u8c03\u7f16\u7801\u5668\u57fa\u7ebf\u7d27\u51d1\u8fd1 9 \u500d\uff0c\u9c81\u68d2\u6027\u9ad8 1.5 \u500d\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u57fa\u7ebf 99% \u7684 5-NN \u5206\u7c7b\u7cbe\u5ea6\u3002\u5728 SNLI \u53e5\u5b50\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\uff0c\u62a5\u544a\u4e86\u7d27\u51d1 3.4 \u500d\u3001\u9c81\u68d2\u6027\u9ad8 1.4 \u500d\u7684\u8868\u793a\uff0c\u800c STSb \u5206\u6570\u7565\u6709\u4e0b\u964d\uff0c\u8fd9\u8bc1\u660e MonoCon \u662f\u4e00\u4e2a\u901a\u7528\u7684\u9886\u57df\u65e0\u5173\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7\u529f\u80fd\u7ea6\u675f\u5b66\u4e60\u7684\u8fd9\u4e9b\u9c81\u68d2\u7684\u8d85\u7d27\u51d1\u8868\u793a\u4e3a\u4ece\u8fb9\u7f18\u8ba1\u7b97\u5230\u4e91\u89c4\u6a21\u68c0\u7d22\u7b49\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24632", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.24632", "abs": "https://arxiv.org/abs/2509.24632", "authors": ["Zan Li", "Jiahui Chen", "Yuan Chai", "Xiaoze Jiang", "Xiaohua Qi", "Zhiheng Qin", "Runbin Zhou", "Shun Zuo", "Guangchao Hao", "Kefeng Wang", "Jingshan Lv", "Yupeng Huang", "Xiao Liang", "Han Li"], "title": "UniDex: Rethinking Search Inverted Indexing with Unified Semantic Modeling", "comment": "11 pages, 6 figures and 5 tables", "summary": "Inverted indexing has traditionally been a cornerstone of modern search\nsystems, leveraging exact term matches to determine relevance between queries\nand documents. However, this term-based approach often emphasizes surface-level\ntoken overlap, limiting the system's generalization capabilities and retrieval\neffectiveness. To address these challenges, we propose UniDex, a novel\nmodel-based method that employs unified semantic modeling to revolutionize\ninverted indexing. UniDex replaces complex manual designs with a streamlined\narchitecture, enhancing semantic generalization while reducing maintenance\noverhead. Our approach involves two key components: UniTouch, which maps\nqueries and documents into semantic IDs for improved retrieval, and UniRank,\nwhich employs semantic matching to rank results effectively. Through\nlarge-scale industrial datasets and real-world online traffic assessments, we\ndemonstrate that UniDex significantly improves retrieval capabilities, marking\na paradigm shift from term-based to model-based indexing. Our deployment within\nKuaishou's short-video search systems further validates UniDex's practical\neffectiveness, serving hundreds of millions of active users efficiently.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5012\u6392\u7d22\u5f15\u65b9\u6cd5\uff0c\u79f0\u4e3a UniDex\uff0c\u5b83\u4f7f\u7528\u7edf\u4e00\u7684\u8bed\u4e49\u5efa\u6a21\u6765\u6539\u8fdb\u68c0\u7d22\u3002", "motivation": "\u4f20\u7edf\u7684\u5012\u6392\u7d22\u5f15\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u672f\u8bed\u5339\u914d\uff0c\u8fd9\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u548c\u68c0\u7d22\u6548\u679c\u3002", "method": "UniDex \u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aUniTouch\uff0c\u5b83\u5c06\u67e5\u8be2\u548c\u6587\u6863\u6620\u5c04\u5230\u8bed\u4e49 ID \u4ee5\u6539\u8fdb\u68c0\u7d22\uff1bUniRank\uff0c\u5b83\u91c7\u7528\u8bed\u4e49\u5339\u914d\u6765\u6709\u6548\u5730\u5bf9\u7ed3\u679c\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u5728\u5927\u578b\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u5728\u7ebf\u6d41\u91cf\u8bc4\u4f30\u4e2d\uff0cUniDex \u663e\u7740\u63d0\u9ad8\u4e86\u68c0\u7d22\u80fd\u529b\u3002", "conclusion": "UniDex \u4ee3\u8868\u4e86\u4ece\u57fa\u4e8e\u672f\u8bed\u7684\u7d22\u5f15\u5230\u57fa\u4e8e\u6a21\u578b\u7684\u7d22\u5f15\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5728\u5feb\u624b\u7684\u77ed\u89c6\u9891\u641c\u7d22\u7cfb\u7edf\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u53ef\u6709\u6548\u5730\u4e3a\u6570\u4ebf\u6d3b\u8dc3\u7528\u6237\u63d0\u4f9b\u670d\u52a1\u3002"}}
{"id": "2509.22845", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3.3; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.22845", "abs": "https://arxiv.org/abs/2509.22845", "authors": ["Kai Hua", "Zhiyuan Feng", "Chongyang Tao", "Rui Yan", "Lu Zhang"], "title": "Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems", "comment": "10 pages, 4 figures, accepted by CIKM 2020", "summary": "Recently, knowledge-grounded conversations in the open domain gain great\nattention from researchers. Existing works on retrieval-based dialogue systems\nhave paid tremendous efforts to utilize neural networks to build a matching\nmodel, where all of the context and knowledge contents are used to match the\nresponse candidate with various representation methods. Actually, different\nparts of the context and knowledge are differentially important for recognizing\nthe proper response candidate, as many utterances are useless due to the topic\nshift. Those excessive useless information in the context and knowledge can\ninfluence the matching process and leads to inferior performance. To address\nthis problem, we propose a multi-turn \\textbf{R}esponse \\textbf{S}election\n\\textbf{M}odel that can \\textbf{D}etect the relevant parts of the\n\\textbf{C}ontext and \\textbf{K}nowledge collection (\\textbf{RSM-DCK}). Our\nmodel first uses the recent context as a query to pre-select relevant parts of\nthe context and knowledge collection at the word-level and utterance-level\nsemantics. Further, the response candidate interacts with the selected context\nand knowledge collection respectively. In the end, The fused representation of\nthe context and response candidate is utilized to post-select the relevant\nparts of the knowledge collection more confidently for matching. We test our\nproposed model on two benchmark datasets. Evaluation results indicate that our\nmodel achieves better performance than the existing methods, and can\neffectively detect the relevant context and knowledge for response selection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u56de\u590d\u9009\u62e9\u6a21\u578b\uff0c\u53ef\u4ee5\u68c0\u6d4b\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u96c6\u5408\u7684\u76f8\u5173\u90e8\u5206\uff0c\u4ece\u800c\u63d0\u9ad8\u56de\u590d\u9009\u62e9\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u5339\u914d\u6a21\u578b\u65f6\uff0c\u4f7f\u7528\u4e86\u6240\u6709\u7684\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u5185\u5bb9\u6765\u5339\u914d\u56de\u590d\u5019\u9009\uff0c\u4f46\u5b9e\u9645\u4e0a\uff0c\u7531\u4e8e\u4e3b\u9898\u8f6c\u79fb\uff0c\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u7684\u4e0d\u540c\u90e8\u5206\u5bf9\u4e8e\u8bc6\u522b\u5408\u9002\u7684\u56de\u590d\u5019\u9009\u7684\u91cd\u8981\u6027\u662f\u4e0d\u540c\u7684\uff0c\u8fc7\u591a\u7684\u65e0\u7528\u4fe1\u606f\u4f1a\u5f71\u54cd\u5339\u914d\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u8be5\u6a21\u578b\u9996\u5148\u4f7f\u7528\u6700\u8fd1\u7684\u4e0a\u4e0b\u6587\u4f5c\u4e3a\u67e5\u8be2\uff0c\u4ee5\u8bcd\u7ea7\u548c\u8bed\u53e5\u7ea7\u8bed\u4e49\u9884\u9009\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u96c6\u5408\u7684\u76f8\u5173\u90e8\u5206\u3002\u7136\u540e\uff0c\u56de\u590d\u5019\u9009\u5206\u522b\u4e0e\u6240\u9009\u7684\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u96c6\u5408\u8fdb\u884c\u4ea4\u4e92\u3002\u6700\u540e\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u548c\u56de\u590d\u5019\u9009\u7684\u878d\u5408\u8868\u793a\uff0c\u66f4\u81ea\u4fe1\u5730\u5bf9\u77e5\u8bc6\u96c6\u5408\u7684\u76f8\u5173\u90e8\u5206\u8fdb\u884c\u540e\u9009\u62e9\u4ee5\u8fdb\u884c\u5339\u914d\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u68c0\u6d4b\u76f8\u5173\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u4ee5\u8fdb\u884c\u56de\u590d\u9009\u62e9\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u76f8\u5173\u4e0a\u4e0b\u6587\u548c\u77e5\u8bc6\u4ee5\u8fdb\u884c\u56de\u590d\u9009\u62e9\uff0c\u4ece\u800c\u63d0\u9ad8\u56de\u590d\u9009\u62e9\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22761", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22761", "abs": "https://arxiv.org/abs/2509.22761", "authors": ["Yapeng Mi", "Hengli Li", "Yanpeng Zhao", "Chenxi Li", "Huimin Wu", "Xiaojian Ma", "Song-Chun Zhu", "Ying Nian Wu", "Qing Li"], "title": "MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning", "comment": "21 pages,13 figures,7 tables", "summary": "Reasoning-augmented machine learning systems have shown improved performance\nin various domains, including image generation. However, existing\nreasoning-based methods for image generation either restrict reasoning to a\nsingle modality (image or text) or rely on high-quality reasoning data for\nfine-tuning. To tackle these limitations, we propose MILR, a test-time method\nthat jointly reasons over image and text in a unified latent vector space.\nReasoning in MILR is performed by searching through vector representations of\ndiscrete image and text tokens. Practically, this is implemented via the policy\ngradient method, guided by an image quality critic. We instantiate MILR within\nthe unified multimodal understanding and generation (MUG) framework that\nnatively supports language reasoning before image synthesis and thus\nfacilitates cross-modal reasoning. The intermediate model outputs, which are to\nbe optimized, serve as the unified latent space, enabling MILR to operate\nentirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE,\nachieving state-of-the-art results on all benchmarks. Notably, on\nknowledge-intensive WISE, MILR attains an overall score of 0.63, improving over\nthe baseline by 80%. Our further analysis indicates that joint reasoning in the\nunified latent space is the key to its strong performance. Moreover, our\nqualitative studies reveal MILR's non-trivial ability in temporal and cultural\nreasoning, highlighting the efficacy of our reasoning method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMILR\u7684\u6d4b\u8bd5\u65f6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u7edf\u4e00\u7684\u6f5c\u5728\u5411\u91cf\u7a7a\u95f4\u4e2d\u5171\u540c\u63a8\u7406\u56fe\u50cf\u548c\u6587\u672c\uff0c\u4ee5\u6539\u8fdb\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63a8\u7406\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u5c06\u63a8\u7406\u9650\u5236\u5728\u5355\u4e00\u6a21\u6001\uff08\u56fe\u50cf\u6216\u6587\u672c\uff09\u4e2d\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "method": "\u901a\u8fc7\u5728\u79bb\u6563\u56fe\u50cf\u548c\u6587\u672ctokens\u7684\u5411\u91cf\u8868\u793a\u4e2d\u641c\u7d22\u6765\u6267\u884c\u63a8\u7406\u3002\u5b9e\u9645\u4e0a\uff0c\u8fd9\u662f\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b9e\u73b0\u7684\uff0c\u7531\u56fe\u50cf\u8d28\u91cf\u8bc4\u8bba\u5bb6\u6307\u5bfc\u3002", "result": "\u5728GenEval\u3001T2I-CompBench\u548cWISE\u4e0a\u8bc4\u4f30MILR\uff0c\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5728\u77e5\u8bc6\u5bc6\u96c6\u578bWISE\u4e0a\uff0cMILR\u7684\u603b\u5206\u8fbe\u5230\u4e860.63\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8680%\u3002", "conclusion": "\u5728\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8054\u5408\u63a8\u7406\u662f\u5176\u5f3a\u5927\u6027\u80fd\u7684\u5173\u952e\u3002"}}
{"id": "2509.23108", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.23108", "abs": "https://arxiv.org/abs/2509.23108", "authors": ["Morgan McCarty", "Jorge Morales"], "title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models", "comment": "30 pages,15 figures", "summary": "This study offers a novel approach for benchmarking complex cognitive\nbehavior in artificial systems. Almost universally, Large Language Models\n(LLMs) perform best on tasks which may be included in their training data and\ncan be accomplished solely using natural language, limiting our understanding\nof their emergent sophisticated cognitive capacities. In this work, we created\ndozens of novel items of a classic mental imagery task from cognitive\npsychology. A task which, traditionally, cognitive psychologists have argued is\nsolvable exclusively via visual mental imagery (i.e., language alone would be\ninsufficient). LLMs are perfect for testing this hypothesis. First, we tested\nseveral state-of-the-art LLMs by giving text-only models written instructions\nand asking them to report the resulting object after performing the\ntransformations in the aforementioned task. Then, we created a baseline by\ntesting 100 human subjects in exactly the same task. We found that the best\nLLMs performed significantly above average human performance. Finally, we\ntested reasoning models set to different levels of reasoning and found the\nstrongest performance when models allocate greater amounts of reasoning tokens.\nThese results provide evidence that the best LLMs may have the capability to\ncomplete imagery-dependent tasks despite the non-pictorial nature of their\narchitectures. Our study not only demonstrates an emergent cognitive capacity\nin LLMs while performing a novel task, but it also provides the field with a\nnew task that leaves lots of room for improvement in otherwise already highly\ncapable models. Finally, our findings reignite the debate over the formats of\nrepresentation of visual imagery in humans, suggesting that propositional\nreasoning (or at least non-imagistic reasoning) may be sufficient to complete\ntasks that were long-thought to be imagery-dependent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u8ba4\u77e5\u884c\u4e3a\uff0c\u901a\u8fc7\u4f7f\u7528\u5fc3\u7406\u5b66\u4e2d\u7684\u7ecf\u5178\u5fc3\u7406\u610f\u8c61\u4efb\u52a1\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5373\u4f7f\u5728\u975e\u56fe\u50cf\u67b6\u6784\u4e0b\u4e5f\u80fd\u5b8c\u6210\u4f9d\u8d56\u56fe\u50cf\u7684\u4efb\u52a1\uff0c\u5e76\u4e14\u6027\u80fd\u751a\u81f3\u8d85\u8fc7\u4e86\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u3002", "motivation": "\u76ee\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8ba4\u77e5\u80fd\u529b\u7406\u89e3\u6709\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u5df2\u5305\u542b\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u8fd9\u4e9b\u4efb\u52a1\u4ec5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5373\u53ef\u5b8c\u6210\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u53ea\u80fd\u901a\u8fc7\u89c6\u89c9\u5fc3\u7406\u610f\u8c61\u89e3\u51b3\u7684\u4efb\u52a1\u6765\u6d4b\u8bd5LLMs\u3002", "method": "1. \u521b\u5efa\u4e86\u6570\u5341\u4e2a\u7ecf\u5178\u5fc3\u7406\u610f\u8c61\u4efb\u52a1\u7684\u65b0\u9879\u76ee\u3002\n2. \u6d4b\u8bd5\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684LLMs\uff0c\u7ed9\u4e88\u6587\u672c\u6307\u4ee4\u5e76\u8981\u6c42\u5b83\u4eec\u62a5\u544a\u6267\u884c\u8f6c\u6362\u540e\u7684\u7ed3\u679c\u5bf9\u8c61\u3002\n3. \u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u7ebf\uff0c\u901a\u8fc7\u5728\u5b8c\u5168\u76f8\u540c\u7684\u4efb\u52a1\u4e2d\u6d4b\u8bd5100\u540d\u4eba\u7c7b\u53d7\u8bd5\u8005\u3002\n4. \u6d4b\u8bd5\u4e86\u8bbe\u7f6e\u4e3a\u4e0d\u540c\u63a8\u7406\u6c34\u5e73\u7684\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u53d1\u73b0\u5f53\u6a21\u578b\u5206\u914d\u66f4\u591a\u7684\u63a8\u7406tokens\u65f6\uff0c\u6027\u80fd\u6700\u5f3a\u3002", "result": "\u6700\u597d\u7684LLMs\u8868\u73b0\u660e\u663e\u9ad8\u4e8e\u4eba\u7c7b\u7684\u5e73\u5747\u6c34\u5e73\u3002\u5f53\u6a21\u578b\u5206\u914d\u66f4\u591a\u7684\u63a8\u7406tokens\u65f6\uff0c\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u6700\u5f3a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6700\u597d\u7684LLMs\u53ef\u80fd\u5177\u6709\u5b8c\u6210\u4f9d\u8d56\u56fe\u50cf\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684\u67b6\u6784\u672c\u8d28\u4e0a\u662f\u975e\u56fe\u50cf\u7684\u3002\u8fd9\u9879\u7814\u7a76\u4e0d\u4ec5\u5c55\u793a\u4e86LLMs\u5728\u65b0\u4efb\u52a1\u4e2d\u6d8c\u73b0\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u800c\u4e14\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u4e3a\u5176\u4ed6\u5df2\u7ecf\u975e\u5e38\u6709\u80fd\u529b\u7684\u6a21\u578b\u7559\u4e0b\u4e86\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002\u7814\u7a76\u7ed3\u679c\u91cd\u65b0\u5f15\u53d1\u4e86\u5173\u4e8e\u4eba\u7c7b\u89c6\u89c9\u610f\u8c61\u8868\u5f81\u5f62\u5f0f\u7684\u8fa9\u8bba\uff0c\u8868\u660e\u547d\u9898\u63a8\u7406\uff08\u6216\u81f3\u5c11\u975e\u56fe\u50cf\u63a8\u7406\uff09\u53ef\u80fd\u8db3\u4ee5\u5b8c\u6210\u957f\u671f\u4ee5\u6765\u88ab\u8ba4\u4e3a\u662f\u4f9d\u8d56\u56fe\u50cf\u7684\u4efb\u52a1\u3002"}}
{"id": "2509.22935", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22935", "abs": "https://arxiv.org/abs/2509.22935", "authors": ["Aleksandr Dremov", "David Grangier", "Angelos Katharopoulos", "Awni Hannun"], "title": "Compute-Optimal Quantization-Aware Training", "comment": null, "summary": "Quantization-aware training (QAT) is a leading technique for improving the\naccuracy of quantized neural networks. Previous work has shown that decomposing\ntraining into a full-precision (FP) phase followed by a QAT phase yields\nsuperior accuracy compared to QAT alone. However, the optimal allocation of\ncompute between the FP and QAT phases remains unclear. We conduct extensive\nexperiments with various compute budgets, QAT bit widths, and model sizes from\n86.0M to 2.2B to investigate how different QAT durations impact final\nperformance. We demonstrate that, contrary to previous findings, the\nloss-optimal ratio of QAT to FP training increases with the total amount of\ncompute. Moreover, the optimal fraction can be accurately predicted for a wide\nrange of model sizes and quantization widths using the\ntokens-per-parameter-byte statistic. From experimental data, we derive a loss\nscaling law that predicts both optimal QAT ratios and final model performance\nacross different QAT/FP compute allocation strategies and QAT bit widths. We\nuse the scaling law to make further predictions, which we verify\nexperimentally, including which QAT bit width is optimal under a given memory\nconstraint and how QAT accuracy with different bit widths compares to\nfull-precision model accuracy. Additionally, we propose a novel cooldown and\nQAT fusion approach that performs learning rate decay jointly with\nquantization-aware training, eliminating redundant full-precision model updates\nand achieving significant compute savings. These findings provide practical\ninsights into efficient QAT planning and enable the training of higher-quality\nquantized models with the same compute budget.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u4e2d\u5168\u7cbe\u5ea6\uff08FP\uff09\u548cQAT\u9636\u6bb5\u7684\u8ba1\u7b97\u5206\u914d\u95ee\u9898\uff0c\u53d1\u73b0\u6700\u4f18QAT/FP\u8bad\u7ec3\u6bd4\u4f8b\u968f\u8ba1\u7b97\u91cf\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u635f\u5931\u7f29\u653e\u5b9a\u5f8b\u6765\u9884\u6d4b\u6700\u4f18QAT\u6bd4\u4f8b\u548c\u6a21\u578b\u6027\u80fd\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u51b7\u5374\u548cQAT\u878d\u5408\u65b9\u6cd5\uff0c\u4ee5\u8282\u7701\u8ba1\u7b97\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c06\u8bad\u7ec3\u5206\u89e3\u4e3a\u5168\u7cbe\u5ea6\uff08FP\uff09\u9636\u6bb5\u548cQAT\u9636\u6bb5\u53ef\u4ee5\u63d0\u9ad8\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0cFP\u548cQAT\u9636\u6bb5\u4e4b\u95f4\u7684\u6700\u4f73\u8ba1\u7b97\u5206\u914d\u4ecd\u7136\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7\u5bf986.0M\u52302.2B\u7684\u5404\u79cd\u8ba1\u7b97\u9884\u7b97\u3001QAT\u4f4d\u5bbd\u548c\u6a21\u578b\u5927\u5c0f\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e0d\u540c\u7684QAT\u6301\u7eed\u65f6\u95f4\u5982\u4f55\u5f71\u54cd\u6700\u7ec8\u6027\u80fd\u3002\u57fa\u4e8e\u5b9e\u9a8c\u6570\u636e\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u635f\u5931\u7f29\u653e\u5b9a\u5f8b\uff0c\u8be5\u5b9a\u5f8b\u53ef\u4ee5\u9884\u6d4b\u4e0d\u540cQAT/FP\u8ba1\u7b97\u5206\u914d\u7b56\u7565\u548cQAT\u4f4d\u5bbd\u4e0b\u7684\u6700\u4f73QAT\u6bd4\u7387\u548c\u6700\u7ec8\u6a21\u578b\u6027\u80fd\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u51b7\u5374\u548cQAT\u878d\u5408\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u7387\u8870\u51cf\u4e0e\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684\u53d1\u73b0\u76f8\u53cd\uff0cQAT\u4e0eFP\u8bad\u7ec3\u7684\u635f\u5931\u6700\u4f18\u6bd4\u7387\u968f\u7740\u8ba1\u7b97\u603b\u91cf\u7684\u589e\u52a0\u800c\u589e\u52a0\u3002\u6b64\u5916\uff0c\u53ef\u4ee5\u4f7f\u7528tokens-per-parameter-byte\u7edf\u8ba1\u6570\u636e\u51c6\u786e\u9884\u6d4b\u5404\u79cd\u6a21\u578b\u5927\u5c0f\u548c\u91cf\u5316\u5bbd\u5ea6\u7684\u6700\u4f73\u6bd4\u4f8b\u3002\u5bfc\u51fa\u7684\u635f\u5931\u7f29\u653e\u5b9a\u5f8b\u53ef\u4ee5\u9884\u6d4b\u4e0d\u540cQAT/FP\u8ba1\u7b97\u5206\u914d\u7b56\u7565\u548cQAT\u4f4d\u5bbd\u4e0b\u7684\u6700\u4f73QAT\u6bd4\u7387\u548c\u6700\u7ec8\u6a21\u578b\u6027\u80fd\u3002\u65b0\u9896\u7684\u51b7\u5374\u548cQAT\u878d\u5408\u65b9\u6cd5\u6d88\u9664\u4e86\u5197\u4f59\u7684\u5168\u7cbe\u5ea6\u6a21\u578b\u66f4\u65b0\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u8ba1\u7b97\u8282\u7701\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6709\u6548\u7684QAT\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c1\u89e3\uff0c\u5e76\u80fd\u591f\u5728\u76f8\u540c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u8bad\u7ec3\u66f4\u9ad8\u8d28\u91cf\u7684\u91cf\u5316\u6a21\u578b\u3002"}}
{"id": "2509.24869", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24869", "abs": "https://arxiv.org/abs/2509.24869", "authors": ["Junwei Lan", "Jianlyu Chen", "Zheng Liu", "Chaofan Li", "Siqi Bao", "Defu Lian"], "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval", "comment": null, "summary": "With the growing popularity of LLM agents and RAG, it has become increasingly\nimportant to retrieve documents that are essential for solving a task, even\nwhen their connection to the task is indirect or implicit. Addressing this\nproblem requires fine-grained reasoning to accurately assess the relevance\nbetween the task and each candidate document. This capability, however, poses a\nsignificant challenge for existing IR techniques. Despite recent progress in\nreasoning-enhanced IR, existing approaches still face significant challenges in\napplicability, scalability, and efficiency. In this work, we propose Retro*, a\nnovel approach for reasoning-intensive document retrieval. Our method\nintroduces a rubric-based relevance scoring mechanism, enabling the model to\nreason about the relationship between a task and a document based on explicitly\ndefined criteria, whereby producing a fine-grained, interpretable relevance\nscore. Retro* also supports test-time scaling by combining multiple reasoning\ntrajectories via score integration, which produces more reliable relevance\nestimates. To optimize Retro*'s reasoning capabilities, we introduce a novel\nreinforcement learning algorithm tailored for its relevance scoring mechanism,\nwhich employs two composite rewards to fully exploit the trajectories of each\ntraining sample. Our experiments show that Retro* outperforms existing document\nretrieval methods with notable advantages, leading to state-of-the-art\nperformance on the BRIGHT benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540d\u4e3aRetro*\u7684\u6587\u6863\u68c0\u7d22\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u76f8\u5173\u6027\u8bc4\u5206\u673a\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u4f18\u5316\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u5728BRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u5728\u5904\u7406\u4efb\u52a1\u4e0e\u6587\u6863\u4e4b\u95f4\u95f4\u63a5\u6216\u9690\u542b\u7684\u8fde\u63a5\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728LLM agents\u548cRAG\u65e5\u76ca\u666e\u53ca\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u7684\u63a8\u7406\u589e\u5f3aIR\u65b9\u6cd5\u5728\u9002\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u663e\u8457\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u76f8\u5173\u6027\u8bc4\u5206\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u660e\u786e\u5b9a\u4e49\u7684\u6807\u51c6\u6765\u63a8\u65ad\u4efb\u52a1\u548c\u6587\u6863\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u3001\u53ef\u89e3\u91ca\u7684\u76f8\u5173\u6027\u5206\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u63a8\u7406\u8f68\u8ff9\u6765\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u4e3a\u76f8\u5173\u6027\u8bc4\u5206\u673a\u5236\u91cf\u8eab\u5b9a\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u4f18\u5316\u63a8\u7406\u80fd\u529b\u3002", "result": "Retro*\u4f18\u4e8e\u73b0\u6709\u7684\u6587\u6863\u68c0\u7d22\u65b9\u6cd5\uff0c\u5e76\u5728BRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Retro*\u662f\u4e00\u79cd\u7528\u4e8e\u63a8\u7406\u5bc6\u96c6\u578b\u6587\u6863\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u76f8\u5173\u6027\u8bc4\u5206\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u6863\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22854", "abs": "https://arxiv.org/abs/2509.22854", "authors": ["Jiaqian Li", "Yanshu Li", "Ligong Han", "Ruixiang Tang", "Wenya Wang"], "title": "Towards Generalizable Implicit In-Context Learning with Attention Routing", "comment": null, "summary": "Implicit in-context learning (ICL) has newly emerged as a promising paradigm\nthat simulates ICL behaviors in the representation space of Large Language\nModels (LLMs), aiming to attain few-shot performance at zero-shot cost.\nHowever, existing approaches largely rely on injecting shift vectors into\nresidual flows, which are typically constructed from labeled demonstrations or\ntask-specific alignment. Such designs fall short of utilizing the structural\nmechanisms underlying ICL and suffer from limited generalizability. To address\nthis, we propose In-Context Routing (ICR), a novel implicit ICL method that\ninternalizes generalizable ICL patterns at the attention logits level. It\nextracts reusable structural directions that emerge during ICL and employs a\nlearnable input-conditioned router to modulate attention logits accordingly,\nenabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world\ndatasets spanning diverse domains and multiple LLMs. The results show that ICR\nconsistently outperforms prior implicit ICL methods that require task-specific\nretrieval or training, while demonstrating robust generalization to\nout-of-domain tasks where existing methods struggle. These findings position\nICR to push the boundary of ICL's practical value.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6ce8\u610f\u529blogits\u5c42\u9762\u5185\u5316\u53ef\u6cdb\u5316\u7684ICL\u6a21\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u5c11\u6837\u672c\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9690\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5c06\u79fb\u4f4d\u5411\u91cf\u6ce8\u5165\u6b8b\u5dee\u6d41\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9ICL\u5e95\u5c42\u7ed3\u6784\u673a\u5236\u7684\u5229\u7528\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIn-Context Routing (ICR) \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u53d6ICL\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u7ed3\u6784\u65b9\u5411\uff0c\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u8f93\u5165\u6761\u4ef6\u8def\u7531\u5668\u6765\u8c03\u8282\u6ce8\u610f\u529blogits\u3002", "result": "\u572812\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cICR\u4f18\u4e8e\u73b0\u6709\u7684\u9690\u5f0fICL\u65b9\u6cd5\uff0c\u5e76\u5728\u9886\u57df\u5916\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ICR\u6709\u6f5c\u529b\u63a8\u52a8ICL\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2509.22763", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 68U10", "I.4.9; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.22763", "abs": "https://arxiv.org/abs/2509.22763", "authors": ["Tangqi Shi", "Pietro Lio"], "title": "UESA-Net: U-Shaped Embedded Multidirectional Shrinkage Attention Network for Ultrasound Nodule Segmentation", "comment": "22 pages,2 figures,4 tables", "summary": "Background: Breast and thyroid cancers pose an increasing public-health\nburden. Ultrasound imaging is a cost-effective, real-time modality for lesion\ndetection and segmentation, yet suffers from speckle noise, overlapping\nstructures, and weak global-local feature interactions. Existing networks\nstruggle to reconcile high-level semantics with low-level spatial details. We\naim to develop a segmentation framework that bridges the semantic gap between\nglobal context and local detail in noisy ultrasound images.\n  Methods: We propose UESA-Net, a U-shaped network with multidirectional\nshrinkage attention. The encoder-decoder architecture captures long-range\ndependencies and fine-grained structures of lesions. Within each encoding\nblock, attention modules operate along horizontal, vertical, and depth\ndirections to exploit spatial details, while a shrinkage (threshold) strategy\nintegrates prior knowledge and local features. The decoder mirrors the encoder\nbut applies a pairwise shrinkage mechanism, combining prior low-level physical\ncues with corresponding encoder features to enhance context modeling.\n  Results: On two public datasets - TN3K (3493 images) and BUSI (780 images) -\nUESA-Net achieved state-of-the-art performance with intersection-over-union\n(IoU) scores of 0.8487 and 0.6495, respectively.\n  Conclusions: UESA-Net effectively aggregates multidirectional spatial\ninformation and prior knowledge to improve robustness and accuracy in breast\nand thyroid ultrasound segmentation, demonstrating superior performance to\nexisting methods on multiple benchmarks.", "AI": {"tldr": "UESA-Net: A novel U-shaped network with multidirectional shrinkage attention for improved breast and thyroid ultrasound segmentation.", "motivation": "Existing methods struggle to reconcile high-level semantics with low-level spatial details in noisy ultrasound images, which motivates the development of a segmentation framework that bridges the semantic gap between global context and local detail.", "method": "UESA-Net, a U-shaped network with multidirectional shrinkage attention, captures long-range dependencies and fine-grained structures. Attention modules exploit spatial details, while a shrinkage strategy integrates prior knowledge and local features. A pairwise shrinkage mechanism combines prior low-level physical cues with corresponding encoder features.", "result": "UESA-Net achieved state-of-the-art performance on TN3K and BUSI datasets with IoU scores of 0.8487 and 0.6495, respectively.", "conclusion": "UESA-Net effectively aggregates multidirectional spatial information and prior knowledge, demonstrating superior performance in breast and thyroid ultrasound segmentation."}}
{"id": "2509.23109", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23109", "abs": "https://arxiv.org/abs/2509.23109", "authors": ["Junyang Zhang", "Tianyi Zhu", "Thierry Tambe"], "title": "AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors", "comment": "31 pages, 17 figures", "summary": "A fundamental reason for the dominance of attention over RNNs and LSTMs in\nLLMs is its ability to capture long-range dependencies by modeling direct\ninteractions between all tokens, overcoming the sequential limitations of\nrecurrent architectures. Similarly, a key reason why today's vision language\nmodels (VLMs) hallucinate and underperform pure language models is that they\nrely on direct concatenation of image and text tokens with a modality-blinded\npositional encoding, which conveniently adopts the pretrained LLM backbone but\nforces unnecessary long-distance attention between semantically related tokens\nacross modalities. This underscores the urgent need for mechanisms that\nefficiently enhance token locality and cross-modal alignment. In response, we\npropose Attention Anchor, a parameter-free framework that efficiently groups\nsemantically similar tokens across modalities, improving cross-modal locality.\nBy inserting text tokens near relevant visual patches, we create semantic\nsignposts that reveal true content-based cross-modal attention scores, guiding\nthe model to focus on the correct image regions for tasks such as VQA, MMBench\nand POPE. This improves answer accuracy and reduces hallucinations without\ndisrupting the prompt's semantic flow. AttAnchor achieves improvements across\n13 out of 15 different metrics and benchmarks, including up to 32% gains on\nreasoning tasks and up to 15% improvements on hallucination benchmarks.\nAttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B\nand QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of\nour knowledge, this work is among the first to investigate mixed-modal token\ngrouping, where text and image tokens are clustered jointly into shared groups\nrather than being grouped within a single modality or merely aligned post-hoc\nwith additional alignment losses.", "AI": {"tldr": "\u63d0\u51fa Attention Anchor \u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8de8\u6a21\u6001\u7684\u8bed\u4e49\u76f8\u4f3c tokens \u8fdb\u884c\u5206\u7ec4\uff0c\u63d0\u9ad8\u8de8\u6a21\u6001\u5c40\u90e8\u6027\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u76ee\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u56fe\u50cf\u548c\u6587\u672c tokens \u7684\u76f4\u63a5\u8fde\u63a5\u4ee5\u53ca\u6a21\u6001\u76f2\u76ee\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u957f\u8ddd\u79bb\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u4ea7\u751f\u5e7b\u89c9\u5e76\u4e14\u6027\u80fd\u4e0d\u5982\u7eaf\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6709\u6548\u7684\u673a\u5236\u6765\u589e\u5f3a token \u5c40\u90e8\u6027\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa Attention Anchor \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u6587\u672c tokens \u63d2\u5165\u5230\u76f8\u5173\u7684\u89c6\u89c9 patches \u9644\u8fd1\uff0c\u521b\u5efa\u8bed\u4e49\u8def\u6807\uff0c\u4ece\u800c\u63ed\u793a\u771f\u5b9e\u7684\u57fa\u4e8e\u5185\u5bb9\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u6b63\u786e\u7684\u56fe\u50cf\u533a\u57df\u3002", "result": "\u5728 15 \u4e2a\u4e0d\u540c\u7684\u6307\u6807\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAttAnchor \u5728 13 \u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6539\u8fdb\uff0c\u5305\u62ec\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u9ad8\u8fbe 32% \u7684\u6536\u76ca\uff0c\u5728\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9ad8\u8fbe 15% \u7684\u6539\u8fdb\u3002AttAnchor \u4f7f TinyLLaVA 1B \u5728 POPE \u4e0a\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u5982 LLaVA 7B \u548c QwenVL 3B\uff0c\u4e14\u53ea\u6709 0.1% \u7684\u63a8\u7406\u65f6\u95f4\u5f00\u9500\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u662f\u9996\u6279\u7814\u7a76\u6df7\u5408\u6a21\u6001 token \u5206\u7ec4\u7684\u5de5\u4f5c\u4e4b\u4e00\uff0c\u5176\u4e2d\u6587\u672c\u548c\u56fe\u50cf tokens \u88ab\u8054\u5408\u805a\u7c7b\u5230\u5171\u4eab\u7ec4\u4e2d\uff0c\u800c\u4e0d\u662f\u5728\u5355\u4e2a\u6a21\u6001\u5185\u5206\u7ec4\u6216\u4ec5\u5728\u4e8b\u540e\u901a\u8fc7\u989d\u5916\u7684\u5bf9\u9f50\u635f\u5931\u8fdb\u884c\u5bf9\u9f50\u3002"}}
{"id": "2509.22938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22938", "abs": "https://arxiv.org/abs/2509.22938", "authors": ["Yanqing Lu", "Letao Wang", "Jinbo Liu"], "title": "Understanding SOAP from the Perspective of Gradient Whitening", "comment": null, "summary": "Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) has recently\nemerged as a promising optimization algorithm for neural network training,\nachieving superior training efficiency over both Adam and Shampoo in language\nmodeling tasks. In this work, we analyze Adam, Shampoo, and SOAP from the\nperspective of gradient whitening, interpreting their preconditioners as\napproximations to the whitening matrix, which captures second-order curvature\ninformation. We further establish a theoretical equivalence between idealized\nversions of SOAP and Shampoo under the Kronecker product assumption. To\nempirically evaluate these insights, we reproduce the language modeling\nexperiments using nanoGPT and grayscale image colorization. Our results show\nthat SOAP exhibits similar convergence rate as Shampoo, and no significant\nadvantage over both Adam and Shampoo in the final loss achieved, which aligns\nwith their equivalence in theory.", "AI": {"tldr": "SOAP\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4f18\u4e8eAdam\u548cShampoo\uff0c\u4f46\u672c\u6587\u53d1\u73b0SOAP\u4e0eShampoo\u5728\u7406\u8bba\u4e0a\u7b49\u4ef7\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e5f\u8868\u660eSOAP\u7684\u6536\u655b\u901f\u5ea6\u4e0eShampoo\u76f8\u4f3c\uff0c\u4e14\u5728\u6700\u7ec8\u635f\u5931\u65b9\u9762\u6ca1\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u5206\u6790Adam, Shampoo\u548cSOAP\u7684\u9884\u5904\u7406\u5668\uff0c\u5e76\u5c06\u5b83\u4eec\u89e3\u91ca\u4e3a\u767d\u5316\u77e9\u9635\u7684\u8fd1\u4f3c\uff0c\u4ece\u800c\u6355\u6349\u4e8c\u9636\u66f2\u7387\u4fe1\u606f\u3002", "method": "\u4ece\u68af\u5ea6\u767d\u5316\u7684\u89d2\u5ea6\u5206\u6790Adam, Shampoo\u548cSOAP\uff0c\u5e76\u5728Kronecker\u79ef\u5047\u8bbe\u4e0b\u5efa\u7acbSOAP\u548cShampoo\u7406\u60f3\u5316\u7248\u672c\u7684\u7406\u8bba\u7b49\u4ef7\u6027\u3002\u4f7f\u7528nanoGPT\u548c\u7070\u5ea6\u56fe\u50cf\u7740\u8272\u590d\u73b0\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\u3002", "result": "SOAP\u8868\u73b0\u51fa\u4e0eShampoo\u76f8\u4f3c\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u4e14\u5728\u6700\u7ec8\u635f\u5931\u65b9\u9762\u6ca1\u6709\u663e\u8457\u4f18\u4e8eAdam\u548cShampoo\u3002", "conclusion": "SOAP\u5728\u6027\u80fd\u4e0a\u4e0eShampoo\u7b49\u4ef7\uff0c\u4e0e\u7406\u8bba\u7ed3\u679c\u4e00\u81f4\u3002"}}
{"id": "2509.22856", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22856", "abs": "https://arxiv.org/abs/2509.22856", "authors": ["R. Alexander Knipper", "Charles S. Knipper", "Kaiqi Zhang", "Valerie Sims", "Clint Bowers", "Santu Karmaker"], "title": "The Bias is in the Details: An Assessment of Cognitive Bias in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly embedded in real-world\ndecision-making processes, it becomes crucial to examine the extent to which\nthey exhibit cognitive biases. Extensively studied in the field of psychology,\ncognitive biases appear as systematic distortions commonly observed in human\njudgments. This paper presents a large-scale evaluation of eight\nwell-established cognitive biases across 45 LLMs, analyzing over 2.8 million\nLLM responses generated through controlled prompt variations. To achieve this,\nwe introduce a novel evaluation framework based on multiple-choice tasks,\nhand-curate a dataset of 220 decision scenarios targeting fundamental cognitive\nbiases in collaboration with psychologists, and propose a scalable approach for\ngenerating diverse prompts from human-authored scenario templates. Our analysis\nshows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances\nacross a range of judgment and decision-making contexts targeting anchoring,\navailability, confirmation, framing, interpretation, overattribution, prospect\ntheory, and representativeness biases. We find that both model size and prompt\nspecificity play a significant role on bias susceptibility as follows: larger\nsize (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt\ndetail reduces most biases by up to 14.9%, except in one case\n(Overattribution), which is exacerbated by up to 8.8%.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u8ba4\u77e5\u504f\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76LLM\u5728\u73b0\u5b9e\u51b3\u7b56\u4e2d\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u56e0\u6b64\u68c0\u9a8c\u5b83\u4eec\u662f\u5426\u5b58\u5728\u8ba4\u77e5\u504f\u5dee\u81f3\u5173\u91cd\u8981\u3002\u8ba4\u77e5\u504f\u5dee\u662f\u4eba\u7c7b\u5224\u65ad\u4e2d\u5e38\u89c1\u7684\u7cfb\u7edf\u6027\u626d\u66f2\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b220\u4e2a\u51b3\u7b56\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u5fc3\u7406\u5b66\u5bb6\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u8bc4\u4f3045\u4e2aLLM\u4e2d\u76848\u79cd\u8ba4\u77e5\u504f\u5dee\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u4ece\u4eba\u5de5\u7f16\u5199\u7684\u573a\u666f\u6a21\u677f\u751f\u6210\u4e0d\u540c\u7684\u63d0\u793a\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cLLM\u572817.8%-57.3%\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4e0e\u504f\u5dee\u4e00\u81f4\u7684\u884c\u4e3a\u3002\u6a21\u578b\u5927\u5c0f\u548c\u63d0\u793a\u7684\u660e\u786e\u7a0b\u5ea6\u5bf9\u504f\u5dee\u654f\u611f\u6027\u6709\u663e\u8457\u5f71\u54cd\uff1a\u8f83\u5927\u7684\u6a21\u578b\uff08>32B\u53c2\u6570\uff09\u53ef\u4ee5\u572839.5%\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u504f\u5dee\uff0c\u800c\u66f4\u8be6\u7ec6\u7684\u63d0\u793a\u6700\u591a\u53ef\u4ee5\u51cf\u5c1114.9%\u7684\u504f\u5dee\uff08Overattribution\u504f\u5dee\u9664\u5916\uff0c\u8be5\u504f\u5dee\u6700\u591a\u53ef\u80fd\u52a0\u52678.8%\uff09\u3002", "conclusion": "LLM \u5b58\u5728\u8ba4\u77e5\u504f\u5dee\uff0c\u4e14\u6a21\u578b\u5927\u5c0f\u548c prompt \u660e\u786e\u7a0b\u5ea6\u4f1a\u5bf9\u504f\u5dee\u4ea7\u751f\u5f71\u54cd\u3002"}}
{"id": "2509.22769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22769", "abs": "https://arxiv.org/abs/2509.22769", "authors": ["Fernando Julio Cendra", "Kai Han"], "title": "PartCo: Part-Level Correspondence Priors Enhance Category Discovery", "comment": "Project page: https://visual-ai.github.io/partco", "summary": "Generalized Category Discovery (GCD) aims to identify both known and novel\ncategories within unlabeled data by leveraging a set of labeled examples from\nknown categories. Existing GCD methods primarily depend on semantic labels and\nglobal image representations, often overlooking the detailed part-level cues\nthat are crucial for distinguishing closely related categories. In this paper,\nwe introduce PartCo, short for Part-Level Correspondence Prior, a novel\nframework that enhances category discovery by incorporating part-level visual\nfeature correspondences. By leveraging part-level relationships, PartCo\ncaptures finer-grained semantic structures, enabling a more nuanced\nunderstanding of category relationships. Importantly, PartCo seamlessly\nintegrates with existing GCD methods without requiring significant\nmodifications. Our extensive experiments on multiple benchmark datasets\ndemonstrate that PartCo significantly improves the performance of current GCD\napproaches, achieving state-of-the-art results by bridging the gap between\nsemantic labels and part-level visual compositions, thereby setting new\nbenchmarks for GCD. Project page: https://visual-ai.github.io/partco", "AI": {"tldr": "PartCo improves category discovery by using part-level visual feature correspondences, enhancing the understanding of category relationships.", "motivation": "Existing GCD methods often overlook detailed part-level cues crucial for distinguishing closely related categories.", "method": "Introduce PartCo, a framework incorporating part-level visual feature correspondences, seamlessly integrating with existing GCD methods.", "result": "PartCo significantly improves the performance of current GCD approaches, achieving state-of-the-art results.", "conclusion": "PartCo bridges the gap between semantic labels and part-level visual compositions, setting new benchmarks for GCD."}}
{"id": "2509.23113", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.23113", "abs": "https://arxiv.org/abs/2509.23113", "authors": ["Xian Yeow Lee", "Lasitha Vidyaratne", "Ahmed Farahat", "Chetan Gupta"], "title": "Exploring LLM-based Frameworks for Fault Diagnosis", "comment": null, "summary": "Large Language Model (LLM)-based systems present new opportunities for\nautonomous health monitoring in sensor-rich industrial environments. This study\nexplores the potential of LLMs to detect and classify faults directly from\nsensor data, while producing inherently explainable outputs through natural\nlanguage reasoning. We systematically evaluate how LLM-system architecture\n(single-LLM vs. multi-LLM), input representations (raw vs. descriptive\nstatistics), and context window size affect diagnostic performance. Our\nfindings show that LLM systems perform most effectively when provided with\nsummarized statistical inputs, and that systems with multiple LLMs using\nspecialized prompts offer improved sensitivity for fault classification\ncompared to single-LLM systems. While LLMs can produce detailed and\nhuman-readable justifications for their decisions, we observe limitations in\ntheir ability to adapt over time in continual learning settings, often\nstruggling to calibrate predictions during repeated fault cycles. These\ninsights point to both the promise and the current boundaries of LLM-based\nsystems as transparent, adaptive diagnostic tools in complex environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\u5728\u4f20\u611f\u5668\u4e30\u5bcc\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u5065\u5eb7\u76d1\u6d4b\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76LLM\u76f4\u63a5\u4ece\u4f20\u611f\u5668\u6570\u636e\u4e2d\u68c0\u6d4b\u548c\u5206\u7c7b\u6545\u969c\uff0c\u5e76\u751f\u6210\u5177\u6709\u5185\u5728\u53ef\u89e3\u91ca\u6027\u7684\u8f93\u51fa\u3002", "method": "\u7cfb\u7edf\u5730\u8bc4\u4f30LLM\u7cfb\u7edf\u67b6\u6784\uff08\u5355LLM vs. \u591aLLM\uff09\u3001\u8f93\u5165\u8868\u793a\uff08\u539f\u59cb\u6570\u636e vs. \u63cf\u8ff0\u6027\u7edf\u8ba1\uff09\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\u5bf9\u8bca\u65ad\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "LLM\u7cfb\u7edf\u5728\u63d0\u4f9b\u6c47\u603b\u7684\u7edf\u8ba1\u8f93\u5165\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u4e0e\u5355LLM\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5177\u6709\u4f7f\u7528\u4e13\u95e8\u63d0\u793a\u7684\u591aLLM\u7cfb\u7edf\u7684\u6545\u969c\u5206\u7c7b\u7075\u654f\u5ea6\u66f4\u9ad8\u3002LLM\u53ef\u4ee5\u4e3a\u5176\u51b3\u7b56\u751f\u6210\u8be6\u7ec6\u4e14\u4eba\u7c7b\u53ef\u8bfb\u7684\u7406\u7531\uff0c\u4f46\u5728\u6301\u7eed\u5b66\u4e60\u73af\u5883\u4e2d\u968f\u65f6\u95f4\u8c03\u6574\u7684\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u6307\u51fa\u4e86\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u4f5c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u900f\u660e\u3001\u81ea\u9002\u5e94\u8bca\u65ad\u5de5\u5177\u7684\u524d\u666f\u548c\u5f53\u524d\u5c40\u9650\u6027\u3002"}}
{"id": "2509.22944", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22944", "abs": "https://arxiv.org/abs/2509.22944", "authors": ["Lorenz K. M\u00fcller", "Philippe Bich", "Jiawei Zhuang", "Ahmet \u00c7elik", "Luca Benfenati", "Lukas Cavigelli"], "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights", "comment": null, "summary": "Post-training quantization has emerged as the most widely used strategy for\ndeploying large language models at low precision. Still, current methods show\nperplexity degradation at bit-widths less than or equal to 4, partly because\nrepresenting outliers causes precision issues in parameters that share the same\nscales as these outliers. This problem is especially pronounced for\ncalibration-free, uniform quantization methods. We introduce SINQ to augment\nexisting post-training quantizers with an additional second-axis scale factor\nand a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize\nper-row and per-column variances, thereby minimizing a novel per-matrix proxy\ntarget for quantization: the matrix imbalance. Our method has no interactions\nbetween layers and can be trivially applied to new architectures to quantize\nany linear layers. We evaluate our method on the Qwen3 model family and\nDeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against\nuncalibrated uniform quantization baselines and can be further enhanced by\ncombining it with calibration and non-uniform quantization levels. Code to\nreproduce the results of this work and to easily quantize models using SINQ is\navailable at https://github.com/huawei-csl/SINQ.", "AI": {"tldr": "\u63d0\u51faSINQ\uff0c\u901a\u8fc7\u5f15\u5165\u7b2c\u4e8c\u8f74\u6bd4\u4f8b\u56e0\u5b50\u548cSinkhorn-Knopp\u7b97\u6cd5\uff0c\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u4f4e\u4e8e\u6216\u7b49\u4e8e4\u4f4d\u65f6\uff0c\u7531\u4e8e\u79bb\u7fa4\u503c\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6821\u51c6\u7684\u5747\u5300\u91cf\u5316\u65b9\u6cd5\u4e2d\u3002", "method": "\u5f15\u5165SINQ\uff0c\u4f7f\u7528\u7b2c\u4e8c\u8f74\u6bd4\u4f8b\u56e0\u5b50\u548cSinkhorn-Knopp\u98ce\u683c\u7b97\u6cd5\u6765\u5f52\u4e00\u5316\u884c\u548c\u5217\u7684\u65b9\u5dee\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u77e9\u9635\u4e0d\u5e73\u8861\u3002", "result": "\u5728Qwen3\u548cDeepSeek-V2.5\u6a21\u578b\u4e0a\uff0cSINQ\u663e\u8457\u63d0\u9ad8\u4e86WikiText2\u548cC4\u7684\u56f0\u60d1\u5ea6\u3002", "conclusion": "SINQ\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u65e0\u6821\u51c6\u5747\u5300\u91cf\u5316\u7684\u6548\u679c\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u6821\u51c6\u548c\u975e\u5747\u5300\u91cf\u5316\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2509.22991", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22991", "abs": "https://arxiv.org/abs/2509.22991", "authors": ["Jasin Cekinmez", "Omid Ghahroodi", "Saad Fowad Chandle", "Dhiman Gupta", "Ehsaneddin Asgari"], "title": "ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning", "comment": null, "summary": "We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating\nand improving multimodal large language models (MLLMs) in biographical\nreasoning. To the best of our knowledge, this is the first work to\nsystematically examine LLM capabilities in biography, a critical yet\nunderexplored dimension of factual knowledge. At its core, AdamDB is a\nmultilingual and multimodal dataset covering over 4 million individuals across\ngeography, time, and profession, while AdamBench provides cognitively\nstructured evaluations based on Bloom's taxonomy, spanning six reasoning levels\nin both English and native languages. To address hallucinations, particularly\nfor lesser-known individuals, we propose AdamRAG, a retrieval-augmented\ngeneration system tailored to biographical contexts. Experiments show that\nAdamRAG substantially improves open-source models and modestly benefits\nclosed-source ones, with the largest gains on lower-order reasoning. Popularity\nstrongly mediates accuracy, and multimodal input via face images offers\nsmaller, less consistent improvements than retrieval. ADAM establishes the\nfirst benchmark and framework for cognitively, culturally, and multimodally\ngrounded biographical evaluation, advancing the development of multilingual,\naccurate, and hallucination-resistant MLLMs.", "AI": {"tldr": "ADAM\u662f\u4e00\u4e2a\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u5728\u4f20\u8bb0\u63a8\u7406\u65b9\u9762\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5bf9LLM\u5728\u4f20\u8bb0\u8fd9\u4e00\u4e8b\u5b9e\u77e5\u8bc6\u7684\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u80fd\u529b\u8003\u5bdf\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u8003\u5bdfLLM\u5728\u4f20\u8bb0\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6570\u636e\u96c6AdamDB\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edfAdamRAG\u3002", "result": "AdamRAG\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u8f83\u4f4e\u5c42\u6b21\u7684\u63a8\u7406\u4e0a\u83b7\u5f97\u4e86\u6700\u5927\u7684\u6536\u76ca\u3002\u4eba\u8138\u56fe\u50cf\u7684\u591a\u6a21\u6001\u8f93\u5165\u63d0\u4f9b\u7684\u6539\u8fdb\u5c0f\u4e8e\u68c0\u7d22\u3002", "conclusion": "ADAM\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u5177\u6709\u8ba4\u77e5\u3001\u6587\u5316\u548c\u591a\u6a21\u6001\u57fa\u7840\u7684\u4f20\u8bb0\u8bc4\u4f30\u57fa\u51c6\u548c\u6846\u67b6\uff0c\u4ece\u800c\u63a8\u8fdb\u591a\u8bed\u8a00\u3001\u51c6\u786e\u548c\u6297\u5e7b\u89c9MLLM\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.22870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22870", "abs": "https://arxiv.org/abs/2509.22870", "authors": ["Passant Elchafei", "Mayar Osama", "Mohamed Rageh", "Mervat Abuelkheir"], "title": "Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction", "comment": null, "summary": "We present a graph-based approach enriched with lexicons to predict\ndocument-level readability in Arabic, developed as part of the Constrained\nTrack of the BAREC Shared Task 2025. Our system models each document as a\nsentence-level graph, where nodes represent sentences and lemmas, and edges\ncapture linguistic relationships such as lexical co-occurrence and class\nmembership. Sentence nodes are enriched with features from the SAMER lexicon as\nwell as contextual embeddings from the Arabic transformer model. The graph\nneural network (GNN) and transformer sentence encoder are trained as two\nindependent branches, and their predictions are combined via late fusion at\ninference. For document-level prediction, sentence-level outputs are aggregated\nusing max pooling to reflect the most difficult sentence. Experimental results\nshow that this hybrid method outperforms standalone GNN or transformer branches\nacross multiple readability metrics. Overall, the findings highlight that\nfusion offers advantages at the document level, but the GNN-only approach\nremains stronger for precise prediction of sentence-level readability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u3001\u7ed3\u5408\u8bcd\u6c47\u4fe1\u606f\u7684\u963f\u62c9\u4f2f\u8bed\u6587\u6863\u53ef\u8bfb\u6027\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u6863\u5c42\u9762\u6709\u6548\uff0c\u4f46\u5728\u53e5\u5b50\u5c42\u9762\u4e0d\u5982\u5355\u72ec\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u4e3a\u4e86\u53c2\u52a0BAREC Shared Task 2025\u7684\u7ea6\u675f\u8d5b\u9053\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u963f\u62c9\u4f2f\u8bed\u6587\u6863\u53ef\u8bfb\u6027\u9884\u6d4b\u7cfb\u7edf\u3002", "method": "\u8be5\u7cfb\u7edf\u5c06\u6587\u6863\u5efa\u6a21\u4e3a\u53e5\u5b50\u7ea7\u56fe\uff0c\u8282\u70b9\u8868\u793a\u53e5\u5b50\u548c\u8bcd\u5143\uff0c\u8fb9\u8868\u793a\u8bcd\u6c47\u5171\u73b0\u548c\u7c7b\u522b\u6210\u5458\u5173\u7cfb\u7b49\u8bed\u8a00\u5173\u7cfb\u3002\u53e5\u5b50\u8282\u70b9\u4f7f\u7528SAMER\u8bcd\u5178\u7684\u7279\u5f81\u548c\u963f\u62c9\u4f2f\u8bedtransformer\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u8fdb\u884c\u589e\u5f3a\u3002\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548ctransformer\u53e5\u5b50\u7f16\u7801\u5668\u4f5c\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u5206\u652f\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u540e\u671f\u878d\u5408\u7ed3\u5408\u5b83\u4eec\u7684\u9884\u6d4b\u3002\u6587\u6863\u5c42\u9762\u7684\u9884\u6d4b\u4f7f\u7528\u6700\u5927\u6c60\u5316\u6765\u805a\u5408\u53e5\u5b50\u5c42\u9762\u7684\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u591a\u4e2a\u53ef\u8bfb\u6027\u6307\u6807\u4e0a\u4f18\u4e8e\u72ec\u7acb\u7684GNN\u6216transformer\u5206\u652f\u3002", "conclusion": "\u878d\u5408\u5728\u6587\u6863\u5c42\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46GNN-only\u65b9\u6cd5\u5728\u7cbe\u786e\u9884\u6d4b\u53e5\u5b50\u5c42\u9762\u7684\u53ef\u8bfb\u6027\u65b9\u9762\u4ecd\u7136\u66f4\u5f3a\u3002"}}
{"id": "2509.22793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22793", "abs": "https://arxiv.org/abs/2509.22793", "authors": ["Komal Kumar", "Rao Muhammad Anwer", "Fahad Shahbaz Khan", "Salman Khan", "Ivan Laptev", "Hisham Cholakkal"], "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models", "comment": "13 Figures, 21 pages, accepted in NeurIPS 2025", "summary": "Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves\nadjusting the model to suit a particular task or dataset while minimizing\ncomputational resources and limiting the number of trainable parameters.\nHowever, it often faces challenges in striking a trade-off between aligning\nwith the target distribution: learning a novel concept from a limited image for\npersonalization and retaining the instruction ability needed for unifying\nmultiple tasks, all while maintaining editability (aligning with a variety of\nprompts or in-context generation). In this work, we introduce DEFT,\nDecompositional Efficient Fine-Tuning, an efficient fine-tuning framework that\nadapts a pre-trained weight matrix by decomposing its update into two\ncomponents with two trainable matrices: (1) a projection onto the complement of\na low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update.\nThe single trainable low-rank matrix defines the subspace, while the other\ntrainable low-rank matrix enables flexible parameter adaptation within that\nsubspace. We conducted extensive experiments on the Dreambooth and Dreambench\nPlus datasets for personalization, the InsDet dataset for object and scene\nadaptation, and the VisualCloze dataset for a universal image generation\nframework through visual in-context learning with both Stable Diffusion and a\nunified model. Our results demonstrated state-of-the-art performance,\nhighlighting the emergent properties of efficient fine-tuning. Our code is\navailable on \\href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.", "AI": {"tldr": "DEFT: An efficient fine-tuning framework for pre-trained Text-to-Image models.", "motivation": "Efficient fine-tuning of T2I models struggles to balance target distribution alignment, novel concept learning, instruction ability retention, and editability.", "method": "Decomposes the update of a pre-trained weight matrix into two trainable matrices: a projection onto the complement of a low-rank subspace and a low-rank update.", "result": "Achieved state-of-the-art performance on Dreambooth, Dreambench Plus, InsDet, and VisualCloze datasets with both Stable Diffusion and a unified model.", "conclusion": "Demonstrates the emergent properties of efficient fine-tuning."}}
{"id": "2509.23121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23121", "abs": "https://arxiv.org/abs/2509.23121", "authors": ["Shuai Li", "Chen Yizhe", "Li Dong", "Liu Sichao", "Lan Dapeng", "Liu Yu", "Zhibo Pang"], "title": "Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges", "comment": "Accepted to IAI 2025 (International Conference on Industrial\n  Artificial Intelligence), Shenyang, China, Aug 21 - 24, 2025. Preprint\n  (before IEEE copyright transfer)", "summary": "The application of artificial intelligence (AI) in industry is accelerating\nthe shift from traditional automation to intelligent systems with perception\nand cognition. Vision language-action (VLA) models have been a key paradigm in\nAI to unify perception, reasoning, and control. Has the performance of the VLA\nmodels met the industrial requirements? In this paper, from the perspective of\nindustrial deployment, we compare the performance of existing state-of-the-art\nVLA models in industrial scenarios and analyze the limitations of VLA models\nfor real-world industrial deployment from the perspectives of data collection\nand model architecture. The results show that the VLA models retain their\nability to perform simple grasping tasks even in industrial settings after\nfine-tuning. However, there is much room for performance improvement in complex\nindustrial environments, diverse object categories, and high precision placing\ntasks. Our findings provide practical insight into the adaptability of VLA\nmodels for industrial use and highlight the need for task-specific enhancements\nto improve their robustness, generalization, and precision.", "AI": {"tldr": "VLA\u6a21\u578b\u5728\u5de5\u4e1a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u548c\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e2d\u3002", "motivation": "\u63a2\u8ba8VLA\u6a21\u578b\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u662f\u5426\u80fd\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\uff0c\u5e76\u5206\u6790\u5176\u5c40\u9650\u6027\u3002", "method": "\u5bf9\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u5728\u5de5\u4e1a\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u4ece\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u67b6\u6784\u7684\u89d2\u5ea6\u5206\u6790\u5176\u5c40\u9650\u6027\u3002", "result": "VLA\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4ecd\u80fd\u5b8c\u6210\u7b80\u5355\u7684\u6293\u53d6\u4efb\u52a1\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u3001\u591a\u6837\u5316\u7684\u7269\u4f53\u7c7b\u522b\u548c\u9ad8\u7cbe\u5ea6\u653e\u7f6e\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u7a7a\u95f4\u5f88\u5927\u3002", "conclusion": "VLA\u6a21\u578b\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5177\u6709\u9002\u5e94\u6027\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2509.22949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22949", "abs": "https://arxiv.org/abs/2509.22949", "authors": ["Hamidreza Moazzami", "Asma Jamali", "Nicholas Kevlahan", "Rodrigo A. Vargas-Hern\u00e1ndez"], "title": "Meta-Learning Fourier Neural Operators for Hessian Inversion and Enhanced Variational Data Assimilation", "comment": "6 pages, 2 figures, Machine Learning and the Physical Sciences\n  Workshop, (NeurIPS 2025)", "summary": "Data assimilation (DA) is crucial for enhancing solutions to partial\ndifferential equations (PDEs), such as those in numerical weather prediction,\nby optimizing initial conditions using observational data. Variational DA\nmethods are widely used in oceanic and atmospheric forecasting, but become\ncomputationally expensive, especially when Hessian information is involved. To\naddress this challenge, we propose a meta-learning framework that employs the\nFourier Neural Operator (FNO) to approximate the inverse Hessian operator\nacross a family of DA problems, thereby providing an effective initialization\nfor the conjugate gradient (CG) method. Numerical experiments on a linear\nadvection equation demonstrate that the resulting FNO-CG approach reduces the\naverage relative error by $62\\%$ and the number of iterations by $17\\%$\ncompared to the standard CG. These improvements are most pronounced in\nill-conditioned scenarios, highlighting the robustness and efficiency of FNO-CG\nfor challenging DA problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u8fd1\u4f3c\u6570\u636e\u540c\u5316\uff08DA\uff09\u95ee\u9898\u4e2d\u7684\u9006Hessian\u7b97\u5b50\uff0c\u4e3a\u5171\u8f6d\u68af\u5ea6\uff08CG\uff09\u65b9\u6cd5\u63d0\u4f9b\u6709\u6548\u7684\u521d\u59cb\u5316\u3002", "motivation": "\u53d8\u5206\u6570\u636e\u540c\u5316\uff08DA\uff09\u65b9\u6cd5\u5728\u6d77\u6d0b\u548c\u5927\u6c14\u9884\u6d4b\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5f53\u6d89\u53ca\u5230Hessian\u4fe1\u606f\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\u3002", "method": "\u5229\u7528\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u6765\u8fd1\u4f3c\u4e00\u7c7bDA\u95ee\u9898\u4e2d\u7684\u9006Hessian\u7b97\u5b50\u3002", "result": "\u5728\u7ebf\u6027\u5e73\u6d41\u65b9\u7a0b\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6807\u51c6CG\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684FNO-CG\u65b9\u6cd5\u5c06\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u964d\u4f4e\u4e8662%\uff0c\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u4e8617%\u3002", "conclusion": "FNO-CG\u65b9\u6cd5\u5728\u75c5\u6001\u60c5\u51b5\u4e0b\u6548\u679c\u6700\u660e\u663e\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u5177\u6709\u6311\u6218\u6027\u7684DA\u95ee\u9898\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.22876", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22876", "abs": "https://arxiv.org/abs/2509.22876", "authors": ["Gabriela Pinto", "Palash Goyal", "Yiwen Song", "Souradip Chakraborty", "Zifeng Wang", "Tomas Pfister", "Hamid Palangi"], "title": "HEART: Emotionally-driven test-time scaling of Language Models", "comment": null, "summary": "Test-time scaling has shown considerable success in improving the performance\nof language models on complex reasoning tasks without requiring fine-tuning.\nHowever, current strategies such as self-reflection primarily focus on logical\nor structural refinement. They do not leverage the guiding potential of\naffective feedback. Inspired by psychological research showing that emotions\ncan modulate cognitive performance, we introduce HEART--a novel framework that\nuses emotionally-driven prompts for iterative self-correction. HEART provides\nfeedback on a model's incorrect response using a curated set of concise,\nemotionally charged phrases based on the six universal emotions categorized by\nDr. Paul Ekman. By systematically varying the emotional tone of the feedback\nacross iterations, our method guides the model to escape flawed reasoning paths\nand explore more promising alternatives. We evaluate our framework on\nchallenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,\nand SimpleQA. Our results reveal a significant new phenomenon: when guided by\nan oracle verifier, this affective iteration protocol unlocks significantly\ndeeper reasoning, leading to consistent and substantial increases in accuracy\nover state-of-the-art baselines with the same verifier. However, we also\nidentify a critical bottleneck for practical deployment. In a verifier-free\nsetting, it struggles to harness these gains consistently, highlighting as a\nkey challenge for future work. Our findings suggest that the next frontier in\nmachine reasoning may lie not just in refining logic, but also in understanding\nand leveraging the `HEART' of the models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHEART\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u60c5\u611f\u9a71\u52a8\u7684\u63d0\u793a\u8fdb\u884c\u8fed\u4ee3\u81ea\u6211\u7ea0\u6b63\uff0c\u4ee5\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u7b56\u7565\u4e3b\u8981\u5173\u6ce8\u903b\u8f91\u6216\u7ed3\u6784\u4e0a\u7684\u6539\u8fdb\uff0c\u800c\u6ca1\u6709\u5229\u7528\u60c5\u611f\u53cd\u9988\u7684\u6307\u5bfc\u6f5c\u529b\u3002\u53d7\u5230\u60c5\u7eea\u53ef\u4ee5\u8c03\u8282\u8ba4\u77e5\u8868\u73b0\u7684\u5fc3\u7406\u5b66\u7814\u7a76\u7684\u542f\u53d1\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8ePaul Ekman\u535a\u58eb\u5206\u7c7b\u7684\u516d\u79cd\u901a\u7528\u60c5\u611f\u7684\u3001\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b80\u6d01\u3001\u60c5\u611f\u5316\u7684\u77ed\u8bed\uff0c\u4e3a\u6a21\u578b\u7684\u4e0d\u6b63\u786e\u54cd\u5e94\u63d0\u4f9b\u53cd\u9988\u3002\u901a\u8fc7\u7cfb\u7edf\u5730\u6539\u53d8\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u53cd\u9988\u7684\u60c5\u611f\u57fa\u8c03\uff0c\u5f15\u5bfc\u6a21\u578b\u6446\u8131\u6709\u7f3a\u9677\u7684\u63a8\u7406\u8def\u5f84\uff0c\u63a2\u7d22\u66f4\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u57fa\u51c6\uff08\u5305\u62ecOlympiadBench\uff0cHumanity's Last Exam\u548cSimpleQA\uff09\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u7531oracle\u9a8c\u8bc1\u5668\u5f15\u5bfc\u65f6\uff0c\u8fd9\u79cd\u60c5\u611f\u8fed\u4ee3\u534f\u8bae\u53ef\u5b9e\u73b0\u66f4\u6df1\u5165\u7684\u63a8\u7406\uff0c\u4ece\u800c\u5728\u5177\u6709\u76f8\u540c\u9a8c\u8bc1\u5668\u7684\u6700\u65b0\u57fa\u7ebf\u4e0a\u5b9e\u73b0\u6301\u7eed\u4e14\u663e\u7740\u5730\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u673a\u5668\u63a8\u7406\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\u53ef\u80fd\u4e0d\u4ec5\u5728\u4e8e\u5b8c\u5584\u903b\u8f91\uff0c\u8fd8\u5728\u4e8e\u7406\u89e3\u548c\u5229\u7528\u6a21\u578b\u7684\u201cHEART\u201d\u3002"}}
{"id": "2509.22799", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22799", "abs": "https://arxiv.org/abs/2509.22799", "authors": ["Xuan He", "Dongfu Jiang", "Ping Nie", "Minghao Liu", "Zhengxuan Jiang", "Mingyi Su", "Wentao Ma", "Junru Lin", "Chun Ye", "Yi Lu", "Keming Wu", "Benjamin Schneider", "Quy Duc Do", "Zhuofeng Li", "Yiming Jia", "Yuxuan Zhang", "Guo Cheng", "Haozhe Wang", "Wangchunshu Zhou", "Qunshu Lin", "Yuanxing Zhang", "Ge Zhang", "Wenhao Huang", "Wenhu Chen"], "title": "VideoScore2: Think before You Score in Generative Video Evaluation", "comment": null, "summary": "Recent advances in text-to-video generation have produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassing visual quality,\nsemantic alignment, and physical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We present VideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluates visual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale dataset VideoFeedback2 containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline of supervised fine-tuning followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nenhance analytical robustness. Extensive experiments demonstrate that\nVideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),\nwhile providing interpretable assessments that bridge the gap between\nevaluation and controllable generation through effective reward modeling for\nBest-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/", "AI": {"tldr": "VideoScore2\uff1a\u4e00\u4e2a\u591a\u7ef4\u5ea6\u3001\u53ef\u89e3\u91ca\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5305\u62ec\u89c6\u89c9\u8d28\u91cf\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u5668\u548c\u5956\u52b1\u6a21\u578b\u5728\u8bc4\u4f30\u89c6\u9891\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u89c6\u9891\u8d28\u91cf\u7684\u7efc\u5408\u6027\u8d28\u3002", "method": "\u63d0\u51faVideoScore2\uff0c\u4e00\u4e2a\u591a\u7ef4\u5ea6\u7684\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f7f\u7528\u5305\u542b\u4eba\u7c7b\u6ce8\u91ca\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6VideoFeedback2\u3002", "result": "VideoScore2\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728VideoScore-Bench-v2\u4e0a\u8fbe\u523044.35\u7684\u51c6\u786e\u7387\uff0c\u5728\u56db\u4e2a\u9886\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8868\u73b0\u4e3a50.37\u3002", "conclusion": "VideoScore2\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u901a\u8fc7\u6709\u6548\u7684\u5956\u52b1\u5efa\u6a21\uff0c\u5f25\u5408\u4e86\u8bc4\u4f30\u548c\u53ef\u63a7\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.23130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23130", "abs": "https://arxiv.org/abs/2509.23130", "authors": ["Qian Cheng", "Ruize Tang", "Emilie Ma", "Finn Hackett", "Peiyang He", "Yiming Su", "Ivan Beschastnikh", "Yu Huang", "Xiaoxing Ma", "Tianyin Xu"], "title": "SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems", "comment": null, "summary": "Formal models are essential to specifying large, complex computer systems and\nverifying their correctness, but are notoriously expensive to write and\nmaintain. Recent advances in generative AI show promise in generating certain\nforms of specifications. However, existing work mostly targets small code, not\ncomplete systems. It is unclear whether AI can deal with realistic system\nartifacts, as this requires abstracting their complex behavioral properties\ninto formal models. We present SysMoBench, a benchmark that evaluates AI's\nability to formally model large, complex systems. We focus on concurrent and\ndistributed systems, which are keystones of today's critical computing\ninfrastructures, encompassing operating systems and cloud infrastructure. We\nuse TLA+, the it de facto specification language for concurrent and distributed\nsystems, though the benchmark can be extended to other specification languages.\nWe address the primary challenge of evaluating AI-generated models by\nautomating metrics like syntactic and runtime correctness, conformance to\nsystem code, and invariant correctness. SysMoBench currently includes nine\ndiverse system artifacts: the Raft implementation of Etcd and Redis, the\nSpinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively\nadded. SysMoBench enables us to understand the capabilities and limitations of\ntoday's LLMs and agents, putting tools in this area on a firm footing and\nopening up promising new research directions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a SysMoBench \u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u5728\u6b63\u5f0f\u5efa\u6a21\u5927\u578b\u590d\u6742\u7cfb\u7edf\u65b9\u9762\u7684\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u5e76\u53d1\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u751f\u6210\u89c4\u8303\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u9488\u5bf9\u5c0f\u578b\u4ee3\u7801\uff0c\u800c\u4e0d\u662f\u5b8c\u6574\u7684\u7cfb\u7edf\u3002\u4eba\u5de5\u667a\u80fd\u662f\u5426\u53ef\u4ee5\u5904\u7406\u5b9e\u9645\u7684\u7cfb\u7edf\u5de5\u4ef6\u5c1a\u4e0d\u6e05\u695a\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u5c06\u5176\u590d\u6742\u7684\u884c\u4e3a\u5c5e\u6027\u62bd\u8c61\u4e3a\u6b63\u5f0f\u6a21\u578b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86 SysMoBench \u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u4f7f\u7528 TLA+ \u89c4\u8303\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u6307\u6807\uff08\u5982\u8bed\u6cd5\u548c\u8fd0\u884c\u65f6\u6b63\u786e\u6027\u3001\u4e0e\u7cfb\u7edf\u4ee3\u7801\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u4e0d\u53d8\u6027\u6b63\u786e\u6027\uff09\u6765\u8bc4\u4f30 AI \u751f\u6210\u7684\u6a21\u578b\u3002", "result": "SysMoBench \u76ee\u524d\u5305\u62ec\u4e5d\u4e2a\u4e0d\u540c\u7684\u7cfb\u7edf\u5de5\u4ef6\uff0c\u4f8b\u5982 Etcd \u548c Redis \u7684 Raft \u5b9e\u73b0\uff0c\u4ee5\u53ca Asterinas OS \u4e2d\u7684 Spinlock \u548c Mutex\u3002\u66f4\u591a\u7684\u5de5\u4ef6\u6b63\u5728\u79ef\u6781\u6dfb\u52a0\u4e2d\u3002", "conclusion": "SysMoBench \u4f7f\u6211\u4eec\u80fd\u591f\u4e86\u89e3\u5f53\u524d LLM \u548c\u4ee3\u7406\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u5de5\u5177\u5960\u5b9a\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u5f00\u8f9f\u6709\u5e0c\u671b\u7684\u65b0\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.22953", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22953", "abs": "https://arxiv.org/abs/2509.22953", "authors": ["Valentyn Melnychuk", "Stefan Feuerriegel"], "title": "GDR-learners: Orthogonal Learning of Generative Models for Potential Outcomes", "comment": null, "summary": "Various deep generative models have been proposed to estimate potential\noutcomes distributions from observational data. However, none of them have the\nfavorable theoretical property of general Neyman-orthogonality and, associated\nwith it, quasi-oracle efficiency and double robustness. In this paper, we\nintroduce a general suite of generative Neyman-orthogonal (doubly-robust)\nlearners that estimate the conditional distributions of potential outcomes. Our\nproposed GDR-learners are flexible and can be instantiated with many\nstate-of-the-art deep generative models. In particular, we develop GDR-learners\nbased on (a) conditional normalizing flows (which we call GDR-CNFs), (b)\nconditional generative adversarial networks (GDR-CGANs), (c) conditional\nvariational autoencoders (GDR-CVAEs), and (d) conditional diffusion models\n(GDR-CDMs). Unlike the existing methods, our GDR-learners possess the\nproperties of quasi-oracle efficiency and rate double robustness, and are thus\nasymptotically optimal. In a series of (semi-)synthetic experiments, we\ndemonstrate that our GDR-learners are very effective and outperform the\nexisting methods in estimating the conditional distributions of potential\noutcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u751f\u6210\u5f0fNeyman\u6b63\u4ea4\u5b66\u4e60\u5668\uff08GDR-Learner\uff09\u6765\u4f30\u8ba1\u6f5c\u5728\u7ed3\u679c\u7684\u6761\u4ef6\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e0d\u5177\u5907\u5e7f\u4e49Neyman\u6b63\u4ea4\u6027\uff0c\u56e0\u6b64\u4e0d\u5177\u5907\u51c6\u9884\u8a00\u6548\u7387\u548c\u53cc\u91cd\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8e\u6761\u4ef6\u6b63\u6001\u5316\u6d41\uff08GDR-CNFs\uff09\u3001\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GDR-CGANs\uff09\u3001\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08GDR-CVAEs\uff09\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08GDR-CDMs\uff09\u6784\u5efaGDR-Learner\u3002", "result": "GDR-Learner\u5177\u6709\u51c6\u9884\u8a00\u6548\u7387\u548c\u901f\u7387\u53cc\u91cd\u9c81\u68d2\u6027\uff0c\u5728\u534a\u5408\u6210\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GDR-Learner\u5728\u4f30\u8ba1\u6f5c\u5728\u7ed3\u679c\u7684\u6761\u4ef6\u5206\u5e03\u65b9\u9762\u975e\u5e38\u6709\u6548\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u662f\u6e10\u8fd1\u6700\u4f18\u7684\u3002"}}
{"id": "2509.23471", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23471", "abs": "https://arxiv.org/abs/2509.23471", "authors": ["Harshil Vejendla"], "title": "Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases", "comment": "EMNLP 2025 Main 12 pages, 6 figures", "summary": "Upgrading embedding models in production vector databases typically requires\nre-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor\n(ANN) index, leading to significant operational disruption and computational\ncost. This paper presents Drift-Adapter, a lightweight, learnable\ntransformation layer designed to bridge embedding spaces between model\nversions. By mapping new queries into the legacy embedding space, Drift-Adapter\nenables the continued use of the existing ANN index, effectively deferring full\nre-computation. We systematically evaluate three adapter parameterizations:\nOrthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on\na small sample of paired old and new embeddings. Experiments on MTEB text\ncorpora and a CLIP image model upgrade (1M items) show that Drift-Adapter\nrecovers 95-99% of the retrieval recall (Recall@10, MRR) of a full\nre-embedding, adding less than 10 microseconds of query latency. Compared to\noperational strategies like full re-indexing or dual-index serving,\nDrift-Adapter reduces recompute costs by over 100 times and facilitates\nupgrades with near-zero operational interruption. We analyze robustness to\nvaried model drift, training data size, scalability to billion-item systems,\nand the impact of design choices like diagonal scaling, demonstrating\nDrift-Adapter's viability as a pragmatic solution for agile model deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u53ef\u5b66\u4e60\u8f6c\u6362\u5c42\uff0c\u65e8\u5728\u6865\u63a5\u6a21\u578b\u7248\u672c\u4e4b\u95f4\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5141\u8bb8\u7ee7\u7eed\u4f7f\u7528\u73b0\u6709\u7684ANN\u7d22\u5f15\uff0c\u4ece\u800c\u6709\u6548\u5730\u5ef6\u8fdf\u4e86\u5b8c\u5168\u91cd\u65b0\u8ba1\u7b97\u3002", "motivation": "\u5728\u751f\u4ea7\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u5347\u7ea7\u5d4c\u5165\u6a21\u578b\u901a\u5e38\u9700\u8981\u91cd\u65b0\u7f16\u7801\u6574\u4e2a\u8bed\u6599\u5e93\u5e76\u91cd\u5efa\u8fd1\u4f3c\u6700\u8fd1\u90bb(ANN)\u7d22\u5f15\uff0c\u4ece\u800c\u5bfc\u81f4\u663e\u8457\u7684\u64cd\u4f5c\u4e2d\u65ad\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u6b63\u4ea4\u666e\u7f57\u514b\u9c81\u65af\u7279\uff0c\u4f4e\u79e9\u4eff\u5c04\u548c\u7d27\u51d1\u6b8b\u5deeMLP", "result": "\u5728MTEB\u6587\u672c\u8bed\u6599\u5e93\u548cCLIP\u56fe\u50cf\u6a21\u578b\u5347\u7ea7(1M\u9879\u76ee)\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDrift-Adapter\u6062\u590d\u4e86\u5b8c\u6574\u91cd\u65b0\u5d4c\u5165\u768495-99%\u7684\u68c0\u7d22\u53ec\u56de\u7387(Recall@10, MRR)\uff0c\u589e\u52a0\u4e86\u4e0d\u523010\u5fae\u79d2\u7684\u67e5\u8be2\u5ef6\u8fdf\u3002\u4e0e\u5b8c\u5168\u91cd\u65b0\u7d22\u5f15\u6216\u53cc\u7d22\u5f15\u670d\u52a1\u7b49\u64cd\u4f5c\u7b56\u7565\u76f8\u6bd4\uff0cDrift-Adapter\u5c06\u91cd\u65b0\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86100\u500d\u4ee5\u4e0a\uff0c\u5e76\u4ee5\u63a5\u8fd1\u96f6\u7684\u64cd\u4f5c\u4e2d\u65ad\u4fc3\u8fdb\u4e86\u5347\u7ea7\u3002", "conclusion": "\u6211\u4eec\u5206\u6790\u4e86\u5bf9\u5404\u79cd\u6a21\u578b\u6f02\u79fb\u3001\u8bad\u7ec3\u6570\u636e\u5927\u5c0f\u3001\u6570\u5341\u4ebf\u9879\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u5bf9\u89d2\u7f29\u653e\u7b49\u8bbe\u8ba1\u9009\u62e9\u7684\u5f71\u54cd\u7684\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86Drift-Adapter\u4f5c\u4e3a\u654f\u6377\u6a21\u578b\u90e8\u7f72\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.22887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22887", "abs": "https://arxiv.org/abs/2509.22887", "authors": ["EunJeong Hwang", "Yuwei Yin", "Giuseppe Carenini", "Peter West", "Vered Shwartz"], "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "comment": null, "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key\naspect of human social intelligence, yet, chatbots and LLM-based social agents\ndo not typically integrate it. In this work, we demonstrate that LLMs that\nexplicitly use ToM get better at dialogue, achieving goals more effectively.\nAfter showing that simply prompting models to generate mental states between\ndialogue turns already provides significant benefit, we further introduce\nToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM\nwith dialogue lookahead to produce mental states that are maximally useful for\nachieving dialogue goals. Experiments on the Sotopia interactive social\nevaluation benchmark demonstrate the effectiveness of our method over a range\nof baselines. Comprehensive analysis shows that ToMA exhibits more strategic,\ngoal-oriented reasoning behaviors, which enable long-horizon adaptation, while\nmaintaining better relationships with their partners. Our results suggest a\nstep forward in integrating ToM for building socially intelligent LLM agents.", "AI": {"tldr": "LLMs that explicitly use Theory of Mind (ToM) perform better in dialogues.", "motivation": "Chatbots and LLM-based social agents typically lack Theory of Mind (ToM), a key aspect of human social intelligence. This work aims to integrate ToM into LLMs to improve their dialogue capabilities.", "method": "The authors introduce ToMAgent (ToMA), a ToM-focused dialogue agent trained by pairing ToM with dialogue lookahead to produce mental states useful for achieving dialogue goals. They also show that prompting models to generate mental states between dialogue turns provides benefit.", "result": "Experiments on the Sotopia benchmark demonstrate the effectiveness of ToMA over baselines. ToMA exhibits more strategic, goal-oriented reasoning and maintains better relationships with partners.", "conclusion": "The results suggest a step forward in integrating ToM for building socially intelligent LLM agents."}}
{"id": "2509.22813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22813", "abs": "https://arxiv.org/abs/2509.22813", "authors": ["Sahar Dastani", "Ali Bahri", "Gustavo Adolfo Vargas Hakim", "Moslem Yazdanpanah", "Mehrdad Noori", "David Osowiechi", "Samuel Barbeau", "Ismail Ben Ayed", "Herve Lombaert", "Christian Desrosiers"], "title": "TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses", "comment": null, "summary": "State Space Models (SSMs) have emerged as efficient alternatives to Vision\nTransformers (ViTs), with VMamba standing out as a pioneering architecture\ndesigned for vision tasks. However, their generalization performance degrades\nsignificantly under distribution shifts. To address this limitation, we propose\nTRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel\ntest-time adaptation (TTA) method that leverages diverse traversal permutations\nto generate multiple causal perspectives of the input image. Model predictions\nserve as pseudo-labels to guide updates of the Mamba-specific parameters, and\nthe adapted weights are averaged to integrate the learned information across\ntraversal scans. Altogether, TRUST is the first approach that explicitly\nleverages the unique architectural properties of SSMs for adaptation.\nExperiments on seven benchmarks show that TRUST consistently improves\nrobustness and outperforms existing TTA methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\uff0c\u540d\u4e3aTRUST\uff0c\u5229\u7528\u4e0d\u540c\u7684\u904d\u5386\u6392\u5217\u751f\u6210\u8f93\u5165\u56fe\u50cf\u7684\u591a\u4e2a\u56e0\u679c\u89c6\u89d2\uff0c\u4ee5\u89e3\u51b3VMamba\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "VMamba\u4f5c\u4e3a\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\u7684\u5f00\u521b\u6027\u67b6\u6784\uff0c\u4f46\u5176\u6cdb\u5316\u6027\u80fd\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4f1a\u663e\u8457\u4e0b\u964d\u3002", "method": "\u5229\u7528\u4e0d\u540c\u7684\u904d\u5386\u6392\u5217\u751f\u6210\u8f93\u5165\u56fe\u50cf\u7684\u591a\u4e2a\u56e0\u679c\u89c6\u89d2\u3002\u6a21\u578b\u9884\u6d4b\u7528\u4f5c\u4f2a\u6807\u7b7e\uff0c\u4ee5\u6307\u5bfcMamba\u7279\u5b9a\u53c2\u6570\u7684\u66f4\u65b0\uff0c\u5e76\u5e73\u5747\u8c03\u6574\u540e\u7684\u6743\u91cd\u4ee5\u6574\u5408\u8de8\u904d\u5386\u626b\u63cf\u7684\u5b66\u4e60\u4fe1\u606f\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTRUST\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u7684TTA\u65b9\u6cd5\u3002", "conclusion": "TRUST\u662f\u7b2c\u4e00\u79cd\u660e\u786e\u5229\u7528SSM\u7684\u72ec\u7279\u67b6\u6784\u5c5e\u6027\u8fdb\u884c\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.23143", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23143", "abs": "https://arxiv.org/abs/2509.23143", "authors": ["Charles L. Wang"], "title": "MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning", "comment": null, "summary": "This paper presents MathBode, a dynamic diagnostic for mathematical reasoning\nin large language models (LLMs). Instead of one-shot accuracy, MathBode treats\neach parametric problem as a system: we drive a single parameter sinusoidally\nand fit first-harmonic responses of model outputs and exact solutions. This\nyields interpretable, frequency-resolved metrics -- gain (amplitude tracking)\nand phase (lag) -- that form Bode-style fingerprints. Across five closed-form\nfamilies (linear solve, ratio/saturation, compound interest, 2x2 linear\nsystems, similar triangles), the diagnostic surfaces systematic low-pass\nbehavior and growing phase lag that accuracy alone obscures. We compare several\nmodels against a symbolic baseline that calibrates the instrument ($G \\approx\n1$, $\\phi \\approx 0$). Results separate frontier from mid-tier models on\ndynamics, providing a compact, reproducible protocol that complements standard\nbenchmarks with actionable measurements of reasoning fidelity and consistency.\nWe open-source the dataset and code to enable further research and adoption.", "AI": {"tldr": "MathBode: \u52a8\u6001\u8bca\u65ad\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6b63\u5f26\u65b9\u5f0f\u9a71\u52a8\u53c2\u6570\uff0c\u89c2\u5bdf\u6a21\u578b\u8f93\u51fa\u548c\u7cbe\u786e\u89e3\u7684\u4e00\u9636\u8c10\u6ce2\u54cd\u5e94\u3002", "motivation": "\u4f20\u7edf\u7684\u4e00\u6b21\u6027\u51c6\u786e\u7387\u8bc4\u4f30\u65e0\u6cd5\u5145\u5206\u63ed\u793aLLM\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "\u5c06\u53c2\u6570\u5316\u95ee\u9898\u89c6\u4e3a\u7cfb\u7edf\uff0c\u7528\u6b63\u5f26\u6ce2\u9a71\u52a8\u5355\u4e2a\u53c2\u6570\uff0c\u62df\u5408\u6a21\u578b\u8f93\u51fa\u548c\u7cbe\u786e\u89e3\u7684\u4e00\u9636\u8c10\u6ce2\u54cd\u5e94\uff0c\u5f97\u5230\u53ef\u89e3\u91ca\u7684\u3001\u9891\u7387\u89e3\u6790\u7684\u6307\u6807\uff08\u589e\u76ca\u548c\u76f8\u4f4d\uff09\uff0c\u5f62\u6210Bode\u56fe\u98ce\u683c\u7684\u6307\u7eb9\u3002", "result": "\u5728\u4e94\u4e2a\u95ed\u5f0f\u95ee\u9898\u65cf\u4e0a\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u4f4e\u901a\u884c\u4e3a\u548c\u4e0d\u65ad\u589e\u957f\u7684\u76f8\u4f4d\u6ede\u540e\uff0c\u800c\u8fd9\u4e9b\u73b0\u8c61\u662f\u4f20\u7edf\u51c6\u786e\u7387\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u7684\u3002\u901a\u8fc7\u4e0e\u7b26\u53f7\u57fa\u7ebf\u7684\u6bd4\u8f83\uff0c\u533a\u5206\u4e86\u524d\u6cbf\u6a21\u578b\u548c\u4e2d\u7b49\u6a21\u578b\u5728\u52a8\u6001\u7279\u6027\u4e0a\u7684\u5dee\u5f02\u3002", "conclusion": "MathBode\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u53ef\u590d\u73b0\u7684\u534f\u8bae\uff0c\u53ef\u4ee5\u8865\u5145\u6807\u51c6\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u63a8\u7406\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u6d4b\u91cf\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.22957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22957", "abs": "https://arxiv.org/abs/2509.22957", "authors": ["Luke Guerdan", "Justin Whitehouse", "Kimberly Truong", "Kenneth Holstein", "Zhiwei Steven Wu"], "title": "Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas", "comment": null, "summary": "As Generative AI (GenAI) systems see growing adoption, a key concern involves\nthe external validity of evaluations, or the extent to which they generalize\nfrom lab-based to real-world deployment conditions. Threats to the external\nvalidity of GenAI evaluations arise when the source sample of human raters and\nsystem outputs used to obtain a system quality estimate differs from the target\ndistribution at deployment time. In this work, we propose a doubly-robust\nestimation framework designed to address this evaluation sampling bias. Key to\nour approach is the use of \"persona\" ratings produced by prompting an LLM\nevaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific\nsociodemographic characteristics. Our doubly-robust framework combines these\ninformative yet imperfect persona ratings with human ratings obtained under\nevaluation sampling bias to produce statistically valid system quality\nestimates. In particular, we show that our approach yields valid system quality\nestimates when either (i) a model trained to predict human ratings using\npersona ratings and source data observed under sampling bias, or (ii) a\nreweighting model that corrects for sampling bias is of sufficient quality. We\nvalidate our framework theoretically and via a novel Persona Simulation\nFramework (PSF) designed to systematically manipulate persona quality and the\ndegree of evaluation sampling bias present in source data. Our work provides a\nprincipled foundation for combining imperfect persona ratings with human\nratings observed under sampling bias to obtain valid system quality estimates.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u9c81\u68d2\u7684\u4f30\u8ba1\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3 GenAI \u8bc4\u4f30\u4e2d\u7684\u5916\u90e8\u6709\u6548\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u5b8c\u5584\u7684 persona \u8bc4\u5206\u548c\u5728\u62bd\u6837\u504f\u5dee\u4e0b\u83b7\u5f97\u7684\u4eba\u5de5\u8bc4\u5206\uff0c\u4ee5\u83b7\u5f97\u6709\u6548\u7684\u7cfb\u7edf\u8d28\u91cf\u4f30\u8ba1\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u7cfb\u7edf\u5f97\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5e94\u7528\uff0c\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u662f\u8bc4\u4f30\u7684\u5916\u90e8\u6709\u6548\u6027\uff0c\u6216\u8005\u8bf4\u5b83\u4eec\u4ece\u57fa\u4e8e\u5b9e\u9a8c\u5ba4\u5230\u771f\u5b9e\u90e8\u7f72\u6761\u4ef6\u4e0b\u7684\u63a8\u5e7f\u7a0b\u5ea6\u3002\u5f53\u7528\u4e8e\u83b7\u5f97\u7cfb\u7edf\u8d28\u91cf\u4f30\u8ba1\u7684\u4eba\u5de5\u8bc4\u4f30\u5458\u548c\u7cfb\u7edf\u8f93\u51fa\u7684\u6e90\u6837\u672c\u4e0e\u90e8\u7f72\u65f6\u7684\u76ee\u6807\u5206\u5e03\u4e0d\u540c\u65f6\uff0c\u5c31\u4f1a\u51fa\u73b0\u5bf9 GenAI \u8bc4\u4f30\u7684\u5916\u90e8\u6709\u6548\u6027\u7684\u5a01\u80c1\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u7531 LLM \u8bc4\u4f30\u5668\u751f\u6210\u7684\u201c\u89d2\u8272\u201d\u8bc4\u5206\uff0c\u8be5\u8bc4\u4f30\u5668\u88ab\u63d0\u793a\u8868\u73b0\u4e3a\u5177\u6709\u7279\u5b9a\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u7684\u4eba\u5de5\u8bc4\u4f30\u5458\u3002\u8be5\u53cc\u91cd\u9c81\u68d2\u6846\u67b6\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4e30\u5bcc\u4f46\u4e0d\u5b8c\u5584\u7684\u89d2\u8272\u8bc4\u5206\u4e0e\u5728\u8bc4\u4f30\u62bd\u6837\u504f\u5dee\u4e0b\u83b7\u5f97\u7684\u4eba\u5de5\u8bc4\u5206\u76f8\u7ed3\u5408\uff0c\u4ee5\u4ea7\u751f\u5177\u6709\u7edf\u8ba1\u6709\u6548\u6027\u7684\u7cfb\u7edf\u8d28\u91cf\u4f30\u8ba1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ee5\u4e0b\u60c5\u51b5\u4e0b\u4ea7\u751f\u6709\u6548\u7684\u7cfb\u7edf\u8d28\u91cf\u4f30\u8ba1\uff1a(i) \u4f7f\u7528\u89d2\u8272\u8bc4\u5206\u548c\u5728\u62bd\u6837\u504f\u5dee\u4e0b\u89c2\u5bdf\u5230\u7684\u6e90\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4eba\u5de5\u8bc4\u5206\uff0c\u6216 (ii) \u7528\u4e8e\u6821\u6b63\u62bd\u6837\u504f\u5dee\u7684\u91cd\u52a0\u6743\u6a21\u578b\u5177\u6709\u8db3\u591f\u7684\u8d28\u91cf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7ed3\u5408\u4e0d\u5b8c\u5584\u7684\u89d2\u8272\u8bc4\u5206\u548c\u5728\u62bd\u6837\u504f\u5dee\u4e0b\u89c2\u5bdf\u5230\u7684\u4eba\u5de5\u8bc4\u5206\uff0c\u4ee5\u83b7\u5f97\u6709\u6548\u7684\u7cfb\u7edf\u8d28\u91cf\u4f30\u8ba1\uff0c\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2509.22906", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22906", "abs": "https://arxiv.org/abs/2509.22906", "authors": ["Henrique Godoy"], "title": "Extract-0: A Specialized Language Model for Document Information Extraction", "comment": null, "summary": "This paper presents Extract-0, a 7-billion parameter language model\nspecifically optimized for document information extraction that achieves\nperformance exceeding models with parameter counts several orders of magnitude\nlarger. Through a novel combination of synthetic data generation, supervised\nfine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via\nGroup Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of\n0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming\nGPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology\nemploys a memory-preserving synthetic data generation pipeline that produces\n280,128 training examples from diverse document sources, followed by\nparameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M\nout of 7.66B parameters). The reinforcement learning phase introduces a novel\nsemantic similarity-based reward function that handles the inherent ambiguity\nin information extraction tasks. This research demonstrates that task-specific\noptimization can yield models that surpass general-purpose systems while\nrequiring substantially fewer computational resource.", "AI": {"tldr": "Extract-0\u662f\u4e00\u4e2a70\u4ebf\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u4e3a\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4f18\u5316\uff0c\u6027\u80fd\u8d85\u8fc7\u53c2\u6570\u91cf\u5927\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u7684\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u4f7f\u7528LoRA\u7684\u76d1\u7763\u5fae\u8c03\u548c\u4f7f\u7528GRPO\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u7ec4\u5408\u3002", "result": "Extract-0\u57281,000\u4e2a\u4e0d\u540c\u7684\u6587\u6863\u63d0\u53d6\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u5956\u52b1\u4e3a0.573\uff0c\u4f18\u4e8eGPT-4.1 (0.457)\u3001o3 (0.464)\u548cGPT-4.1-2025 (0.459)\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u7684\u4f18\u5316\u53ef\u4ee5\u4ea7\u751f\u8d85\u8d8a\u901a\u7528\u7cfb\u7edf\u7684\u6a21\u578b\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2509.22820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22820", "abs": "https://arxiv.org/abs/2509.22820", "authors": ["Jaeik Kim", "Woojin Kim", "Woohyeon Park", "Jaeyoung Do"], "title": "MMPB: It's Time for Multi-Modal Personalization", "comment": null, "summary": "Visual personalization is essential in user-facing AI systems such as smart\nhomes and healthcare, where aligning model behavior with user-centric concepts\nis critical. However, recent large Vision-Language Models (VLMs), despite their\nbroad applicability, remain underexplored in their ability to adapt to\nindividual users. In this paper, we introduce MMPB, the first extensive\nbenchmark for evaluating VLMs on personalization. MMPB comprises 10k\nimage-query pairs and includes 111 personalizable concepts across four\ncategories: humans, animals, objects, and characters, with the human category\nenriched with preference-grounded queries. We structure personalization into\nthree main task types, each highlighting a different key property of VLMs.\nUsing 23 widely used VLMs including both open- and closed-source models, we\nevaluate personalization performance via a three-stage protocol: concept\ninjection, multi-turn dialogue, and personalized querying. Our findings\nindicate that most VLMs (including some closed-source models) struggle with\npersonalization, particularly in maintaining consistency over dialogue,\nhandling user preferences, and adapting to visual cues. Our analysis reveals\nthat the challenges in VLM personalization (such as refusal behaviors and\nlong-context forgetting) highlight substantial room for improvement. By\nidentifying these limitations and offering a scalable benchmark, MMPB offers\nvaluable insights and a solid foundation for future research toward truly\npersonalized multi-modal AI. Project Page: aidaslab.github.io/MMPB", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5MMPB\uff0c\u5305\u542b10k\u56fe\u50cf-\u67e5\u8be2\u5bf9\u548c111\u4e2a\u53ef\u4e2a\u6027\u5316\u6982\u5ff5\u3002", "motivation": "\u89c6\u89c9\u4e2a\u6027\u5316\u5728\u667a\u80fd\u5bb6\u5c45\u548c\u533b\u7597\u4fdd\u5065\u7b49\u9762\u5411\u7528\u6237\u7684AI\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86MMPB\u57fa\u51c6\uff0c\u5305\u542b\u6982\u5ff5\u6ce8\u5165\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u4e2a\u6027\u5316\u67e5\u8be2\u4e09\u4e2a\u9636\u6bb5\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u4f7f\u752823\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5927\u591a\u6570\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u4fdd\u6301\u5bf9\u8bdd\u4e00\u81f4\u6027\u3001\u5904\u7406\u7528\u6237\u504f\u597d\u548c\u9002\u5e94\u89c6\u89c9\u63d0\u793a\u65b9\u9762\u3002", "conclusion": "MMPB\u57fa\u51c6\u7684\u63d0\u51fa\u80fd\u591f\u5e2e\u52a9\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u57fa\u7840\u3002"}}
{"id": "2509.23144", "categories": ["cs.AI", "cond-mat.stat-mech", "cs.MA", "nlin.AO", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2509.23144", "abs": "https://arxiv.org/abs/2509.23144", "authors": ["Atma Anand"], "title": "Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence", "comment": "11 pages, 1 figure, 6 pages supplementary material, submitted to\n  Physical Review E", "summary": "Information-processing systems coordinating across multiple agents and\nobjectives face fundamental thermodynamic constraints. We show that solutions\nwith maximum utility to act as coordination focal points have much higher\nselection pressure for being findable across agents rather than accuracy. We\nderive that the information-theoretic minimum description length of\ncoordination protocols to precision $\\varepsilon$ scales as $L(P)\\geq NK\\log_2\nK+N^2d^2\\log (1/\\varepsilon)$ for $N$ agents with $d$ potentially conflicting\nobjectives and internal model complexity $K$. This scaling forces progressive\nsimplification, with coordination dynamics changing the environment itself and\nshifting optimization across hierarchical levels. Moving from established focal\npoints requires re-coordination, creating persistent metastable states and\nhysteresis until significant environmental shifts trigger phase transitions\nthrough spontaneous symmetry breaking. We operationally define coordination\ntemperature to predict critical phenomena and estimate coordination work costs,\nidentifying measurable signatures across systems from neural networks to\nrestaurant bills to bureaucracies. Extending the topological version of Arrow's\ntheorem on the impossibility of consistent preference aggregation, we find it\nrecursively binds whenever preferences are combined. This potentially explains\nthe indefinite cycling in multi-objective gradient descent and alignment faking\nin Large Language Models trained with reinforcement learning with human\nfeedback. We term this framework Thermodynamic Coordination Theory (TCT), which\ndemonstrates that coordination requires radical information loss.", "AI": {"tldr": "\u534f\u8c03\u7cfb\u7edf\u9762\u4e34\u70ed\u529b\u5b66\u7ea6\u675f\uff0c\u6700\u5927\u6548\u7528\u89e3\u66f4\u6ce8\u91cd\u53ef\u53d1\u73b0\u6027\u800c\u975e\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u8de8\u591a\u4e2a\u667a\u80fd\u4f53\u548c\u76ee\u6807\u7684\u534f\u8c03\u7cfb\u7edf\u4e2d\u7684\u57fa\u672c\u70ed\u529b\u5b66\u7ea6\u675f\u3002", "method": "\u63a8\u5bfc\u4e86\u7cbe\u5ea6\u4e3a $\\varepsilon$ \u7684\u534f\u8c03\u534f\u8bae\u7684\u4fe1\u606f\u8bba\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u7684\u7f29\u653e\u6bd4\u4f8b\u3002", "result": "\u53d1\u73b0\u534f\u8c03\u534f\u8bae\u7684\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u3001\u76ee\u6807\u51b2\u7a81\u7a0b\u5ea6\u548c\u5185\u90e8\u6a21\u578b\u590d\u6742\u5ea6\u6709\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u534f\u8c03\u52a8\u6001\u5982\u4f55\u6539\u53d8\u73af\u5883\u4ee5\u53ca\u4f18\u5316\u5982\u4f55\u5728\u5c42\u7ea7\u95f4\u8f6c\u79fb\u3002\u8fd8\u53d1\u73b0\u4ece\u5df2\u5efa\u7acb\u7684\u7126\u70b9\u8f6c\u79fb\u9700\u8981\u91cd\u65b0\u534f\u8c03\uff0c\u4ece\u800c\u5bfc\u81f4\u6301\u4e45\u7684\u4e9a\u7a33\u6001\u548c\u6ede\u540e\u73b0\u8c61\u3002", "conclusion": "\u63d0\u51fa\u4e86\u70ed\u529b\u5b66\u534f\u8c03\u7406\u8bba\uff08TCT\uff09\uff0c\u8868\u660e\u534f\u8c03\u9700\u8981 \u0440\u0430\u0434\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u4fe1\u606f\u635f\u5931\uff0c\u5e76\u89e3\u91ca\u4e86\u591a\u76ee\u6807\u68af\u5ea6\u4e0b\u964d\u4e2d\u7684\u65e0\u9650\u5faa\u73af\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5bf9\u9f50\u4f2a\u9020\u73b0\u8c61\u3002"}}
{"id": "2509.22963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22963", "abs": "https://arxiv.org/abs/2509.22963", "authors": ["Haitong Ma", "Ofir Nabati", "Aviv Rosenberg", "Bo Dai", "Oran Lang", "Idan Szpektor", "Craig Boutilier", "Na Li", "Shie Mannor", "Lior Shani", "Guy Tenneholtz"], "title": "Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces", "comment": "22 pages, 10 figures. Haitong Ma and Ofir Nabati contributed equally\n  to this paper", "summary": "Reinforcement learning (RL) struggles to scale to large, combinatorial action\nspaces common in many real-world problems. This paper introduces a novel\nframework for training discrete diffusion models as highly effective policies\nin these complex settings. Our key innovation is an efficient online training\nprocess that ensures stable and effective policy improvement. By leveraging\npolicy mirror descent (PMD) to define an ideal, regularized target policy\ndistribution, we frame the policy update as a distributional matching problem,\ntraining the expressive diffusion model to replicate this stable target. This\ndecoupled approach stabilizes learning and significantly enhances training\nperformance. Our method achieves state-of-the-art results and superior sample\nefficiency across a diverse set of challenging combinatorial benchmarks,\nincluding DNA sequence generation, RL with macro-actions, and multi-agent\nsystems. Experiments demonstrate that our diffusion policies attain superior\nperformance compared to other baselines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u73af\u5883\u4e2d\u8bad\u7ec3\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u5176\u6210\u4e3a\u9ad8\u6548\u7684\u7b56\u7565\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u6269\u5c55\u5230\u5927\u578b\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\uff0c\u800c\u5b9e\u9645\u95ee\u9898\u4e2d\u5f88\u5e38\u89c1\u8fd9\u79cd\u52a8\u4f5c\u7a7a\u95f4\u3002", "method": "\u5229\u7528\u7b56\u7565\u955c\u50cf\u4e0b\u964d\uff08PMD\uff09\u5b9a\u4e49\u4e00\u4e2a\u7406\u60f3\u7684\u3001\u6b63\u5219\u5316\u7684\u76ee\u6807\u7b56\u7565\u5206\u5e03\uff0c\u5e76\u5c06\u7b56\u7565\u66f4\u65b0\u6784\u5efa\u4e3a\u4e00\u4e2a\u5206\u5e03\u5339\u914d\u95ee\u9898\uff0c\u8bad\u7ec3\u6709\u8868\u8fbe\u529b\u7684\u6269\u6563\u6a21\u578b\u6765\u590d\u5236\u8fd9\u4e2a\u7a33\u5b9a\u7684\u76ee\u6807\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u548c\u5353\u8d8a\u7684\u6837\u672c\u6548\u7387\uff0c\u5305\u62ecDNA\u5e8f\u5217\u751f\u6210\u3001\u5e26\u6709\u5b8f\u52a8\u4f5c\u7684RL\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6269\u6563\u7b56\u7565\u83b7\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.23742", "categories": ["cs.LG", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23742", "abs": "https://arxiv.org/abs/2509.23742", "authors": ["Yewang Chen", "Junfeng Li", "Shuyin Xia", "Qinghong Lai", "Xinbo Gao", "Guoyin Wang", "Dongdong Cheng", "Yi Liu", "Yi Wang"], "title": "GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data", "comment": null, "summary": "To effectively handle clustering task for large-scale datasets, we propose a\nnovel scalable skeleton clustering algorithm, namely GBSK, which leverages the\ngranular-ball technique to capture the underlying structure of data. By\nmulti-sampling the dataset and constructing multi-grained granular-balls, GBSK\nprogressively uncovers a statistical \"skeleton\" -- a spatial abstraction that\napproximates the essential structure and distribution of the original data.\nThis strategy enables GBSK to dramatically reduce computational overhead while\nmaintaining high clustering accuracy. In addition, we introduce an adaptive\nversion, AGBSK, with simplified parameter settings to enhance usability and\nfacilitate deployment in real-world scenarios. Extensive experiments conducted\non standard computing hardware demonstrate that GBSK achieves high efficiency\nand strong clustering performance on large-scale datasets, including one with\nup to 100 million instances across 256 dimensions. Our implementation and\nexperimental results are available at: https://github.com/XFastDataLab/GBSK/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u6269\u5c55\u9aa8\u67b6\u805a\u7c7b\u7b97\u6cd5GBSK\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u7c92\u7403\u6280\u672f\u6355\u83b7\u6570\u636e\u7684\u5e95\u5c42\u7ed3\u6784\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u805a\u7c7b\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u591a\u91cd\u62bd\u6837\u5e76\u6784\u5efa\u591a\u7c92\u5ea6\u7c92\u7403\uff0cGBSK\u9010\u6b65\u53d1\u73b0\u4e00\u4e2a\u7edf\u8ba1\u201c\u9aa8\u67b6\u201d\u2014\u2014\u4e00\u79cd\u7a7a\u95f4\u62bd\u8c61\uff0c\u5b83\u8fd1\u4f3c\u4e8e\u539f\u59cb\u6570\u636e\u7684\u57fa\u672c\u7ed3\u6784\u548c\u5206\u5e03\u3002", "result": "GBSK\u5728\u6807\u51c6\u8ba1\u7b97\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u5f3a\u5927\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u805a\u7c7b\u6027\u80fd\uff0c\u5305\u62ec\u4e00\u4e2a\u5177\u6709\u9ad8\u8fbe1\u4ebf\u4e2a\u5b9e\u4f8b\u548c256\u4e2a\u7ef4\u5ea6\u7684\u6570\u636e\u96c6\u3002", "conclusion": "GBSK \u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u805a\u7c7b\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u805a\u7c7b\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7248\u672cAGBSK\uff0c\u5b83\u5177\u6709\u7b80\u5316\u7684\u53c2\u6570\u8bbe\u7f6e\uff0c\u4ee5\u589e\u5f3a\u53ef\u7528\u6027\uff0c\u5e76\u6709\u52a9\u4e8e\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2509.22926", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22926", "abs": "https://arxiv.org/abs/2509.22926", "authors": ["Kelli Henry", "Steven Xu", "Kaitlin Blotske", "Moriah Cargile", "Erin F. Barreto", "Brian Murray", "Susan Smith", "Seth R. Bauer", "Yanjun Gao", "Tianming Liu", "Andrea Sikora"], "title": "Large language models management of medications: three performance analyses", "comment": null, "summary": "Background: Large language models (LLMs) can be useful in diagnosing medical\nconditions, but few studies have evaluated their consistency in recommending\nappropriate medication regimens. The purpose of this evaluation was to test\nGPT-4o on three medication benchmarking tests including mapping a drug name to\nits correct formulation, identifying drug-drug interactions using both its\ninternal knowledge and using a web search, and preparing a medication order\nsentence after being given the medication name. Methods: Using GTP-4o three\nexperiments were completed. Accuracy was quantified by computing cosine\nsimilarity on TF-IDF vectors, normalized Levenshtein similarity, and\nROUGE-1/ROUGE-L F1 between each response and its reference string or by manual\nevaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation\nmatching, with frequent omissions of available drug formulations (mean 1.23 per\nmedication) and hallucinations of formulations that do not exist (mean 1.14 per\nmedication). Only 49% of tested medications were correctly matched to all\navailable formulations. Accuracy was decreased for medications with more\nformulations (p<0.0001). GPT-4o was also inconsistent at identifying\ndrug-drug-interactions, although it had better performance with the\nsearch-augmented assessment compared to its internal knowledge (54.7% vs.\n69.2%, p=0.013). However, allowing a web-search worsened performance when there\nwas no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,\nGPT-4o performed moderately with preparing a medication order sentence, with\nonly 65.8% of medication order sentences containing no medication or\nabbreviation errors. Conclusions: Model performance was overall poor for all\ntests. This highlights the need for domain-specific training through\nclinician-annotated datasets and a comprehensive evaluation framework for\nbenchmarking performance.", "AI": {"tldr": "GPT-4o \u5728\u836f\u7269\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5305\u62ec\u836f\u7269\u5236\u5242\u5339\u914d\u3001\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u8bc6\u522b\u548c\u751f\u6210\u836f\u7269\u533b\u5631\u8bed\u53e5\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u5408\u9002\u7684\u836f\u7269\u6cbb\u7597\u65b9\u6848\u65f6\u7684\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528 GPT-4o \u5b8c\u6210\u4e09\u4e2a\u5b9e\u9a8c\uff0c\u5305\u62ec\u836f\u7269\u540d\u79f0\u5230\u6b63\u786e\u5236\u5242\u7684\u6620\u5c04\uff0c\u4f7f\u7528\u5176\u5185\u90e8\u77e5\u8bc6\u548c\u7f51\u7edc\u641c\u7d22\u8bc6\u522b\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u53ca\u5728\u7ed9\u51fa\u836f\u7269\u540d\u79f0\u540e\u51c6\u5907\u836f\u7269\u533b\u5631\u8bed\u53e5\u3002\u4f7f\u7528 TF-IDF \u5411\u91cf\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6807\u51c6\u5316 Levenshtein \u76f8\u4f3c\u5ea6\u548c ROUGE-1/ROUGE-L F1 \u4ee5\u53ca\u4e34\u5e8a\u533b\u751f\u7684\u624b\u52a8\u8bc4\u4f30\u6765\u91cf\u5316\u51c6\u786e\u6027\u3002", "result": "GPT-4o \u5728\u836f\u7269\u5236\u5242\u5339\u914d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7ecf\u5e38\u9057\u6f0f\u53ef\u7528\u7684\u836f\u7269\u5236\u5242\uff0c\u5e76\u4e14\u51fa\u73b0\u4e0d\u5b58\u5728\u7684\u5236\u5242\u3002\u5728\u8bc6\u522b\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\u4e5f\u4e0d\u4e00\u81f4\uff0c\u867d\u7136\u5728\u641c\u7d22\u589e\u5f3a\u8bc4\u4f30\u4e2d\u6bd4\u5185\u90e8\u77e5\u8bc6\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u6ca1\u6709\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u65f6\uff0c\u5141\u8bb8\u7f51\u7edc\u641c\u7d22\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u5728\u51c6\u5907\u836f\u7269\u533b\u5631\u8bed\u53e5\u65b9\u9762\u8868\u73b0\u4e00\u822c\uff0c\u53ea\u6709 65.8% \u7684\u836f\u7269\u533b\u5631\u8bed\u53e5\u4e0d\u5305\u542b\u836f\u7269\u6216\u7f29\u5199\u9519\u8bef\u3002", "conclusion": "\u6a21\u578b\u5728\u6240\u6709\u6d4b\u8bd5\u4e2d\u603b\u4f53\u8868\u73b0\u8f83\u5dee\u3002\u8fd9\u7a81\u51fa\u4e86\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u4ee5\u53ca\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.22836", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22836", "abs": "https://arxiv.org/abs/2509.22836", "authors": ["Roie Kazoom", "Alon Goldberg", "Hodaya Cohen", "Ofer Hadar"], "title": "Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN", "comment": null, "summary": "Adversarial patch attacks pose a severe threat to deep neural networks, yet\nmost existing approaches rely on unrealistic white-box assumptions, untargeted\nobjectives, or produce visually conspicuous patches that limit real-world\napplicability. In this work, we introduce a novel framework for fully\ncontrollable adversarial patch generation, where the attacker can freely choose\nboth the input image x and the target class y target, thereby dictating the\nexact misclassification outcome. Our method combines a generative U-Net design\nwith Grad-CAM-guided patch placement, enabling semantic-aware localization that\nmaximizes attack effectiveness while preserving visual realism. Extensive\nexperiments across convolutional networks (DenseNet-121, ResNet-50) and vision\ntransformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach\nachieves state-of-the-art performance across all settings, with attack success\nrates (ASR) and target-class success (TCS) consistently exceeding 99%.\n  Importantly, we show that our method not only outperforms prior white-box\nattacks and untargeted baselines, but also surpasses existing non-realistic\napproaches that produce detectable artifacts. By simultaneously ensuring\nrealism, targeted control, and black-box applicability-the three most\nchallenging dimensions of patch-based attacks-our framework establishes a new\nbenchmark for adversarial robustness research, bridging the gap between\ntheoretical attack strength and practical stealthiness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u8865\u4e01\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u5b8c\u5168\u53ef\u63a7\u6027\uff0c\u80fd\u591f\u81ea\u7531\u9009\u62e9\u8f93\u5165\u56fe\u50cf\u548c\u76ee\u6807\u7c7b\u522b\uff0c\u4ece\u800c\u63a7\u5236\u7cbe\u786e\u7684\u9519\u8bef\u5206\u7c7b\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e0d\u5207\u5b9e\u9645\u7684\u767d\u76d2\u5047\u8bbe\u3001\u65e0\u76ee\u6807\u7684\u5bf9\u8c61\uff0c\u6216\u4ea7\u751f\u89c6\u89c9\u4e0a\u660e\u663e\u7684\u8865\u4e01\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u751f\u6210\u5f0f U-Net \u8bbe\u8ba1\u548c Grad-CAM \u5f15\u5bfc\u7684\u8865\u4e01\u653e\u7f6e\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u611f\u77e5\u7684\u5b9a\u4f4d\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4e86\u653b\u51fb\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u89c9\u771f\u5b9e\u611f\u3002", "result": "\u5728\u5377\u79ef\u7f51\u7edc\u548c\u89c6\u89c9 Transformer \u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u653b\u51fb\u6210\u529f\u7387\u548c\u76ee\u6807\u7c7b\u522b\u6210\u529f\u7387\u59cb\u7ec8\u8d85\u8fc7 99%\u3002", "conclusion": "\u8be5\u6846\u67b6\u540c\u65f6\u786e\u4fdd\u4e86\u771f\u5b9e\u6027\u3001\u6709\u9488\u5bf9\u6027\u7684\u63a7\u5236\u548c\u9ed1\u76d2\u9002\u7528\u6027\uff0c\u4e3a\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u5f25\u5408\u4e86\u7406\u8bba\u653b\u51fb\u5f3a\u5ea6\u548c\u5b9e\u9645\u9690\u853d\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.23154", "categories": ["cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23154", "abs": "https://arxiv.org/abs/2509.23154", "authors": ["Jinzhe Pan", "Jingqing Wang", "Yuehui Ouyang", "Wenchi Cheng", "Wei Zhang"], "title": "AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8", "comment": "6 pages,6 figures, accepted by Globalcom 2025", "summary": "The exponential growth of wireless devices and stringent reliability\nrequirements of emerging applications demand fundamental improvements in\ndistributed channel access mechanisms for unlicensed bands. Current Wi-Fi\nsystems, which rely on binary exponential backoff (BEB), suffer from suboptimal\ncollision resolution in dense deployments and persistent fairness challenges\ndue to inherent randomness. This paper introduces a multi-agent reinforcement\nlearning framework that integrates artificial intelligence (AI) optimization\nwith legacy device coexistence. We first develop a dynamic backoff selection\nmechanism that adapts to real-time channel conditions through access deferral\nevents while maintaining full compatibility with conventional CSMA/CA\noperations. Second, we introduce a fairness quantification metric aligned with\nenhanced distributed channel access (EDCA) principles to ensure equitable\nmedium access opportunities. Finally, we propose a centralized training\ndecentralized execution (CTDE) architecture incorporating neighborhood activity\npatterns as observational inputs, optimized via constrained multi-agent\nproximal policy optimization (MAPPO) to jointly minimize collisions and\nguarantee fairness. Experimental results demonstrate that our solution\nsignificantly reduces collision probability compared to conventional BEB while\npreserving backward compatibility with commercial Wi-Fi devices. The proposed\nfairness metric effectively eliminates starvation risks in heterogeneous\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u4fe1\u9053\u63a5\u5165\u673a\u5236\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edfWi-Fi\u7cfb\u7edf\u5728\u5bc6\u96c6\u90e8\u7f72\u4e2d\u7684\u78b0\u649e\u95ee\u9898\u548c\u516c\u5e73\u6027\u6311\u6218\u3002", "motivation": "\u5f53\u524dWi-Fi\u7cfb\u7edf\u4f9d\u8d56\u7684\u4e8c\u8fdb\u5236\u6307\u6570\u9000\u907f\u7b97\u6cd5\u5728\u5bc6\u96c6\u90e8\u7f72\u4e2d\u78b0\u649e\u89e3\u51b3\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u7531\u4e8e\u5185\u5728\u968f\u673a\u6027\u5b58\u5728\u6301\u7eed\u7684\u516c\u5e73\u6027\u6311\u6218\u3002\u65b0\u5174\u5e94\u7528\u5bf9\u65e0\u7ebf\u8bbe\u5907\u548c\u4e25\u683c\u7684\u53ef\u9760\u6027\u8981\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u6539\u8fdb\u514d\u8bb8\u53ef\u9891\u6bb5\u7684\u5206\u5e03\u5f0f\u4fe1\u9053\u63a5\u5165\u673a\u5236\u3002", "method": "1. \u5f00\u53d1\u4e86\u4e00\u79cd\u52a8\u6001\u9000\u907f\u9009\u62e9\u673a\u5236\uff0c\u901a\u8fc7\u8bbf\u95ee\u5ef6\u8fdf\u4e8b\u4ef6\u9002\u5e94\u5b9e\u65f6\u4fe1\u9053\u6761\u4ef6\uff0c\u540c\u65f6\u4e0e\u4f20\u7edfCSMA/CA\u64cd\u4f5c\u5b8c\u5168\u517c\u5bb9\u30022. \u5f15\u5165\u4e86\u4e0e\u589e\u5f3a\u578b\u5206\u5e03\u5f0f\u4fe1\u9053\u63a5\u5165\uff08EDCA\uff09\u539f\u5219\u5bf9\u9f50\u7684\u516c\u5e73\u6027\u91cf\u5316\u6307\u6807\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u7684\u4ecb\u8d28\u8bbf\u95ee\u673a\u4f1a\u30023. \u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\uff08CTDE\uff09\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u90bb\u57df\u6d3b\u52a8\u6a21\u5f0f\u4f5c\u4e3a\u89c2\u5bdf\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08MAPPO\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u5171\u540c\u6700\u5c0f\u5316\u51b2\u7a81\u5e76\u4fdd\u8bc1\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u663e\u8457\u964d\u4f4e\u4e86\u4e0e\u4f20\u7edfBEB\u76f8\u6bd4\u7684\u78b0\u649e\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e0e\u5546\u7528Wi-Fi\u8bbe\u5907\u7684\u5411\u540e\u517c\u5bb9\u6027\u3002\u6240\u63d0\u51fa\u7684\u516c\u5e73\u6027\u6307\u6807\u6709\u6548\u5730\u6d88\u9664\u4e86\u5f02\u6784\u573a\u666f\u4e2d\u7684\u8d44\u6e90\u532e\u4e4f\u98ce\u9669\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edfWi-Fi\u7cfb\u7edf\u5728\u5bc6\u96c6\u90e8\u7f72\u4e2d\u7684\u78b0\u649e\u95ee\u9898\u548c\u516c\u5e73\u6027\u6311\u6218\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u5411\u540e\u517c\u5bb9\u6027\u3002"}}
{"id": "2509.22964", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22964", "abs": "https://arxiv.org/abs/2509.22964", "authors": ["Qinxun Bai", "Yuxuan Han", "Wei Xu", "Zhengyuan Zhou"], "title": "Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic", "comment": null, "summary": "Off-policy reinforcement learning (RL) with function approximation offers an\neffective way to improve sample efficiency by reusing past experience. Within\nthis setting, the actor-critic (AC) framework has achieved strong empirical\nsuccess. However, both the critic and actor learning is challenging for the\noff-policy AC methods: first of all, in addition to the classic \"deadly triad\"\ninstability of off-policy evaluation, it also suffers from a \"moving target\"\nproblem, where the policy being evaluated changes continually; secondly, actor\nlearning becomes less efficient due to the difficulty of estimating the exact\noff-policy policy gradient. The first challenge essentially reduces the problem\nto repeatedly performing off-policy evaluation for changing policies. For the\nsecond challenge, the off-policy policy gradient theorem requires a complex and\noften impractical algorithm to estimate an additional emphasis critic, which is\ntypically neglected in practice, thereby reducing to the on-policy policy\ngradient as an approximation. In this work, we introduce a novel concept of\nfunctional critic modeling, which leads to a new AC framework that addresses\nboth challenges for actor-critic learning under the deadly triad setting. We\nprovide a theoretical analysis in the linear function setting, establishing the\nprovable convergence of our framework, which, to the best of our knowledge, is\nthe first convergent off-policy target-based AC algorithm. From a practical\nperspective, we further propose a carefully designed neural network\narchitecture for the functional critic modeling and demonstrate its\neffectiveness through preliminary experiments on widely used RL tasks from the\nDeepMind Control Benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684actor-critic (AC) \u6846\u67b6\uff0c\u4ee5\u89e3\u51b3 off-policy \u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "Off-policy \u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u901a\u8fc7\u91cd\u7528\u8fc7\u53bb\u7684\u7ecf\u9a8c\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4f46\u5176actor\u548ccritic\u5b66\u4e60\u9762\u4e34\u201c\u81f4\u547d\u4e09\u5143\u7ec4\u201d\u4e0d\u7a33\u5b9a\u548c\u201c\u79fb\u52a8\u76ee\u6807\u201d\u95ee\u9898\uff0c\u4ee5\u53ca\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1 off-policy \u7b56\u7565\u68af\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u51fd\u6570\u5f0f critic \u5efa\u6a21\u7684\u6982\u5ff5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684 AC \u6846\u67b6\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u672c\u6587\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u5728 DeepMind Control Benchmark \u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u51fd\u6570\u5f0f critic \u5efa\u6a21\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3 off-policy AC \u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u4e3a off-policy \u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2509.23883", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23883", "abs": "https://arxiv.org/abs/2509.23883", "authors": ["Yibo Yan", "Guangwei Xu", "Xin Zou", "Shuliang Liu", "James Kwok", "Xuming Hu"], "title": "DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning", "comment": "Under review", "summary": "Visual Document Retrieval (VDR), the task of retrieving visually-rich\ndocument pages using queries that combine visual and textual cues, is crucial\nfor numerous real-world applications. Recent state-of-the-art methods leverage\nLarge Vision-Language Models (LVLMs) in a multi-vector paradigm, representing\neach document as patch-level embeddings to capture fine-grained details. While\nhighly effective, this approach introduces a critical challenge: prohibitive\nstorage overhead, as storing hundreds of vectors per page makes large-scale\ndeployment costly and impractical. To address this, we introduce DocPruner, the\nfirst framework to employ adaptive patch-level embedding pruning for VDR to\neffectively reduce the storage overhead. DocPruner leverages the intra-document\npatch attention distribution to dynamically identify and discard redundant\nembeddings for each document. This adaptive mechanism enables a significant\n50-60% reduction in storage for leading multi-vector VDR models with negligible\ndegradation in document retrieval performance. Extensive experiments across\nmore than ten representative datasets validate that DocPruner offers a robust,\nflexible, and effective solution for building storage-efficient, large-scale\nVDR systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDocPruner\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11\u89c6\u89c9\u6587\u6863\u68c0\u7d22\uff08VDR\uff09\u4e2d\u56e0\u4f7f\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u800c\u4ea7\u751f\u7684\u5927\u91cf\u5b58\u50a8\u5f00\u9500\u3002DocPruner\u901a\u8fc7\u81ea\u9002\u5e94\u7684patch\u7ea7\u522b\u5d4c\u5165\u526a\u679d\uff0c\u52a8\u6001\u8bc6\u522b\u5e76\u4e22\u5f03\u6bcf\u4e2a\u6587\u6863\u4e2d\u7684\u5197\u4f59\u5d4c\u5165\uff0c\u4ece\u800c\u5728\u51e0\u4e4e\u4e0d\u964d\u4f4e\u68c0\u7d22\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u51cf\u5c1150-60%\u7684\u5b58\u50a8\u3002", "motivation": "\u73b0\u6709VDR\u65b9\u6cd5\u4f7f\u7528LVLM\u7684\u591a\u5411\u91cf\u8303\u5f0f\u867d\u7136\u6709\u6548\uff0c\u4f46\u6bcf\u4e2a\u6587\u6863\u9700\u8981\u5b58\u50a8\u6570\u767e\u4e2a\u5411\u91cf\uff0c\u5bfc\u81f4\u5b58\u50a8\u5f00\u9500\u8fc7\u5927\uff0c\u5927\u89c4\u6a21\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u672c\u6587\u63d0\u51faDocPruner\u6846\u67b6\uff0c\u5229\u7528\u6587\u6863\u5185\u7684patch\u6ce8\u610f\u529b\u5206\u5e03\u6765\u52a8\u6001\u8bc6\u522b\u548c\u4e22\u5f03\u5197\u4f59\u5d4c\u5165\u3002", "result": "DocPruner\u80fd\u591f\u663e\u8457\u51cf\u5c1150-60%\u7684\u5b58\u50a8\uff0c\u4e14\u6587\u6863\u68c0\u7d22\u6027\u80fd\u51e0\u4e4e\u6ca1\u6709\u4e0b\u964d\u3002\u5728\u8d85\u8fc7\u5341\u4e2a\u4ee3\u8868\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DocPruner\u7684\u6709\u6548\u6027\u3002", "conclusion": "DocPruner\u4e3a\u6784\u5efa\u5b58\u50a8\u9ad8\u6548\u7684\u5927\u89c4\u6a21VDR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u7075\u6d3b\u548c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22940", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22940", "abs": "https://arxiv.org/abs/2509.22940", "authors": ["Melissa Roemmele", "John Joon Young Chung", "Taewook Kim", "Yuqian Sun", "Alex Calderwood", "Max Kreminski"], "title": "LLMs Behind the Scenes: Enabling Narrative Scene Illustration", "comment": "Accepted at EMNLP 2025", "summary": "Generative AI has established the opportunity to readily transform content\nfrom one medium to another. This capability is especially powerful for\nstorytelling, where visual illustrations can illuminate a story originally\nexpressed in text. In this paper, we focus on the task of narrative scene\nillustration, which involves automatically generating an image depicting a\nscene in a story. Motivated by recent progress on text-to-image models, we\nconsider a pipeline that uses LLMs as an interface for prompting text-to-image\nmodels to generate scene illustrations given raw story text. We apply\nvariations of this pipeline to a prominent story corpus in order to synthesize\nillustrations for scenes in these stories. We conduct a human annotation task\nto obtain pairwise quality judgments for these illustrations. The outcome of\nthis process is the SceneIllustrations dataset, which we release as a new\nresource for future work on cross-modal narrative transformation. Through our\nanalysis of this dataset and experiments modeling illustration quality, we\ndemonstrate that LLMs can effectively verbalize scene knowledge implicitly\nevoked by story text. Moreover, this capability is impactful for generating and\nevaluating illustrations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u4f5c\u4e3a\u63a5\u53e3\uff0c\u63d0\u793a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4ece\u539f\u59cb\u6545\u4e8b\u6587\u672c\u751f\u6210\u573a\u666f\u63d2\u56fe\u7684\u6d41\u7a0b\u3002\u8be5\u6d41\u7a0b\u5e94\u7528\u4e8e\u4e00\u4e2a\u7a81\u51fa\u7684\u6545\u4e8b\u8bed\u6599\u5e93\uff0c\u4ee5\u5408\u6210\u8fd9\u4e9b\u6545\u4e8b\u4e2d\u573a\u666f\u7684\u63d2\u56fe\u3002\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a SceneIllustrations \u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u672a\u6765\u5173\u4e8e\u8de8\u6a21\u5f0f\u53d9\u4e8b\u8f6c\u6362\u7684\u5de5\u4f5c\u3002", "motivation": "\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5c06\u5185\u5bb9\u4ece\u4e00\u79cd\u5a92\u4ecb\u8f6c\u6362\u4e3a\u53e6\u4e00\u79cd\u5a92\u4ecb\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8bb2\u6545\u4e8b\u65b9\u9762\uff0c\u89c6\u89c9\u63d2\u56fe\u53ef\u4ee5\u9610\u660e\u6700\u521d\u7528\u6587\u672c\u8868\u8fbe\u7684\u6545\u4e8b\u3002\u672c\u6587\u4fa7\u91cd\u4e8e\u53d9\u4e8b\u573a\u666f\u63d2\u56fe\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u6d89\u53ca\u81ea\u52a8\u751f\u6210\u63cf\u8ff0\u6545\u4e8b\u4e2d\u573a\u666f\u7684\u56fe\u50cf\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u4f5c\u4e3a\u63a5\u53e3\uff0c\u63d0\u793a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4ece\u539f\u59cb\u6545\u4e8b\u6587\u672c\u751f\u6210\u573a\u666f\u63d2\u56fe\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6210\u5bf9\u8d28\u91cf\u5224\u65ad\u7684\u63d2\u56fe\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u8868\u8fbe\u6545\u4e8b\u6587\u672c\u4e2d\u9690\u542b\u7684\u573a\u666f\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u80fd\u529b\u5bf9\u4e8e\u751f\u6210\u548c\u8bc4\u4f30\u63d2\u56fe\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u8868\u8fbe\u6545\u4e8b\u6587\u672c\u4e2d\u9690\u542b\u7684\u573a\u666f\u77e5\u8bc6\uff0c\u5e76\u4e14\u8fd9\u79cd\u80fd\u529b\u5bf9\u4e8e\u751f\u6210\u548c\u8bc4\u4f30\u63d2\u56fe\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2509.22839", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22839", "abs": "https://arxiv.org/abs/2509.22839", "authors": ["Ibrahim Delibasoglu", "Fredrik Heintz"], "title": "Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention", "comment": null, "summary": "Explainability in time series forecasting is essential for improving model\ntransparency and supporting informed decision-making. In this work, we present\nCrossScaleNet, an innovative architecture that combines a patch-based\ncross-attention mechanism with multi-scale processing to achieve both high\nperformance and enhanced temporal explainability. By embedding attention\nmechanisms into the training process, our model provides intrinsic\nexplainability for temporal saliency, making its decision-making process more\ntransparent. Traditional post-hoc methods for temporal saliency detection are\ncomputationally expensive, particularly when compared to feature importance\ndetection. While ablation techniques may suffice for datasets with fewer\nfeatures, identifying temporal saliency poses greater challenges due to its\ncomplexity. We validate CrossScaleNet on synthetic datasets with known saliency\nground truth and on established public benchmarks, demonstrating the robustness\nof our method in identifying temporal saliency. Experiments on real-world\ndatasets for forecasting task show that our approach consistently outperforms\nmost transformer-based models, offering better explainability without\nsacrificing predictive accuracy. Our evaluations demonstrate superior\nperformance in both temporal saliency detection and forecasting accuracy.\nMoreover, we highlight that existing models claiming explainability often fail\nto maintain strong performance on standard benchmarks. CrossScaleNet addresses\nthis gap, offering a balanced approach that captures temporal saliency\neffectively while delivering state-of-the-art forecasting performance across\ndatasets of varying complexity.", "AI": {"tldr": "CrossScaleNet: A new architecture combining patch-based cross-attention with multi-scale processing for high performance and temporal explainability in time series forecasting. It outperforms transformer-based models and maintains strong performance on benchmarks.", "motivation": "Explainability in time series forecasting is essential for improving model transparency and supporting informed decision-making. Traditional post-hoc methods are computationally expensive, and ablation techniques face challenges with temporal saliency detection.", "method": "CrossScaleNet uses a patch-based cross-attention mechanism with multi-scale processing to provide intrinsic explainability for temporal saliency.", "result": "CrossScaleNet demonstrates robustness in identifying temporal saliency on synthetic and public benchmark datasets. It outperforms most transformer-based models on real-world datasets, offering better explainability without sacrificing predictive accuracy.", "conclusion": "CrossScaleNet captures temporal saliency effectively while delivering state-of-the-art forecasting performance, addressing the gap where existing explainable models often fail to maintain strong performance."}}
{"id": "2509.23178", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23178", "abs": "https://arxiv.org/abs/2509.23178", "authors": ["Tian Qin", "Yuhan Chen", "Zhiwei Wang", "Zhi-Qin John Xu"], "title": "Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers", "comment": null, "summary": "Transformers are able to perform reasoning tasks, however the intrinsic\nmechanism remains widely open. In this paper we propose a set of information\npropagation rules based on Transformers and utilize symbolic reasoning tasks to\ntheoretically analyze the limit reasoning steps. We show that the limit number\nof reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with\n$L$ attention layers in a single-pass.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "Transformer\u80fd\u591f\u6267\u884c\u63a8\u7406\u4efb\u52a1\uff0c\u4f46\u5176\u5185\u5728\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7ec4\u57fa\u4e8eTransformer\u7684\u4fe1\u606f\u4f20\u64ad\u89c4\u5219\uff0c\u5e76\u5229\u7528\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u6781\u9650\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5bf9\u4e8e\u5177\u6709L\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u6a21\u578b\uff0c\u5355\u6b21\u901a\u8fc7\u7684\u6781\u9650\u63a8\u7406\u6b65\u9aa4\u7684\u6570\u91cf\u5728O(3^{L-1})\u548cO(2^{L-1})\u4e4b\u95f4\u3002", "conclusion": "\u672c\u6587\u8868\u660eTransformer\u5177\u6709\u4e00\u5b9a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7ed9\u51fa\u4e86\u63a8\u7406\u6b65\u9aa4\u7684\u7406\u8bba\u4e0a\u9650\u3002"}}
{"id": "2509.22969", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22969", "abs": "https://arxiv.org/abs/2509.22969", "authors": ["Samuel V. Singh", "Shirley Coyle", "Mimi Zhang"], "title": "Shape-Informed Clustering of Multi-Dimensional Functional Data via Deep Functional Autoencoders", "comment": null, "summary": "We introduce FAEclust, a novel functional autoencoder framework for cluster\nanalysis of multi-dimensional functional data, data that are random\nrealizations of vector-valued random functions. Our framework features a\nuniversal-approximator encoder that captures complex nonlinear\ninterdependencies among component functions, and a universal-approximator\ndecoder capable of accurately reconstructing both Euclidean and manifold-valued\nfunctional data. Stability and robustness are enhanced through innovative\nregularization strategies applied to functional weights and biases.\nAdditionally, we incorporate a clustering loss into the network's training\nobjective, promoting the learning of latent representations that are conducive\nto effective clustering. A key innovation is our shape-informed clustering\nobjective, ensuring that the clustering results are resistant to phase\nvariations in the functions. We establish the universal approximation property\nof our non-linear decoder and validate the effectiveness of our model through\nextensive experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u805a\u7c7b\u5206\u6790\u591a\u7ef4\u51fd\u6570\u6570\u636e\u7684\u529f\u80fd\u81ea\u52a8\u7f16\u7801\u5668\u6846\u67b6 (FAEclust)\u3002", "motivation": "\u65e8\u5728\u6355\u83b7\u7ec4\u4ef6\u51fd\u6570\u4e4b\u95f4\u590d\u6742\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u51c6\u786e\u91cd\u5efa\u6b27\u51e0\u91cc\u5f97\u548c\u6d41\u5f62\u503c\u51fd\u6570\u6570\u636e\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u901a\u7528\u903c\u8fd1\u5668\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u5e94\u7528\u4e8e\u529f\u80fd\u6743\u91cd\u548c\u504f\u5dee\u7684\u521b\u65b0\u6b63\u5219\u5316\u7b56\u7565\u6765\u589e\u5f3a\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5c06\u805a\u7c7b\u635f\u5931\u7eb3\u5165\u7f51\u7edc\u7684\u8bad\u7ec3\u76ee\u6807\u4e2d\uff0c\u4ece\u800c\u4fc3\u8fdb\u6f5c\u5728\u8868\u793a\u7684\u5b66\u4e60\uff0c\u8fd9\u6709\u52a9\u4e8e\u6709\u6548\u7684\u805a\u7c7b\u3002\u4e00\u4e2a\u5173\u952e\u7684\u521b\u65b0\u662f\u5f62\u72b6\u611f\u77e5\u7684\u805a\u7c7b\u76ee\u6807\uff0c\u786e\u4fdd\u805a\u7c7b\u7ed3\u679c\u80fd\u591f\u62b5\u6297\u51fd\u6570\u4e2d\u7684\u76f8\u4f4d\u53d8\u5316\u3002", "result": "\u5efa\u7acb\u4e86\u975e\u7ebf\u6027\u89e3\u7801\u5668\u7684\u901a\u7528\u903c\u8fd1\u6027\u8d28\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6709\u6548\u805a\u7c7b"}}
{"id": "2509.24193", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24193", "abs": "https://arxiv.org/abs/2509.24193", "authors": ["Ran Xu", "Yuchen Zhuang", "Zihan Dong", "Jonathan Wang", "Yue Yu", "Joyce C. Ho", "Linjun Zhang", "Haoyu Wang", "Wenqi Shi", "Carl Yang"], "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "comment": "Accepted to NeurIPS 2025 (Spotlight)", "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.", "AI": {"tldr": "AceSearcher, a cooperative self-play framework, trains a single LLM to improve complex reasoning by alternating between decomposing queries and integrating retrieved contexts.", "motivation": "Search-augmented LLMs struggle with complex reasoning due to ineffective multi-hop retrieval and limited reasoning ability.", "method": "A cooperative self-play framework trains a single LLM to alternate between a decomposer and a solver. It uses supervised fine-tuning and reinforcement fine-tuning optimized for final answer accuracy.", "result": "AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. On document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters.", "conclusion": "AceSearcher demonstrates exceptional efficiency and effectiveness in tackling complex reasoning tasks, even at smaller scales."}}
{"id": "2509.22947", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22947", "abs": "https://arxiv.org/abs/2509.22947", "authors": ["Mohammed Sabry", "Anya Belz"], "title": "What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?", "comment": null, "summary": "Does explicitly exercising the induction circuit during pretraining improve\nin-context learning (ICL), or is natural text sufficient when compute is held\nconstant (iso-FLOPs)? To test whether targeted synthetic data can accelerate\ninduction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight\ncurriculum that injects forward-copy (Induction), backward-copy (Anti), or a\nbalanced mix into the pretraining stream. We train models from 0.13B to 1B\nparameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)\nhead-level telemetry, and (iii) held-out language modeling perplexity. Our\nfindings challenge the assumption that early induction circuit activation\ndirectly improves ICL. While Bi-Induct accelerates induction-head emergence at\nsmall scales, this does not consistently yield stronger generalization. On\nstandard LM benchmarks, Bi-Induct matches natural-only training; on\nfunction-style ICL probes, the 1B natural-only performs best. Stress tests\n(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these\ntrends. Telemetry shows larger natural-only models develop broader, earlier\ninduction heads without explicit induction patterns. Anti-induction data fails\nto elicit meaningful activation. Perplexity penalties from synthetic data\nshrink with scale, suggesting larger models can absorb non-natural patterns\nwith minimal cost. Crucially, ablating the top 2% of induction heads degrades\nICL more than random ablations, especially for natural-only models, indicating\nmore centralized, load-bearing circuits. Bi-Induct variants exhibit more\nredundant induction activity, implying different circuit utilization. Overall,\ninducing activation is not sufficient: ICL gains depend on these circuits\nbecoming functionally necessary. These results underscore mechanism-aware\npretraining diagnostics and data mixtures that foster load-bearing, not merely\npresent, structure.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u663e\u5f0f\u5730\u8bad\u7ec3\u5f52\u7eb3\u7535\u8def\u662f\u5426\u80fd\u63d0\u9ad8\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u663e\u5f0f\u5f52\u7eb3\u53ef\u4ee5\u52a0\u901f\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u5f52\u7eb3\u5934\u7684\u51fa\u73b0\uff0c\u4f46\u4e0d\u80fd\u6301\u7eed\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002\u5728\u6807\u51c6LM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBi-Induct\u4e0e\u7eaf\u81ea\u7136\u8bad\u7ec3\u76f8\u5339\u914d\uff1b\u5728\u51fd\u6570\u5f0fICL\u63a2\u9488\u4e2d\uff0c1B\u7eaf\u81ea\u7136\u6a21\u578b\u8868\u73b0\u6700\u597d\u3002\u603b\u7684\u6765\u8bf4\uff0c\u8bf1\u5bfc\u6fc0\u6d3b\u662f\u4e0d\u591f\u7684\uff1aICL\u7684\u63d0\u5347\u53d6\u51b3\u4e8e\u8fd9\u4e9b\u7535\u8def\u5728\u529f\u80fd\u4e0a\u662f\u5426\u5fc5\u8981\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u6d4b\u8bd5\u6709\u9488\u5bf9\u6027\u7684\u5408\u6210\u6570\u636e\u662f\u5426\u53ef\u4ee5\u52a0\u901f\u5f52\u7eb3\u5934\u7684\u51fa\u73b0\u5e76\u589e\u5f3aICL\uff0c\u4ee5\u6b64\u6765\u5224\u65ad\u5728\u8ba1\u7b97\u91cf\u6052\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u5f0f\u5730\u8bad\u7ec3\u5f52\u7eb3\u7535\u8def\u662f\u5426\u80fd\u63d0\u9ad8\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4f7f\u7528\u81ea\u7136\u6587\u672c\u5c31\u8db3\u591f\u4e86\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86Bi-Induct\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8bfe\u7a0b\uff0c\u5c06\u524d\u5411\u590d\u5236\uff08\u5f52\u7eb3\uff09\u3001\u540e\u5411\u590d\u5236\uff08\u53cd\u5f52\u7eb3\uff09\u6216\u5e73\u8861\u6df7\u5408\u6ce8\u5165\u5230\u9884\u8bad\u7ec3\u6d41\u4e2d\u3002\u5728\u7b49\u8ba1\u7b97\u91cf\u4e0b\u8bad\u7ec3\u4e86\u4ece0.13B\u52301B\u53c2\u6570\u7684\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86(i)\u5c11\u6837\u672cICL\u57fa\u51c6\u6d4b\u8bd5\uff0c(ii)\u5934\u90e8\u5c42\u9762\u7684\u9065\u6d4b\uff0c\u4ee5\u53ca(iii)\u4fdd\u7559\u7684\u8bed\u8a00\u5efa\u6a21\u56f0\u60d1\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cBi-Induct\u867d\u7136\u52a0\u901f\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u5f52\u7eb3\u5934\u7684\u51fa\u73b0\uff0c\u4f46\u4e0d\u80fd\u6301\u7eed\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002\u5728\u6807\u51c6LM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBi-Induct\u4e0e\u7eaf\u81ea\u7136\u8bad\u7ec3\u76f8\u5339\u914d\uff1b\u5728\u51fd\u6570\u5f0fICL\u63a2\u9488\u4e2d\uff0c1B\u7eaf\u81ea\u7136\u6a21\u578b\u8868\u73b0\u6700\u597d\u3002\u9065\u6d4b\u663e\u793a\uff0c\u66f4\u5927\u7684\u7eaf\u81ea\u7136\u6a21\u578b\u5728\u6ca1\u6709\u663e\u5f0f\u5f52\u7eb3\u6a21\u5f0f\u7684\u60c5\u51b5\u4e0b\uff0c\u53d1\u5c55\u51fa\u66f4\u5e7f\u6cdb\u3001\u66f4\u65e9\u7684\u5f52\u7eb3\u5934\u3002\u6d88\u878d\u524d2%\u7684\u5f52\u7eb3\u5934\u6bd4\u968f\u673a\u6d88\u878d\u66f4\u80fd\u964d\u4f4eICL\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7eaf\u81ea\u7136\u6a21\u578b\uff0c\u8868\u660e\u7535\u8def\u66f4\u52a0\u96c6\u4e2d\uff0c\u4e14\u8d1f\u8f7d\u66f4\u5927\u3002Bi-Induct\u53d8\u4f53\u8868\u73b0\u51fa\u66f4\u5197\u4f59\u7684\u5f52\u7eb3\u6d3b\u52a8\uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u540c\u7684\u7535\u8def\u5229\u7528\u7387\u3002", "conclusion": "\u603b\u7684\u6765\u8bf4\uff0c\u8bf1\u5bfc\u6fc0\u6d3b\u662f\u4e0d\u591f\u7684\uff1aICL\u7684\u63d0\u5347\u53d6\u51b3\u4e8e\u8fd9\u4e9b\u7535\u8def\u5728\u529f\u80fd\u4e0a\u662f\u5426\u5fc5\u8981\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u673a\u5236\u611f\u77e5\u7684\u9884\u8bad\u7ec3\u8bca\u65ad\u548c\u6570\u636e\u6df7\u5408\uff0c\u8fd9\u4e9b\u8bca\u65ad\u548c\u6570\u636e\u6df7\u5408\u57f9\u517b\u4e86\u8d1f\u8f7d\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b58\u5728\u7684\u7ed3\u6784\u3002"}}
{"id": "2509.22841", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22841", "abs": "https://arxiv.org/abs/2509.22841", "authors": ["Yi Luo", "Yike Guo", "Hamed Hooshangnejad", "Rui Zhang", "Xue Feng", "Quan Chen", "Wil Ngwa", "Kai Ding"], "title": "Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging", "comment": "11 pages, 5 figures", "summary": "Lung cancer remains the leading cause of cancerrelated deaths globally.\nAccurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is\npivotal for optimal radiation therapy in mobile tumors such as lung cancer to\naccount for tumor motion, yet is hindered by the limited availability of\nannotated IGTV datasets and attenuated PET signal intensity at tumor\nboundaries. In this study, we present a transfer learningbased methodology\nutilizing a multimodal interactive perception network with MAMBA, pre-trained\non extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a\nprivate IGTV cohort. This cohort constitutes the PET/CT subset of the\nLung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the\nchallenge of weak PET intensities in IGTV peripheral slices, we introduce a\nslice interaction module (SIM) within a 2.5D segmentation framework to\neffectively model inter-slice relationships. Our proposed module integrates\nchannel and spatial attention branches with depthwise convolutions, enabling\nmore robust learning of slice-to-slice dependencies and thereby improving\noverall segmentation performance. A comprehensive experimental evaluation\ndemonstrates that our approach achieves a Dice of 0.609 on the private IGTV\ndataset, substantially surpassing the conventional baseline score of 0.385.\nThis work highlights the potential of transfer learning, coupled with advanced\nmultimodal techniques and a SIM to enhance the reliability and clinical\nrelevance of IGTV segmentation for lung cancer radiation therapy planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684PET/CT\u56fe\u50cf\u5185\u80bf\u7624\u4f53\u79ef(IGTV)\u5206\u5272\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u6a21\u6001\u4ea4\u4e92\u611f\u77e5\u7f51\u7edc\u548c\u5207\u7247\u4ea4\u4e92\u6a21\u5757(SIM)\u6765\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u7cbe\u786e\u7684IGTV delineation\u5bf9\u4e8e\u80ba\u764c\u7b49\u79fb\u52a8\u80bf\u7624\u7684\u653e\u5c04\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u5230\u5e26\u6ce8\u91ca\u7684IGTV\u6570\u636e\u96c6\u7684\u9650\u5236\u4ee5\u53ca\u80bf\u7624\u8fb9\u754c\u5904PET\u4fe1\u53f7\u5f3a\u5ea6\u8870\u51cf\u7684\u963b\u788d\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u5728\u5927\u91cfGTV\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u5e76\u5728\u79c1\u6709IGTV\u961f\u5217\u4e0a\u5fae\u8c03\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u5207\u7247\u4ea4\u4e92\u6a21\u5757(SIM)\u4ee5\u6709\u6548\u5efa\u6a21\u5207\u7247\u95f4\u5173\u7cfb\u3002", "result": "\u5728\u79c1\u6709IGTV\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.609\u7684Dice\u7cfb\u6570\uff0c\u663e\u8457\u8d85\u8fc7\u4e86\u4f20\u7edf\u57fa\u7ebf\u76840.385\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8fc1\u79fb\u5b66\u4e60\u3001\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6280\u672f\u548cSIM\u5728\u63d0\u9ad8\u80ba\u764c\u653e\u5c04\u6cbb\u7597\u8ba1\u5212\u4e2dIGTV\u5206\u5272\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.23186", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23186", "abs": "https://arxiv.org/abs/2509.23186", "authors": ["Qimin Zhong", "Hao Liao", "Siwei Wang", "Mingyang Zhou", "Xiaoqun Wu", "Rui Mao", "Wei Chen"], "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse tasks but continue to struggle with learning transitive relations, a\ncornerstone for complex planning. To address this issue, we investigate the\nMulti-Token Prediction (MTP) paradigm and its impact to transitive relation\nlearning. We theoretically analyze the MTP paradigm using a Transformer\narchitecture composed of a shared output head and a transfer layer. Our\nanalysis reveals that the transfer layer gradually learns the multi-step\nadjacency information, which in turn enables the backbone model to capture\nunobserved transitive reachability relations beyond those directly present in\nthe training data, albeit with some inevitable noise in adjacency estimation.\nBuilding on this foundation, we propose two strategies to enhance the transfer\nlayer and overall learning quality: Next-Token Injection (NTI) and a\nTransformer-based transfer layer. Our experiments on both synthetic graphs and\nthe Blocksworld planning benchmark validate our theoretical findings and\ndemonstrate that the improvements significantly enhance the model's\npath-planning capability. These findings deepen our understanding of how\nTransformers with MTP learn in complex planning tasks, and provide practical\nstrategies to overcome the transitivity bottleneck, paving the way toward\nstructurally aware and general-purpose planning models.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u4e60\u4f20\u9012\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u5bf9\u4e8e\u590d\u6742\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u7814\u7a76\u4e86\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u8303\u5f0f\u53ca\u5176\u5bf9\u4f20\u9012\u5173\u7cfb\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b66\u4e60\u4f20\u9012\u5173\u7cfb\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u800c\u4f20\u9012\u5173\u7cfb\u662f\u590d\u6742\u89c4\u5212\u7684\u57fa\u77f3\u3002", "method": "\u672c\u6587\u4f7f\u7528\u5177\u6709\u5171\u4eab\u8f93\u51fa\u5934\u548c\u8f6c\u79fb\u5c42\u7684Transformer\u67b6\u6784\uff0c\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86MTP\u8303\u5f0f\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b56\u7565\u6765\u589e\u5f3a\u8f6c\u79fb\u5c42\u548c\u6574\u4f53\u5b66\u4e60\u8d28\u91cf\uff1aNext-Token Injection (NTI) \u548c\u57fa\u4e8eTransformer\u7684\u8f6c\u79fb\u5c42\u3002", "result": "\u5728\u5408\u6210\u56fe\u548cBlocksworld\u89c4\u5212\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\uff0c\u5e76\u8868\u660e\u6539\u8fdb\u663e\u7740\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8def\u5f84\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u5177\u6709MTP\u7684Transformer\u5982\u4f55\u5728\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u5b66\u4e60\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u514b\u670d\u4f20\u9012\u6027\u74f6\u9888\u7684\u5b9e\u7528\u7b56\u7565\uff0c\u4e3a\u7ed3\u6784\u611f\u77e5\u548c\u901a\u7528\u89c4\u5212\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.22979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22979", "abs": "https://arxiv.org/abs/2509.22979", "authors": ["Zeyi Chen", "Xinzhi Zhang", "Humishka Zope", "Hugo Barbalho", "Konstantina Mellou", "Marco Molinaro", "Janardhan Kulkarni", "Ishai Menache", "Sirui Li"], "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts", "comment": null, "summary": "Mathematical programming -- the task of expressing operations and\ndecision-making problems in precise mathematical language -- is fundamental\nacross domains, yet remains a skill-intensive process requiring operations\nresearch expertise. Recent advances in large language models for complex\nreasoning have spurred interest in automating this task, translating natural\nlanguage into executable optimization models. Current approaches, however,\nachieve limited accuracy, hindered by scarce and noisy training data without\nleveraging domain knowledge. In this work, we systematically integrate\noptimization expertise to improve formulation accuracy for mixed-integer linear\nprogramming, a key family of mathematical programs. Our approach first cleans\ntraining data through class-based error analysis to explicitly prevent common\nmistakes within each optimization class. We then develop multi-turn inference\nstrategies that guide LLMs with class-specific error summaries and solver\nfeedback, enabling iterative refinement. Experiments across multiple base LLMs\ndemonstrate that combining cleaned data with domain-informed prompting and\nfeedback improves formulation accuracy by 14 percentage points on average,\nenabling further progress toward robust LLM-assisted optimization formulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f18\u5316\u4e13\u4e1a\u77e5\u8bc6\u6765\u63d0\u9ad8\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u516c\u5f0f\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u4f18\u5316\u6a21\u578b\u662f\u4e00\u9879\u91cd\u8981\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u6709\u9650\uff0c\u539f\u56e0\u662f\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u548c\u672a\u5145\u5206\u5229\u7528\u9886\u57df\u77e5\u8bc6\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u57fa\u4e8e\u7c7b\u522b\u7684\u9519\u8bef\u5206\u6790\u6765\u6e05\u7406\u8bad\u7ec3\u6570\u636e\uff0c\u7136\u540e\u5f00\u53d1\u591a\u8f6e\u63a8\u7406\u7b56\u7565\uff0c\u5229\u7528\u7c7b\u7279\u5b9a\u7684\u9519\u8bef\u6458\u8981\u548c\u6c42\u89e3\u5668\u53cd\u9988\u6765\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u6e05\u7406\u540e\u7684\u6570\u636e\u4e0e\u9886\u57df\u63d0\u793a\u548c\u53cd\u9988\u76f8\u7ed3\u5408\uff0c\u53ef\u4f7f\u516c\u5f0f\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8 14 \u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u516c\u5f0f\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u73b0\u7a33\u5065\u7684 LLM \u8f85\u52a9\u4f18\u5316\u516c\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.22973", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22973", "abs": "https://arxiv.org/abs/2509.22973", "authors": ["Jon Gauthier", "Canaan Breiss", "Matthew Leonard", "Edward F. Chang"], "title": "Emergent morpho-phonological representations in self-supervised speech models", "comment": "Accepted at EMNLP 2025", "summary": "Self-supervised speech models can be trained to efficiently recognize spoken\nwords in naturalistic, noisy environments. However, we do not understand the\ntypes of linguistic representations these models use to accomplish this task.\nTo address this question, we study how S3M variants optimized for word\nrecognition represent phonological and morphological phenomena in frequent\nEnglish noun and verb inflections. We find that their representations exhibit a\nglobal linear geometry which can be used to link English nouns and verbs to\ntheir regular inflected forms.\n  This geometric structure does not directly track phonological or\nmorphological units. Instead, it tracks the regular distributional\nrelationships linking many word pairs in the English lexicon -- often, but not\nalways, due to morphological inflection. These findings point to candidate\nrepresentational strategies that may support human spoken word recognition,\nchallenging the presumed necessity of distinct linguistic representations of\nphonology and morphology.", "AI": {"tldr": "\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u53ef\u4ee5\u5728\u5608\u6742\u73af\u5883\u4e2d\u6709\u6548\u5730\u8bc6\u522b\u53e3\u8bed\u5355\u8bcd\u3002\u7136\u800c\uff0c\u6211\u4eec\u5e76\u4e0d\u4e86\u89e3\u8fd9\u4e9b\u6a21\u578b\u4f7f\u7528\u4ec0\u4e48\u7c7b\u578b\u7684\u8bed\u8a00\u8868\u793a\u6765\u5b8c\u6210\u8fd9\u9879\u4efb\u52a1\u3002\u672c\u7814\u7a76\u65e8\u5728\u7814\u7a76\u7528\u4e8e\u8bcd\u8bed\u8bc6\u522b\u7684 S3M \u53d8\u4f53\u5982\u4f55\u8868\u793a\u5e38\u89c1\u82f1\u8bed\u540d\u8bcd\u548c\u52a8\u8bcd\u5c48\u6298\u4e2d\u7684\u97f3\u7cfb\u548c\u5f62\u6001\u73b0\u8c61\u3002", "motivation": "\u63a2\u7a76\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u5982\u4f55\u5229\u7528\u8bed\u8a00\u8868\u793a\u6765\u8bc6\u522b\u53e3\u8bed\u5355\u8bcd\uff0c\u7279\u522b\u662f\u5173\u6ce8\u6a21\u578b\u5982\u4f55\u5904\u7406\u82f1\u8bed\u540d\u8bcd\u548c\u52a8\u8bcd\u7684\u5c48\u6298\u53d8\u5316\u3002", "method": "\u7814\u7a76\u9488\u5bf9\u5355\u8bcd\u8bc6\u522b\u4f18\u5316\u7684 S3M \u53d8\u4f53\uff0c\u5206\u6790\u5b83\u4eec\u5982\u4f55\u8868\u793a\u5e38\u89c1\u7684\u82f1\u8bed\u540d\u8bcd\u548c\u52a8\u8bcd\u5c48\u6298\u4e2d\u7684\u97f3\u7cfb\u548c\u5f62\u6001\u73b0\u8c61\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u7684\u8868\u793a\u5177\u6709\u5168\u5c40\u7ebf\u6027\u51e0\u4f55\u7ed3\u6784\uff0c\u53ef\u4ee5\u5c06\u82f1\u8bed\u540d\u8bcd\u548c\u52a8\u8bcd\u4e0e\u5176\u89c4\u5219\u5c48\u6298\u5f62\u5f0f\u8054\u7cfb\u8d77\u6765\u3002\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u5e76\u4e0d\u76f4\u63a5\u8ddf\u8e2a\u97f3\u7cfb\u6216\u5f62\u6001\u5355\u4f4d\uff0c\u800c\u662f\u8ddf\u8e2a\u82f1\u8bed\u8bcd\u5178\u4e2d\u94fe\u63a5\u8bb8\u591a\u8bcd\u5bf9\u7684\u89c4\u5219\u5206\u5e03\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u53ef\u80fd\u91c7\u7528\u5019\u9009\u8868\u793a\u7b56\u7565\u6765\u652f\u6301\u4eba\u7c7b\u53e3\u8bed\u5355\u8bcd\u8bc6\u522b\uff0c\u6311\u6218\u4e86\u97f3\u7cfb\u548c\u5f62\u6001\u7684\u72ec\u7279\u8bed\u8a00\u8868\u793a\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.22864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22864", "abs": "https://arxiv.org/abs/2509.22864", "authors": ["Yixuan Hu", "Yuxuan Xue", "Simon Klenk", "Daniel Cremers", "Gerard Pons-Moll"], "title": "ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models", "comment": null, "summary": "In recent years, event cameras have gained significant attention due to their\nbio-inspired properties, such as high temporal resolution and high dynamic\nrange. However, obtaining large-scale labeled ground-truth data for event-based\nvision tasks remains challenging and costly. In this paper, we present\nControlEvents, a diffusion-based generative model designed to synthesize\nhigh-quality event data guided by diverse control signals such as class text\nlabels, 2D skeletons, and 3D body poses. Our key insight is to leverage the\ndiffusion prior from foundation models, such as Stable Diffusion, enabling\nhigh-quality event data generation with minimal fine-tuning and limited labeled\ndata. Our method streamlines the data generation process and significantly\nreduces the cost of producing labeled event datasets. We demonstrate the\neffectiveness of our approach by synthesizing event data for visual\nrecognition, 2D skeleton estimation, and 3D body pose estimation. Our\nexperiments show that the synthesized labeled event data enhances model\nperformance in all tasks. Additionally, our approach can generate events based\non unseen text labels during training, illustrating the powerful text-based\ngeneration capabilities inherited from foundation models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4e8b\u4ef6\u6570\u636e\uff0c\u8be5\u6a21\u578b\u7531\u591a\u79cd\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u7c7b\u6587\u672c\u6807\u7b7e\u30012D\u9aa8\u67b6\u548c3D\u8eab\u4f53\u59ff\u52bf\uff09\u5f15\u5bfc\u3002", "motivation": "\u83b7\u53d6\u5927\u89c4\u6a21\u6807\u8bb0\u7684\u4e8b\u4ef6\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5229\u7528\u7a33\u5b9a\u6269\u6563\u7b49\u57fa\u7840\u6a21\u578b\u7684\u6269\u6563\u5148\u9a8c\uff0c\u901a\u8fc7\u6700\u5c11\u7684\u5fae\u8c03\u548c\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e8b\u4ef6\u6570\u636e\u3002", "result": "\u5408\u6210\u7684\u6807\u8bb0\u4e8b\u4ef6\u6570\u636e\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u89c6\u89c9\u8bc6\u522b\u30012D\u9aa8\u9abc\u4f30\u8ba1\u548c3D\u8eab\u4f53\u59ff\u52bf\u4f30\u8ba1\u65b9\u9762\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u57fa\u4e8e\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u6587\u672c\u6807\u7b7e\u751f\u6210\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u751f\u6210\u6807\u8bb0\u4e8b\u4ef6\u6570\u636e\u96c6\u7684\u6210\u672c\u3002"}}
{"id": "2509.23189", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23189", "abs": "https://arxiv.org/abs/2509.23189", "authors": ["Zhenxing Xu", "Yizhe Zhang", "Weidong Bao", "Hao Wang", "Ming Chen", "Haoran Ye", "Wenzheng Jiang", "Hui Yan", "Ji Wang"], "title": "AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms", "comment": null, "summary": "Dynamically configuring algorithm hyperparameters is a fundamental challenge\nin computational intelligence. While learning-based methods offer automation,\nthey suffer from prohibitive sample complexity and poor generalization. We\nintroduce AutoEP, a novel framework that bypasses training entirely by\nleveraging Large Language Models (LLMs) as zero-shot reasoning engines for\nalgorithm control. AutoEP's core innovation lies in a tight synergy between two\ncomponents: (1) an online Exploratory Landscape Analysis (ELA) module that\nprovides real-time, quantitative feedback on the search dynamics, and (2) a\nmulti-LLM reasoning chain that interprets this feedback to generate adaptive\nhyperparameter strategies. This approach grounds high-level reasoning in\nempirical data, mitigating hallucination. Evaluated on three distinct\nmetaheuristics across diverse combinatorial optimization benchmarks, AutoEP\nconsistently outperforms state-of-the-art tuners, including neural evolution\nand other LLM-based methods. Notably, our framework enables open-source models\nlike Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and\naccessible new paradigm for automated hyperparameter design. Our code is\navailable at https://anonymous.4open.science/r/AutoEP-3E11", "AI": {"tldr": "AutoEP\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u4f5c\u4e3a\u96f6\u6837\u672c\u63a8\u7406\u5f15\u64ce\u8fdb\u884c\u7b97\u6cd5\u63a7\u5236\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u8ba1\u7b97\u667a\u80fd\u4e2d\u52a8\u6001\u914d\u7f6e\u7b97\u6cd5\u8d85\u53c2\u6570\u662f\u4e00\u4e2a\u6839\u672c\u6311\u6218\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u6837\u672c\u590d\u6742\u5ea6\u9ad8\uff0c\u6cdb\u5316\u6027\u5dee\u3002", "method": "AutoEP\u6846\u67b6\u7ed3\u5408\u5728\u7ebf\u63a2\u7d22\u6027\u666f\u89c2\u5206\u6790(ELA)\u6a21\u5757\u548c\u591allm\u63a8\u7406\u94fe\uff0c\u524d\u8005\u63d0\u4f9b\u5b9e\u65f6\u641c\u7d22\u52a8\u6001\u7684\u91cf\u5316\u53cd\u9988\uff0c\u540e\u8005\u89e3\u91ca\u53cd\u9988\u4ee5\u751f\u6210\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u7b56\u7565\u3002", "result": "AutoEP\u5728\u5404\u79cd\u7ec4\u5408\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8c03\u4f18\u5668\uff0c\u5305\u62ec\u795e\u7ecf\u8fdb\u5316\u548c\u5176\u4ed6\u57fa\u4e8ellm\u7684\u65b9\u6cd5\u3002Qwen3-30B\u7b49\u5f00\u6e90\u6a21\u578b\u53ef\u4ee5\u4e0eGPT-4\u7684\u6027\u80fd\u76f8\u5339\u914d\u3002", "conclusion": "AutoEP\u4e3a\u81ea\u52a8\u5316\u8d85\u53c2\u6570\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u6613\u4e8e\u8bbf\u95ee\u7684\u65b0\u8303\u4f8b\u3002"}}
