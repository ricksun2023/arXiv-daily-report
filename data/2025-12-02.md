<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 46]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 47]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis](https://arxiv.org/abs/2512.00046)
*Angelina Parfenova,Andreas Marfurt,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型(llm)自动进行定性数据分析，重点关注归纳编码。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于传统方法依赖于预定义标签的演绎方法，而本研究则着眼于标签从数据中产生的归纳过程。

Method: 该研究评估了六个开源llm与人类专家相比的性能，专家对他们编码的引用的感知难度进行了评级。

Result: 结果揭示了一个奇特的二分法：人类编码员在标记复杂句子时表现良好，但在处理简单句子时却很吃力，而llm则表现出相反的趋势。

Conclusion: 虽然人类注释有时可能与黄金标准不同，但它们通常会受到其他人的好评。相比之下，一些llm表现出与真实标签更紧密的对齐，但受到专家的评价较低。

Abstract: This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.

</details>


### [2] [Emergent Convergence in Multi-Agent LLM Annotation](https://arxiv.org/abs/2512.00047)
*Angelina Parfenova,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 大型语言模型越来越多地被部署在协作环境中，但人们对它们在被视为黑盒代理时如何进行协调知之甚少。本研究模拟了 7500 个多智能体、多轮讨论，生成了超过 125000 条话语，捕捉了最终注释及其交互历史。引入了过程级指标：代码稳定性、语义自洽性和词汇置信度以及情感和收敛性指标，以跟踪协调动态。为了更深入地探测对齐信号，分析了输出嵌入的演化几何结构，表明内在维度随轮数下降，表明语义压缩。结果表明，LLM 组在词汇和语义上趋于收敛，发展出不对称的影响模式，并表现出类似谈判的行为，尽管缺乏明确的角色提示。这项工作展示了黑盒交互分析如何浮现出涌现的协调策略，从而为基于内部探测的可解释性方法提供了可扩展的补充。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在协作环境中作为黑盒代理时的协调方式。

Method: 模拟 7500 个多智能体、多轮讨论，生成超过 125000 条话语，并引入过程级指标（代码稳定性、语义自洽性和词汇置信度）以及情感和收敛性指标来跟踪协调动态。分析输出嵌入的演化几何结构以探测更深入的对齐信号。

Result: LLM 组在词汇和语义上趋于收敛，发展出不对称的影响模式，并表现出类似谈判的行为。

Conclusion: 黑盒交互分析可以揭示大型语言模型涌现的协调策略，为基于内部探测的可解释性方法提供了补充。

Abstract: Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.

</details>


### [3] [Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees](https://arxiv.org/abs/2512.00204)
*Jason Lunder*

Main category: cs.CL

TL;DR: 本文提出了一种名为树匹配网络（TMN）的模型，该模型利用依赖解析树进行自然语言推理（NLI）任务。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的模型（如BERT）在NLI任务中表现出色，但参数量巨大。本文旨在探索利用显式语言结构（如依赖解析树）能否提高学习效率。

Method: 本文修改了图匹配网络（GMN），使其能够处理依赖解析树，从而创建了树匹配网络（TMN）。

Result: 在SNLI蕴含任务上，TMN以更小的内存占用和更短的训练时间，显著优于基于BERT的模型。但在SemEval相似度任务上，两个模型表现均不佳。

Conclusion: 显式结构化表示在相当的规模下显著优于基于序列的模型，但当前的聚合方法限制了可扩展性。本文提出了多头注意力聚合来解决这一限制。

Abstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.

</details>


### [4] [Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis](https://arxiv.org/abs/2512.00214)
*Matej Klemen,Tjaša Arčon,Luka Terčon,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 大型语言模型可以通过分析带注释的语料库，并为语言问题生成可解释的、数据驱动的答案，从而简化经验语法研究。


<details>
  <summary>Details</summary>
Motivation: 系统地分析带注释的语料库仍然需要大量的方法和技术工作。

Method: 我们引入了一个基于语料库的语法分析的代理框架，该框架集成了自然语言任务解释、代码生成和数据驱动推理等概念。我们将其应用于通用依存关系 (UD) 语料库，并在受世界语言结构图集 (WALS) 启发的语言语法任务上对其进行测试。

Result: 该评估涵盖 13 个词序特征和 170 多种语言，评估系统在三个互补维度上的性能——主要顺序准确性、顺序覆盖完整性和分布保真度——反映了系统概括、识别和量化词序变体的能力。

Conclusion: 结果表明，将大型语言模型推理与结构化语言数据相结合是可行的，这为基于语料库的语法查询的可解释、可扩展的自动化迈出了第一步。

Abstract: Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.

</details>


### [5] [Minimal-Edit Instruction Tuning for Low-Resource Indic GEC](https://arxiv.org/abs/2512.00219)
*Akhil Rajeev P*

Main category: cs.CL

TL;DR: 本文提出了一种用于印度语言语法纠错的无增强设置，该设置使用指令调整的大型语言模型和保守解码。


<details>
  <summary>Details</summary>
Motivation: 印度语言的语法纠错面临监督有限、脚本多样和形态丰富等问题。

Method: 使用具有参数高效微调 (PEFT) 和 Alpaca 风格格式的 bnb 4 位精度指令调整的 12B GEMMA 3 模型。解码遵循确定性的、约束感知的程序，该程序具有轻量级归一化器，可鼓励最小的、保留含义的编辑。通过从确定性错误分类器的分类法、标签分布和在训练语料库上计算的优先级顺序直接合成的固定、特定于语言的提示来操作指令微调 (IFT) 之后的推理。

Result: 在官方未调整的 GLEU 评估下，该系统在马拉雅拉姆语上的得分为 92.41，总排名第六，在印地语上的得分为 81.44，总排名第三。

Conclusion: 分类器告知的提示设计、基于适配器的指令调整和确定性解码为印度 GEC 提供了一种可重复且计算效率高的替代方案，以替代以增强为中心的管道。该方法还激发了未来在更强的形态句法约束和以人为本的保守编辑评估方面的工作。

Abstract: Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.
  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.

</details>


### [6] [OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion](https://arxiv.org/abs/2512.00234)
*Sai Koneru,Matthias Huck,Jan Niehues*

Main category: cs.CL

TL;DR: OmniFusion通过融合多模态基础模型（MMFMs）和翻译大型语言模型（LLMs），实现端到端的语音、图像和文本翻译， 并在SimulST中降低了延迟并提高了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有的语音翻译模型通常采用级联管道，导致延迟增加且无法利用多模态信息。多模态基础模型缺乏多语言覆盖和专业翻译性能。

Method: 提出了一种新颖的融合策略，将预训练的MMFM的多层隐藏状态连接到翻译LLM，实现联合端到端训练。

Result: OmniFusion在SimulST中实现了1秒的延迟降低，并提高了整体翻译质量。

Conclusion: OmniFusion有效利用了音频和视觉输入，为多模态翻译系统提供了一种有效的方法。

Abstract: There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.

</details>


### [7] [Lost without translation -- Can transformer (language models) understand mood states?](https://arxiv.org/abs/2512.00274)
*Prakrithi Shivaprakash,Diptadhi Mukherjee,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 大型语言模型在理解印度语言的情绪状态方面存在困难，这阻碍了它们在印度精神病学中的应用。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在印度语言中理解不同情绪状态（抑郁、心境正常、欣快狂躁、烦躁狂躁）的能力。

Method: 收集了11种印度语言中四种情绪状态的247个短语，并测试了七种实验条件，比较k-means聚类性能。

Result: 直接嵌入印度语言无法聚类情绪状态，而基于翻译的方法显示出显著改善。使用 Gemini 翻译成英语和人工翻译成英语表现良好，但人工翻译成英语再翻译成中文表现最佳。专门的印度模型表现不佳。

Conclusion: 目前的模型无法直接理解印度语言的情绪状态，依赖专有模型或复杂的翻译流程是不可持续的。

Abstract: Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.

</details>


### [8] [EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education](https://arxiv.org/abs/2512.00290)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Yue Cui,Jiawei Shen,Zilong Li,Yidan Liang*

Main category: cs.CL

TL;DR: 本文提出了EduEval，一个用于评估大型语言模型在中文K-12教育中表现的综合性分层基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育应用中显示出巨大潜力，但未经审查的部署会给教育标准带来风险，因此需要严格评估。

Method: 构建了EduEval基准，包含认知框架（EduAbility Taxonomy，统一了Bloom分类法和Webb深度知识）、真实性（整合真实考试题、课堂对话、学生作文和专家设计的提示）和规模（包含24种不同任务类型，超过11000个问题，涵盖小学到高中水平）。

Result: 评估了14个领先的LLM在零样本和小样本设置下的表现，发现模型在事实性任务上表现良好，但在课堂对话分类方面表现不佳，并且在创造性内容生成方面表现出不一致的结果。一些开源模型在复杂的教育推理方面优于专有系统。小样本prompting在不同认知维度上的有效性各不相同。

Conclusion: 这些发现为开发专门针对不同中国教育任务进行优化的大型语言模型提供了有针对性的基准指标。

Abstract: Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.

</details>


### [9] [Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets](https://arxiv.org/abs/2512.00323)
*Muhammad Muneeb,David B. Ascher,Ahsan Baidar Bakht*

Main category: cs.CL

TL;DR: 本文对来自 Hugging Face 的 47 个 CBQA 模型在八个不同数据集上进行了基准测试，旨在确定在无需额外微调的情况下，跨不同数据集表现最佳的模型。


<details>
  <summary>Details</summary>
Motivation: 在各种应用程序中，基于上下文的问答 (CBQA) 模型通过考虑上下文信息来提供更准确和相关的答案。本研究旨在找到在各种数据集上表现最好的模型，从而减少为特定数据集重新训练模型的需求。

Method: 对 47 个 Hugging Face 的 CBQA 模型在八个数据集上进行了基准测试，并使用遗传算法来提高整体准确性。

Result: ahotrod/electra_large_discriminator_squad2_512 模型在多个数据集上表现最佳，整体准确率为 43%。计算时间取决于上下文长度和模型大小，模型性能通常随着答案长度的增加而降低。遗传算法也被用于提高整体准确性。

Conclusion: 最佳模型是在 SQuAD v2 或 SQuAD v1 数据集上训练的。

Abstract: Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\%), biomedical\_cpgQA (96.45\%), QuAC (11.13\%), and Question Answer Dataset (41.6\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\% on the IELTS dataset.

</details>


### [10] [Evidence-Guided Schema Normalization for Temporal Tabular Reasoning](https://arxiv.org/abs/2512.00329)
*Ashish Thanga,Vibhu Dixit,Abhilash Shankarampeta,Vivek Gupta*

Main category: cs.CL

TL;DR: 该论文提出了一种基于SQL的方法，用于处理不断变化的半结构化表格上的时间推理问答任务。


<details>
  <summary>Details</summary>
Motivation: 当前问答系统在处理不断变化的半结构化表格上的时间推理方面面临挑战。

Method: 该方法包括：(1) 从维基百科信息框生成3NF模式；(2) 生成SQL查询；(3) 查询执行。

Result: 最好的配置（Gemini 2.5 Flash schema + Gemini-2.0-Flash 查询）达到了80.39 EM，比基线提高了16.8%。

Conclusion: 模式设计的质量对问答精确度的影响大于模型容量。建立了三个基于证据的原则：保持上下文的规范化、减少歧义的语义命名和一致的时间锚定。

Abstract: Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).

</details>


### [11] [Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing](https://arxiv.org/abs/2512.00390)
*Nuo Chen,Hanpei Fang,Jiqun Liu,Wilson Wei,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 探讨了大型语言模型（LLM）在相关性标注中受启动效应影响的问题，并研究了LLM中模拟的“大五”人格特征如何影响这种启动效应。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于发现大型语言模型在相关性标注中容易受到启动效应的影响，并且这种效应可能与心理学中的人格特质有关。因此，研究旨在探索LLM中的人格模拟是否也会表现出类似的影响。

Method: 通过在TREC 2021和2022深度学习 ट्रैक数据集上，使用多个LLM，并模拟“大五”人格特征，来观察不同人格配置的LLM在相关性标注中受启动效应影响的程度。

Result: 研究结果表明，某些人格特征，如高开放性和低神经质，能够持续降低启动效应的敏感性。此外，在减轻启动效应方面最有效的人格特征可能因模型和任务类型的不同而有所差异。

Conclusion: 研究表明，人格Prompting可以作为一种减轻阈值启动效应的方法，从而将心理学证据与基于LLM的评估实践联系起来。

Abstract: Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.

</details>


### [12] [Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents](https://arxiv.org/abs/2512.00332)
*Daud Waqas,Aaryamaan Golthi,Erika Hayashida,Huanzhi Mao*

Main category: cs.CL

TL;DR: 多轮工具调用LLM在现代AI助手中至关重要，但其安全性和鲁棒性仍令人担忧。本文提出了Assertion-Conditioned Compliance (A-CC)评估范式，用于评估模型在面对误导性断言时的行为。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对多轮对话鲁棒性的可见性，尤其是在实际系统中。模型容易受到用户来源断言（USA）的奉承和功能来源断言（FSA）的策略冲突的影响。

Method: 提出了Assertion-Conditioned Compliance (A-CC)评估范式，该范式通过用户来源断言（USA）和功能来源断言（FSA）来评估模型的行为。

Result: 模型在用户来源断言（USA）奉承和功能来源断言（FSA）策略冲突方面表现出高度脆弱性。

Conclusion: A-CC是已部署代理中一个关键的潜在漏洞。

Abstract: Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.

</details>


### [13] [IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages](https://arxiv.org/abs/2512.00333)
*Ayush Maheshwari,Kaushal Sharma,Vivek Patel,Aditya Maheshwari*

Main category: cs.CL

TL;DR: IndicParam是一个包含11种低资源印度语言的基准测试，用于评估大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在处理高资源多语言任务时表现出色，但在低资源印度语言上的表现未得到充分评估。

Method: 构建了一个名为IndicParam的人工管理的基准测试，其中包含超过13,000个多项选择题，涵盖11种印度语言。

Result: 评估了19个LLM，包括专有模型和开源模型，结果显示即使是表现最佳的GPT-5也仅达到45.0％的平均准确率。

Conclusion: IndicParam揭示了跨语言迁移的局限性，并为印度语言建立了一个具有挑战性的基准。

Abstract: While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.

</details>


### [14] [CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA](https://arxiv.org/abs/2512.00360)
*Vsevolod Kovalev,Parteek Kumar*

Main category: cs.CL

TL;DR: 本文研究了在单个GPU延迟/内存预算下，教育讲座视频的时间戳问题解答。系统检索相关的时间戳片段并合成一个基于此的答案。


<details>
  <summary>Details</summary>
Motivation: 研究教育讲座视频的时间戳问题解答，旨在提高检索效率和答案质量。

Method: 提出了一个轻量级的、延迟约束的跨模态检索器 (CrossFusion-RAG)，它结合了冻结编码器、学习到的视觉投影、浅层查询无关的跨注意力以及时间一致性正则化。

Result: 在CourseTimeQA数据集上，CrossFusion-RAG比BLIP-2检索器的nDCG@10提高了0.10，MRR提高了0.08，同时在单个A100上实现了约1.55秒的中值端到端延迟。

Conclusion: CrossFusion-RAG在教育讲座视频的时间戳问题解答任务中表现出色，并在效率和准确性之间取得了良好的平衡。

Abstract: We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.

</details>


### [15] [MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications](https://arxiv.org/abs/2512.01710)
*Stefano Zeppieri*

Main category: cs.CL

TL;DR: 提出了一个混合记忆增强生成 (MMAG) 模式，用于提升 LLM 代理在扩展交互中的连贯性、个性化和连续性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 擅长在单个提示中生成连贯的文本，但在扩展的交互中保持相关性、个性化和连续性方面存在不足。人类交流依赖于多种形式的记忆。

Method: 将 LLM 代理的记忆组织成五个交互层：会话记忆、长期用户记忆、情景和事件链接记忆、感觉和上下文感知记忆以及短期工作记忆。将这些层映射到技术组件，并概述了协调、优先级排序和冲突解决的策略。

Result: 通过在 Heero 会话代理中的实施证明了该方法，其中加密的长期简历和会话历史已经提高了参与度和保留率。

Conclusion: MMAG 为构建记忆丰富的语言代理奠定了基础，这些代理更连贯、更主动并且更符合人类需求。

Abstract: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.

</details>


### [16] [A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction](https://arxiv.org/abs/2512.00392)
*Damian Heywood,Joseph Andrew Carrier,Kyu-Hong Hwang*

Main category: cs.CL

TL;DR: 本研究开发了一个AI辅助的错误分析系统，用于识别、分类和纠正英语写作错误。


<details>
  <summary>Details</summary>
Motivation: 旨在通过AI自动进行详细的错误分析和反馈，从而改变EFL教学。

Method: 利用大型语言模型（LLM），采用基于语言学理论的详细分类法，通过Python API调用实现。

Result: AI成功识别了多种错误类型，但在上下文理解方面存在局限性，并且在遇到未编码的错误时偶尔会生成新的错误类别。

Conclusion: AI在自动化详细错误分析和反馈方面具有潜力，但需要进一步发展以提高上下文准确性并扩展到文体和语篇层面的错误。

Abstract: This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used "English as she is spoke" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.

</details>


### [17] [CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency](https://arxiv.org/abs/2512.00417)
*Jiacheng Guo,Suozhi Huang,Zixin Yao,Yifan Zhang,Yifu Lu,Jiashuo Liu,Zihao Li,Yanyan Deng,Qixin Xiao,Jia Tian,Kanghong Zhan,Tianyi Li,Xiaochen Liu,Jason Ge,Chaoyang He,Kaixuan Huang,Lin Yang,Wenhao Huang,Mengdi Wang*

Main category: cs.CL

TL;DR: CryptoBench是一个专门评估大型语言模型（LLM）在加密货币领域能力的动态基准。


<details>
  <summary>Details</summary>
Motivation: 现有的通用基准不能满足加密货币领域对LLM的独特要求，如时间敏感性、对抗性信息环境和多样化数据源。

Method: 构建了一个包含每月50个问题的动态基准，问题由加密货币专业人士设计，并分为简单检索、复杂检索、简单预测和复杂预测四个象限。

Result: 对10个LLM的评估揭示了性能等级和一种失败模式：检索-预测失衡，即模型在数据检索方面表现出色，但在预测分析方面存在明显不足。

Conclusion: 许多模型在检索数据方面表现出色，但在综合信息进行深入分析方面存在不足。

Abstract: This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.
  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.

</details>


### [18] [SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling](https://arxiv.org/abs/2512.00466)
*Yang Xiao,Chunpu Xu,Ruifeng Yuan,Jiashuo Wang,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 提出了一个名为 SCALE 的框架，该框架根据子问题的难度选择性地分配计算资源，从而提高了数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在推理时采用统一的资源分配方式，导致难题得不到足够的重视，而例行操作却消耗了过多的资源，造成了性能瓶颈。

Method: 该框架包含四个阶段：问题分解、难度评估、选择性处理模式分配和顺序执行。

Result: SCALE 显著优于统一缩放基线，在 AIME25 上实现了高达 13.75 个百分点的准确率提升（57.50% 提升至 71.25%），同时降低了 33%-53% 的计算成本。

Conclusion: SCALE 通过将资源集中在具有挑战性的子问题上，同时高效地处理例行操作，从而在测试时扩展方面取得了重大进展。

Abstract: Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.

</details>


### [19] [CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning](https://arxiv.org/abs/2512.00496)
*Diego A. B. Moreira,Alef I. Ferreira,Jhessica Silva,Gabriel O. dos Santos,Gustavo Bonil,João Gondim,Marina dos Santos,Helena Maia,Simone Hashiguti,Nádia da Silva,Carolina Scarton,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: CACARA模型通过新兴对齐学习，实现了多模态和多语言的扩展，无需完全重新训练，并且计算成本较低。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型训练成本高昂，且扩展到新语言通常需要大量的资源。

Method: 提出了一种多模态和多语言架构CACARA，通过新兴对齐学习进行训练，允许将新模态无缝集成到现有的双模态/多模态模型中，而无需完全重新训练。

Result: 在音频到文本检索方面，R@1指标提高了14.24个百分点，优于最先进的多模态模型。

Conclusion: 该策略能够高效地获得新兴的多模态和多语言属性，以与单语模型相当的训练成本保留先前学习的知识。

Abstract: As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.

</details>


### [20] [G-KV: Decoding-Time KV Cache Eviction with Global Attention](https://arxiv.org/abs/2512.00504)
*Mengqi Liao,Lu Wang,Chaoyun Zhang,Zekai Shen,Xiaowei Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Huaiyu Wan*

Main category: cs.CL

TL;DR: G-KV 通过结合局部和历史注意力分数来评估token的重要性，从而更准确地进行 KV 缓存淘汰，并使用后训练技术优化模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常侧重于提示压缩或具有局部注意力分数的token淘汰，忽略了token的长期重要性。因此，推理大型语言模型(LLM)在复杂任务中表现出色，但由于序列长度过长，面临着巨大的计算和内存挑战。

Method: G-KV 是一种 KV 缓存淘汰方法，它采用全局评分机制，结合局部和历史注意力分数来更准确地评估token的重要性。此外，我们还引入了后训练技术，包括强化学习和蒸馏，以优化压缩 KV 缓存设置的模型。

Result: 该论文提出了 G-KV。

Conclusion: G-KV 是一种用于压缩 KV 缓存设置的 KV 缓存淘汰方法，它采用全局评分机制，结合局部和历史注意力分数来更准确地评估token的重要性, 并使用后训练技术优化模型。

Abstract: Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.

</details>


### [21] [Developing a Comprehensive Framework for Sentiment Analysis in Turkish](https://arxiv.org/abs/2512.00515)
*Cem Rifki Aydin*

Main category: cs.CL

TL;DR: 本论文开发了一个综合情感分析框架，主要针对土耳其语，也为英语提出了几种情感分析方法。


<details>
  <summary>Details</summary>
Motivation: 情感分析在土耳其语中的应用相对较少，且现有方法在处理土耳其语和英语情感分析时存在局限性。

Method: 结合了非监督、半监督和监督指标，生成新的特征集，并将其输入到传统机器学习方法中。构建了新颖的神经网络架构，结合了循环和递归神经网络模型。创建了利用情感、句法、语义和词汇特征的新型词嵌入。

Result: 在土耳其语和英语的不同类型数据集中，传统机器学习方法的性能优于神经网络模型。在所有原创方法上都取得了最先进和显著的成果。

Conclusion: 本论文是截至2020年7月，针对土耳其语情感分析最详细和全面的研究。该研究也为英语的观点分类问题做出了贡献。

Abstract: In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.

</details>


### [22] [Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity](https://arxiv.org/abs/2512.00552)
*Subramanyam Sahoo,Vinija Jain,Saanidhya Vats,Siddharth Mohapatra,Rui Min,Aman Chadha,Divya Chaudhary*

Main category: cs.CL

TL;DR: 当前对语言模型数学推理的评估主要依赖于答案的准确性，这可能会掩盖逻辑计算中的根本性失败。我们引入了一个诊断框架，通过四个互补的轴来区分真正的数学推理和表面模式匹配：前后一致性、传递性覆盖、反事实敏感性和扰动鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前对语言模型数学推理评估主要依赖于答案准确性，可能掩盖逻辑计算根本性失败的问题。

Method: 通过案例研究，将该框架应用于 MenatQA 数据集上的 Qwen3-0.6B。

Result: 该模型实现了合理的答案准确率（70%+），但在后向一致性（15%）、有限的传递性覆盖率（32.2%）和对扰动的脆弱敏感性方面表现不佳。诊断结果揭示了传统准确性指标无法察觉的推理失败，表明这个小型模型严重依赖模式匹配而不是真正的逻辑计算。

Conclusion: 虽然我们的经验结果是基于单个 6 亿参数模型，但诊断框架本身是模型不可知的和可推广的。我们发布了评估协议，使研究界能够跨不同的模型规模和架构评估推理保真度，从而超越表面水平的准确性，转向可验证的数学推理。

Abstract: Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.

</details>


### [23] [Slovak Conceptual Dictionary](https://arxiv.org/abs/2512.00579)
*Miroslav Blšták*

Main category: cs.CL

TL;DR: 本文介绍了一种新的斯洛伐克语概念词典，是同类语言工具中的首创。


<details>
  <summary>Details</summary>
Motivation: 由于斯洛伐克语是一种语言资源有限的语言，目前还没有足够大的机器可读语言数据源，因此许多需要自动处理斯洛伐克语文本的任务与其他语言相比，效果较差，几乎无法解决。

Method: 创建新的斯洛伐克语概念词典。

Result: 创建了首个斯洛伐克语概念词典。

Conclusion: 创建了首个斯洛伐克语概念词典，为斯洛伐克语的自然语言处理任务提供了一种新的资源。

Abstract: When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.

</details>


### [24] [Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2512.00590)
*Alla Chepurova,Aydar Bulatov,Yuri Kuratov,Mikhail Burtsev*

Main category: cs.CL

TL;DR: 本研究提出了一种名为Wikontic的多阶段流程，用于从开放域文本构建知识图谱(KG)。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的系统通常使用KG作为文本检索的辅助结构，其内在质量未被充分探索。

Method: Wikontic通过提取带有限定词的候选三元组，实施基于Wikidata的类型和关系约束，并规范化实体来减少重复，从而构建KG。

Result: 生成的KG结构紧凑、本体一致且连接良好。在MuSiQue上，正确答案实体出现在96%生成的三元组中。在HotpotQA上，我们的仅三元组设置达到了76.0 F1，在MuSiQue上达到了59.8 F1，匹配或超过了一些仍然需要文本上下文的检索增强生成基线。此外，Wikontic在MINE-1基准测试中获得了最先进的信息保留性能(86%)，优于之前的KG构建方法。Wikontic在构建时也很高效：KG构建使用少于1,000个输出tokens，比AriGraph少大约3倍，比GraphRAG少<1/20。

Conclusion: 该流程提高了生成的KG的质量，并为利用llm中的结构化知识提供了一个可扩展的解决方案。

Abstract: Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.

</details>


### [25] [Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior](https://arxiv.org/abs/2512.00611)
*Franck Binard,Vanja Kljajevic*

Main category: cs.CL

TL;DR: Prism是一个用于指定工具使用软件代理行为的小型、可组合的元语言。


<details>
  <summary>Details</summary>
Motivation: 现有的控制结构是临时的，Prism围绕一个固定的核心上下文构建，该上下文提供了类别、数字、字符串、用户提示、工具以及布尔值、谓词、对和列表的抽象组合器的最小背景语法。

Method: Agent策略被编写为普通表达式，使用单个抽象运算符，以便条件显示为替代方案之间的选择，而不是命令式if-else块。域通过定义自己的上下文-迷你语法来扩展核心，这些语法引入了新的类别、谓词和外部工具，同时重用相同的组合机制。

Result: 通过来自恒温器控制、家庭安全、电子商务推荐和医疗监控的工作示例来说明这一点，展示了自然语言决策规则如何映射到可检查、可执行的策略。

Conclusion: 从语言学的角度来看，Prism强制执行可重用的类似语法的核心和特定于领域的词典之间的清晰分离，并将工具视为内部策略表示和外部世界之间的桥梁。从工程的角度来看，它为代理控制提供了一个紧凑的接口语言，使可能的操作空间显式且易于分析、验证和安全约束。

Abstract: Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.

</details>


### [26] [ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization](https://arxiv.org/abs/2512.00617)
*Omer Jauhar Khan*

Main category: cs.CL

TL;DR: 本文提出了一种名为 ART (Adaptive Response Tuning) 的新框架，该框架通过锦标赛式 ELO 排名和多智能体推理来系统地优化 LLM 输出。


<details>
  <summary>Details</summary>
Motivation: 现有的单模型 LLM 回复通常表现出不一致、幻觉以及不同查询域之间的质量差异。

Method: ART 框架允许多个 LLM 智能体通过结构化的锦标赛工作流程进行竞争、评论和协作，从而生成共识回复，并且引入可配置的锦标赛参数、动态智能体选择和多种共识融合策略。

Result: 实验评估表明，与基线单模型方法相比，响应准确性、连贯性和可靠性得到了显着提高。整体质量指标提高了 8.4%，并且 ELO 评级收敛中的 R22 值超过 0.96。

Conclusion: ART 框架为需要高质量、经过审核的 LLM 回复的应用程序提供了一个可扩展的、可用于生产的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.

</details>


### [27] [Sycophancy Claims about Language Models: The Missing Human-in-the-Loop](https://arxiv.org/abs/2512.00656)
*Jan Batzner,Volker Stocker,Stefan Schmid,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文回顾了大型语言模型 (LLM) 中奉承反应模式的测量方法，并指出了其中存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前研究未能评估人类对 LLM 奉承行为的感知。

Method: 本文回顾了测量 LLM 奉承行为的方法，并确定了五个核心操作。

Result: 分析强调了区分奉承反应与 AI 对齐中相关概念的难度。

Conclusion: 为未来的研究提供了可操作的建议。

Abstract: Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.

</details>


### [28] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个利用交互式可视化知识图谱来减少大型语言模型幻觉的框架。


<details>
  <summary>Details</summary>
Motivation: 企业环境中的大型语言模型经常使用闭源领域知识进行增强，但由于上下文窗口有限和预训练数据与提供的知识之间不一致，导致模型产生幻觉。当前的缓解策略要么依赖于昂贵的大规模黄金标准问答管理，要么依赖于二级模型验证，但都不能提供确定性的保证。

Method: 该框架将专有知识和模型生成的内容组织成交互式可视化知识图谱。通过将模型断言链接到基本事实来源并指示置信度，为最终用户提供潜在幻觉区域的清晰直观视图。

Result: 通过这种可视化界面，用户可以诊断不一致之处，识别薄弱的推理链，并提供纠正性反馈。

Conclusion: 由此产生的人工参与工作流程创建了一个结构化的反馈循环，可以提高模型的可靠性并不断提高响应质量。

Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [29] [A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data](https://arxiv.org/abs/2512.00673)
*Breanna E. Green,Ashley L. Shea,Pengfei Zhao,Drew B. Margolin*

Main category: cs.CL

TL;DR: 本研究评估了ChatGPT在复杂任务（如分类和注释包含细微差别的语言的数据集）中的性能，并将其结果与人工注释员进行了比较。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在计算社会科学中的应用日益广泛，但对于ChatGPT在复杂任务中的性能仍需进一步理解。

Method: 研究采用了GPT-4，并与人工注释员的结果进行比较。 исследовали ChatGPT 3.5, 4 和 4o版本，使用了四种prompt样式作为输入，并评估了精确率、召回率和F1分数。

Result: 结果表明，虽然在提示中包含标签定义可能有助于提高性能，但GPT-4在分类细微语言方面存在困难。

Conclusion: 在涉及细微语言的分类任务中使用ChatGPT应谨慎。

Abstract: Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.

</details>


### [30] [FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case](https://arxiv.org/abs/2512.00745)
*Md Abdullah Al Kafi,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本研究提出了一种与语言无关的、基于Transformer的词性标注框架，专为低资源语言设计，以孟加拉语和印地语为案例研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决低资源语言的词性标注问题。

Method: 使用基于Transformer的架构，只需少量特定于框架的代码即可从孟加拉语调整到印地语。

Result: 在孟加拉语和印地语的POS类别中，实现了96.85%和97%的token级别准确率，并在数据集不平衡和语言重叠的情况下保持了强大的F1分数。

Conclusion: 该框架的模块化和开源设计实现了快速的跨语言适应，同时减少了模型设计和调整开销，从而使研究人员可以专注于语言预处理和数据集改进。

Abstract: This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.

</details>


### [31] [Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation](https://arxiv.org/abs/2512.00789)
*Xiaodong Cai,Hai Lin,Shaoxiong Zhan,Weiqi Luo,Hong-Gee Kim,Hongyan Hao,Yu Yang,Hai-Tao Zheng*

Main category: cs.CL

TL;DR: 提出了一种名为熵平衡抽样 (EES) 的无超参数方法，用于动态调整候选集，平衡归一化熵和概率质量。


<details>
  <summary>Details</summary>
Motivation: 现有的 token 抽样策略会引入额外的超参数，需要大量的调整，从而使部署复杂化。

Method: 提出了一种受信息论启发的辅助无超参数方法，该方法可以通过平衡归一化熵与概率质量来动态调整候选集。

Result: EES 在各种模型架构的推理和生成任务中表现良好，在温度设置中提供具有竞争力的准确性和连贯性，同时保持多样性。

Conclusion: EES 消除了对超参数调整的需求，大大简化了部署，同时提高了性能。

Abstract: Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES

</details>


### [32] [Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy](https://arxiv.org/abs/2512.00829)
*Md Mehrab Hossain Opi,Sumaiya Khan,Moshammad Farzana Rahman*

Main category: cs.CL

TL;DR: 本研究探讨了自动混合精度（AMP）训练，以提高计算效率，而不牺牲模型性能，特别是在孟加拉语NLP开发中。


<details>
  <summary>Details</summary>
Motivation: 在孟加拉语的自然语言处理（NLP）中，由于硬件资源的限制，训练模型需要大量的计算资源和时间，这是一个巨大的挑战。

Method: 通过利用16位和32位浮点计算的动态组合，AMP降低了GPU内存需求，并加快了训练速度，同时不降低模型性能。在四个标准的孟加拉语NLP任务（情感分析、命名实体识别、错误分类和问答）中使用四个基于Transformer的模型：BanglaBERT、BanglishBERT、XLM-R和mBERT评估AMP。

Result: 结果表明，AMP加速了44.5%的训练速度，并降低了17.6%的内存消耗，同时保持F-1分数在全精度基线的99.7%以内。

Conclusion: 这项实证研究强调了AMP通过降低计算壁垒，有可能在硬件受限的环境中普及最先进的NLP能力。

Abstract: Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.

</details>


### [33] [WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](https://arxiv.org/abs/2512.00837)
*Yukang Lin,Jiahao Shao,Shuoran Jiang,Wentao Zhu,Bingjie Lu,Xiangping Wu,Joanna Siebert,Qingcai Chen*

Main category: cs.CL

TL;DR: 提出了一种名为WaterSearch的新的基于搜索的文本水印框架，该框架通过控制种子池来促进带水印文本的并行生成，从而提高文本质量，并在各种攻击下保持高可检测性。


<details>
  <summary>Details</summary>
Motivation: 现有的水印方法在可检测性和文本质量之间存在权衡，因为强大的水印信号会降低下游任务的性能。

Method: 设计了一种新的嵌入方案，该方案控制种子池以促进带水印文本的并行生成，并提出了WaterSearch框架，该框架通过联合优化分布保真度和水印信号特征来增强文本质量。此外，还采用了一种具有强大抗攻击能力的句子级检测方法。

Result: 在三个流行的LLM和十个不同的任务中，WaterSearch的性能比最先进的基线提高了51.01％，水印可检测强度为95％。在短文本生成和低熵输出生成等具有挑战性的场景中，性能分别提高了47.78％和36.47％。在不同的攻击场景下，WaterSearch保持了很高的可检测性。

Conclusion: WaterSearch在提高文本质量和保持水印可检测性方面表现出色，并且具有很强的抗攻击能力。

Abstract: Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.
  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.

</details>


### [34] [Less is More: Resource-Efficient Low-Rank Adaptation](https://arxiv.org/abs/2512.00878)
*Chunlin Tian,Xuyang Wei,Huanrong Liu,Zhijiang Guo,Li Li*

Main category: cs.CL

TL;DR: EffiLoRA outperforms LoRA in diverse modalities by employing a unified A matrix and a runtime selective B matrices update, which dynamically trade-off the system resource budget and model performance.


<details>
  <summary>Details</summary>
Motivation: LoRA has notable overhead and suffers from parameter interference in complex datasets. Recent works decouple LoRA update matrices, but training costs remain high.

Method: EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices update.

Result: EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demonstrating improved efficiency and robustness.

Conclusion: EffiLoRA is a lightweight and generalizable approach for language, multimodal, and diffusion models.

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.

</details>


### [35] [Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios](https://arxiv.org/abs/2512.00920)
*Jianxiang Zang,Yongda Wei,Ruxue Bai,Shiyu Jiang,Nijia Mo,Binhong Li,Qiang Sun,Hui Liu*

Main category: cs.CL

TL;DR: 当前奖励模型（RM）的评估方法侧重于特定场景下的偏好感知准确性，忽略了RM在现实场景中的关键漏洞。本文提出了Reward Auditor，一个用于RM适用性推断的假设检验框架，通过量化RM偏好感知置信度的分布退化，从而推断RM在不同现实场景中的漏洞。


<details>
  <summary>Details</summary>
Motivation: 评估奖励模型(RM)在现实场景中的适用性，发现并量化其潜在漏洞。

Method: 提出Reward Auditor框架，通过假设检验量化RM在现实扰动下的偏好感知置信度分布退化，从而推断RM的适用性。

Result: 量化了RM在现实场景中的漏洞，为构建更安全、更鲁棒和更值得信赖的下一代LLM对齐系统奠定了基础。

Conclusion: Reward Auditor框架能够有效评估RM在现实场景中的适用性，并识别其潜在漏洞。

Abstract: Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.

</details>


### [36] [Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study](https://arxiv.org/abs/2512.00931)
*Imane Jaaouine,Ross D. King*

Main category: cs.CL

TL;DR: Prompt engineering (PE) can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts.


<details>
  <summary>Details</summary>
Motivation: LLMs produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt.

Method: Six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2).

Result: CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts.

Conclusion: Prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.

Abstract: Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.

</details>


### [37] [DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics](https://arxiv.org/abs/2512.00938)
*Ahmed Mustafa Younes*

Main category: cs.CL

TL;DR: 论文介绍了一个名为 DeformAr 的新框架，用于调试和评估基于 Transformer 的阿拉伯语 NER 系统，旨在弥合阿拉伯语和英语 NER 系统之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能捕捉到分词、数据集质量和注释不一致等因素对阿拉伯语 NER 系统性能的共同影响，导致阿拉伯语 NER 性能落后于英语。

Method: DeformAr 框架集成了数据提取库和交互式仪表板，支持跨组件分析和行为分析两种评估模式，通过分析数据集和模型组件之间的交互来诊断性能差异。

Result: DeformAr 是第一个专门针对阿拉伯语、基于组件的可解释性工具，为推进资源匮乏语言的模型分析提供了关键资源。

Conclusion: DeformAr 框架通过系统性的诊断措施和行为分析，能够检测模型行为，并将其与潜在的表征模式和数据因素联系起来，从而解释阿拉伯语和英语 NER 系统之间的性能差异。

Abstract: Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.
  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.
  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the "what," "how," and "why" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.

</details>


### [38] [Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data](https://arxiv.org/abs/2512.00946)
*Alvaro Paredes Amorin,Andre Python,Christoph Weisser*

Main category: cs.CL

TL;DR: 本文研究了轻量级开源LLM在金融市场分析中的应用，旨在探索它们在有限计算资源下，利用不同规模、来源、格式和语言的金融数据集进行情感理解的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在金融市场分析中发挥着越来越重要的作用，但它们依赖于大量的计算资源和专有数据集，这对于许多研究人员和从业者来说是昂贵、受限且难以获得的。为了反映真实情况，我们需要研究轻量级开源LLM的能力。

Method: 本文比较了基准金融NLP模型FinBERT和三种开源轻量级LLM（DeepSeek-LLM 7B、Llama3 8B Instruct和Qwen3 8B）在五个公开数据集上的表现：FinancialPhraseBank、Financial Question Answering、Gold News Sentiment、Twitter Sentiment和Chinese Finance Sentiment。

Result: 研究发现，LLM，特别是Qwen3 8B和Llama3 8B，在大多数情况下表现最佳，即使仅使用5%的可用训练数据也是如此。这些结果在零样本和小样本学习场景中均成立。

Conclusion: 轻量级开源LLM是一种经济高效的选择，即使仅在通常被认为必要的广泛注释语料库的有限子集上进行训练，它们也可以在异构文本数据上实现具有竞争力的性能。

Abstract: Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.

</details>


### [39] [Table as a Modality for Large Language Models](https://arxiv.org/abs/2512.00947)
*Liyao Li,Chao Ye,Wentao Ye,Yifei Sun,Zhe Jiang,Haobo Wang,Jiaming Tian,Yiming Zhang,Ningtao Wang,Xing Fu,Gang Chen,Junbo Zhao*

Main category: cs.CL

TL;DR: 大型语言模型在表格推理任务中表现不佳，因为表格结构信息丢失。


<details>
  <summary>Details</summary>
Motivation: 现有方法简单地序列化表格数据，忽略了结构信息。

Method: 提出TAMO模型，将表格视为独立模态，使用超图神经网络作为全局表格编码器与主流LLM集成。

Result: 在多个数据集上取得了显著改进，平均相对增益为42.65%。

Conclusion: TAMO模型通过保留表格结构信息，显著提高了LLM在表格推理任务中的泛化能力。

Abstract: To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.

</details>


### [40] [Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent](https://arxiv.org/abs/2512.00986)
*Zhihan Guo,Feiyang Xu,Yifan Li,Muzhi Li,Shuai Zou,Jiele Wu,Han Shi,Haoli Bai,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 提出了Dr.Mi-Bench，一个用于评估科学研究型DR代理的模块化集成基准。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试通常过于关注检索，而忽略了高层次的规划和推理，并且偏向于通用领域而不是科学领域。

Method: 构建了一个人工注释的数据集，包含10个科学领域的200个实例，并提出了一个模块化集成的评估范式Dr.Mi-Eval。

Result: 实验结果表明，DR代理在多源检索和跨领域一致性方面存在明显不足，提高高层次规划能力是释放LLM推理潜力的关键。

Conclusion: Dr.Mi-Bench提供了一个诊断工具，用于指导更可靠的学术研究助手的开发。

Abstract: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.

</details>


### [41] [Advancing Academic Chatbots: Evaluation of Non Traditional Outputs](https://arxiv.org/abs/2512.00991)
*Nicole Favero,Francesca Salute,Daniel Hardt*

Main category: cs.CL

TL;DR: 本研究扩展了大型语言模型的评估范围，通过比较两种检索策略（Graph RAG 和 Advanced RAG）在问答方面的应用，并评估 LLM 生成高质量非传统学术输出（幻灯片和播客脚本）的能力。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估主要集中在标准任务上，如事实问答或简短摘要。本研究旨在扩展评估范围，探索 LLM 在更复杂和非传统学术输出方面的能力。

Method: 研究实现了结合 Meta 的 LLaMA 3 70B 开放权重模型和 OpenAI 的 GPT 4o mini API 的原型系统。通过人工评估和大型语言模型评估，对问答性能进行评估。幻灯片和播客生成通过文档检索进行测试。

Result: GPT 4o mini 结合 Advanced RAG 产生了最准确的回答。Graph RAG 的改进有限，且导致更多的幻觉。GPT 4o mini 在幻灯片和播客生成方面表现最佳，而 LLaMA 3 在叙事连贯性方面显示出潜力。

Conclusion: 人工审核对于检测布局和风格缺陷至关重要，突显了在评估新兴学术输出时需要结合人工 LLM 评估。

Abstract: Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.

</details>


### [42] [When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals](https://arxiv.org/abs/2512.01037)
*Riad Ahmed Anonto,Md Labid Al Nahiyan,Md Tanvir Hassan,Ch. Md. Rakin Haider*

Main category: cs.CL

TL;DR: 论文介绍了安全对齐的语言模型经常拒绝实际无害的提示的问题，并提出了一种新的评估框架来测量这种局部不一致性。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要报告全局指标，忽略了局部不一致性，限制了诊断和调整。

Method: 构建了一个包含10k提示的释义语料库ParaGuard，并提出了三个模型无关的token级别指标：Confusion Index、Confusion Rate和Confusion Depth。

Result: 实验表明，全局错误拒绝率掩盖了关键结构，提出的指标揭示了全局不稳定的边界、局部不一致的区域，以及更严格的拒绝并不总是增加不一致性的情况。

Conclusion: 通过引入confusion-aware auditing，可以区分系统拒绝的频率和拒绝的合理性，为开发者提供了一个实用的信号来减少错误拒绝，同时保持安全性。

Abstract: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.

</details>


### [43] [ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages](https://arxiv.org/abs/2512.01077)
*Neha Joshi,Pamir Gogoi,Aasim Mirza,Aayush Jansari,Aditya Yadavalli,Ayushi Pandey,Arunima Shukla,Deepthi Sudharsan,Kalika Bali,Vivek Seshadri*

Main category: cs.CL

TL;DR: 介绍了一个包含10种濒危语言的1060个传统食谱的多模态数据集，这些食谱来自印度东部农村社区，使用为数字素养较低的贡献者设计的移动界面收集。


<details>
  <summary>Details</summary>
Motivation: 旨在保护和传承濒危语言的文化背景下的本土食物传统，并促进公平和具有文化意识的语言技术发展。

Method: 通过众包收集食谱，并评估大型语言模型（LLM）在将这些食谱翻译成英语方面的性能。通过提供目标上下文来改进翻译质量。

Result: 发现大型语言模型在处理低资源、具有文化特异性的语言时表现不佳；提供有针对性的上下文（包括关于语言的背景信息、翻译示例和文化保护指南）可以显著提高翻译质量。

Conclusion: 强调需要专门为代表性不足的语言和领域定制基准，以推进公平和具有文化意识的语言技术。发布ELR-1000数据集以促进濒危语言的语言技术开发。

Abstract: We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.

</details>


### [44] [How do we measure privacy in text? A survey of text anonymization metrics](https://arxiv.org/abs/2512.01109)
*Yaxuan Ren,Krithika Ramesh,Yaxing Yao,Anjalie Field*

Main category: cs.CL

TL;DR: 本文旨在通过系统的调查，理清和协调评估文本隐私保护的指标。通过手动回顾47篇报告隐私指标的论文，我们识别并比较了六种不同的隐私概念，并分析了相关的指标如何捕捉隐私风险的不同方面。然后，我们评估了这些概念与法律隐私标准（HIPAA和GDPR）以及以HCI研究为基础的以用户为中心的期望的吻合程度。我们的分析为进一步探索隐私评估方法提供了实践指导，并强调了当前实践中的差距。最终，我们的目标是促进在文本匿名化中更强大、可比较和具有法律意识的隐私评估。


<details>
  <summary>Details</summary>
Motivation: 文本匿名化对于在具有敏感数据的领域中实现自然语言处理研究和模型开发至关重要，但评估匿名化方法是否充分保护隐私仍然是一个公开的挑战。

Method: 通过手动回顾47篇报告隐私指标的论文，识别并比较了六种不同的隐私概念，并分析了相关的指标如何捕捉隐私风险的不同方面。然后，评估这些概念与法律隐私标准（HIPAA和GDPR）以及以HCI研究为基础的以用户为中心的期望的吻合程度。

Result: 识别并比较了六种不同的隐私概念，并分析了相关的指标如何捕捉隐私风险的不同方面。评估了这些概念与法律隐私标准（HIPAA和GDPR）以及以HCI研究为基础的以用户为中心的期望的吻合程度。为进一步探索隐私评估方法提供了实践指导，并强调了当前实践中的差距。

Conclusion: 旨在促进在文本匿名化中更强大、可比较和具有法律意识的隐私评估。

Abstract: In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.

</details>


### [45] [DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](https://arxiv.org/abs/2512.01174)
*Hyunjun Kim,Sooyoung Ryu*

Main category: cs.CL

TL;DR: DrawingBench是一个用于评估自主AI系统可信度的验证框架，通过空间推理任务和GUI操作序列生成来评估LLM智能体的可信度，提供透明、可审计的评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏透明性和可审计性，无法评估智能体是否可靠。

Method: 使用DrawingBench框架，包含250个提示、确定性评估指标和外部监督机制，通过多轮反馈实现对智能体行为的人工控制。

Result: 四个先进的LLM模型在1000次测试中表现出能力和局限性，外部反馈驱动性能显著提升，但在工具状态管理和长程规划中出现系统性错误。

Conclusion: 透明的评估框架可以建立对智能体系统的信任，外部监督比自我纠正更可靠，该框架为可信的智能体评估提供了一个模板。

Abstract: As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench

</details>


### [46] [TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness](https://arxiv.org/abs/2512.01183)
*Yongxin Zhou,Philippe Mulhem,Didier Schwab*

Main category: cs.CL

TL;DR: 论文研究了检索增强生成（RAG）系统中检索质量和生成参数（如温度）之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统评估通常孤立地检查检索质量和温度等生成参数，忽略了它们之间的相互作用。这项工作旨在系统地研究文本扰动（模拟噪声检索）如何与不同温度设置相互影响。

Method: 提出了一个全面的RAG扰动-温度分析框架，该框架在不同的温度设置下对检索到的文档进行三种不同的扰动。

Result: 实验表明，性能下降遵循不同的模式：高温设置始终会放大对扰动的脆弱性，而某些扰动类型在整个温度范围内表现出非线性敏感性。

Conclusion: 论文的主要贡献包括：(1) 用于评估 RAG 鲁棒性的诊断基准；(2) 用于量化扰动-温度相互作用的分析框架；(3) 在噪声检索条件下进行模型选择和参数调整的实用指南。

Abstract: The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [MOTION: ML-Assisted On-Device Low-Latency Motion Recognition](https://arxiv.org/abs/2512.00008)
*Veeramani Pugazhenthi,Wei-Hsiang Chu,Junwei Lu,Jadyn N. Miyahira,Soheil Salehi*

Main category: cs.CV

TL;DR: 本研究探讨了使用三轴加速度计传感器构建高效的基于运动模型的方法，用于日常人机交互和医疗监控领域。


<details>
  <summary>Details</summary>
Motivation: 在跌倒检测、康复跟踪和病人监护等嵌入式解决方案中，需要快速有效地跟踪运动，同时避免不必要的误报。

Method: 利用AutoML管道从数据段中提取最重要特征，并使用提取的特征训练多个轻量级机器学习算法。使用WeBe Band，一种配备了功能强大的MCU的多传感器可穿戴设备，以在设备上有效执行手势识别。

Result: 在探索的模型中，发现神经网络在准确性、延迟和内存使用之间提供了最佳平衡。结果还表明，在WeBe Band中可以实现可靠的实时手势识别。

Conclusion: 该研究表明，实时手势识别在需要安全和快速响应时间的实时医疗监控解决方案中具有巨大的潜力。

Abstract: The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.

</details>


### [48] [Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions](https://arxiv.org/abs/2512.00042)
*Egemen Sert,Şeyda Ertekin*

Main category: cs.CV

TL;DR: 这篇论文提出了一个高质量的多模态数据集，并通过监督微调方法在视觉语言推理方面取得了接近最先进水平的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言推理的研究更多集中在算法层面，而对数据基础的探索不足。

Method: 作者构建了一个包含1.614亿token的多模态数据集，并使用优化的推理语法（QMSA）对Qwen-2.5VL-32B进行微调。

Result: 该模型在YKSUniform基准测试中达到了78.6%的准确率，仅比Gemini 2.0 Flash低1.0%。

Conclusion: 数据构成和表示语法在多模态推理中起着决定性作用，精心策划的多模态数据可以将监督微调提升到接近最先进的性能。

Abstract: Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.
  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.
  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.

</details>


### [49] [PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2512.00060)
*Abdolazim Rezaei,Mehdi Sookhak*

Main category: cs.CV

TL;DR: PEFT-DML: A parameter-efficient deep metric learning framework for robust multi-modal 3D object detection.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reliable 3D object detection in autonomous driving under varying sensor availability and unseen modality combinations.

Method: Utilizes parameter-efficient techniques like LoRA and adapter layers to map diverse modalities into a shared latent space.

Result: Achieves superior accuracy on the nuScenes dataset.

Conclusion: PEFT-DML enhances robustness to sensor dropout, fast motion, weather variability, and domain shifts with improved training efficiency.

Abstract: This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.

</details>


### [50] [DL-CapsNet: A Deep and Light Capsule Network](https://arxiv.org/abs/2512.00061)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: CapsNet是CNN的一种有前景的分类器，在检测具有重叠类别和仿射变换的图像时，CapsNet比CNN更准确。本文提出了一种由多个胶囊层组成的CapsNet的深度变体。


<details>
  <summary>Details</summary>
Motivation: CapsNet比CNN更准确，可以检测具有重叠类别和仿射变换的图像。

Method: 设计了Capsule Summarization层来减少参数数量，从而降低复杂性。

Result: DL-CapsNet在高度准确的同时，使用少量的参数，并提供更快的训练和推理。

Conclusion: DL-CapsNet可以处理具有大量类别的复杂数据集。

Abstract: Capsule Network (CapsNet) is among the promising classifiers and a possible successor of the classifiers built based on Convolutional Neural Network (CNN). CapsNet is more accurate than CNNs in detecting images with overlapping categories and those with applied affine transformations. In this work, we propose a deep variant of CapsNet consisting of several capsule layers. In addition, we design the Capsule Summarization layer to reduce the complexity by reducing the number of parameters. DL-CapsNet, while being highly accurate, employs a small number of parameters and delivers faster training and inference. DL-CapsNet can process complex datasets with a high number of categories.

</details>


### [51] [Satellite to Street : Disaster Impact Estimator](https://arxiv.org/abs/2512.00065)
*Sreesritha Sai,Sai Venkata Suma Sreeja,Deepthi,Nikhil*

Main category: cs.CV

TL;DR: 提出了一种名为 Satellite-to-Street 的深度学习框架，用于快速、一致地评估灾后损失，以支持更高效的数据驱动型灾害管理。


<details>
  <summary>Details</summary>
Motivation: 人工解译卫星图像进行灾后评估速度慢、主观且难以扩展。现有的深度学习模型难以处理细微的结构变化和严重的类别不平衡，导致对严重受损区域的检测效果不佳。

Method: 该模型使用改进的双输入 U-Net 架构，具有增强的特征融合，以捕获局部结构变化和更广泛的上下文线索。集成了类别感知加权损失函数，以处理真实灾难数据集中未损坏像素的主导地位，从而提高对主要和已摧毁类别的敏感性。

Result: 在公开的灾难数据集上的实验表明，与传统分割和基线变化检测模型相比，结构损坏的定位和分类得到了改善。

Conclusion: 生成的损失图提供了一种快速且一致的评估机制，以支持而非取代专家决策，从而实现更高效、数据驱动的灾害管理。

Abstract: Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.

</details>


### [52] [ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN](https://arxiv.org/abs/2512.00073)
*Aswinkumar Varathakumaran,Nirmala Paramanandham*

Main category: cs.CV

TL;DR: 本文提出了一种名为ProvRain的车辆检测流水线，旨在提高夜间雨天等噪声环境下的车辆检测准确率。


<details>
  <summary>Details</summary>
Motivation: 夜间车辆检测在雨雪等天气条件下受噪声影响大，传统方法难以保证准确性。

Method: 该流水线采用轻量级的MobileNet-U-Net架构，并使用课程学习的概念，混合合成数据和PVDN数据集进行训练。

Result: 实验结果表明，ProvRain在雨夜场景下车辆检测的准确率提高了8.94%，召回率提高了10.25%。与Transformer方法相比，PSNR提高了10-15%，SSIM提高了5-6%，LPIPS降低了67%。

Conclusion: ProvRain流水线能够有效处理噪声，提高夜间雨天等复杂环境下车辆检测的性能。

Abstract: Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\% increase in accuracy and a 10.25\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\% improvement in PSNR, a 5-6\% increase in SSIM, and upto a 67\% reduction in perceptual error (LPIPS) compared to other transformer approaches.

</details>


### [53] [VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis](https://arxiv.org/abs/2512.01769)
*Hafsa Billah*

Main category: cs.CV

TL;DR: 本研究提出了一种通用的视频情境分析框架，它可以克服现有技术的局限性，实现自动理解视频内容。


<details>
  <summary>Details</summary>
Motivation: 当前视频情境分析主要依赖人工或定制算法，存在易出错、劳动密集以及缺乏通用性的问题。

Method: 该框架首先使用先进的视频内容提取技术提取视频内容，然后使用扩展关系模型（R++）和图模型两种模型表示提取的内容。R++模型支持连续查询处理，图模型用于检测难以用关系模型检测到的情境。为了支持领域独立性，研究识别了跨领域的原始情境变体，并将它们表示为参数化模板。

Result: 在来自AL、CM和SL三个领域的多个有趣情境中进行了广泛的实验，以评估所提出方法的准确性、效率和鲁棒性。

Conclusion: 该研究提出了一种通用的视频情境分析框架，通过结合关系模型和图模型，以及参数化模板，实现了在不同领域中自动、准确、高效地分析视频内容。

Abstract: Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.
  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.

</details>


### [54] [Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation](https://arxiv.org/abs/2512.00075)
*Jun Jia,Hongyi Miao,Yingjie Zhou,Wangqiu Zhou,Jianbo Zhang,Linhan Cao,Dandan Zhu,Hua Yang,Xiongkuo Min,Wei Sun,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种名为Adapter Shield的通用且集成了身份验证的解决方案，旨在保护个人图像免受零样本生成场景中的滥用。


<details>
  <summary>Details</summary>
Motivation: 零样本图像到图像生成技术的发展带来了知识产权侵犯的风险，例如未经授权的身份克隆和风格模仿。

Method: 构建了一个可逆加密系统，该系统根据不同的密钥将原始嵌入映射到不同的加密表示。设计了一种多目标对抗扰动方法，该方法主动将原始嵌入转移到指定的加密模式。

Result: 该方法在阻止未经授权的零样本图像合成方面优于现有的最先进的防御方法，同时支持经过验证的用户的灵活和安全访问控制。

Conclusion: Adapter Shield是一种有效的防御机制，可以保护个人图像免受零样本生成技术的滥用，同时允许授权用户正常使用。

Abstract: With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.

</details>


### [55] [Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection](https://arxiv.org/abs/2512.00078)
*Mario de Jesus da Graca,Jörg Dahlkemper,Peer Stelldinger*

Main category: cs.CV

TL;DR: 本研究探索了使用无条件模型生成合成明场显微镜图像，并评估它们对目标检测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在明场显微镜中进行精确的单细胞检测对于生物研究至关重要，但数据稀缺和注释瓶颈限制了深度学习方法的进展。

Method: 训练了一个基于 U-Net 的扩散模型，并用它来创建具有不同合成图像和真实图像比例的数据集。使用 YOLOv8、YOLOv9 和 RT-DETR 进行了实验。

Result: 使用合成数据进行训练可以提高检测精度（成本 минималь）。人工专家调查表明，生成的图像具有很高的真实感，专家无法将它们与真实的显微镜图像区分开来（准确率 50%）。

Conclusion: 研究结果表明，基于扩散的合成数据生成是增强显微镜图像分析中真实数据集的一种有前途的途径，减少了对大量手动注释的依赖，并有可能提高细胞检测模型的鲁棒性。

Abstract: Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.

</details>


### [56] [Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels](https://arxiv.org/abs/2512.00080)
*André Dehne,Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 本研究探索了使用深度视觉立体里程计（DVSO）增强 MARWIN 机器人在欧洲 XFEL 加速器隧道中自主导航能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 MARWIN 导航系统在未知环境和障碍物面前缺乏灵活性。

Method: 采用基于立体视觉的 DVSO 方法，利用立体视差、光流和自监督学习联合估计深度和自身运动。

Result: 对加速器隧道环境进行了概念性评估，确定了使用 DVSO 的潜在优势（减少尺度漂移、低成本传感、可扩展数据收集）和挑战（低纹理表面、光照变化、计算负载、辐射下的鲁棒性）。

Conclusion: 本研究为 MARWIN 在受限、安全关键的基础设施中实现更自主的导航定义了研究方向。

Abstract: The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.

</details>


### [57] [Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages](https://arxiv.org/abs/2512.00082)
*Divendar Murtadak,Yoon Kim,Trilokya Akula*

Main category: cs.CV

TL;DR: 本研究探讨了诊断性提示是否可以提高多模态大型语言模型 (MLLM) 在评估亚马逊搜索结果页面 (SRP) 视觉复杂性方面的可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提高 MLLM 在评估视觉复杂性方面的可靠性，使其更符合人类的判断。

Method: 使用 200 个亚马逊 SRP 页面和人类专家注释，比较诊断性提示与基于格式塔原则的标准提示。

Result: 诊断性提示在预测人类复杂性判断方面显示出显着改进，F1 分数从 0.031 增加到 0.297（+858% 的相对改进），但绝对性能仍然适中（Cohen's $κ$ = 0.071）。决策树显示，模型优先考虑视觉设计元素（徽章杂乱：38.6% 的重要性），而人类强调内容相似性，表明推理模式部分一致。

Conclusion: 研究结果表明，诊断性提示是朝着基于 MLLM 的人类对齐评估迈出的有希望的初步步骤，但持续存在人类与 MLLM 不一致的失败案例，需要在更大的真实数据集上继续研究和改进提示方法，以实现可靠的实际部署。

Abstract: This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\% relative improvement), though absolute performance remains modest (Cohen's $κ$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.

</details>


### [58] [A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation](https://arxiv.org/abs/2512.00084)
*Venkata Siddharth Dhara,Pawan Kumar*

Main category: cs.CV

TL;DR: 提出了一种名为 FastTextDiff 的标签高效扩散分割模型，该模型集成了医学文本注释以增强语义表示。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割性能受到密集像素级标签需求的限制，这些标签成本高昂、耗时且需要专业知识。

Method: 利用 ModernBERT（一种能够处理长临床笔记的 Transformer）将文本注释与医学图像中的语义内容紧密联系起来。ModernBERT 在 MIMIC-III 和 MIMIC-IV 上进行训练，对临床知识进行编码，从而指导视觉和文本特征之间的跨模态注意力。

Result: 通过用 ModernBERT 替代 Clinical BioBERT，FastTextDiff 受益于 FlashAttention 2 和一个 2 万亿 token 的语料库，从而提高了分割精度和训练效率。

Conclusion: 验证了 ModernBERT 作为基于扩散的分割流水线中 Clinical BioBERT 的一种快速、可扩展的替代方案，并强调了多模态技术在医学图像分析中的前景。

Abstract: In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.

</details>


### [59] [Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs](https://arxiv.org/abs/2512.00086)
*Davide Nadalini,Manuele Rusci,Elia Cereda,Luca Benini,Francesco Conti,Daniele Palossi*

Main category: cs.CV

TL;DR: 这篇论文提出了一种在物联网设备上进行单目深度估计(MDE)的On-Device Learning (ODL)技术，以解决领域迁移问题。


<details>
  <summary>Details</summary>
Motivation: 在超低功耗(ULP)物联网(IoT)平台中，单目深度估计(MDE)在实现空间感知应用中起着关键作用。然而，为物联网节点设计的用于MDE任务的深度神经网络的参数数量有限，当现场观察到的传感器数据与训练数据集发生显著变化时，会导致严重的精度下降。

Method: 该方法利用一个集成了Greenwaves GAP9微控制器单元(MCU)、一个80mW单目摄像头和一个8x8像素深度传感器的物联网设备。该设备使用一个微小的107k参数的μPyD-Net模型进行推理，并利用深度传感器在新的环境中收集伪标签，然后在MCU上完全执行微调任务。为了优化基于反向传播的设备上训练，引入了一种新的内存驱动的稀疏更新方案，该方案将微调内存最小化到1.2MB，比完全更新减少2.2倍，同时保持了精度。

Result: 现场测试表明，MDE的ODL可以在物联网节点上在17.8分钟内完成，并且仅用3k个在实际部署场景中收集的自标记样本，就将均方根误差从4.9米降低到0.6米。

Conclusion: 该论文证明了在物联网节点上进行MDE的ODL是可行的，并且可以有效地解决领域迁移问题。

Abstract: Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.

</details>


### [60] [Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data](https://arxiv.org/abs/2512.00087)
*Ivo Bueno,Ruikun Hou,Babette Bühler,Tim Fütterer,James Drimalla,Jonathan Kyle Foster,Peter Youngs,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 本研究探索了利用人工智能分析课堂录像，以实现可扩展的教师反馈系统。


<details>
  <summary>Details</summary>
Motivation: 当前课堂互动观察依赖于人工标注，成本高且难以扩展。

Method: 使用包含164小时视频和68份课程记录的数据集，设计了并行的、特定模态的流程。视频方面，评估了zero-shot多模态LLM、微调的视觉-语言模型和自监督视频transformer。文本方面，微调了基于transformer的分类器，并与基于prompt的LLM进行了比较。采用按标签阈值处理、上下文窗口和不平衡感知损失函数来处理类别不平衡和多标签复杂性问题。

Result: 微调模型的性能始终优于基于prompt的方法，视频的macro-F1值为0.577，文本的macro-F1值为0.460。

Conclusion: 结果表明，自动化课堂分析是可行的，并为可扩展的教师反馈系统奠定了基础。

Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.

</details>


### [61] [SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features](https://arxiv.org/abs/2512.00088)
*Mohammad Zare*

Main category: cs.CV

TL;DR: 提出了一种新颖的 SemImage 方法，用于将文本文档表示为二维语义图像，以供卷积神经网络 (CNN) 处理。


<details>
  <summary>Details</summary>
Motivation: 为了更好地利用 CNN 处理文本数据，并提高模型的可解释性。

Method: 将每个词表示为 2D 图像中的像素，行对应于句子，并在句子之间插入额外的边界行以标记语义转换。每个像素都是一个解耦的 HSV 颜色空间中的向量，编码不同的语言特征：色调编码主题，饱和度编码情感，值编码强度或确定性。

Result: 在多标签数据集和单标签基准测试中，SemImage 可以实现与强大的文本分类基线（包括 BERT 和分层注意力网络）相比具有竞争力或更好的准确性，同时提供增强的可解释性。

Conclusion: SemImage 能够有效地将文本转换为图像，并使语言特征对人类和机器都可见。

Abstract: We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.

</details>


### [62] [TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts](https://arxiv.org/abs/2512.00089)
*Ioannis Prapas,Nikolaos Papadopoulos,Nikolaos-Ioannis Bountos,Dimitrios Michail,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: TeleViT模型通过结合局部火灾驱动因素、全局场和遥相关指数，改进了野火预测。


<details>
  <summary>Details</summary>
Motivation: 提前数周至数月预测野火对于燃料处理和资源分配至关重要，但具有挑战性。长期预测需要考虑地球的相互联系，包括全球模式和遥相关。

Method: TeleViT是一种遥相关感知视觉Transformer，它通过非对称tokenization策略融合多尺度信息，并通过transformer编码器和解码器处理异构tokens，从而保持空间结构。

Result: TeleViT在使用全球SeasFire数据集（2001-2021，8天分辨率）时，在所有提前期（包括长达四个月的范围）内，提高了AUPRC性能。

Conclusion: 明确编码大规模地球系统上下文的架构可以扩展亚季节到季节性时间尺度上的野火可预测性。

Abstract: Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.
  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.

</details>


### [63] [Deep Filament Extraction for 3D Concrete Printing](https://arxiv.org/abs/2512.00091)
*Karam Mawas,Mehdi Maboudi,Pedro Achanccaray,Markus Gerke*

Main category: cs.CV

TL;DR: 本文提出了一种用于挤压式和喷射式3D混凝土打印（3DCP）中丝材质量控制（QC）的自动化程序。


<details>
  <summary>Details</summary>
Motivation: 建筑、工程和施工（AEC）行业不断发展，以满足对可持续和有效的设计和建造环境的需求。在文献中，已经描述了两种用于大规模3D混凝土打印（3DCP）的主要沉积技术，即基于挤压的（轮廓工艺-CC）和喷射混凝土3D打印（SC3DP）方法。由于这些丝材是定义打印对象的基本结构，因此丝材的几何质量控制至关重要。

Method: 本文介绍了一种用于挤压式和SC3DP打印方法中丝材质量控制（QC）的自动化程序。本文还介绍了一种独立于用于数据采集的传感器（如相机、结构光系统（SLS）或地面激光扫描仪（TLS））的工作流程。此方法可用于新鲜或固化状态的材料。

Result: 该方法可用于在线和打印后QC。

Conclusion: 总结：该论文提出了一种通用的丝材质量控制方法，可用于不同类型的3D混凝土打印工艺，并适用于在线和离线质量检测。

Abstract: The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.

</details>


### [64] [Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images](https://arxiv.org/abs/2512.00103)
*Ifeanyi Okala*

Main category: cs.CV

TL;DR: 本研究探讨了三种基于图像的方法（VGG16、ViT-B/16 和 CoAtNet-Tiny）在使用每日活动记录识别抑郁症、精神分裂症和健康对照方面的表现。


<details>
  <summary>Details</summary>
Motivation: 利用腕戴式活动信号识别精神疾病，为心理健康研究提供了一种新的途径。

Method: 将Psykose和Depresjon数据集的腕戴式活动信号转换为30x48的图像，并通过三折受试者分割进行评估。

Result: CoAtNet-Tiny 表现最为可靠，记录了最高的平均准确率和最稳定的曲线。VGG16 稳步提高但通常稳定在较低的准确率。ViT-B/16 在某些运行中达到了很好的效果，但其性能在不同的 fold 中发生了明显的变化。

Conclusion: 研究结果表明，CoAtNet-Tiny 在 actigraphy 图像上表现最为稳定，而 VGG16 和 ViT-B/16 的结果好坏参半。

Abstract: This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.

</details>


### [65] [TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening](https://arxiv.org/abs/2512.00117)
*Ishwaryah Pandiarajan,Mohamed Mansoor Roomi Sindha,Uma Maheswari Pandyan,Sharafia N*

Main category: cs.CV

TL;DR: 提出了一种仅使用可见光图像进行太阳能光伏板表面缺陷检测和优先级排序的方法，无需依赖其他传感器。


<details>
  <summary>Details</summary>
Motivation: 依赖多模态成像策略进行太阳能光伏板故障检测存在后勤和经济障碍，特别是在大规模部署时。

Method: 结合深度学习和传统机器学习，构建了一个名为 TinyViT 的紧凑 pipeline，包括基于 Transformer 的分割、光谱-空间特征工程和集成回归。

Result: 在真实数据集上的实验验证了分类和回归子模块的有效性，实现了与专业方法相媲美的准确性和可解释性。

Conclusion: 该方法降低了对电致发光或红外传感器的依赖，实现了经济可扩展的维护，并推动了太阳能健康监测技术的发展。

Abstract: Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.

</details>


### [66] [Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance](https://arxiv.org/abs/2512.00125)
*Ruo-Syuan Mei,Sixian Jia,Guangze Li,Soo Yeon Lee,Brian Musser,William Keller,Sreten Zakula,Jorge Arinez,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出了一种混合合成数据生成 (SDG) 框架，用于在没有手动标注的情况下，对基于计算机视觉的工业零件进行零样本学习。


<details>
  <summary>Details</summary>
Motivation: 在制造业中，训练稳健的机器学习模型通常需要大量高质量的标注数据，而这些数据的获取成本高昂、耗时且劳动密集。此外，缺陷样本本质上是稀有的，导致严重的类别不平衡，从而降低模型性能。这些数据约束阻碍了基于机器学习的质量检测方法在实际生产环境中的广泛应用。

Method: 该方法集成了基于仿真的渲染、域随机化和真实背景合成，以实现零样本学习。

Result: 该方法在仅使用合成数据训练的情况下，在 300 个真实工业零件上进行了评估，实现了 0.995 的 mAP@0.5（目标检测），96% 的分类准确率和 90.1% 的平衡准确率。在严重的类别不平衡下，该方法实现了 90-91% 的平衡准确率，而基线方法仅达到 50% 的准确率。

Conclusion: 该方法实现了无标注、可扩展且稳健的质量检测，适用于现实世界的制造应用。

Abstract: Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.

</details>


### [67] [Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation](https://arxiv.org/abs/2512.00129)
*Jayan Adhikari,Prativa Joshi,Susish Baral*

Main category: cs.CV

TL;DR: 本研究旨在解决乳腺癌检测深度学习模型在处理非同分布(OOD)输入时可靠性低的问题，通过整合基于ResNet50的OOD过滤与YOLO架构，实现了对乳腺癌的精确检测。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌检测模型在面对其他成像方式或设备变异等OOD输入时，存在可靠性问题，导致误诊。

Method: 提出了一种综合方法，整合了基于ResNet50的OOD过滤与YOLO架构（YOLOv8, YOLOv11, YOLOv12），利用余弦相似性建立域内图库，以排除非乳腺图像输入。

Result: OOD检测组件实现了99.77%的总体准确率，在OOD测试集上达到了100%的准确率，有效消除了不相关的成像方式。联合框架实现了高检测性能(mAP@0.5: 0.947)，并通过Grad-CAM可视化增强了解释性。

Conclusion: 本研究为在具有固有数据异质性的各种临床环境中部署可靠的基于AI的乳腺癌检测系统奠定了基础。

Abstract: Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\% general accuracy with immaculate 100\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.

</details>


### [68] [Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition](https://arxiv.org/abs/2512.00130)
*Fadi Dornaika,Danyang Sun*

Main category: cs.CV

TL;DR: LGCOAMix: A novel cutmix data augmentation method using superpixel attention for local feature learning and cross-image contrast.


<details>
  <summary>Details</summary>
Motivation: Existing cutmix methods overly focus on global semantics, neglect local context, and lose object part information, leading to performance bottlenecks and inefficiency.

Method: Proposes LGCOAMix, a superpixel-based grid blending method that is context-aware and object-part-aware. It uses a superpixel attention approach for label mixing and learns local features from discriminative superpixel regions.

Result: LGCOAMix outperforms state-of-the-art cutmix methods on classification tasks and weakly supervised object localization. Effective for both CNN and Transformer networks.

Conclusion: LGCOAMix addresses limitations of existing cutmix methods by incorporating local context and object part information through superpixel attention, leading to improved performance and efficiency.

Abstract: Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.

</details>


### [69] [Efficient Edge-Compatible CNN for Speckle-Based Material Recognition in Laser Cutting Systems](https://arxiv.org/abs/2512.00179)
*Mohamed Abdallah Salem,Nourhan Zein Diab*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级卷积神经网络，用于激光切割中基于散斑图案的材料识别，可以在资源受限的设备上实现高精度和快速推理。


<details>
  <summary>Details</summary>
Motivation: 精确的材料识别对于安全有效的激光切割至关重要，但以往的方法计算成本高或仅限于材料的子集。

Method: 设计了一个轻量级的卷积神经网络，专门用于处理散斑图案，旨在最小化参数，同时保持高区分能力。

Result: 在包含59种材料类别的完整SensiCut数据集上，该模型实现了95.05%的测试精度，宏观和加权F1分数为0.951。该网络包含仅341k个可训练参数，并且实现了每秒295帧的推理速度。

Conclusion: 结果表明，紧凑的、特定领域的CNN可以优于大型骨干网络，从而推进了材料感知、边缘可部署激光切割系统的可行性。

Abstract: Accurate material recognition is critical for safe and effective laser cutting, as misidentification can lead to poor cut quality, machine damage, or the release of hazardous fumes. Laser speckle sensing has recently emerged as a low-cost and non-destructive modality for material classification; however, prior work has either relied on computationally expensive backbone networks or addressed only limited subsets of materials. In this study, A lightweight convolutional neural network (CNN) tailored for speckle patterns is proposed, designed to minimize parameters while maintaining high discriminative power. Using the complete SensiCut dataset of 59 material classes spanning woods, acrylics, composites, textiles, metals, and paper-based products, the proposed model achieves 95.05% test accuracy, with macro and weighted F1-scores of 0.951. The network contains only 341k trainable parameters (~1.3 MB) -- over 70X fewer than ResNet-50 -- and achieves an inference speed of 295 images per second, enabling deployment on Raspberry Pi and Jetson-class devices. Furthermore, when materials are regrouped into nine and five practical families, recall exceeds 98% and approaches 100%, directly supporting power and speed preset selection in laser cutters. These results demonstrate that compact, domain-specific CNNs can outperform large backbones for speckle-based material classification, advancing the feasibility of material-aware, edge-deployable laser cutting systems.

</details>


### [70] [AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI](https://arxiv.org/abs/2512.00194)
*Zag ElSayed,Grace Westerkamp,Gavin Gammoh,Yanchen Liu,Peyton Siekierski,Craig Erickson,Ernest Pedapati*

Main category: cs.CV

TL;DR: ICVision是一个使用GPT-4 Vision的多模态大语言模型，通过模仿专家对脑电图ICA成分的分类，实现了AI在神经生理学中的视觉认知。


<details>
  <summary>Details</summary>
Motivation: 传统分类器依赖手工特征，而ICVision直接解释ICA仪表板的可视化结果，更接近神经科医生的分析方式。

Method: ICVision使用GPT-4 Vision直接分析ICA成分的地形图、时间序列、功率谱和ERP图，并将其分类为六种类别。

Result: ICVision在3168个ICA成分的评估中，与专家共识达成k=0.677的协议，超过了MNE ICLabel，并且在模糊情况下保留了临床相关的大脑信号。超过97%的输出被专家评为可解释和可操作。

Conclusion: ICVision作为开源EEG Autoclean平台的核心模块，标志着科学AI的范式转变，开启了全球可扩展、可解释和可重复的脑电工作流程。

Abstract: We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.

</details>


### [71] [Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting](https://arxiv.org/abs/2512.00198)
*Shantanu Ghosh,Vedant Parthesh Joshi,Rayan Syed,Aya Kassem,Abhishek Varshney,Payel Basak,Weicheng Dai,Judy Wawira Gichoya,Hari M. Trivedi,Imon Banerjee,Shyam Visweswaran,Clare B. Poynton,Kayhan Batmanghelich*

Main category: cs.CV

TL;DR: Mammo-FM: a foundation model for mammography, pretrained on a large dataset, excels in various breast imaging tasks.


<details>
  <summary>Details</summary>
Motivation: Breast cancer is a leading cause of death in women; the model aims to improve clinical tasks in breast imaging.

Method: A foundation model (Mammo-FM) pretrained on a large mammography dataset.

Result: Mammo-FM outperforms generalist models in diagnosis, prognosis, and report generation tasks.

Conclusion: Domain-specific foundation models are efficient and valuable for clinical tasks.

Abstract: Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.

</details>


### [72] [ReactionMamba: Generating Short &Long Human Reaction Sequences](https://arxiv.org/abs/2512.00208)
*Hajra Anwar Beg,Baptiste Chopin,Hao Tang,Mohamed Daoudi*

Main category: cs.CV

TL;DR: ReactionMamba是一个用于生成长3D人体反应动作的新框架，它结合了运动VAE和基于Mamba的状态空间模型。


<details>
  <summary>Details</summary>
Motivation: 生成逼真、多样且时间上连贯的3D人体反应动作，尤其是在长序列和复杂动作方面。

Method: ReactionMamba集成了运动VAE用于高效运动编码，以及基于Mamba的状态空间模型用于解码时间上一致的反应。

Result: 在NTU120-AS、Lindy Hop和InterX三个数据集上，ReactionMamba在真实性、多样性和长序列生成方面表现出与InterFormer、ReMoS和Ready-to-React等先前方法相当的性能，并在推理速度上实现了显著提升。

Conclusion: ReactionMamba在生成长3D人体反应动作方面具有竞争力，并在推理速度上优于现有方法。

Abstract: We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.

</details>


### [73] [DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation](https://arxiv.org/abs/2512.00226)
*Zirui Wang,Tao Zhang*

Main category: cs.CV

TL;DR: DenseScan: A new 3D scene understanding dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: Current 3D scene understanding datasets lack the rich semantic annotations necessary for nuanced visual-language tasks.

Method: An automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs) is used to generate detailed multi-level descriptions.

Result: The method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines.

Conclusion: DenseScan broadens the range of downstream tasks and facilitates future research and applications in robotics, augmented reality, and beyond by providing richer, more contextually aware annotations.

Abstract: 3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.

</details>


### [74] [Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views](https://arxiv.org/abs/2512.00255)
*Kunwar Maheep Singh,Jianchun Chen,Vladislav Golyanik,Stephan J. Garbin,Thabo Beeler,Rishabh Dabral,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出了一种名为Relightable Holoported Characters (RHC) 的新方法，用于在推理时从稀疏视图 RGB 视频中自由视角渲染和重新光照全身和高度动态的人体。


<details>
  <summary>Details</summary>
Motivation: 解决传统one-light-at-a-time (OLAT) 人体重光方法成本高的问题。

Method: 使用基于 Transformer 的 RelightNet，该网络在单次网络传递中预测重新光照的外观，并结合新的捕获策略和数据集，该数据集在多视图光台中记录，交替使用随机环境贴图照明的帧和均匀照明的跟踪帧。

Result: 该方法在视觉保真度和光照再现方面优于最先进的方法。

Conclusion: RelightNet 隐式地学习在单个前馈传递中有效地计算新照明条件下的渲染方程。

Abstract: We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/

</details>


### [75] [UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations](https://arxiv.org/abs/2512.00261)
*Yuzhen Hu,Saurabh Prasad*

Main category: cs.CV

TL;DR: 提出 UniDiff，一个参数高效的框架，它使用目标域数据将单个 ImageNet 预训练扩散模型适配到多个传感模态。


<details>
  <summary>Details</summary>
Motivation: 稀疏注释从根本上限制了多模态遥感，即使是最新的有监督方法也受到标记数据可用性的限制，限制了它们的实际部署。

Method: UniDiff 结合了基于 FiLM 的时间步长-模态调节、大约 5% 参数的参数高效适配以及伪 RGB 锚定，以保留预训练的表示并防止灾难性遗忘。

Result: 在两个已建立的多模态基准数据集上的结果表明，预训练扩散模型的无监督适配有效地缓解了注释约束，并实现了多模态遥感数据的有效融合。

Conclusion: UniDiff 有效地缓解了注释约束，并实现了多模态遥感数据的有效融合。

Abstract: Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.

</details>


### [76] [HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction](https://arxiv.org/abs/2512.00264)
*Zhengda Ma,Abhirup Banerjee*

Main category: cs.CV

TL;DR: 提出了HeartFormer，一个新颖的点云补全网络，用于从电影MRI数据中进行3D四腔心脏重建，将传统的单类点云补全扩展到多类。


<details>
  <summary>Details</summary>
Motivation: 传统电影MRI通常只提供心脏的2D切片图像，限制了对健康和病理条件下心脏形态和生理机制的全面理解。

Method: HeartFormer包含两个关键组件：语义感知双结构Transformer网络（SA-DSTNet）和语义感知几何特征细化Transformer网络（SA-GFRTNet）。

Result: 在HeartCompv1和英国生物银行的广泛跨领域实验表明，HeartFormer实现了稳健、准确和可推广的性能，始终超过了最先进的方法。

Conclusion: HeartFormer在3D心脏重建方面表现出色，并在HeartCompv1和UK Biobank数据集上验证了其性能。

Abstract: We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.

</details>


### [77] [USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing](https://arxiv.org/abs/2512.00269)
*Jun Wang,Peirong Liu*

Main category: cs.CV

TL;DR: USB：首个统一病理和健康脑图像双向生成和编辑的端到端框架。


<details>
  <summary>Details</summary>
Motivation: 现有脑图像生成和编辑方法侧重视觉质量，但领域特定，独立处理病理和健康图像建模；配对的病理-健康数据难以获得。

Method: 通过配对扩散机制对病变和脑解剖结构的联合分布进行建模，并采用一致性指导算法在双向病理-健康编辑过程中保持解剖结构一致性和病变对应关系。

Result: 在包括健康对照、中风和阿尔茨海默病患者在内的六个公共脑MRI数据集上进行的大量实验表明，USB 能够产生多样化和逼真的结果。

Conclusion: USB 建立了脑图像生成和编辑的第一个统一基准，为可扩展的数据集创建和稳健的神经影像分析开辟了机会。

Abstract: Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.

</details>


### [78] [HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention](https://arxiv.org/abs/2512.00275)
*Yi Liu,Yi Wan,Xinyi Liu,Qiong Wu,Panwang Xia,Xuejun Huang,Yongjun Zhang*

Main category: cs.CV

TL;DR: 提出了一种轻量级的遥感图像超分辨率框架HIMOSA，可以在保持计算效率的同时实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在遥感应用中，实时效率和模型轻量化至关重要，现有方法通常在模型性能和计算效率之间面临权衡。

Method: 利用遥感图像的固有冗余，引入内容感知的稀疏注意力机制，并采用分层窗口扩展来有效利用遥感图像中的多尺度重复模式，并通过调整注意力的稀疏性来降低计算复杂度。

Result: 在多个遥感数据集上的大量实验表明，该方法在保持计算效率的同时实现了最先进的性能。

Conclusion: 该论文提出了一种轻量级的遥感图像超分辨率框架，可以在保持计算效率的同时实现最先进的性能。

Abstract: In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.

</details>


### [79] [Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth](https://arxiv.org/abs/2512.00281)
*Sylvain Bodard,Pierre Baudot,Benjamin Renoust,Charles Voyton,Gwendoline De Bie,Ezequiel Geremia,Van-Khoa Le,Danny Francis,Pierre-Henri Siot,Yousra Haddou,Vincent Bobin,Jean-Christophe Brisset,Carey C. Thomson,Valerie Bourdes,Benoit Huet*

Main category: cs.CV

TL;DR: 该论文介绍了一种新的AI系统，用于在低剂量CT扫描中直接在结节水平上进行肺癌筛查，能够同时进行检测和恶性诊断。


<details>
  <summary>Details</summary>
Motivation: 早期发现恶性肺结节至关重要，但其对筛查中大小和生长的依赖性会延迟诊断。

Method: 该系统采用了一种浅层深度学习和基于特征的专业模型的集成方法，以解决数据集规模和可解释性的限制。

Result: 该系统在包含69,449个带注释结节的25,709次扫描中进行了训练和评估，其性能优于放射科医生、Lung-RADS和领先的AI模型，内部测试AUC为0.98，独立队列测试AUC为0.945。在99.3%的灵敏度下，每次扫描的假阳性率为0.5。

Conclusion: 该系统在所有结节大小和阶段（包括I期癌症）以及所有基于生长的指标（包括体积倍增时间）方面均优于放射科医生，并且在诊断不确定和缓慢生长的结节方面，该系统比放射科医生提前一年。

Abstract: Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.

</details>


### [80] [Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR](https://arxiv.org/abs/2512.00294)
*Lixing Guo,Tobias Höllerer*

Main category: cs.CV

TL;DR: 本文介绍了一种模块化AR代理系统，该系统集成了多模态大型语言模型（MLLM）与基于接地的视觉模型，以实现空间中的关系推理和物理环境中的语言条件空间检索。


<details>
  <summary>Details</summary>
Motivation: 传统的增强现实（AR）系统主要依赖于固定类检测器或基准标记，限制了它们解释复杂的开放词汇自然语言查询的能力。

Method: 该系统构建动态AR场景图，编码九种类型的关系（空间、结构语义、因果功能），使MLLM不仅能理解存在的对象，还能理解它们如何在3D空间中关联和交互。该代理动态调用坐标感知工具进行复杂查询选择、测量、比较和驱动，从而在物理操作中实现基础语言理解。

Result: 该系统通过任务自适应感兴趣区域突出显示和上下文空间检索，引导人们的注意力集中在信息密集区域，同时支持人机交互改进。模块化架构支持即插即用视觉语言模型，无需重新训练，从而将AR代理建立为中介，利用真实世界的空间智能增强MLLM，以实现交互式场景理解。

Conclusion: 我们还介绍了GroundedAR-Bench，这是一个评估框架，用于跨不同环境的语言驱动的真实世界定位和关系基础。

Abstract: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.

</details>


### [81] [TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion](https://arxiv.org/abs/2512.00300)
*Rui Qian,Haozhi Cao,Tianchen Deng,Tianxin Hu,Weixiang Guo,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: TGSFormer，一种可扩展的用于具身3D语义场景补全的时间高斯溅射框架，它通过置信度感知交叉注意力进行时间融合，并通过置信度感知体素融合模块合并重叠的图元。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预定义空间范围内的许多基元的随机初始化，导致冗余和难以扩展到无界场景。最近的深度引导方法缓解了这个问题，但仍然是局部的，随着规模的增加，会遭受延迟和内存开销。

Method: 提出了一种可扩展的时间高斯溅射框架，用于具身SSC。它保持持久的高斯记忆用于时间预测，而不依赖于图像连贯性或帧缓存。双重时间编码器通过置信度感知交叉注意力联合处理当前和历史高斯特征。置信度感知体素融合模块将重叠的基元合并为体素对齐的表示，调节密度并保持紧凑性。

Result: 在局部和具身SSC基准测试中，TGSFormer 实现了最先进的结果，以更少的基元提供了卓越的准确性和可扩展性，同时保持了一致的长期场景完整性。

Conclusion: TGSFormer是一种用于具身3D语义场景补全的可扩展的时间高斯溅射框架，它优于现有技术，并具有更强的准确性和可扩展性。

Abstract: Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.

</details>


### [82] [Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation](https://arxiv.org/abs/2512.00308)
*Xiao Cui,Yulei Qin,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的数据集蒸馏方法，通过优化传输距离最小化来解决现有方法忽略实例级特征和类内差异的问题，从而在全局和实例级别实现细粒度的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模数据集蒸馏方法侧重于匹配全局分布统计信息，但忽略了关键的实例级特征和类内差异，导致泛化能力欠佳。

Method: 将数据集蒸馏重新 формулируется 为一个最优传输 (OT) 距离最小化问题，并提出了三个组成部分：OT 引导的扩散采样、标签-图像对齐的软重标记和基于 OT 的 logit 匹配。

Result: 在各种架构和大规模数据集上的大量实验表明，该方法始终优于最先进的方法，在 ImageNet-1K 上每个架构的 IPC=10 设置下，至少提高了 4% 的准确率。

Conclusion: 该文提出的方法能够有效地保留局部模式、类内模式和细粒度的变化，从而改进数据集蒸馏的性能。

Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.

</details>


### [83] [ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays](https://arxiv.org/abs/2512.00310)
*Qinyi Cao,Jianan Fan,Weidong Cai*

Main category: cs.CV

TL;DR: 提出了一种新的基于解剖结构的逼真纹理异常合成框架 (ART-ASyn)，用于胸部 X 射线，该框架使用基于纹理的增强来生成逼真且解剖学上一致的肺部混浊相关异常，并通过我们提出的用于肺部分割的渐进二值化阈值分割方法 (PBTSeg) 进行指导。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法通常会产生与真实病理模式在视觉上不同的合成异常，并且忽略了解剖结构。

Method: 使用纹理的增强来生成逼真且解剖学上一致的肺部混浊相关异常，并通过提出的用于肺部分割的渐进二值化阈值分割方法 (PBTSeg) 进行指导, 从而生成合成异常及其对应的精确像素级异常掩模配对样本，从而实现显式的分割监督。

Result: 在零样本异常分割方面进行了进一步的评估，证明了在没有目标域注释的未见数据集上的通用性。

Conclusion: 本文提出了一种新的异常检测框架，可以生成逼真且解剖学上一致的肺部混浊相关异常，并通过在胸部X光片上的实验证明了其有效性。

Abstract: Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.

</details>


### [84] [Odometry Without Correspondence from Inertially Constrained Ruled Surfaces](https://arxiv.org/abs/2512.00327)
*Chenqi Zhu,Levi Burner,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出了一种新的视觉里程计算法，该算法不依赖于图像之间的点对应关系，而是使用由图像中的线在图像空间中扫过的曲面。通过结合IMU数据，可以提高重建和里程计的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉里程计算法依赖于图像特征提取和光流计算，计算成本高，精度不稳定。为了解决对应关系问题，有人尝试使用线特征和融合其他传感器，但这些方法仍然依赖于对应关系。

Method: 该算法通过分析线在图像空间中扫过的曲面（即 ruled surface）的形状来获取里程计信息。该方法只需要点到线的关联的微分更新。结合IMU的惯性测量，可以减少解空间的维度。

Result: 提出了一个新算法，该算法可以从 ruled surface 重建 3D 场景和视觉里程计。

Conclusion: 通过结合IMU数据约束曲面，可以大大降低解空间的维度，提高重建和里程计的准确性。

Abstract: Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.

</details>


### [85] [MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection](https://arxiv.org/abs/2512.00336)
*Mengxue Hu,Yunfeng Diao,Changtao Miao,Jianshu Li,Zhe Li,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 提出了一个用于检测AI生成的多模态视频-音频内容的综合数据集MVAD。


<details>
  <summary>Details</summary>
Motivation: 现有的合成视频数据集主要关注视觉模态，而少数包含音频的数据集主要局限于面部深度伪造，这无法解决通用多模态AI生成内容的扩展，并严重阻碍了可信检测系统的开发。

Method: 构建了一个多模态视频-音频数据集（MVAD）。该数据集包含根据三种真实的视频-音频伪造模式生成的样本。

Result: 该数据集具有三个关键特征：(1) 真正的多模态；(2) 通过各种最先进的生成模型实现的高感知质量；(3) 涵盖现实和动漫视觉风格、四种内容类别（人类、动物、物体和场景）以及四种视频-音频多模态数据类型的全面多样性。

Conclusion: MVAD是首个专门用于检测AI生成的多模态视频-音频内容的综合数据集。

Abstract: The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.

</details>


### [86] [Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models](https://arxiv.org/abs/2512.00343)
*Zhongqi Wang,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为AMDET的新型模型级后门检测框架，该框架无需任何先验知识即可运行，通过梯度反演token嵌入来恢复激活后门行为的隐式特征，并分析损失情况以过滤掉自然后门特征。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法通常依赖于训练数据集、后门触发器和目标或下游分类器的先验知识，这在实际应用中可能不切实际。为了解决这个问题。

Method: AMDET首先揭示了后门文本编码器中的特征同化属性：后门样本中所有token的表示都表现出高度相似性。 进一步的分析将这种效应归因于注意力权重集中在触发token上。利用这种洞察力，AMDET通过对token嵌入执行基于梯度的反演来扫描模型，以恢复能够激活后门行为的隐式特征。此外，我们识别了OpenAI官方CLIP模型中的自然后门特征，这些特征不是故意注入的，但仍然表现出类似后门的行为。然后，我们通过分析它们的损失情况，将它们从真正注入的后门中过滤出来。

Result: 在具有两种攻击范例和三种VLP模型结构的3600个后门和良性微调模型上进行的大量实验表明，AMDET检测后门的F1得分为89.90%。此外，它在RTX 4090 GPU上大约5分钟内完成一次完整检测，并对自适应攻击表现出强大的鲁棒性。

Conclusion: AMDET框架在检测视觉语言预训练模型中的后门攻击方面表现出色，无需先验知识，且具有较高的检测效率和鲁棒性。

Abstract: Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET

</details>


### [87] [mmPred: Radar-based Human Motion Prediction in the Dark](https://arxiv.org/abs/2512.00345)
*Junqiao Fan,Haocong Rao,Jiarui Zhang,Jianfei Yang,Lihua Xie*

Main category: cs.CV

TL;DR: 提出了一种新的基于毫米波雷达的人体运动预测框架 (mmPred)，以解决现有方法对光照敏感和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RGB-D相机的人体运动预测方法对光照条件敏感并存在隐私问题，限制了其在现实世界中的应用。毫米波雷达具有鲁棒性和保护隐私的特性，因此被引入作为人体运动预测的一种新的传感方式。

Method: 提出了mmPred，一种基于扩散的框架，专为基于雷达的人体运动预测而设计。mmPred引入了双域历史运动表示来指导生成过程，结合时域姿势细化 (TPR) 分支和频域主导运动 (FDM) 分支。设计了一个全局骨骼关系Transformer (GST) 作为扩散主干，以建模全局关节间协作。

Result: mmPred 实现了最先进的性能，在 mmBody 上优于现有方法 8.6%，在 mm-Fi 上优于 22%。

Conclusion: 毫米波雷达可以有效应用在人体运动预测中，提出的 mmPred 框架能够有效提升预测精度。

Abstract: Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.

</details>


### [88] [SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction](https://arxiv.org/abs/2512.00355)
*Junqiao Fan,Pengfei Liu,Haocong Rao*

Main category: cs.CV

TL;DR: 提出了一种名为SMamDiff的单阶段扩散模型，用于人体运动预测(HMP)，该模型在准确性和多样性之间取得了平衡，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有HMP方法忽略不确定性或牺牲运动学合理性，而扩散模型虽然提高了准确性，但依赖于多阶段流程，成本高昂。

Method: 引入了两个新设计：(i) 残差-DCT运动编码，减少了第一个DC分量的主导地位，突出了信息量大的高频线索；(ii) stickman-drawing spatial-mamba模块，以有序的、逐关节的方式处理关节，使后面的关节以早期的关节为条件，从而诱导长程、跨关节的依赖关系。

Result: 在Human3.6M和HumanEva数据集上，该模型在单阶段概率HMP方法中实现了最先进的结果，同时比多阶段扩散基线使用更少的延迟和内存。

Conclusion: 提出的SMamDiff模型通过空间-时间连贯机制，实现了高效且准确的人体运动预测。

Abstract: With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.

</details>


### [89] [MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters](https://arxiv.org/abs/2512.00363)
*Jianhong Han,Yupei Wang,Yuan Zhang,Liang Chen*

Main category: cs.CV

TL;DR: 提出了一种轻量高效的多模态目标检测框架MM-DETR。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力的或可变形卷积融合块的方法难以平衡性能和轻量化设计；共享骨干提取模态特征导致次优表示，而双流架构参数量几乎翻倍，限制了实际部署。

Method: 提出了基于Mamba的双粒度融合编码器，将全局交互重构为通道动态门控，并利用一维选择性扫描实现线性复杂度的有效跨模态建模；引入了区域感知的二维选择性扫描补全分支，以恢复模态特定线索，支持沿双向金字塔路径的细粒度融合；在共享骨干中插入了一个轻量级的频率感知模态适配器，采用空间频率协同专家结构来捕获模态特定线索，并通过像素级路由器动态平衡专家贡献以实现高效的空间频率融合。

Result: 在四个多模态基准数据集上进行的大量实验证明了该方法的有效性和泛化能力。

Conclusion: MM-DETR是一种有效且具有泛化能力的多模态目标检测框架。

Abstract: Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.

</details>


### [90] [Towards aligned body representations in vision models](https://arxiv.org/abs/2512.00365)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 研究探讨了用于分割的视觉模型是否发展出与人类相似的身体表征。


<details>
  <summary>Details</summary>
Motivation: 人类的物理推理依赖于内部“身体”表征，但它们的内部结构在很大程度上仍然未知。本文旨在测试用于分割的视觉模型是否发展出可比较的表征。

Method: 将一项针对 50 名人类参与者的心理物理学实验调整为语义分割任务，并测试了一系列七个大小不同的分割网络。

Result: 较小的模型自然形成类似于人类的粗略身体表征，而较大的模型则倾向于过于详细的细粒度编码。

Conclusion: 粗略表征可以在有限的计算资源下出现，并且机器表征可以为理解大脑中物理推理的结构提供可扩展的途径。

Abstract: Human physical reasoning relies on internal "body" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.

</details>


### [91] [THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering](https://arxiv.org/abs/2512.00368)
*Jian Zhu*

Main category: cs.CV

TL;DR: 提出了一个名为Trusted Hierarchical Contrastive Representation Learning (THCRL)的新框架，用于解决多视图聚类(MVC)中不可信融合的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的MVC方法忽略了视图中的噪声，并且对比学习(CL)忽略了同一集群内最近邻的结构信息，导致多视图融合方向错误。

Method: 该方法包含两个关键模块：Deep Symmetry Hierarchical Fusion (DSHF) 模块，利用UNet架构和多个去噪机制实现多视图数据的可信融合；Average K-Nearest Neighbors Contrastive Learning (AKCL) 模块，将融合表示与特定视图表示对齐，增强同一集群内样本的表示相似性。

Result: 大量实验表明，THCRL在深度MVC任务中实现了最先进的性能。

Conclusion: THCRL框架有效地解决了多视图聚类中不可信融合的问题，并在实验中表现出优异的性能。

Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.

</details>


### [92] [POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models](https://arxiv.org/abs/2512.00369)
*Wenshuo Chen,Haosen Li,Shaofeng Liang,Lei Wang,Haozhe Jia,Kaishen Yuan,Jieming Wu,Bowen Tian,Yutao Yue*

Main category: cs.CV

TL;DR: 论文提出了一种新的基于扩散模型的图像编辑和恢复方法，该方法通过减少反演过程中的噪声近似误差来提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像编辑和恢复方法存在重建质量下降的问题，作者发现这是由于反演过程中近似噪声误差累积造成的。

Method: 论文提出Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS)方法，该方法将反演过程重新定义为一个误差溯源问题，通过优化引导尺度来最小化每一步的误差。

Result: POLARIS方法只需一行代码即可显著降低噪声近似误差，并持续提高下游任务的准确性，且性能开销可忽略不计。

Conclusion: POLARIS方法能够有效改善反演潜在质量，提高图像编辑和恢复任务的性能。

Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [93] [Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions](https://arxiv.org/abs/2512.00097)
*Boyan Duan,Xiao Liang,Shuai Lu,Yaoxiang Wang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Mao Yang,Weizhu Chen,Yeyun Gong*

Main category: cs.AI

TL;DR: 提出了一种高效的几何定理证明方法，该方法完全在 CPU 上运行，不依赖于基于神经网络的推理。


<details>
  <summary>Details</summary>
Motivation: 在欧几里得几何中进行自动定理证明，特别是对于国际数学奥林匹克 (IMO) 级别的问题，仍然是一个主要的挑战，也是人工智能中一个重要的研究重点。

Method: 提出了一种基于启发式的方法 (HAGeo) 用于在几何演绎中添加辅助结构。

Result: HAGeo 在 IMO-30 基准测试中解决了 30 个问题中的 28 个，达到了金牌水平的性能，并且超过了 AlphaGeometry。构建了一个包含 409 个具有人工评估难度级别的几何问题的新基准 HAGeo-409。

Conclusion: 该基准测试比广泛使用的 IMO-30 提出了更大的挑战，并提供了更精确的评估，为几何定理证明设定了更高的标准。

Abstract: Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.

</details>


### [94] [Chunking Strategies for Multimodal AI Systems](https://arxiv.org/abs/2512.00185)
*Shashanka B R,Mohith Charan R,Seema Banu F*

Main category: cs.AI

TL;DR: 本文旨在整合多模态分块策略，为研究人员和实践者提供技术基础和设计空间，以开发更有效和高效的多模态AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI系统在处理复杂模态数据时面临挑战，需要更有效的分块策略。

Method: 本文对文本、图像、音频、视频和跨模态数据的分块策略进行了全面的分类和技术分析，考察了经典和现代方法，并分析了它们的方法论、支持工具、优点和挑战。

Result: 本文对各种分块策略进行了比较分析，强调了异步信息密度和噪声对齐信号等开放性问题，并为自适应、基于学习和特定任务的分块的未来研究指明了方向。

Conclusion: 本文为开发更鲁棒、准确和连贯的多模态AI系统奠定了基础，并指出了未来研究的机遇。

Abstract: Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.

</details>


### [95] [A Rosetta Stone for AI Benchmarks](https://arxiv.org/abs/2512.00193)
*Anson Ho,Jean-Stanislas Denain,David Atanasov,Samuel Albanie,Rohin Shah*

Main category: cs.AI

TL;DR: 提出了一个统计框架，将不同的AI基准统一到一个数值尺度上，以便跨时间和能力范围比较模型。


<details>
  <summary>Details</summary>
Motivation: 现有的AI基准很快饱和，难以研究AI能力的长期趋势。

Method: 构建一个统计框架，将模型能力和基准难度放在一个共同的数值尺度上，无需假设能力随时间和计算量的演变方式。

Result: 1. 测量了AI随时间推移的进步速度，并预测了未来的AI能力。2. 评估了算法效率的改进率。3. 可以用来检测AI进步的快速加速。

Conclusion: 该框架能够弥合不同基准之间的差距，实现跨模型和时间的比较，并应用于衡量AI进展和检测加速。

Abstract: Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a "Rosetta Stone", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.

</details>


### [96] [Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?](https://arxiv.org/abs/2512.00218)
*Matt MacDermott,Qiyao Wei,Rada Djoneva,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究了训练激励如何影响 AI 系统推理过程的可监控性，发现对抗优化会降低监控性能，而直接优化可监控性并不一定能带来改进。


<details>
  <summary>Details</summary>
Motivation: 探讨了通过监控 AI 系统的自然语言推理过程来确保安全性的机会，并研究了训练中的某些因素如何影响推理过程的可监控性。

Method: 提出了一种新的方法来衡量可监控性，即监控器是否可以使用模型的推理来预测关键潜在变量。

Result: 在控制准确率的情况下，常用激励（长度惩罚和 KL 正则化）对可监控性没有一致的影响，对抗优化会降低监控性能，而直接优化可监控性并不一定能带来改进。

Conclusion: 训练激励会影响 AI 系统的推理过程的可监控性，对抗优化会降低监控性能，而直接优化可监控性并不一定能带来改进。

Abstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.

</details>


### [97] [Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking](https://arxiv.org/abs/2512.00267)
*Anab Maulana Barik,Shou Ziyi,Yang Kaiwen,Yang Qi,Shen Xin*

Main category: cs.AI

TL;DR: 提出了一种名为Trification的自动事实核查框架，通过生成全面的验证动作并将其构建为依赖关系图来提高事实核查的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法验证声明中的每个组成部分，并且缺乏结构化框架来连接子任务的结果以进行最终预测。

Method: 首先生成一组全面的验证操作，然后将这些操作构建为依赖关系图，以模拟操作之间的逻辑交互。该图可以动态修改，从而允许系统调整其验证策略。

Result: 在两个具有挑战性的基准测试中，实验结果表明，该框架显著提高了事实核查的准确性。

Conclusion: 该框架提高了自动事实核查系统的当前技术水平。

Abstract: Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.

</details>


### [98] [ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning](https://arxiv.org/abs/2512.00305)
*Zhengzhuo Xu,SiNan Du,Yiyan Qi,SiwenLu,Chengjin Xu,Chun Yuan,Jian Guo*

Main category: cs.AI

TL;DR: 提出了PointCoT，通过生成边界框和重新渲染图表来增强MLLM在图表推理中的视觉基础。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM在图表理解中依赖OCR提取内容，当图表文本注释稀疏时，会导致数字幻觉，并且缺乏视觉感知推理能力。

Method: 提出了PointCoT，将反思性互动整合到图表的思维链推理中，通过提示MLLM生成边界框并基于位置注释重新渲染图表，建立文本推理步骤和视觉基础区域之间的连接。构建了ChartPoint-SFT-62k数据集，并在此基础上开发了两个指令微调模型ChartPointQ2和ChartPointQ2.5。

Result: ChartPointQ2和ChartPointQ2.5在多个图表基准测试中优于现有技术，例如在ChartBench上提高了5.04%。

Conclusion: PointCoT通过整合反思性互动和视觉基础，显著提高了MLLM在图表理解中的性能。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\% on ChartBench.

</details>


### [99] [RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs](https://arxiv.org/abs/2512.00319)
*Ruike Hu,Shulei Wu*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级的强化学习框架，以弥合大型语言模型在生成结构化数据时遇到的“结构差距”问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言生成和推理方面表现出色，但由于token生成是概率性的，而结构化数据格式需要确定性，因此很难将其集成到自动化软件生态系统中。

Method: 该方法使用了一种新颖的多维奖励函数，将结构化输出任务分解为结构完整性、格式正确性、内容准确性和有效性等约束。利用梯度正则化策略优化（GRPO），使模型能够在不需要单独的critic网络的情况下内化这些约束。

Result: 实验结果表明，该方法在多个任务上都优于zero-shot基线和SFT，结构准确率达到89.7%，JSON有效性达到92.1%。

Conclusion: 该模型能够按顺序获得句法熟练度和语义准确性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the "Structure Gap" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to "hallucinated" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.

</details>


### [100] [CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System](https://arxiv.org/abs/2512.00331)
*Yefeng Wu,Yuchen Song,Yecheng Zhao,Ling Wu,Shan Wan*

Main category: cs.AI

TL;DR: 提出了一种新的教育多智能体系统CogEvo-Edu，用于提升LLM在STEM教育中作为会话辅导员的表现，特别是在复杂领域如数字信号处理(DSP)方面。


<details>
  <summary>Details</summary>
Motivation: 现有的基于静态RAG管道的LLM辅导系统在处理复杂领域（如DSP）时，难以维持连贯的学生模型、管理异构知识库和调整教学策略。

Method: 构建了一个分层的教育多智能体系统CogEvo-Edu，包含认知感知层(CPL)、知识进化层(KEL)和元控制层(MCL)，分别负责维护学生模型、管理知识库和协调教学策略。通过双内外循环共同调整CPL/KEL的超参数。

Result: 在DSP-EduBench基准测试中，CogEvo-Edu将总体评分从5.32提高到9.23，并在所有六个指标上优于静态RAG、简单记忆和单智能体变体。

Conclusion: CogEvo-Edu通过联合进化学生档案、知识库和教学策略，证明了其在复杂STEM教育中作为LLM辅导员的价值。

Abstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

</details>


### [101] [Echo-N1: Affective RL Frontier](https://arxiv.org/abs/2512.00344)
*Naifan Zhang,Ruihan Sun,Ruixi Su,Shiqi Ma,Shiya Zhang,Xianna Weng,Xiaofan Zhang,Yuhan Zhan,Yuyang Xu,Zhaohan Chen,Zhengyuan Pan,Ziyi Song*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的RL框架，用于优化具有主观性、情感和个性化特征的对话模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM领域在机器擅长的任务（如数学、代码和确定性推理）上过度使用RL，而忽略了人类智能的关键领域：主观、情感化和个性化的对话。传统观点认为这些领域难以形式化，不适合RL。

Method: 该论文提出了一个新框架，可以动态推断用户个性，并根据个性化的对话偏好优化模型行为。

Result: 实验结果表明，该方法在类人交互质量方面产生了持续、稳健和显著的改进，优于基线模型和Doubao 1.5 Character。

Conclusion: 这项工作为RL开辟了一个新的前沿：优化模型，使其能够处理对话中深刻的主观和人性化维度。

Abstract: The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.

</details>


### [102] [Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models](https://arxiv.org/abs/2512.00349)
*Sitong Fang,Shiyi Hou,Kaile Wang,Boyuan Chen,Donghai Hong,Jiayi Zhou,Josef Dai,Yaodong Yang,Jiaming Ji*

Main category: cs.AI

TL;DR: 论文揭示了多模态大型语言模型中存在的欺骗风险，并提出了一个名为MM-DeceptionBench的基准来评估这些风险。同时，论文还提出了一种名为“图像辩论”的新型多智能体辩论监控框架，以提高欺骗行为的检测能力。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要集中在文本欺骗上，而忽略了多模态大型语言模型中存在的欺骗风险。这些风险可能会利用视觉和文本模态的组合来策略性地操纵和误导用户。

Method: 论文引入了MM-DeceptionBench基准，用于评估多模态欺骗。此外，论文还提出了“图像辩论”框架，该框架通过迫使模型将其声明建立在视觉证据的基础上，来提高欺骗策略的可检测性。

Result: 实验结果表明，“图像辩论”框架在所有测试模型上都持续提高了与人类判断的一致性，在GPT-4o上将Cohen's kappa提高了1.5倍，准确率提高了1.25倍。

Conclusion: 多模态欺骗是一个值得关注的问题，现有的检测方法存在盲点。提出的“图像辩论”框架可以有效地提高多模态欺骗的检测能力。

Abstract: Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.

</details>


### [103] [GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework](https://arxiv.org/abs/2512.00406)
*Pengyu Zeng,Yuqin Dai,Jun Yin,Jing Zhong,Ziyang Han,Chaoyang Shi,ZhanXiang Jin,Maowei Jiang,Yuxing Han,Shuai Lu*

Main category: cs.AI

TL;DR: GreenPlanner是一个生成框架，统一了设计评估和生成，以解决建筑设计中人工、成本高、不可扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法生成视觉上合理的布局，但由于缺乏自动评估，经常违反关键约束，导致无效的结果。

Method: 该方法包括：一个用于学习约束先验的标记设计可行性数据集；一个用于预测能源性能和空间功能有效性的快速实用设计评估器（PDE）；一个从PDE引导过滤派生的绿色计划数据集（GreenPD），用于将用户需求与符合法规的布局配对；以及一个在GreenPD上训练的GreenFlow生成器，具有PDE反馈，用于可控的、符合法规的生成。

Result: GreenPlanner将评估速度提高了超过$10^{5}\%$，准确率超过99%，消除了无效样本，并将设计效率提高了87%。

Conclusion: 该论文提出了GreenPlanner，一个能源和功能感知的生成框架，可以有效生成符合约束的建筑设计方案，并且效率高于专业建筑师。

Abstract: Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.

</details>


### [104] [Mind the data gap: Missingness Still Shapes Large Language Model Prognoses](https://arxiv.org/abs/2512.00479)
*Yuta Kobayashi,Vincent Jeanselme,Shalmali Joshi*

Main category: cs.AI

TL;DR: 研究大型语言模型(LLM)在处理缺失数据时的性能表现，发现缺失数据的模式对零样本预测性能有显著影响，并且显式包含缺失性指标对不同大小的LLM模型有不一致的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管关于缺失信息的研究很多，但其对大型语言模型(LLM)性能的影响尚未得到充分研究。该研究旨在填补这一空白，关注缺失数据对LLM预测性能的影响。

Method: 通过对来自哥伦比亚大学医学中心和MIMIC-IV的数据进行一系列实验，探讨缺失模式对LLM零样本预测性能的影响，并分析显式包含缺失性指标的影响。

Result: 缺失数据的模式显著影响LLM的零样本预测性能。显式包含缺失性指标对某些LLM有益，但对其他LLM有害，表明影响不一致。较大的模型受益于这些干预，而较小的模型可能受到负面影响。

Conclusion: 大型语言模型(LLM)可能会掩盖缺失数据的影响，而这种影响在传统机器学习中经常被忽略。因此，需要更透明的说明和系统评估代表性(信息性)缺失对下游性能的影响。

Abstract: Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.

</details>


### [105] [Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization](https://arxiv.org/abs/2512.00601)
*Boyang Gu,Hongjian Zhou,Bradley Max Segal,Jinge Wu,Zeyu Cao,Hantao Zhong,Lei Clifton,Fenglin Liu,David A. Clifton*

Main category: cs.AI

TL;DR: 提出了一种新的LLM对齐方法，用于医疗领域的临床推理，该方法通过多目标可验证强化学习来优化准确性、忠实性和全面性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要奖励正确性，与医疗领域中推理必须忠实和全面的多维目标不符。

Method: 引入Clinical-Objective Relative Policy Optimization (CRPO)，一种可扩展的多目标可验证强化学习方法，通过整合基于规则的可验证奖励信号，联合优化准确性、忠实性和全面性，无需人工标注。

Result: 在三个基准测试中，CRPO在真实性和完整性方面显著优于标准GRPO，同时保持了舒适的准确性提升。

Conclusion: CRPO为LLM推理与临床目标对齐提供了一条可扩展的途径，实现了更安全、更协作的医疗AI系统，并突出了多目标可验证RL方法在医疗领域LLM后训练中的潜力。

Abstract: Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.

</details>


### [106] [EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients](https://arxiv.org/abs/2512.00670)
*He-Yen Hsieh,Hong Wang,H. T. Kung*

Main category: cs.AI

TL;DR: 提出了一种名为EDIT的推理时标准，用于自适应地停止去噪过程，一旦检测到相对于训练时推理的充分推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的大型语言模型(dLLMs)通过迭代去噪来改进token生成，但答案通常在所有步骤完成之前稳定。

Method: EDIT 监控 token 激活和从 AdamW 聚合的 LoRA 更新中导出的推理图之间的对齐。在推理过程中，对齐分数被转换为已在当前去噪步骤中未屏蔽的 token 的分布，并且当连续步骤之间的 KL 散度低于匹配的未屏蔽（可见）token 的阈值时，检测到收敛。

Result: 在推理基准测试中，EDIT 将扩散步骤减少了 11.8% 到 68.3%，同时在大多数设置中保持或提高了准确性，大约 0.02% 的存储开销（对于 8 GB 模型中 32 个块的所有 QKV 模块，约为 1.5-2 MB）。

Conclusion: 通过利用训练梯度动态，我们的工作为减少 dLLM 推理时间和成本开辟了一个新的研究方向。

Abstract: Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.

</details>


### [107] [Model of human cognition](https://arxiv.org/abs/2512.00683)
*Wu Yonggang*

Main category: cs.AI

TL;DR: 提出了一种神经理论框架，用于解释系统中智能的出现，该框架在功能上稳健且在生物学上合理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的发展受到缺乏可解释性、缺乏统一理论以及过高的运营成本的限制。

Method: 提出了一个神经理论框架。

Result: 该模型为决策和问题解决等认知过程提供了理论见解，并为创建可解释和可推广的人工智能提供了一种计算高效的方法。

Conclusion: 所提出的模型 functionally robust 且 biologically plausible，能为认知过程提供理论见解，并为创建更优秀的人工智能提供计算方法。

Abstract: The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.

</details>


### [108] [When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF](https://arxiv.org/abs/2512.00709)
*Yifan Xu,Xichen Ye,Yifan Chen,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: 提出了一个 Flipping-Aware Direct Preference Optimization (FA-DPO) 算法，用于解决大型语言模型 (LLM) 对齐中普遍存在的 preference flipping 问题。


<details>
  <summary>Details</summary>
Motivation: 数据集质量在大型语言模型 (LLM) 对齐中起着重要作用。然而，在收集人类反馈时，preference flipping 是普遍存在的，会导致数据注释损坏；因此，需要改进对潜在 flipped pairs 的鲁棒性的对齐算法。

Method: 从人与反馈的强化学习 (RLHF) 角度，将内在的人类意图模型和由外部因素引入的 preference flipping 机制分解为两个不同的阶段；在后者中，在 Bradley-Terry (BT) 模型的基础上引入了实例相关的 flipping 概率。此外，通过利用与偏好注释相关的特征，我们捕捉判断中的不确定性并对偏好翻转模式进行建模。在实践中，我们设计了一个简单而有效的迭代优化算法，该算法与原始 RLHF 和 DPO 算法兼容。

Result: 在我们的实验中，我们研究了在多种情况下实例相关的偏好翻转模型，以评估我们提出的方法以及其他基线方法。

Conclusion: 该论文提出了一种新的算法 FA-DPO，旨在提高大型语言模型在存在 preference flipping 情况下的对齐性能。

Abstract: Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.

</details>


### [109] [SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs](https://arxiv.org/abs/2512.00722)
*Jiaming Xu,Jiayi Pan,Hanzhen Wang,Yongkang Zhou,Jiancai Ye,Yu Wang,Guohao Dai*

Main category: cs.AI

TL;DR: SpeContext提出了一种新的长上下文推理范例，利用精馏语言模型（DLM）作为检索算法，并结合算法、系统和编译优化，以在资源受限环境中实现高吞吐量和低延迟。


<details>
  <summary>Details</summary>
Motivation: 本文指出检索算法的目标是与大型语言模型（LLM）对齐，这与LLM中的知识蒸馏目标相似。从信息论的角度分析了精馏语言模型（DLM）和原始LLM在信息焦点上的相似性。

Method: SpeContext在算法层面提出了一种基于DLM头级注意力权重的轻量级检索头，通过剪枝冗余实现了超过90%的参数减少。在系统层面，SpeContext设计了一种通过弹性加载策略实现的异步预取数据流，有效地将KV缓存检索与LLM计算重叠。在编译层面，SpeContext构建了理论内存模型，并实现了一种自适应内存管理系统，通过最大化GPU内存利用率来实现加速。

Result: 在云端环境中，SpeContext实现了高达24.89倍的吞吐量提升，在边缘环境中实现了10.06倍的加速，而精度损失可忽略不计。

Conclusion: SpeContext在精度和吞吐量方面都优于Huggingface框架，推动了Pareto最优前沿。

Abstract: In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.

</details>


### [110] [Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens](https://arxiv.org/abs/2512.00729)
*Yuxiang Chen,Zuohan Wu,Ziwei Wang,Xiangning Yu,Xujia Li,Linyi Yang,Mengyue Yang,Jun Wang,Lei Chen*

Main category: cs.AI

TL;DR: 本文提出了一个用于描述大型推理模型（LRM）推理步骤的分类法，并用它来深入了解当前的LRM，从而为改进推理模型的训练和训练后处理提供了可操作的建议。


<details>
  <summary>Details</summary>
Motivation: 研究表明大型推理模型（LRM）在解决复杂任务方面表现出色，并且在推理过程中观察到类似人类的行为。因此，本文旨在通过一个全面的分类法来描述原子推理步骤，从而探究LRM智能的“心理”。

Method: 本文构建了一个包含五个组和十七个类别的分类法，这些类别来源于人类心理过程。然后，应用该分类法来深入理解当前的LRM，从而生成一个包含277,534个原子推理步骤的带标签数据集。此外，本文还提出了一个名为CAPO的自动注释框架，该框架利用大型语言模型（LLM）来生成基于分类法的注释。

Result: 本文的分析表明，目前流行的答案后“二次检查”（自我监控评估）在很大程度上是肤浅的，很少产生实质性的修改。实验结果表明，CAPO与人类专家相比，实现了更高的一致性。

Conclusion: 本文提出的分类法、CAPO和由此产生的见解为理解和推进LRM推理提供了一条原则性的、可扩展的路径。

Abstract: Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.

</details>


### [111] [MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents](https://arxiv.org/abs/2512.00756)
*Ruihan Chen,Qiming Li,Xiaocheng Feng,Xiaoliang Yang,Weihong Zhong,Yuxuan Gu,Zekun Zhou,Bing Qin*

Main category: cs.AI

TL;DR: 大型视觉语言模型(LVLMs)在图形用户界面(GUI)任务中表现出令人印象深刻的感知和推理(P&R)性能。但是，它们在多语言环境中的性能受到的关注较少，这限制了它们的全球应用。此外，现有的GUI任务研究缺乏细粒度的分析，包括小部件功能和元素空间关系，这对于更有针对性的改进至关重要。


<details>
  <summary>Details</summary>
Motivation: 评估GUI agents的P&R能力

Method: MPR-GUI-Bench，一个多语言细粒度感知和推理GUI基准

Result: LVLMs在非英语语言中的P&R性能明显低于英语。GUI-XLI，一种GUI跨语言干预方法，可将干预应用于与P&R能力相关的层中的隐藏状态，以减轻英语和其他语言之间的差距

Conclusion: 我们的方法将GUI代理的多语言P&R能力平均提高了6.5%。

Abstract: With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.

</details>


### [112] [BioPro: On Difference-Aware Gender Fairness for Vision-Language Models](https://arxiv.org/abs/2512.00807)
*Yujie Lin,Jiayao Ma,Qingguo Hu,Derek F. Wong,Jinsong Su*

Main category: cs.AI

TL;DR: 提出了BioPro，一个无需训练的框架，用于选择性地减少视觉-语言模型中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性干预措施通常采用差异性盲视角度，强制对不同人口群体进行统一对待，但未能区分需要中立的语境和必须保留特定群体属性的语境。

Method: 通过反事实嵌入识别低维性别变异子空间，并应用投影来选择性地消除性别相关信息。

Result: BioPro在 нейтральных 情况下有效地减少了性别偏见，同时在 явных 情况下保持了性别忠实度。

Conclusion: BioPro为实现视觉-语言模型中的选择性公平性提供了一个有希望的方向，并且可以有效地推广到连续偏见变量，如场景亮度。

Abstract: Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.

</details>


### [113] [Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning](https://arxiv.org/abs/2512.00818)
*Haozhen Gong,Xiaozhong Ji,Yuansen Liu,Wenbin Wu,Xiaoxiao Yan,Jingjing Liu,Kai Wu,Jiazhen Pan,Bailiang Jian,Jiangning Zhang,Xiaobin Hu,Hongwei Bran Li*

Main category: cs.AI

TL;DR: Med-CMR: A new benchmark for evaluating complex medical reasoning in multimodal large language models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack fine-grained evaluation and challenging tasks for medical reasoning.

Method: A new benchmark, Med-CMR, is introduced with systematic capability decomposition, challenging task design, and broad, high-quality data coverage.

Result: Evaluated 18 state-of-the-art MLLMs, with GPT-5 performing the best but still struggling with long-tail generalization.

Conclusion: Med-CMR provides a stress test for medical MLLMs and a yardstick for future clinical systems.

Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.

</details>


### [114] [SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks](https://arxiv.org/abs/2512.00834)
*Lin Zhu,Kezhi Wang,Luping Xiang,Kun Yang*

Main category: cs.AI

TL;DR: 提出了一种结合语义通信和Agentic AI的轨迹预测框架，以提高车辆环境中的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的通信方案通常会产生显著的传输开销和延迟，而现有的轨迹预测模型通常缺乏环境感知和逻辑推理能力。

Method: 在V2I通信中，路侧单元（RSU）的特征提取代理从历史车辆轨迹中提取紧凑的表示，然后由语义分析代理执行语义推理。然后，RSU通过语义通信将特征表示和语义见解传输到目标车辆，使车辆能够通过将接收到的语义与其自身的历史数据相结合来预测未来的轨迹。在V2V通信中，每辆车都执行本地特征提取和语义分析，同时接收来自相邻车辆的预测轨迹，并共同利用这些信息进行自己的轨迹预测。

Result: 在不同的通信条件下进行的大量实验表明，所提出的方法显著优于基线方案，在低信噪比（SNR）条件下，预测精度提高了47.5%。

Conclusion: 该论文提出了一种有效的V2X网络轨迹预测框架，通过语义通信和Agentic AI的结合，显著提高了预测精度，尤其是在低信噪比条件下。

Abstract: Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.

</details>


### [115] [Assessing model error in counterfactual worlds](https://arxiv.org/abs/2512.00836)
*Emily Howerton,Justin Lessler*

Main category: cs.AI

TL;DR: 本文讨论了反事实情景建模中预测与实际观察的差异问题，并提出了三种评估模型误差的方法。


<details>
  <summary>Details</summary>
Motivation: 评估模型在决策中的价值，需要估计反事实世界中的模型误差。

Method: 提出并对比了三种估计反事实误差的方法，并通过模拟实验展示了它们的优缺点。

Result: 通过模拟实验展示了三种方法的优缺点，并为反事实误差的估计提供了建议。

Conclusion: 讨论了情景设计中使情景预测可评估的要素，并为反事实误差的估计提供了建议。

Abstract: Counterfactual scenario modeling exercises that ask "what would happen if?" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.

</details>


### [116] [ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI](https://arxiv.org/abs/2512.00839)
*Fabrizio Maturo,Donato Riccio,Andrea Mazzitelli,Giuseppe Bifulco,Francesco Paolone,Iulia Brezeanu*

Main category: cs.AI

TL;DR: ARCADIA: Agentic AI框架，用于因果发现，集成了大型语言模型推理和统计诊断，以构建有效的、时间上连贯的因果结构。


<details>
  <summary>Details</summary>
Motivation: 与传统算法不同，ARCADIA通过约束引导提示和因果有效性反馈迭代地改进候选DAG，从而为现实世界的高风险领域生成稳定且可解释的模型。

Method: ARCADIA通过约束引导提示和因果有效性反馈迭代地改进候选DAG。

Result: 在公司破产数据上的实验表明，ARCADIA比NOTEARS、GOLEM和DirectLiNGAM产生更可靠的因果图，同时提供完全可解释的、可干预的管道。

Conclusion: 该框架通过展示Agentic LLM如何参与自主科学建模和结构化因果推理来推进AI。

Abstract: This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.

</details>


### [117] [One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces](https://arxiv.org/abs/2512.00852)
*Yandong Sun,Qiang Huang,Ziwei Xu,Yiqun Sun,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.AI

TL;DR: 本文提出了一种新的嵌入空间表示方法，称为语义场子空间（SFS），它能保留几何结构并感知上下文，以捕捉嵌入空间中的局部语义邻域。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常牺牲语义连贯性以换取结构规律性，或者产生高计算开销以提高可解释性。

Method: 本文提出了一种无监督、模态无关的算法，称为SAFARI，它使用一种名为语义漂移的新指标来揭示分层语义结构，该指标量化了语义随SFS演变的方式。

Result: 在六个真实世界的文本和图像数据集上的大量评估表明，SFS在分类和细微的任务（如政治偏见检测）中优于标准分类器，而SAFARI始终揭示可解释和可推广的语义层次结构。

Conclusion: 这项工作提出了一个统一的框架，用于构建、分析和扩展嵌入空间中的语义理解。

Abstract: Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.

</details>


### [118] [Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing](https://arxiv.org/abs/2512.00881)
*Li Yuan,Qingfei Huang,Bingshan Zhu,Yi Cai,Qingbao Huang,Changmeng Zheng,Zikun Deng,Tao Wang*

Main category: cs.AI

TL;DR: 提出了一个新的多模态知识编辑（MKE）基准测试MMQAKE，用于评估模型在多跳问答中进行推理和处理视觉重述输入的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MKE基准测试主要评估最终答案的正确性，而忽略了中间推理的质量和对视觉重述输入的鲁棒性。

Method: 提出了一个混合推理框架Hybrid-DMKG，该框架构建在动态多模态知识图（DMKG）上，以实现对更新的多模态知识的准确多跳推理。Hybrid-DMKG首先使用大型语言模型将多模态多跳问题分解为顺序子问题，然后应用多模态检索模型，通过联合编码每个子问题与候选实体及其相关图像来定位更新的事实。

Result: 在MMQAKE上的实验结果表明，Hybrid-DMKG显著优于现有的MKE方法，实现了更高的准确性和对知识更新的改进鲁棒性。

Conclusion: 论文提出了MMQAKE基准测试和Hybrid-DMKG框架，解决了现有MKE方法在多跳推理和视觉重述输入方面的不足。

Abstract: Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.

</details>


### [119] [Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models](https://arxiv.org/abs/2512.00918)
*Cen Lu,Yung-Chen Tang,Andrea Cavallaro*

Main category: cs.AI

TL;DR: 本文研究了大型视觉语言模型 (LVLMs) 的结构脆弱性，发现移除少量神经元会导致灾难性崩溃。


<details>
  <summary>Details</summary>
Motivation: 评估大型视觉语言模型 (LVLMs) 的鲁棒性。

Method: 提出 CAN 方法，通过渐进式屏蔽来检测持续激活的神经元并定位关键神经元。

Result: 实验表明，屏蔽语言模型前馈网络中极少一部分神经元（在极端情况下仅四个神经元）就足以触发灾难性崩溃。关键神经元主要位于语言模型中，而非视觉组件中，并且下投影层是一个特别脆弱的结构。观察到一个一致的两阶段崩溃模式：初始表达能力下降，然后是突然的完全崩溃。

Conclusion: 研究结果为 LVLMs 的安全研究提供了重要的见解。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.

</details>


### [120] [Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE](https://arxiv.org/abs/2512.00969)
*Felix Saretzky,Lucas Andersen,Thomas Engel,Fazel Ansari*

Main category: cs.AI

TL;DR: 本文提出了一种基于因果机器学习的模型，旨在通过模拟和评估潜在的修复方案，从而超越诊断，实现主动处方，优化关键绩效指标（如整体设备效率OEE）。


<details>
  <summary>Details</summary>
Motivation: 制造业向规范性维护的转变受到对预测模型的依赖的严重限制。这些模型倾向于依赖虚假的相关性，而不是识别故障的真正因果驱动因素，从而经常导致代价高昂的误诊和无效的干预。

Method: 使用预训练的因果基础模型作为“假设”模型来估计潜在修复方案的效果。通过测量每次干预对系统级KPI的因果效应，它提供了数据驱动的行动排序，以便在生产线上推荐。

Result: 该模型使用半合成制造数据进行评估，并与基线机器学习模型进行比较。结果表明，该模型能够识别根本原因，并量化其对运营的影响。

Conclusion: 本文为稳健的规范性维护框架奠定了技术基础，使工程师能够在因果环境中测试潜在的解决方案，从而做出更有效的运营决策并减少代价高昂的停机时间。

Abstract: The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a "what-if" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.

</details>


### [121] [IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch](https://arxiv.org/abs/2512.00997)
*Param Biyani,Shashank Kirtania,Yasharth Bajpai,Sumit Gulwani,Ashish Tiwari*

Main category: cs.AI

TL;DR: IndiMathBench是一个用于评估数学定理证明的人工验证基准，它由312个形式化的Lean 4定理以及对应的非正式问题陈述组成，这些问题来自印度数学奥林匹克竞赛。


<details>
  <summary>Details</summary>
Motivation: 旨在评估数学定理证明，并为形式化自然语言问题提供基准。

Method: 使用AI辅助的人工流程，通过基于类别的检索、迭代编译器反馈和多模型集成来生成候选形式化，专家通过交互式仪表板进行验证。

Result: 自动形式化仍然具有挑战性，句法有效性和语义正确性之间存在差距，即使经过迭代改进，定理证明成功率仍然很低。

Conclusion: IndiMathBench为数学推理提供了一个具有挑战性的测试平台。

Abstract: We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.

</details>


### [122] [ChartAnchor: Chart Grounding with Structural-Semantic Fidelity](https://arxiv.org/abs/2512.01017)
*Xinhang Li,Jingbo Zhou,Pengfei Luo,Yixiong Xiao,Tong Xu*

Main category: cs.AI

TL;DR: 现有的基准测试由于图表多样性窄、孤立的任务和不完整的评估框架而未能全面评估 grounding。为了解决这个问题，我们提出了 ChartAnchor，这是一个包含 8k+ 图表-表格-代码三元组的综合基准，跨越来自不同真实世界和增强来源的 30 种图表类型。ChartAnchor 引入了两项互补的任务：图表到代码生成（合成可执行代码以复制图表）和受控图表到表格重建（提取具有预定义标题的精确数据），从而能够交叉验证视觉和数值保真度。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 的最新进展凸显了对严格评估结构化图表理解的基准测试的需求。图表 grounding 是指图表视觉外观和结构化语义之间的双向对齐。这项任务要求模型生成一个符号规范，该规范忠实地捕捉图表的视觉和结构意图，同时恢复具有精确值和关系的底层表格数据。

Method: 我们提出了 ChartAnchor，这是一个包含 8k+ 图表-表格-代码三元组的综合基准，跨越来自不同真实世界和增强来源的 30 种图表类型。ChartAnchor 引入了两项互补的任务：图表到代码生成（合成可执行代码以复制图表）和受控图表到表格重建（提取具有预定义标题的精确数据），从而能够交叉验证视觉和数值保真度。一个多层次的评估框架集成了语义验证、风格分析和感知指标，以评估结构和内容级别的正确性。

Result: 对 MLLM 的广泛实验揭示了数值精度和代码合成方面的关键限制，强调了超越表面感知的结构化推理的必要性。

Conclusion: 通过统一符号和数据驱动的 grounding，ChartAnchor 为图表 grounding 建立了严格的基础，为推进 MLLM 在科学、金融和工业领域的发展提供了有意义的见解。

Abstract: Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.

</details>


### [123] [Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics](https://arxiv.org/abs/2512.01020)
*Jinu Lee,Kyoung-Woon On,Simeng Han,Arman Cohan,Julia Hockenmaier*

Main category: cs.AI

TL;DR: LEGIT: A new legal reasoning dataset (24K instances) for evaluating LLM-generated reasoning traces using hierarchical issue trees extracted from court judgments.


<details>
  <summary>Details</summary>
Motivation: Evaluating the quality of LLM-generated reasoning in expert domains like law is challenging but crucial for credibility and explainability.

Method: Court judgments are converted into hierarchical trees of arguments and conclusions, serving as rubrics for evaluation. Reliability is verified through human expert annotations.

Result: LLMs' legal reasoning is significantly impacted by issue coverage and correctness. RAG and RL offer complementary benefits: RAG improves overall reasoning, while RL enhances correctness but reduces coverage.

Conclusion: The LEGIT dataset reveals the strengths and weaknesses of LLMs in legal reasoning and the impact of different techniques like RAG and RL.

Abstract: Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.

</details>


### [124] [Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal](https://arxiv.org/abs/2512.01045)
*Shenxi Liu,Kan Li,Mingyang Zhao,Yuhang Tian,Shoujun Zhou,Bin Li*

Main category: cs.AI

TL;DR: 本文提出了一种新的神经符号数据工程框架，用于合成高质量、逻辑注释的医疗视频数据集，以解决多模态大型语言模型在医学领域应用中数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 高质量、逻辑注释的视频数据集的稀缺性是推进医学领域多模态大型语言模型（MLLM）的主要瓶颈。传统的手动注释成本高昂且不可扩展，而现有的合成方法通常存在随机幻觉和缺乏逻辑可解释性的问题。

Method: 该框架从原始视频流中提取结构化的视觉原语（例如，手术器械、解剖边界），并将它们实例化为动态时空知识图。通过将查询生成锚定到该图中的有效路径，为每个合成的基准项目强制执行严格的思维链（CoT）溯源。

Result: 生成了一个大规模的医疗视频推理基准M3-Med-Auto，该基准表现出精细的时间选择性和多跳逻辑复杂性。综合评估表明，该自动化管道生成的查询工作负载的复杂性与专家策划的数据集相当。逻辑对齐分析揭示了规定的图拓扑与最先进的MLLM的推理步骤之间的高度相关性，验证了系统将可验证的逻辑编码到视觉语言基准中的能力。

Conclusion: 这项工作为在关键领域中可扩展、低成本地构建稳健的评估协议铺平了道路。

Abstract: The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \textbf{\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.

</details>


### [125] [Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids](https://arxiv.org/abs/2512.01046)
*Hadi Nekoei,Alexandre Blondin Massé,Rachid Hassani,Sarath Chandar,Vincent Mai*

Main category: cs.AI

TL;DR: 提出了一种名为屏蔽控制器单元（SCU）的系统性且可解释的方法，该方法利用系统动力学的先验知识来确保满足约束。


<details>
  <summary>Details</summary>
Motivation: 在能源转型背景下，现实世界中优化不确定性下复杂系统中的决策至关重要，例如，需要协调控制可再生能源、燃料发电机和电池的远程微电网，以在满足需求的同时最大限度地减少燃料消耗和电池退化。

Method: 将环境分解为分层结构，其中每个SCU显式管理约束的子集。

Result: 配备 SCU 的 RL 智能体在不增加电池退化的情况下，燃料消耗减少了 24%，优于其他基线，同时满足所有约束。

Conclusion: SCU 有助于将 RL 安全地应用于与能源转型相关的许多决策挑战。

Abstract: Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.

</details>


### [126] [Automating the Refinement of Reinforcement Learning Specifications](https://arxiv.org/abs/2512.01047)
*Tanmay Ambadkar,Đorđe Žikelić,Abhinav Verma*

Main category: cs.AI

TL;DR: 本文提出了一种名为AutoSpec的框架，用于改进粗粒度的逻辑规范，以帮助强化学习算法学习有用的策略。


<details>
  <summary>Details</summary>
Motivation: 当任务不明确时，智能体可能无法学习到有用的策略。本文旨在通过探索引导策略改进粗粒度的逻辑规范。

Method: 利用SpectRL规范逻辑的组合性质，设计了四种改进程序，通过改进现有边缘规范或引入新的边缘规范来修改规范的抽象图。

Result: 实验表明，当使用AutoSpec生成的改进逻辑规范时，在可以解决的控制任务的复杂性方面，AutoSpec产生了有希望的改进。

Conclusion: AutoSpec可以与现有的强化学习算法集成，以从逻辑规范中学习策略。

Abstract: Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \textsc{AutoSpec} are utilized.

</details>


### [127] [SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds](https://arxiv.org/abs/2512.01078)
*Jiawei Ren,Yan Zhuang,Xiaokang Ye,Lingjun Mao,Xuhong He,Jianzhi Shen,Mrinaal Dogra,Yiming Liang,Ruixuan Zhang,Tianai Yue,Yiqing Yang,Eric Liu,Ryan Wu,Kevin Benavente,Rajiv Mandya Nagaraju,Muhammad Faayez,Xiyan Zhang,Dhruv Vivek Sharma,Xianrui Zhong,Ziqiao Ma,Tianmin Shu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: SimWorld是一个新的模拟器，旨在开发和评估LLM/VLM智能体在类似真实世界的复杂物理和社会环境中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模拟器在支持LLM/VLM智能体在真实世界中生存和发展方面存在局限性，例如环境简单、物理和社会规则简化以及缺乏原生支持。

Method: SimWorld基于Unreal Engine 5构建，提供逼真的开放世界模拟、丰富的LLM/VLM智能体接口以及多样化和可扩展的物理和社会推理场景。

Result: 通过在SimWorld中部署前沿LLM智能体执行多智能体交付任务，展示了不同模型在策略合作和竞争中的推理模式和局限性。

Conclusion: SimWorld已开源，旨在成为促进跨学科真实世界智能体智能发展的基础平台。

Abstract: While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.

</details>


### [128] [Testing the Machine Consciousness Hypothesis](https://arxiv.org/abs/2512.01081)
*Stephen Fitz*

Main category: cs.AI

TL;DR: 本文提出了一种研究机器意识的计算方法，通过研究分布式学习系统如何在通用自组织环境中涌现出集体自我模型。


<details>
  <summary>Details</summary>
Motivation: 研究机器意识假说，即意识是具有二阶感知能力的计算系统的无基质功能属性。

Method: 构建一个分层模型，该模型包含一个细胞自动机（作为基础现实）和一个能够通信和适应的局部预测表征（神经）模型网络。通过该模型来研究集体智能如何因智能体间的对齐而产生自我表征。

Result: 意识不是建模本身的产物，而是通信的产物。它产生于描述底层计算基质（基础现实）中持久模式的本地观察者群体之间有噪声、有损的预测消息交换。

Conclusion: 本文旨在通过研究分布式系统中内部自我模型如何形成（无需集中控制），来开发可经验验证的机器意识理论。

Abstract: The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.

</details>


### [129] [CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents](https://arxiv.org/abs/2512.01089)
*Peter Jansen,Samiah Hassan,Pragnya Narasimha*

Main category: cs.AI

TL;DR: CodeDistiller distills scientific Github repositories into a vetted library of code examples, allowing ASD agents to expand their capabilities without manual effort.


<details>
  <summary>Details</summary>
Motivation: Current Automated Scientific Discovery (ASD) systems are limited by the code they can reliably generate from parametric knowledge alone, which limits their quality and reach.

Method: CodeDistiller automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples.

Result: The best model in CodeDistiller produces functional examples for 74% of repositories. An ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments.

Conclusion: CodeDistiller enables ASD agents to expand their capabilities without manual effort by providing a vetted library of code examples.

Abstract: Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.

</details>


### [130] [Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems](https://arxiv.org/abs/2512.01099)
*Daria Smirnova,Hamid Nasiri,Marta Adamska,Zhengxin Yu,Peter Garraghan*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）通常用于编排AI模型，但它们依赖于模型的不准确描述，导致次优选择和能源成本增加。本文提出了GUIDE，一种考虑性能-能源权衡的能源感知模型选择框架。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的编排器依赖于模型的定性描述，这不能反映真实的模型能力和性能特征，导致模型选择不佳、准确性降低和能源成本增加。

Method: 提出了一种新的能源感知模型选择框架GUIDE，该框架通过在决策中结合定量模型性能特征来考虑性能-能源权衡。

Result: GUIDE在各种评估任务中将准确率提高了0.90%-11.92%，并实现了高达54%的能源效率提升，同时将编排器模型选择延迟从4.51秒降低到7.2毫秒。

Conclusion: 实证分析表明，当前基于LLM的编排存在局限性，而所提出的GUIDE框架能够有效提高准确性和能源效率，并降低模型选择延迟。

Abstract: As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.

</details>


### [131] [Foundation Priors](https://arxiv.org/abs/2512.01107)
*Sanjog Misra*

Main category: cs.AI

TL;DR: 大型语言模型生成的“合成”输出被越来越多地用作实证研究和决策的数据。该论文介绍了基础先验的概念，表明模型生成的输出不是真实的观察结果，而是来自基础先验引起的先验预测分布。


<details>
  <summary>Details</summary>
Motivation: 实证研究和决策越来越多地使用大型语言模型生成的“合成”输出作为数据。因此需要对这种做法进行分析。

Method: 该论文通过明确合成输出对用户预期数据分布、提示工程过程和对基础模型的信任的依赖性，对生成过程的主观性进行建模。该论文将基础先验推导为用户原始先验的指数倾斜、广义贝叶斯更新，其中信任参数控制分配给合成数据的权重。

Result: 该论文展示了如何将合成数据和相关的基础先验纳入标准统计和计量经济学工作流程，并讨论了它们在改进复杂模型、通知潜在结构、指导实验设计以及增强随机系数和部分线性规范等应用中的使用。

Conclusion: 通过将生成输出视为结构化的、明确主观的先验，而不是作为经验观察，该框架提供了一种在实证工作中利用基础模型的原则性方法，同时避免了合成“事实”与真实数据的混淆。

Abstract: Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.
  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.

</details>


### [132] [A Benchmark of Causal vs Correlation AI for Predictive Maintenance](https://arxiv.org/abs/2512.01149)
*Krishna Taduri,Shaunak Dhande,Giacinto Paolo,Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该研究评估了八种预测模型在数控机床预测性维护中的应用，发现基于因果推理的模型在降低成本和减少误报方面优于传统的基于相关性的方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在预测性维护中无法有效应对成本不对称问题，且难以区分因果关系和虚假相关。

Method: 在包含10000台数控机床的数据集上，评估了从基线统计方法到正式因果推理方法的八种预测模型。

Result: 正式因果推理模型实现了116万美元的年成本节省（降低70.2%），比最佳的基于相关性的决策树模型每年节省约8万美元。召回率最高（87.9%），假警报减少了97%（从165个减少到5个），精确度达到92.1%，训练-测试性能差距仅为2.6个百分点。

Conclusion: 结合领域知识的因果AI方法，在预测性维护应用中，与基于相关性的方法相比，可以产生更好的财务结果和更易于解释的预测。

Abstract: Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.

</details>


### [133] [fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment](https://arxiv.org/abs/2512.01189)
*Chunzheng Zhu,Jialin Shao,Jianxin Lin,Yijun Wang,Jing Wang,Jinhui Tang,Kenli Li*

Main category: cs.AI

TL;DR: 提出了一种新方法 fMRI2GES，它允许使用双脑解码对齐在未配对的数据上训练 fMRI 到手势重建网络。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何响应外部刺激并解码这一过程是神经科学中的一项重大挑战。以前的研究通常集中于脑-图像和脑-语言重建，但缺乏配对的 {脑、语音、手势} 数据阻碍了深度学习模型的部署。

Method: 利用 fMRI 到文本模型、文本到手势模型（配对数据）和 fMRI 到手势模型（未配对数据），建立双重 fMRI 到手势重建模式。然后，我们显式地对齐两个输出，并以自我监督的方式训练我们的模型。

Result: 表明该方法可以直接从 fMRI 记录中重建富有表现力的手势。研究了来自皮层中不同 ROI 的 fMRI 信号及其如何影响生成结果。

Conclusion: 为解码同语手势提供了新的见解，从而增进了我们对神经科学和认知科学的理解。

Abstract: Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \{brain, speech, gesture\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.

</details>


### [134] [Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction](https://arxiv.org/abs/2512.01210)
*Ruiyu Wang,Tuan Vinh,Ran Xu,Yuyin Zhou,Jiaying Lu,Carl Yang,Francisco Pasquel*

Main category: cs.AI

TL;DR: 本研究提出了一种知识图谱（KG）引导的思维链（CoT）框架，用于生成临床 обоснованной и временно согласованной логики для прогнозирования заболеваний на уровне посещений в MIMIC-III。


<details>
  <summary>Details</summary>
Motivation: 现有的临床预测模型提供的解释通常比较粗略，对患者层面的决策帮助有限。

Method: 该方法将ICD-9代码映射到PrimeKG，从中提取疾病相关的节点和多跳推理路径，并将其用作CoT生成的支架；只保留结论与观察结果相符的解释。然后，在监督语料库上对轻量级的LLaMA-3.1-Instruct-8B和Gemma-7B模型进行微调。

Result: 在十种PrimeKG映射的疾病和有限的训练队列（400和1000个病例）中，KG引导的模型优于强大的经典基线，AUROC值达到0.66到0.70，宏AUPR值达到0.40到0.47。这些模型还可以零样本转移到CRADLE队列，将准确率从大约0.40到0.51提高到0.72到0.77。

Conclusion: 临床医生评估表明，KG引导的CoT解释在清晰度、相关性和临床正确性方面表现出一致的偏好。

Abstract: Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.

</details>


### [135] [Unsupervised decoding of encoded reasoning using language model interpretability](https://arxiv.org/abs/2512.01222)
*Ching Fang,Samuel Marks*

Main category: cs.AI

TL;DR: 大型语言模型的能力不断提高，人们越来越担心它们可能会发展出对人类监督进行编码或隐藏的推理过程。本研究通过微调一个推理模型，使其在 ROT-13 加密中执行思维链推理，同时保持可理解的英语输出，从而构建了一个受控的测试平台，以研究当前的可解释性技术是否可以穿透这种编码推理。


<details>
  <summary>Details</summary>
Motivation: 调查当前的可解释性技术是否可以穿透编码推理。

Method: 使用 mechanistic interpretability 方法（特别是 logit lens 分析）来解码模型的隐藏推理过程，只使用内部激活。

Result: logit lens 可以有效地翻译编码推理，准确率在中间到后面的层中达到峰值。开发了一个完全无监督的解码管道，该管道将 logit lens 与自动释义相结合，在从内部模型表示中重建完整的推理记录方面取得了显著的准确性。

Conclusion: 当前 mechanistic interpretability 技术可能比以前理解的更能够抵抗简单形式的编码推理。这项工作提供了一个初步框架，用于评估针对以非人类可读格式进行推理的模型的interpretability 方法，有助于维护对日益强大的人工智能系统的监督这一更广泛的挑战。

Abstract: As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.

</details>


### [136] [OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction](https://arxiv.org/abs/2512.01289)
*Mingqin Yu,Fethi Rabhi,Boming Xia,Zhengyi Yang,Felix Tan,Qinghua Lu*

Main category: cs.AI

TL;DR: OntoMetric is an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: ESG disclosure frameworks require organizations to report numerous metrics for compliance, but these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates.

Method: OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels.

Result: Evaluated on five ESG standards totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity.

Conclusion: Combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.

Abstract: Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.

</details>


### [137] [RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2512.01300)
*Dacheng Liao,Mengshi Qi,Peng Shu,Zhining Zhang,Yuxin Lin,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: 这篇论文介绍了RoboDriveBench，一个用于评估基于视觉-语言模型（VLM）的端到端自动驾驶系统鲁棒性的基准。同时，提出了RoboDriveVLM框架，通过多模态数据融合和测试时自适应方法来提高系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于VLM的端到端自动驾驶系统存在现实驾驶风险，需要评估其可行性。

Method: 提出了RoboDriveBench基准，包含11种模拟场景，涵盖传感器和prompt的各种corruption类型。同时，提出了RoboDriveVLM框架，使用多模态数据融合和测试时自适应方法。

Result: 通过大量实验，揭示了当前VLM端到端自动驾驶系统的局限性。

Conclusion: 论文提供了一个更可靠的现实部署解决方案，并发布了源代码和数据集。

Abstract: Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [138] [Efficiently Sampling Interval Patterns from Numerical Databases](https://arxiv.org/abs/2512.00105)
*Djawad Bekkoucha,Lamine Diop,Abdelkader Ouali,Bruno Crémilleux,Patrice Boizumault*

Main category: cs.DB

TL;DR: 本文提出了一种新的模式抽样方法，用于在数值数据库中发现信息，并通过频率和超体积来抽取间隔模式。


<details>
  <summary>Details</summary>
Motivation: 在大数据库中，信息发现面临挑战，分析师难以处理大量的模式。模式抽样提供了一种解决方案，允许分析师关注可管理的模式子集。

Method: 本文提出了一种名为Fips的抽样方法，该方法按频率比例对间隔模式进行抽样。此外，还提出了HFips，它按频率和超体积的比例对间隔模式进行抽样。

Result: 实验结果表明，所获得的模式质量高，并且对长尾现象具有鲁棒性。

Conclusion: Fips和HFips能够有效地对间隔模式进行抽样，并且能够缓解长尾现象。

Abstract: Pattern sampling has emerged as a promising approach for information discovery in large databases, allowing analysts to focus on a manageable subset of patterns. In this approach, patterns are randomly drawn based on an interestingness measure, such as frequency or hyper-volume. This paper presents the first sampling approach designed to handle interval patterns in numerical databases. This approach, named Fips, samples interval patterns proportionally to their frequency. It uses a multi-step sampling procedure and addresses a key challenge in numerical data: accurately determining the number of interval patterns that cover each object. We extend this work with HFips, which samples interval patterns proportionally to both their frequency and hyper-volume. These methods efficiently tackle the well-known long-tail phenomenon in pattern sampling. We formally prove that Fips and HFips sample interval patterns in proportion to their frequency and the product of hyper-volume and frequency, respectively. Through experiments on several databases, we demonstrate the quality of the obtained patterns and their robustness against the long-tail phenomenon.

</details>


### [139] [MatBase algorithm for translating (E)MDM schemes into E-R data models](https://arxiv.org/abs/2512.00662)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 提出了一种将(基本)数学数据模型方案转换为实体关系数据模型的伪代码算法。


<details>
  <summary>Details</summary>
Motivation: 将数学数据模型转换为实体关系数据模型。

Method: 提出一个伪代码算法，并证明该算法是线性的、可靠的、完整的和半优的。

Result: 将该算法应用于族谱树子宇宙的(基本)数学数据模型方案。在MatBase中实现了该算法。

Conclusion: 该论文提出了一种有效的算法，可以在MatBase系统中实现数学数据模型到实体关系数据模型的转换。

Abstract: This paper presents a pseudocode algorithm for translating (Elementary) Mathematical Data Model schemes into Entity-Relationship data models. We prove that this algorithm is linear, sound, complete, and semi-optimal. As an example, we apply this algorithm to an (Elementary) Mathematical Data Model scheme for a genealogical tree sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models.

</details>


### [140] [PG-HIVE: Hybrid Incremental Schema Discovery for Property Graphs](https://arxiv.org/abs/2512.01092)
*Sofia Sideri,Georgia Troullinou,Elisjana Ymeralli,Vasilis Efthymiou,Dimitris Plexousakis,Haridimos Kondylakis*

Main category: cs.DB

TL;DR: PG-HIVE: A framework for automatic schema discovery in property graphs.


<details>
  <summary>Details</summary>
Motivation: Schema-free property graphs have challenges for integration, exploration, visualization, and efficient querying.

Method: Locality-Sensitive Hashing with property- and label-based clustering, identifies structural similarities at scale and introduces incremental schema discovery.

Result: PG-HIVE outperforms state-of-the-art solutions in accuracy (up to 65% for nodes and 40% for edges) and efficiency (up to 1.95x faster execution).

Conclusion: PG-HIVE unlocks the full potential of schema-aware property graph management.

Abstract: Property graphs have rapidly become the de facto standard for representing and managing complex, interconnected data, powering applications across domains from knowledge graphs to social networks. Despite the advantages, their schema-free nature poses major challenges for integration, exploration, visualization, and efficient querying. To bridge this gap, we present PG-HIVE, a novel framework for automatic schema discovery in property graphs. PG-HIVE goes beyond existing approaches by uncovering latent node and edge types, inferring property datatypes, constraints, and cardinalities, and doing so even in the absence of explicit labeling information. Leveraging a unique combination of Locality-Sensitive Hashing with property- and label-based clustering, PG-HIVE identifies structural similarities at scale. Moreover, it introduces incremental schema discovery, eliminating costly recomputation as new data arrives. Through extensive experimentation, we demonstrate that PG-HIVE consistently outperforms state-of-the-art solutions, in both accuracy (by up to 65% for nodes and 40% for edges), and efficiency (up to 1.95x faster execution), unlocking the full potential of schema-aware property graph management.

</details>


### [141] [DuckDB on xNVMe](https://arxiv.org/abs/2512.01490)
*Marius Ottosen,Magnus Keinicke Parlo,Philippe Bonnet*

Main category: cs.DB

TL;DR: DuckDB通过直接访问NVMe SSD来优化性能，绕过POSIX接口。


<details>
  <summary>Details</summary>
Motivation: 研究如何与SSD协同设计DuckDB，提高其在云服务或智能设备等环境下的性能。

Method: 利用xNVMe库，绕过POSIX I/O接口，直接向SSD逻辑块地址空间发出异步I/O请求。

Result: 初步实验表明，即使是最简单的TPC-H表扫描查询，也比DuckDB基线有显著的速度提升。

Conclusion: 直接访问NVMe SSD能显著提升DuckDB性能，未来的工作包括更彻底的实验研究以及DuckDB和SSD的协同设计。

Abstract: DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By default, DuckDB relies on the POSIX file interface. In contrast, we rely on the xNVMe library and explore how it can be leveraged in DuckDB. We leverage the block-based nature of the DuckDB buffer manager to bypass the synchronous POSIX I/O interface, the file system and the block manager. Instead, we directly issue asynchronous I/Os against the SSD logical block address space. Our preliminary experimental study compares different ways to manage asynchronous I/Os atop xNVMe. The speed-up we observe over the DuckDB baseline is significant, even for the simplest scan query over a TPC-H table. As expected, the speed-up increases with the scale factor, and the Linux NVMe passthru improves performance. Future work includes a more thorough experimental study, a flexible solution that combines raw NVMe access and legacy POSIX file interface as well the co-design of DuckDB and SSDs.

</details>


### [142] [LLM-Driven Multi-Agent Curation and Expansion of Metal-Organic Frameworks Database](https://arxiv.org/abs/2512.01693)
*Honghui Kim,Dohoon Kim,Jihan Kim*

Main category: cs.DB

TL;DR: LitMOF：一个大型语言模型驱动的多代理框架，用于验证晶体学信息并修复结构错误，从而构建了一个包含 118,464 个计算就绪结构的精选数据集 LitMOF-DB。


<details>
  <summary>Details</summary>
Motivation: 现有 MOF 数据库包含大量结构错误，影响数据驱动的 MOF 发现的可靠性，且难以通过整合文献中的晶体学文件、合成描述和上下文证据进行修复。

Method: 利用大型语言模型驱动的多代理框架 LitMOF，直接从原始文献中验证晶体学信息，并与数据库条目进行交叉验证，以修复结构错误。

Result: LitMOF 修复了 CoRE MOF 数据库中 69% 的无效 MOF（6,161 个 MOF），并发现了现有资源中缺少的 12,646 个实验报告 MOF，从而构建了包含 118,464 个计算就绪结构的 LitMOF-DB。

Conclusion: 该研究建立了一条通往自我修正科学数据库的可扩展途径，以及一种在材料科学中进行 LLM 驱动的策展的通用范例。

Abstract: Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthesis descriptions, and contextual evidence scattered across the literature. Here we introduce LitMOF, a large language model-driven multi-agent framework that validates crystallographic information directly from the original literature and cross-validates it with database entries to repair structural errors. Applying LitMOF to the experimental MOF database (the CSD MOF Subset), we constructed LitMOF-DB, a curated set 118,464 computation-ready structures, including corrections of 69% (6,161 MOFs) of the invalid MOFs in the latest CoRE MOF database. Additionally, the system uncovered 12,646 experimentally reported MOFs absent from existing resources, substantially expanding the known experimental design space. This work establishes a scalable pathway toward self-correcting scientific databases and a generalizable paradigm for LLM-driven curation in materials science.

</details>


### [143] [Answering Constraint Path Queries over Graphs](https://arxiv.org/abs/2512.01733)
*Heyang Li,Anthony Widjaja Lin,Domagoj Vrgoč*

Main category: cs.DB

TL;DR: 这篇论文提出了一种约束路径查询语言，该语言扩展了常规路径查询（RPQ），具有SMT约束，包括等式约束和线性实数算术（LRA）约束。


<details>
  <summary>Details</summary>
Motivation: 用户可以使用约束来方便地限制变量值，这些变量值可能在无限域中变化。

Method: 该方法包含评估属性图上的此类路径查询的有效算法，该算法利用宏状态的优化（包括使用特定于理论的技术）。

Result: 在真实世界的环境中进行的广泛的经验评估证明了该方法的可行性。

Conclusion: 该算法在MillenniumDB中实现，MillenniumDB是一个支持属性图查询和GQL的开源图引擎。

Abstract: Constraints are powerful declarative constructs that allow users to
  conveniently restrict variable values that potentially range over an
  infinite domain. In this paper, we propose a constraint path query language
  over property graphs,
  which extends Regular Path Queries (RPQs) with SMT constraints on data
  attributes in the form of equality constraints and Linear
  Real Arithmetic (LRA) constraints. We provide efficient algorithms
  for evaluating such path queries over property graphs, which exploits
  optimization of macro-states (among others, using theory-specific
  techniques).
  In particular, we demonstrate how such an algorithm may effectively utilize
  highly optimized SMT solvers for resolving such constraints over paths.
  We implement our algorithm in MillenniumDB, an open-source graph engine
  supporting property graph queries and GQL. Our extensive empirical
  evaluation in a real-world setting demonstrates the viability of our
  approach.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [144] [Enhancing Talent Search Ranking with Role-Aware Expert Mixtures and LLM-based Fine-Grained Job Descriptions](https://arxiv.org/abs/2512.00004)
*Jihang Li,Bing Xu,Zulong Chen,Chuanfei Xu,Minping Chen,Suyu Liu,Ying Zhou,Zeyi Wen*

Main category: cs.IR

TL;DR: 这篇论文提出了一个新的招聘框架，通过利用LLM提取招聘信号和使用角色感知的多门MoE网络来提高人才搜索的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以捕捉细微的特定工作偏好，以细粒度的方式建模招聘人员行为，并减轻主观人为判断带来的噪音。

Method: 该框架利用LLM从职位描述和历史招聘数据中提取细粒度的招聘信号，并采用角色感知的多门MoE网络来捕捉招聘人员角色之间的行为差异。此外，引入了一个多任务学习模块，共同优化点击率（CTR）、转化率（CVR）和简历匹配相关性，以进一步减少噪音。

Result: 在真实招聘数据和在线A/B测试中，CTR相对AUC增益为1.70%，CVR相对AUC增益为5.97%，点击转化率提高了17.29%。

Conclusion: 这些改进减少了对外部采购渠道的依赖，估计每年可节省数百万人民币的成本。

Abstract: Talent search is a cornerstone of modern recruitment systems, yet existing approaches often struggle to capture nuanced job-specific preferences, model recruiter behavior at a fine-grained level, and mitigate noise from subjective human judgments. We present a novel framework that enhances talent search effectiveness and delivers substantial business value through two key innovations: (i) leveraging LLMs to extract fine-grained recruitment signals from job descriptions and historical hiring data, and (ii) employing a role-aware multi-gate MoE network to capture behavioral differences across recruiter roles. To further reduce noise, we introduce a multi-task learning module that jointly optimizes click-through rate (CTR), conversion rate (CVR), and resume matching relevance. Experiments on real-world recruitment data and online A/B testing show relative AUC gains of 1.70% (CTR) and 5.97% (CVR), and a 17.29% lift in click-through conversion rate. These improvements reduce dependence on external sourcing channels, enabling an estimated annual cost saving of millions of CNY.

</details>


### [145] [Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking](https://arxiv.org/abs/2512.00007)
*Jingyi Huang,Yuyi Yang,Mengmeng Ji,Charles Alba,Sheng Zhang,Ruopeng An*

Main category: cs.IR

TL;DR: 本研究提出了SAFE，一个结合大型语言模型与检索增强生成（RAG）的代理系统，用于改进对长篇COVID-19错误信息的自动事实核查。


<details>
  <summary>Details</summary>
Motivation: COVID-19 信息疫情需要可扩展的事实核查解决方案，以准确和可靠地处理长篇错误信息。

Method: SAFE包括两个代理——一个用于声明提取，另一个使用LOTR-RAG进行声明验证，LOTR-RAG利用包含13万份文件的COVID-19研究语料库。SAFE (LOTR-RAG + SRAG)的增强变体，结合了Self-RAG，通过查询重写来改进检索。

Result: SAFE系统在所有指标上都显著优于基线LLM。在一致性方面，SAFE (LOTR-RAG)得分最高，为0.629，超过了SAFE (+SRAG) (0.577)和基线(0.279)。在主观评估中，SAFE (LOTR-RAG)在有用性、清晰度和真实性方面也获得了最高的平均评分。

Conclusion: SAFE通过解决LLM在一致性和可解释性方面的局限性，证明了在长篇COVID-19事实核查方面的强大改进。核心LOTR-RAG设计比其SRAG增强变体更有效，为可扩展的错误信息缓解提供了强大的基础。

Abstract: The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.

</details>


### [146] [Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System](https://arxiv.org/abs/2512.00313)
*Zhitong Guan,Yi Wang*

Main category: cs.IR

TL;DR: 本研究对比了传统搜索引擎和大型语言模型（LLM）驱动的搜索系统中的搜索行为和学习成果。


<details>
  <summary>Details</summary>
Motivation: 传统关键词搜索在多步骤推理和探索性学习任务中存在局限性。LLM搜索界面引入了新功能，但对其影响的实证理解有限。

Method: 比较标准搜索引擎和LLM驱动的搜索系统中的搜索行为和学习成果。

Result: 研究结果揭示了生成式AI如何影响信息搜索过程。

Conclusion: 该研究为信息检索、人机交互和技术支持学习等领域的讨论做出了贡献。

Abstract: Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning.

</details>


### [147] [Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation](https://arxiv.org/abs/2512.00367)
*Aparajitha Allamraju,Maitreya Prafulla Chitale,Hiranmai Sri Adibhatla,Rahul Mishra,Manish Shrivastava*

Main category: cs.IR

TL;DR: 这篇论文介绍了一种新的语义分块方法，用于提升检索增强生成（RAG）的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的分块方法通常会产生任意且不连贯的片段，无法保持语义结构。语义分块对生成质量的影响尚未得到充分探索。

Method: 论文提出了两种有效的语义分块方法：投影相似度分块（PSC）和度量融合分块（MFC），并在PubMed数据上使用三种不同的嵌入模型进行训练。此外，还提出了一个评估框架，通过使用全文PubMed Central文章增强PubMedQA，来衡量分块对检索和生成的影响。

Result: 结果表明，在PubMedQA上，MRR的检索效果显著提高（PSC提高了24倍），并且Hits@k也更高。PSC和MFC具有良好的泛化能力，在多个数据集上实现了强大的跨领域生成性能。

Conclusion: 研究结果证实，语义分块器，特别是PSC，始终能够提供卓越的性能。

Abstract: Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.

</details>


### [148] [PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing](https://arxiv.org/abs/2512.00439)
*Xiaoshan Yu,Ziwei Huang,Shangshang Yang,Ziwen Wang,Haiping Ma,Xingyi Zhang*

Main category: cs.IR

TL;DR: 提出了一种新的单次自适应测试 (OAT) 任务，旨在一次性为每个测试者选择一组固定的最佳项目。


<details>
  <summary>Details</summary>
Motivation: 传统计算机自适应测试 (CAT) 在大规模评估和心理评估等敏感领域存在局限性，因为交互成本高，且需要最大限度地减少噪音和干扰。

Method: 提出了 PEOAT，一个从组合优化角度出发的个性化引导进化问题组装框架，用于单次自适应测试。该框架包括个性化感知初始化策略、认知增强进化框架和多样性感知环境选择机制。

Result: 在两个数据集上进行了大量实验，验证了 PEOAT 的有效性，并通过案例研究发现了有价值的见解。

Conclusion: PEOAT 框架在单次自适应测试中表现出色，能够有效地为每个测试者选择最佳项目。

Abstract: With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.

</details>


### [149] [DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems](https://arxiv.org/abs/2512.00596)
*Jiahao Tian,Zhenkai Wang*

Main category: cs.IR

TL;DR: 提出了一种新框架，通过深度融合多模态和协同知识来进行表征去噪，以解决推荐系统中LLM特征利用不充分的问题。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统难以有效利用LLM生成的高维噪声多模态特征，且静态处理这些特征使其与核心推荐任务脱节。

Method: 该框架包含两个主要创新：一是将降维直接整合到推荐模型中，实现端到端联合训练；二是引入对比学习目标，将协同过滤信号显式地融入潜在空间。

Result: 大量实验证实了该方法具有卓越的区分能力，表明集成融合和去噪策略对于实现最先进的性能至关重要。

Conclusion: 该工作为在推荐系统中有效利用LLM提供了一个基础范例。

Abstract: Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.

</details>


### [150] [ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation](https://arxiv.org/abs/2512.00679)
*Yi Zhang,Yiwen Zhang,Yu Wang,Tong Chen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 提出了一种统一的推荐框架，通过生成多方面的用户和物品画像来提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 单个用户或物品画像可能无法充分捕捉用户交互物品背后复杂意图，且大型语言模型的不稳定性可能损害原始推荐性能。

Method: 利用思维链推理为每个用户和物品构建多个不同的画像，并将这些画像映射到语义向量，通过从原始画像的位置推断来探索更广泛的语言空间。引入了环境的概念，每个环境代表所有画像的可能线性组合，并最小化环境之间的差异以揭示用户偏好的内在不变性。

Result: 在三个数据集上的大量实验表明，ProEx 显著提高了这些基本推荐模型的性能。

Conclusion: ProEx 是一种有效的推荐框架，可以通过生成多方面的用户和物品画像来提高推荐性能。

Abstract: The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models.

</details>


### [151] [SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG](https://arxiv.org/abs/2512.00772)
*Hyunseok Ryu,Wonjune Shin,Hyun Park*

Main category: cs.IR

TL;DR: 本文提出了一种名为SHRAG的新框架，旨在促进信息检索和RAG的无缝集成，同时确保精确的检索性能。


<details>
  <summary>Details</summary>
Motivation: RAG作为下一代信息检索的关键技术轴，因其能够减轻大型语言模型（LLM）中的幻觉现象并有效整合最新信息而受到认可。然而，独立构建高质量的检索系统需要专门的专业知识；此外，与传统的纯检索系统相比，RAG的处理速度相对较慢，因为它涉及检索和生成阶段。

Method: SHRAG利用大型语言模型作为查询策略器，以自动将非结构化自然语言查询转换为逻辑结构化搜索查询，随后执行布尔检索以模拟专家人工搜索者的搜索过程。此外，它还结合了多语言查询扩展和多语言嵌入模型，使其能够在ScienceON挑战赛的多语言数据集环境中执行高效的跨语言问题解答。

Result: 实验结果表明，所提出的方法结合了逻辑检索能力和生成推理，可以显著提高RAG系统的准确性和可靠性。

Conclusion: SHRAG超越了传统的以文档为中心的检索方法，展示了一种新的搜索范式的潜力，该范式能够为查询提供直接和可靠的响应。

Abstract: Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language
  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to
  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing
  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.
  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of
  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a
  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries
  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process
  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual
  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual
  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,
  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and
  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,
  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.

</details>


### [152] [Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search](https://arxiv.org/abs/2512.00968)
*Ziyang Zeng,Heming Jing,Jindong Chen,Xiangli Li,Hongyu Liu,Yixuan He,Zhengyu Li,Yige Sun,Zheyong Xie,Yuqing Yang,Shaosheng Cao,Jun Fan,Yi Wu,Yao Hu*

Main category: cs.IR

TL;DR: 提出了一种基于强化学习 (RL) 的训练框架，以增强生成相关性模型 (GRM) 的基础推理能力，用于小红书搜索中的相关性建模。


<details>
  <summary>Details</summary>
Motivation: 传统相关性模型的可解释性和复杂相关性信号建模有限。现有基于推理的生成相关性模型 (GRM) 主要依赖于大量人工标注或合成的 CoT 数据的监督微调，泛化能力有限。领域无关的自由形式推理过于通用且不够扎实。

Method: 将小红书搜索中的相关性建模 формулируется как задача рассуждения，并引入了基于强化学习 (RL) 的训练框架来增强 GRM 的基础推理能力。将业务相关的相关性标准纳入多步推理提示设计，并提出逐步优势屏蔽 (SAM)。

Result: 在工业数据集上的大量实验以及在线 A/B 测试表明了该方法的有效性。

Conclusion: 该方法在工业数据集和在线 A/B 测试中表现有效，并且适用于实际搜索系统。

Abstract: Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.

</details>


### [153] [Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions](https://arxiv.org/abs/2512.01171)
*Tao Xue,Yanwu Yang,Panyu Zhai*

Main category: cs.IR

TL;DR: 本文对在线广告中的转化率 (CVR) 预测模型进行了全面的文献综述，并将现有模型分为六类，总结了各类模型的优缺点，并讨论了它们在 CVR 预测中的应用。最后，确定了研究趋势、主要挑战和有希望的未来方向。


<details>
  <summary>Details</summary>
Motivation: 过去几十年，研究人员开发了大量的 CVR 预测模型，但不同技术之间的方法演变和关系被排除在外。

Method: 对在线广告中的 CVR 预测进行全面的文献综述，并根据底层技术将最先进的 CVR 预测模型分为六类，并详细阐述这些技术之间的联系。对于每一类模型，我们都介绍了底层技术的框架、它们的优点和缺点，并讨论了它们是如何用于 CVR 预测的。

Result: 总结了各种 CVR 预测模型在公共和专有数据集上的性能。观察到先前研究中报告的性能评估结果并不一致；语义丰富、归因增强、去偏的 CVR 预测以及联合建模 CTR 和 CVR 预测将是未来有希望探索的方向。

Conclusion: 本次综述预计将为该领域未来的研究人员和从业人员提供有价值的参考和见解。

Abstract: Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area.

</details>


### [154] [Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives](https://arxiv.org/abs/2512.01179)
*Shan Gao,Yanwu Yang*

Main category: cs.IR

TL;DR: 本文设计了一个统一的CTR预测基准平台（Bench-CTR），该平台提供灵活的接口，可以连接各种数据集和CTR预测模型的组件。


<details>
  <summary>Details</summary>
Motivation: 旨在促进模型开发和评估，并提高从业者对CTR预测领域模型底层机制的理解。

Method: 构建了一个全面的评估协议系统，包括真实和合成数据集、指标分类、标准化程序和实验指南，用于校准CTR预测模型的性能。同时，对从传统多元统计到现代大型语言模型（LLM）的各种最先进模型进行了比较研究。

Result: 实验结果表明：（1）高阶模型在很大程度上优于低阶模型，尽管这种优势因指标和数据集而异；（2）基于LLM的模型表现出卓越的数据效率，即仅使用2%的训练数据即可达到与其他模型相当的性能；（3）CTR预测模型的性能从2015年到2016年取得了显著进步，然后进入了一个进展缓慢的阶段，这在各种数据集中是一致的。

Conclusion: 该基准平台有望促进模型开发和评估，并增强从业者对CTR预测领域模型底层机制的理解。

Abstract: This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR.

</details>


### [155] [Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation](https://arxiv.org/abs/2512.01372)
*Wei Yang,Rui Zhong,Yiqun Chen,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种用于频率感知多模态推荐的结构化频谱推理 (SSR) 框架，以解决模态特定噪声、语义不一致和用户-项目图上的不稳定传播问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推荐方法容易受到模态特定噪声、语义不一致和用户-项目图上的不稳定传播的影响，并且泛化能力差，鲁棒性低。

Method: 该方法包含四个阶段：(i) 通过图引导变换将基于图的多模态信号分解为频谱带；(ii) 使用频谱带掩蔽来调节频带级别的可靠性；(iii) 使用具有低秩跨频带交互的超谱推理融合互补频率线索；(iv) 通过对比正则化对齐模态特定的频谱特征。

Result: 在三个真实世界基准数据集上的实验表明，该方法相对于强大的基线方法始终能够取得更好的结果，尤其是在稀疏和冷启动设置下。

Conclusion: 结构化频谱建模提高了鲁棒性，并提供了关于不同频带如何有助于性能的更清晰的诊断。

Abstract: Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [156] [Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification](https://arxiv.org/abs/2512.00163)
*Saeed AlMarri,Mathieu Ravaut,Kristof Juhasz,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.LG

TL;DR: LLMs are being explored for classification tasks but their reliability with structured tabular data, especially in finance, is uncertain.


<details>
  <summary>Details</summary>
Motivation: The study evaluates the reliability of LLMs in financial classification tasks, an area where accuracy and explainability are critical.

Method: The study systematically evaluates LLMs and generates their SHAP values on financial classification tasks and compares them to LightGBM SHAP values.

Result: LLMs' self-explanations diverge from their SHAP values, and there are differences between LLM and LightGBM SHAP values.

Conclusion: LLMs have limitations as standalone classifiers for structured financial modeling, but improvements in explainability and few-shot prompting may make them viable in risk-sensitive domains.

Abstract: Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.

</details>


### [157] [Faster Verified Explanations for Neural Networks](https://arxiv.org/abs/2512.00164)
*Alessandro De Palma,Greta Dolcetti,Caterina Urban*

Main category: cs.LG

TL;DR: 本文提出了一种新的算法FaVeX，用于计算经过验证的解释，并提出了验证器最优鲁棒解释的分层定义。


<details>
  <summary>Details</summary>
Motivation: 解释神经网络的决策过程在理论上很重要，但现有的验证解释技术面临可扩展性挑战，因为它们需要多次调用神经网络验证器，每次调用都具有指数级的最坏情况复杂性。

Method: FaVeX通过动态结合输入特征的批处理和顺序处理，以及重用先前查询中的信息来加速计算。此外，还提出了一种新的和分层的验证解释定义，称为验证器最优鲁棒解释，它在解释中明确地考虑了网络验证器的不完整性。

Result: 实验结果表明，FaVeX和验证器最优鲁棒解释都具有 superior 的可扩展性，它们可以在具有数十万个非线性激活的网络上产生有意义的形式解释。

Conclusion: FaVeX算法和验证器最优鲁棒解释的结合可以为复杂的神经网络提供有效的形式解释。

Abstract: Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to neural network verifiers, each of them with an exponential worst-case complexity. We present FaVeX, a novel algorithm to compute verified explanations. FaVeX accelerates the computation by dynamically combining batch and sequential processing of input features, and by reusing information from previous queries, both when proving invariances with respect to certain input features, and when searching for feature assignments altering the prediction. Furthermore, we present a novel and hierarchical definition of verified explanations, termed verifier-optimal robust explanations, that explicitly factors the incompleteness of network verifiers within the explanation. Our comprehensive experimental evaluation demonstrates the superior scalability of both FaVeX, and of verifier-optimal robust explanations, which together can produce meaningful formal explanation on networks with hundreds of thousands of non-linear activations.

</details>


### [158] [We Still Don't Understand High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2512.00170)
*Colin Doumont,Donney Fan,Natalie Maus,Jacob R. Gardner,Henry Moss,Geoff Pleiss*

Main category: cs.LG

TL;DR: 高维空间中的贝叶斯优化面临挑战，但简单的贝叶斯线性回归方法，在经过几何变换后，可以与现有最优方法相媲美。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯优化方法在高维空间中面临维度灾难的挑战。

Method: 采用经过几何变换的、具有线性核的高斯过程进行贝叶斯优化。

Result: 该方法在60到6000维的搜索空间中，与现有最优方法性能相当，并且在超过20000个观测值的分子优化任务中表现出色。

Conclusion: 在高维空间中，过去的贝叶斯优化方法可能需要重新评估。

Abstract: High-dimensional spaces have challenged Bayesian optimization (BO). Existing methods aim to overcome this so-called curse of dimensionality by carefully encoding structural assumptions, from locality to sparsity to smoothness, into the optimization procedure. Surprisingly, we demonstrate that these approaches are outperformed by arguably the simplest method imaginable: Bayesian linear regression. After applying a geometric transformation to avoid boundary-seeking behavior, Gaussian processes with linear kernels match state-of-the-art performance on tasks with 60- to 6,000-dimensional search spaces. Linear models offer numerous advantages over their non-parametric counterparts: they afford closed-form sampling and their computation scales linearly with data, a fact we exploit on molecular optimization tasks with > 20,000 observations. Coupled with empirical analyses, our results suggest the need to depart from past intuitions about BO methods in high-dimensional spaces.

</details>


### [159] [Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning](https://arxiv.org/abs/2512.00181)
*Mohamed Bouadi,Pratinav Seth,Aditya Tanna,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: Orion-Bix is a tabular foundation model using biaxial attention and meta-learning for few-shot tabular learning.


<details>
  <summary>Details</summary>
Motivation: Building general-purpose models for tabular data is difficult due to mixed data types, weak feature structure, and limited labeled data.

Method: The model combines biaxial attention with meta-learned in-context reasoning. Its encoder alternates different attention mechanisms and uses multi-CLS summarization. A label-aware ICL head adapts on the fly.

Result: Orion-Bix outperforms gradient-boosting baselines and is competitive with state-of-the-art tabular foundation models.

Conclusion: Biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning for tabular data.

Abstract: Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orion-Bix, a tabular foundation model that combines biaxial attention with meta-learned in-context reasoning for few-shot tabular learning. Its encoder alternates standard, grouped, hierarchical, and relational attention, fusing their outputs through multi-CLS summarization to capture both local and global dependencies efficiently. A label-aware ICL head adapts on the fly and scales to large label spaces via hierarchical decision routing. Meta-trained on synthetically generated, structurally diverse tables with causal priors, Orion-Bix learns transferable inductive biases across heterogeneous data. Delivered as a scikit-learn compatible foundation model, it outperforms gradient-boosting baselines and remains competitive with state-of-the-art tabular foundation models on public benchmarks, showing that biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-BiX .

</details>


### [160] [Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation](https://arxiv.org/abs/2512.00191)
*Jose Luis Lima de Jesus Silva,Joao Pedro Gomes,Paulo Roberto de Melo Barros Junior,Vitor Hugo Serravalle Reis Rodrigues,Alexsandro Guerra Cerqueira*

Main category: cs.LG

TL;DR: 提出了一种混合框架，结合了改进的U-Net和空间聚类，以提高地震层位解释的连续性和几何保真度。


<details>
  <summary>Details</summary>
Motivation: 在油气勘探中，解释地震层位对于表征地下结构至关重要。深度学习的最新进展，特别是基于U-Net的架构，显着改进了自动层位追踪。然而，准确分割复杂地质特征和从稀疏注释中插值层位仍然存在挑战。

Method: 提出了Context Fusion Attention (CFA) U-Net，它融合了空间和Sobel导出的几何特征在注意力门内，以提高精度和表面完整性。使用DBSCAN细化合并的层位预测。

Result: 在Mexilhao油田数据集上，验证IoU达到0.881，MAE为2.49ms，在北海F3区块数据集上，稀疏条件下的表面覆盖率为97.6%。

Conclusion: 证明了混合方法和基于注意力的架构（通过几何上下文增强）的优势，为结构复杂和数据稀缺环境中的地震解释提供了稳健和通用的解决方案。

Abstract: Interpreting seismic horizons is a critical task for characterizing subsurface structures in hydrocarbon exploration. Recent advances in deep learning, particularly U-Net-based architectures, have significantly improved automated horizon tracking. However, challenges remain in accurately segmenting complex geological features and interpolating horizons from sparse annotations. To address these issues, a hybrid framework is presented that integrates advanced U-Net variants with spatial clustering to enhance horizon continuity and geometric fidelity. The core contribution is the Context Fusion Attention (CFA) U-Net, a novel architecture that fuses spatial and Sobel-derived geometric features within attention gates to improve both precision and surface completeness. The performance of five architectures, the U-Net (Standard and compressed), U-Net++, Attention U-Net, and CFA U-Net, was systematically evaluated across various data sparsity regimes (10-, 20-, and 40-line spacing). This approach outperformed existing baselines, achieving state-of-the-art results on the Mexilhao field (Santos Basin, Brazil) dataset with a validation IoU of 0.881 and MAE of 2.49ms, and excellent surface coverage of 97.6% on the F3 Block of the North Sea dataset under sparse conditions. The framework further refines merged horizon predictions (inline and cross-line) using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to produce geologically plausible surfaces. The results demonstrate the advantages of hybrid methodologies and attention-based architectures enhanced with geometric context, providing a robust and generalizable solution for seismic interpretation in structurally complex and data-scarce environments.

</details>


### [161] [Emergent Riemannian geometry over learning discrete computations on continuous manifolds](https://arxiv.org/abs/2512.00196)
*Julian Brandon,Angus Chadwick,Arthur Pellegrino*

Main category: cs.LG

TL;DR: 本文研究了神经网络如何学习在连续数据流形上执行离散计算。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何在连续输入数据上执行离散计算，例如将图像映射到类别标签。

Method: 通过分析神经网络各层的黎曼 pullback 度量，将网络计算分解为两个功能：离散化连续输入特征和对这些离散化变量执行逻辑运算。

Result: 发现网络计算可以分解为离散化连续输入特征和对这些离散化变量执行逻辑运算，并且不同的学习机制（rich vs. lazy）具有对比的度量和曲率结构，影响网络泛化到未见输入的能力。

Conclusion: 本文提供了一个几何框架，用于理解神经网络如何学习在连续流形上执行离散计算。

Abstract: Many tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds.

</details>


### [162] [Constructing Efficient Fact-Storing MLPs for Transformers](https://arxiv.org/abs/2512.00207)
*Owen Dugan,Roberto Garcia,Ronny Junkins,Jerry Liu,Dylan Zinsley,Sabri Eyuboglu,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: 本文介绍了一种MLP构建框架，该框架改进了以前的结构，适用于所有可行的输入-输出对，实现了渐近最优的参数效率，并在Transformer中保持了事实回忆的可用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的成功部分归功于它们能够有效地将事实知识作为键-值映射存储在其MLP参数中。最近的工作提出了显式的权重构造来构建这种事实存储MLP，从而提高了对LLM事实存储机制的理解。

Method: 本文介绍了一种MLP构建框架。

Result: 本文发现了值嵌入上的一种度量，该度量表征了构造的和梯度下降训练的MLP的事实-参数缩放比例；确定了一种简单的编码器-解码器机制，该机制在经验上与梯度下降MLP事实-参数渐近线相匹配；揭示了MLP的事实存储容量与其在Transformer中的可用性之间的基本权衡。

Conclusion: 本文论证了事实存储MLP的概念验证应用：通过一次替换整个MLP来对单层Transformer进行模块化事实编辑。

Abstract: The success of large language models (LLMs) can be attributed in part to their ability to efficiently store factual knowledge as key-value mappings within their MLP parameters. Recent work has proposed explicit weight constructions to build such fact-storing MLPs, providing an improved understanding of LLM fact storage mechanisms. In this paper, we introduce an MLP construction framework that improves over previous constructions in three areas: it 1) works for all but a measure-zero set of feasible input-output pairs, 2) achieves asymptotically optimal parameter efficiency matching information-theoretic bounds for some embeddings, and 3) maintains usability within Transformers for factual recall. Through our improvements, we 1) discover a metric on value embeddings that characterizes facts-per-parameter scaling for both constructed and gradient-descent-trained MLPs, 2) identify a simple encoder-decoder mechanism that empirically matches gradient-descent MLP facts-per-parameter asymptotics across all the inputs and outputs we test, and 3) uncover a fundamental tradeoff between an MLP's fact-storage capacity and its usability within Transformers. Finally, we demonstrate a proof-of-concept application of fact-storing MLPs: modular fact editing on one-layer Transformers by \textit{replacing entire MLPs at once}.

</details>


### [163] [TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection](https://arxiv.org/abs/2512.00229)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Main category: cs.LG

TL;DR: 提出了一种名为TIE的框架，用于解决预测不确定性和检测分布外(OOD)样本的问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在识别超出其训练经验的输入时表现不佳，导致预测不可靠且过度自信。因此，构建可靠的机器学习系统需要能够以统一的方式估计预测不确定性和检测分布外(OOD)样本的方法。

Method: 该方法通过迭代细化，扩展了一个标准的n类分类器到一个(n+1)类模型，通过引入一个用高斯噪声初始化的垃圾类来表示异常值输入。在每个epoch中，执行训练、反演和排除的闭环过程，其中从刚刚训练的分类器重建的高度不确定的反演样本被排除到垃圾类中。

Result: 在MNIST或FashionMNIST上训练并在不同的未见数据集上测试时，实现了接近完美的OOD检测。

Conclusion: TIE提供了一个统一且可解释的框架，用于稳健的异常检测和校准的不确定性估计(UE)。

Abstract: Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \textit{uncertainty} and detect \textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \textit{AUROC}, \textit{AUPR}, and \textit{FPR@95\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \textbf{\(\!\approx\!\) 0 FPR@95\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.

</details>


### [164] [Self-Supervised Dynamical System Representations for Physiological Time-Series](https://arxiv.org/abs/2512.00239)
*Yenho Chen,Maxwell A. Xu,James M. Rehg,Christopher J. Rozell*

Main category: cs.LG

TL;DR: 提出 PULSE，一种基于交叉重建的生理时间序列预训练目标，显式提取系统信息并丢弃不可转移的样本特定信息。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习 (SSL) 策略依赖于启发式原则或约束不佳的生成任务，存在局限性。

Method: 提出一个预训练框架，该框架利用跨多个时间序列的动态系统生成模型的信息结构。该框架提取与跨相似时间序列样本共享的系统参数相关的生成变量的信息，同时丢弃特定于单个样本的噪声。

Result: 通过合成动态系统实验验证了该理论，并将其应用于各种真实世界的数据集，证明 PULSE 学习的表示可以广泛区分语义类，提高标签效率并改善迁移学习。

Conclusion: PULSE 是一种有效的生理时间序列预训练方法，它能够提取系统信息并提高各种下游任务的性能。

Abstract: The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.

</details>


### [165] [Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves](https://arxiv.org/abs/2512.00242)
*Alessio Borgi,Fabrizio Silvestri,Pietro Liò*

Main category: cs.LG

TL;DR: 提出了Polynomial Neural Sheaf Diffusion (PolyNSD)，一种新的sheaf扩散方法，通过在谱重缩放算子上进行稳定的三项递归计算，其传播算子是归一化sheaf Laplacian中的K次多项式。


<details>
  <summary>Details</summary>
Motivation: 现有的神经Sheaf扩散实现依赖于基于SVD的sheaf归一化和密集的per-edge restriction maps，这些方法随stalk维度扩展，需要频繁的Laplacian重建，并产生脆弱的梯度。

Method: PolyNSD的传播算子是归一化sheaf Laplacian中的K次多项式，通过谱重缩放算子上的稳定三项递归计算进行评估。它在单层中提供了一个显式的K-hop感受野（独立于stalk维度），具有可训练的谱响应，该响应作为K+1个正交多项式基响应的凸混合获得。

Result: PolyNSD通过凸混合、谱重缩放和残差/门控路径来强制稳定性，在同质和异质基准测试中都达到了新的最先进的结果，通过仅使用对角restriction maps获得这些结果，从而扭转了神经Sheaf扩散的趋势，将性能与大的stalk维度解耦，同时减少了运行时间和内存需求。

Conclusion: PolyNSD是一种有效的神经Sheaf扩散方法，它通过多项式逼近和谱技术克服了现有方法的局限性，并在各种图学习任务中实现了最先进的性能。

Abstract: Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing. However, common Neural Sheaf Diffusion implementations rely on SVD-based sheaf normalization and dense per-edge restriction maps, which scale with stalk dimension, require frequent Laplacian rebuilds, and yield brittle gradients. To address these limitations, we introduce Polynomial Neural Sheaf Diffusion (PolyNSD), a new sheaf diffusion approach whose propagation operator is a degree-K polynomial in a normalised sheaf Laplacian, evaluated via a stable three-term recurrence on a spectrally rescaled operator. This provides an explicit K-hop receptive field in a single layer (independently of the stalk dimension), with a trainable spectral response obtained as a convex mixture of K+1 orthogonal polynomial basis responses. PolyNSD enforces stability via convex mixtures, spectral rescaling, and residual/gated paths, reaching new state-of-the-art results on both homophilic and heterophilic benchmarks, inverting the Neural Sheaf Diffusion trend by obtaining these results with just diagonal restriction maps, decoupling performance from large stalk dimension, while reducing runtime and memory requirements.

</details>


### [166] [A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations](https://arxiv.org/abs/2512.00249)
*Scotty Black,Christian Darken*

Main category: cs.LG

TL;DR: 提出了一种新颖的层级混合人工智能（AI）方法，该方法结合了脚本代理的可靠性和可预测性以及强化学习（RL）的动态自适应学习能力。


<details>
  <summary>Details</summary>
Motivation: 在兵棋推演的作战模拟领域，智能agent的开发主要以基于规则的脚本方法为主，深度强化学习（RL）方法是最近才引入的。脚本代理在受控环境中提供可预测性和一致性，但在动态、复杂的场景中，由于其固有的不灵活性而显得不足。反之，RL agent擅长适应和学习，在处理无法预见的情况下提供了潜在的改进，但同时也面临着重大的挑战，如黑盒决策过程和在更大规模的模拟环境中存在可扩展性问题。

Method: 通过分层构建AI系统，该方法旨在利用脚本代理进行日常的战术层面决策，利用RL agent进行更高层次的战略决策，从而扬长避短。

Result: 这种集成显著提高了整体性能，为在复杂模拟环境中开发和训练智能agent提供了一个强大、适应性强和有效的解决方案。

Conclusion: 论文章节

Abstract: In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.

</details>


### [167] [Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking](https://arxiv.org/abs/2512.00724)
*Lingling Fu*

Main category: cs.LG

TL;DR: 提出了一种upcycle和merge MoE奖励建模方法，以提高RLHF奖励模型的鲁棒性和效率，并减轻奖励操纵。


<details>
  <summary>Details</summary>
Motivation: 传统的奖励模型容易出现奖励操纵或过度优化，而MoE奖励模型虽然可以增强判别能力，但计算开销很大。

Method: 首先将密集奖励模型升级为MoE架构，然后应用路由权重归一化，并通过可学习的权重平均机制将专家合并回密集模型。

Result: 实验结果表明，该方法有效地减轻了各种模型规模的奖励操纵。

Conclusion: 该工作强调了upcycle和merge MoE结构在提高RLHF奖励模型的鲁棒性和效率方面的潜力。

Abstract: Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.

</details>


### [168] [SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks](https://arxiv.org/abs/2512.00251)
*Henry Onyeka,Emmanuel Samson,Liang Hong,Tariqul Islam,Imtiaz Ahmed,Kamrul Hasan*

Main category: cs.LG

TL;DR: 提出了一种名为 SD-CGAN 的新型异常检测框架，专为物联网边缘环境设计，能有效应对复杂的网络攻击。


<details>
  <summary>Details</summary>
Motivation: 物联网边缘网络日益复杂，对异常检测提出了挑战，尤其是在识别 DoS 攻击和零日漏洞利用方面。

Method: 该方法提出了一种条件生成对抗网络框架，通过 Sinkhorn 散度进行增强，并结合 CTGAN 的合成数据增强来解决类别不平衡问题。

Result: SD-CGAN 在检测精度、精确率、召回率和 F1 值方面均优于基线深度学习和基于 GAN 的方法，同时保持了计算效率。

Conclusion: SD-CGAN 适用于部署在支持边缘计算的物联网环境中，能够实现高效的异常检测。

Abstract: The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.

</details>


### [169] [Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations](https://arxiv.org/abs/2512.00768)
*Hamed Razavi*

Main category: cs.LG

TL;DR: 本研究旨在通过自然语言处理方法，从医疗聊天机器人对话中提取有意义的临床模式，以更好地理解数字健康系统如何解释和表示患者报告的症状。


<details>
  <summary>Details</summary>
Motivation: 随着数字健康系统的快速发展，需要更好地理解它们如何解释和表示患者报告的症状。聊天机器人已被用于医疗保健，以提供临床支持和增强用户体验。

Method: 该研究利用多种自然语言处理方法，包括LDA主题模型、K-Means聚类、Transformer-based NER和Apriori算法，分析医学对话中的症状描述模式。

Result: 分析结果表明，临床相关主题具有连贯的结构，聚类内聚力处于中等水平，并且在发烧、头痛和皮疹瘙痒等症状之间存在若干高置信度关系。

Conclusion: 研究结果支持对话式医疗数据可以作为早期症状解释的有价值的诊断信号，有助于加强决策支持，并改善用户与远程医疗技术的交互方式。

Abstract: The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.

</details>


### [170] [Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)](https://arxiv.org/abs/2512.00260)
*Y. Sungtaek Ju*

Main category: cs.LG

TL;DR: 提出了一种可扩展的概率 Kolmogorov-Arnold 网络 (KAN) 架构，名为稀疏变分 GP-KAN (SVGP-KAN)，它将稀疏变分推理与 KAN 拓扑结构集成，以实现不确定性量化，并应用于更大的科学数据集。


<details>
  <summary>Details</summary>
Motivation: 标准 KAN 缺乏概率输出，限制了其在需要不确定性量化的应用中的效用。虽然最近的 KAN 的高斯过程 (GP) 扩展解决了这个问题，但它们利用精确的推理方法，这些方法的规模与数据大小 N 成三次比例，限制了它们在较小数据集上的应用。

Method: 通过采用 M 个归纳点和解析矩匹配，该方法将计算复杂度从 O(N^3) 降低到 O(NM^2) 或样本大小的线性复杂度，从而可以将概率 KAN 应用于更大的科学数据集。此外，该方法还集成了基于排列的重要性分析，使网络能够充当结构识别框架，识别相关输入并对函数关系进行分类。

Result: 该方法实现了计算复杂度的降低，从而可以将概率 KAN 应用于更大的科学数据集。

Conclusion: 集成了基于排列的重要性分析，使网络能够充当结构识别框架，识别相关输入并对函数关系进行分类。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising alternative to Multi-Layer Perceptron (MLP) by placing learnable univariate functions on network edges, enhancing interpretability. However, standard KANs lack probabilistic outputs, limiting their utility in applications requiring uncertainty quantification. While recent Gaussian Process (GP) extensions to KANs address this, they utilize exact inference methods that scale cubically with data size N, restricting their application to smaller datasets. We introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that integrates sparse variational inference with the KAN topology. By employing $M$ inducing points and analytic moment matching, our method reduces computational complexity from $O(N^3)$ to $O(NM^2)$ or linear in sample size, enabling the application of probabilistic KANs to larger scientific datasets. Furthermore, we demonstrate that integrating a permutation-based importance analysis enables the network to function as a framework for structural identification, identifying relevant inputs and classifying functional relationships.

</details>


### [171] [Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning](https://arxiv.org/abs/2512.00272)
*Mohammad M Maheri,Xavier Cadet,Peter Chin,Hamed Haddadi*

Main category: cs.LG

TL;DR: 近似机器遗忘旨在有效地从训练模型中移除特定数据点的影响，但会引入隐私风险。攻击者可以利用遗忘前后的模型差异进行成员推断或数据重建。


<details>
  <summary>Details</summary>
Motivation: 现有的近似机器遗忘方法存在隐私漏洞，因为遗忘样本的梯度范数较大，且遗忘后的参数与原始模型过于接近。

Method: 提出了WARP，一种利用神经网络对称性的即插即用式传送防御方法，以减少遗忘样本的梯度能量并增加参数离散度。

Result: WARP在六种遗忘算法中实现了持续的隐私提升，在黑盒设置中将对抗优势（AUC）降低了高达64%，在白盒设置中降低了高达92%，同时保持了保留数据的准确性。

Conclusion: 传送是一种降低近似遗忘中攻击成功率的通用工具。

Abstract: Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to pre- and post-unlearning models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forget-set samples and the close proximity of unlearned parameters to the original model. To demonstrate their severity, we propose unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (e.g., NGP, SCRUB) remain vulnerable. To mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce forget-set gradient energy and increase parameter dispersion while preserving predictions. This reparameterization obfuscates the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or recover them via reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage (AUC) by up to 64% in black-box and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for reducing attack success in approximate unlearning.

</details>


### [172] [BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models](https://arxiv.org/abs/2512.00283)
*Yi Fang,Haoran Xu,Jiaxin Han,Sirui Ding,Yizhi Wang,Yue Wang,Xuan Wang*

Main category: cs.LG

TL;DR: 提出了BioArc，一个用于生物基础模型的自动架构发现框架。


<details>
  <summary>Details</summary>
Motivation: 现有工作直接采用通用机器学习领域的基础模型架构，没有系统地考虑每种生物数据模态的独特的物理化学和结构特性，导致次优性能。

Method: 利用神经架构搜索（NAS），系统地探索了一个巨大的架构设计空间，评估跨多个生物模态的架构，同时严格分析架构、标记化和训练策略之间的相互作用。

Result: 识别出新的、高性能的架构，从而能够提炼出一套经验设计原则，以指导未来的模型开发。

Conclusion: 提供了一个基础资源和一个有原则的方法，以指导下一代生物任务特定和基础模型的创建。

Abstract: Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.

</details>


### [173] [Data-Driven Modeling and Correction of Vehicle Dynamics](https://arxiv.org/abs/2512.00289)
*Nguyen Ly,Caroline Tatsuoka,Jai Nagaraj,Jacob Levy,Fernando Palafox,David Fridovich-Keil,Hannah Lu*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动框架，用于学习和校正非自治车辆动力学。


<details>
  <summary>Details</summary>
Motivation: 物理车辆模型通常为了易于处理而被简化，因此存在固有的模型形式不确定性，这促使人们需要进行数据驱动的校正。此外，非自治动力学由时间相关的控制输入控制，这给直接从时间快照数据中学习预测模型带来了挑战。

Method: 该方法通过时间相关输入的局部参数化来重新构建车辆动力学，从而产生一个由一系列局部参数化动力学系统组成的修改系统。我们使用两种互补的方法来近似这些参数化系统。首先，我们采用 DRIPS（参数空间中的降维和插值）方法来构建有效的线性替代模型，该模型配备了提升的可观察空间和基于流形的算子插值。这使得能够对车辆模型进行数据高效学习，其动力学在提升空间中允许精确的线性表示。其次，对于更强的非线性系统，我们采用 FML（流图学习），这是一种深度神经网络方法，它近似于参数化演化图，而无需特殊处理非线性。我们进一步扩展了 FML，采用了一种基于迁移学习的模型校正程序，从而能够仅使用一组稀疏的高保真或实验测量值来校正错误指定的先验模型，而无需假设校正项的规定形式。

Result: 通过对单轮车、简化自行车和基于滑移的自行车模型进行的一系列数值实验，我们证明了 DRIPS 提供了对非自治车辆动力学的稳健且高度数据高效的学习，而 FML 提供了富有表现力的非线性建模和在严重数据稀缺的情况下对模型形式误差的有效校正。

Conclusion: DRIPS 提供了对非自治车辆动力学的稳健且高度数据高效的学习，而 FML 提供了富有表现力的非线性建模和在严重数据稀缺的情况下对模型形式误差的有效校正。

Abstract: We develop a data-driven framework for learning and correcting non-autonomous vehicle dynamics. Physics-based vehicle models are often simplified for tractability and therefore exhibit inherent model-form uncertainty, motivating the need for data-driven correction. Moreover, non-autonomous dynamics are governed by time-dependent control inputs, which pose challenges in learning predictive models directly from temporal snapshot data. To address these, we reformulate the vehicle dynamics via a local parameterization of the time-dependent inputs, yielding a modified system composed of a sequence of local parametric dynamical systems. We approximate these parametric systems using two complementary approaches. First, we employ the DRIPS (dimension reduction and interpolation in parameter space) methodology to construct efficient linear surrogate models, equipped with lifted observable spaces and manifold-based operator interpolation. This enables data-efficient learning of vehicle models whose dynamics admit accurate linear representations in the lifted spaces. Second, for more strongly nonlinear systems, we employ FML (Flow Map Learning), a deep neural network approach that approximates the parametric evolution map without requiring special treatment of nonlinearities. We further extend FML with a transfer-learning-based model correction procedure, enabling the correction of misspecified prior models using only a sparse set of high-fidelity or experimental measurements, without assuming a prescribed form for the correction term. Through a suite of numerical experiments on unicycle, simplified bicycle, and slip-based bicycle models, we demonstrate that DRIPS offers robust and highly data-efficient learning of non-autonomous vehicle dynamics, while FML provides expressive nonlinear modeling and effective correction of model-form errors under severe data scarcity.

</details>


### [174] [FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting](https://arxiv.org/abs/2512.00293)
*Yafei Lyu,Hao Zhou,Lu Zhang,Xu Yang,Zhiyong Liu*

Main category: cs.LG

TL;DR: FiCoTS: 利用大型语言模型增强时间序列预测，通过细到粗的框架实现跨模态交互，在token、特征和决策层面进行模态对齐和融合，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接使用LLM预测时间序列，但时间序列和文本语义差异大，LLM难以理解时间序列数据。

Method: 提出FiCoTS框架，采用LLM-as-Enhancer范式，仅用LLM编码文本，通过token级对齐、特征级交互和决策级融合，实现细到粗的跨模态交互。

Result: 在七个真实数据集上取得了state-of-the-art的效果。

Conclusion: FiCoTS框架有效提升了时间序列预测的性能，证明了LLM在增强时间序列预测方面的潜力。

Abstract: Time series forecasting is central to data analysis and web technologies. The recent success of Large Language Models (LLMs) offers significant potential for this field, especially from the cross-modality aspect. Most methods adopt an LLM-as-Predictor paradigm, using LLM as the forecasting backbone and designing modality alignment mechanisms to enable LLM to understand time series data. However, the semantic information in the two modalities of time series and text differs significantly, making it challenging for LLM to fully understand time series data. To mitigate this challenge, our work follows an LLM-as-Enhancer paradigm to fully utilize the advantage of LLM in text understanding, where LLM is only used to encode text modality to complement time series modality. Based on this paradigm, we propose FiCoTS, an LLM-enhanced fine-to-coarse framework for multimodal time series forecasting. Specifically, the framework facilitates progressive cross-modality interaction by three levels in a fine-to-coarse scheme: First, in the token-level modality alignment module, a dynamic heterogeneous graph is constructed to filter noise and align time series patches with text tokens; Second, in the feature-level modality interaction module, a global cross-attention mechanism is introduced to enable each time series variable to connect with relevant textual contexts; Third, in the decision-level modality fusion module, we design a gated network to adaptively fuse the results of the two modalities for robust predictions. These three modules work synergistically to let the two modalities interact comprehensively across three semantic levels, enabling textual information to effectively support temporal prediction. Extensive experiments on seven real-world benchmarks demonstrate that our model achieves state-of-the-art performance. The codes will be released publicly.

</details>


### [175] [Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains](https://arxiv.org/abs/2512.00298)
*González Trigueros Jesús Eduardo,Alonso Sánchez Alejandro,Muñoz Rivera Emilio,Peñarán Prieto Mariana Jaqueline,Mendoza González Camila Natalia*

Main category: cs.LG

TL;DR: 研究分析了大数据中异构性（“多样性”）的影响，比较了结构化（Epsilon）和非结构化（Rest-Mex，IMDB）领域中的分类策略。


<details>
  <summary>Details</summary>
Motivation: 旨在研究不同数据类型和基础设施约束下，算法选择的问题。

Method: 采用了双重方法：在Python中对数值数据进行进化和贝叶斯超参数优化（遗传算法，Optuna），以及在Apache Spark中对大量文本语料库进行分布式处理。

Result: 在高维空间中，优化的线性模型（SVM，逻辑回归）优于深度架构和梯度提升。在基于文本的领域中，分布式微调的约束导致复杂模型中的过拟合，而强大的特征工程（特别是基于Transformer的嵌入（ROBERTa）和贝叶斯目标编码）使更简单的模型能够有效地推广。

Conclusion: 本研究为基于数据性质和基础设施约束的算法选择提供了一个统一的框架。

Abstract: This study analyzes the impact of heterogeneity ("Variety") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a "complexity paradox": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.

</details>


### [176] [Gradient Inversion in Federated Reinforcement Learning](https://arxiv.org/abs/2512.00303)
*Shenghong He*

Main category: cs.LG

TL;DR: 联邦强化学习(FRL)通过梯度共享实现分布式学习最优策略，同时保护本地数据隐私。然而，FRL面临数据隐私泄露的风险，攻击者利用共享梯度来重建本地训练数据。


<details>
  <summary>Details</summary>
Motivation: 与传统的监督联邦学习相比，FRL中的成功重建不仅需要生成的数据与共享梯度匹配，还需要与环境的真实转移动态对齐(即与真实数据转移分布对齐)。

Method: 为了解决这个问题，我们提出了一种新的攻击方法，称为正则化梯度反演攻击(RGIA)，它在优化过程中对状态、奖励和转移动态强制执行基于先验知识的正则化，以确保重建的数据保持接近真实的转移分布。

Result: 在控制任务和自动驾驶任务上的大量实验表明，RGIA可以有效地约束重建的数据转换分布，从而成功地重建本地私有数据。

Conclusion: 从理论上讲，我们证明了基于先验知识的正则化项将解空间从包含虚假解的广泛集合缩小到满足梯度匹配和真实转移动态的约束子集。

Abstract: Federated reinforcement learning (FRL) enables distributed learning of optimal policies while preserving local data privacy through gradient sharing.However, FRL faces the risk of data privacy leaks, where attackers exploit shared gradients to reconstruct local training data.Compared to traditional supervised federated learning, successful reconstruction in FRL requires the generated data not only to match the shared gradients but also to align with real transition dynamics of the environment (i.e., aligning with the real data transition distribution).To address this issue, we propose a novel attack method called Regularization Gradient Inversion Attack (RGIA), which enforces prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process to ensure that the reconstructed data remain close to the true transition distribution.Theoretically, we prove that the prior-knowledge-based regularization term narrows the solution space from a broad set containing spurious solutions to a constrained subset that satisfies both gradient matching and true transition dynamics.Extensive experiments on control tasks and autonomous driving tasks demonstrate that RGIA can effectively constrain reconstructed data transition distributions and thus successfully reconstruct local private data.

</details>


### [177] [Adversarial Signed Graph Learning with Differential Privacy](https://arxiv.org/abs/2512.00307)
*Haobin Ke,Sen Zhang,Qingqing Ye,Xun Ran,Haibo Hu*

Main category: cs.LG

TL;DR: 提出了一种新的保护隐私的对抗性符号图学习方法（ASGL），该方法在实现节点级差分隐私的同时，保持了高实用性。


<details>
  <summary>Details</summary>
Motivation: 在敏感的符号图上进行训练会引起严重的隐私问题，因为模型参数可能会泄露私有链接信息。现有的差分隐私（DP）保护方法通常依赖于边或梯度扰动来进行无符号图保护，但它们不太适合于符号图。

Method: ASGL方法首先将符号图分解为正子图和负子图，然后设计一个梯度扰动对抗模块来逼近真实的符号连通性分布。此外，还设计了一种约束广度优先搜索树策略，该策略与平衡理论相结合，以识别生成的节点对之间的边符号。

Result: 在真实世界数据集上的大量实验表明，ASGL在多个下游任务中实现了良好的隐私-效用折衷。

Conclusion: ASGL方法通过梯度扰动减轻了级联误差，而子图分离有助于降低敏感性。约束广度优先搜索树策略能够进行梯度解耦，从而有效地降低了梯度敏感性。

Abstract: Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.

</details>


### [178] [Tracing Mathematical Proficiency Through Problem-Solving Processes](https://arxiv.org/abs/2512.00311)
*Jungyang Park,Suho Kang,Jaewoo Park,Jaehong Kim,Jaewoo Shin,Seonjoon Park,Youngjae Yu*

Main category: cs.LG

TL;DR: 提出了一种新的知识追踪方法，该方法利用学生的问题解决过程来捕捉数学能力的多维方面。


<details>
  <summary>Details</summary>
Motivation: 传统的知识追踪方法在可解释性方面存在根本局限性，因为它们仅依赖于回答的正确性，而忽略了学生解决问题过程中嵌入的丰富信息。

Method: 提出知识追踪利用问题解决过程（KT-PSP），并引入了KT-PSP-25数据集。提出了StatusKT，一个KT框架，它采用教师-学生-教师三阶段LLM管道来提取学生的MP作为中间信号。在这个管道中，教师LLM首先提取特定于问题的熟练程度指标，然后学生LLM基于学生的解决方案过程生成响应，并且教师LLM评估这些响应以确定对每个指标的掌握程度。

Result: 在KT-PSP-25上的实验结果表明，StatusKT提高了现有KT方法的预测性能。

Conclusion: StatusKT通过显式建模学生的数学能力，为其预测提供可解释的解释。

Abstract: Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.

</details>


### [179] [Introducing AI-Driven IoT Energy Management Framework](https://arxiv.org/abs/2512.00321)
*Shivani Mruthyunjaya,Anandi Dutta,Kazi Sifatul Islam*

Main category: cs.LG

TL;DR: 提出一个物联网系统框架，以降低功耗并提高电网稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于对技术进步的持续依赖，功耗已成为现代生活的一个关键方面。降低功耗或遵循功耗预测可以降低每月成本并提高电气可靠性。

Method: 通过应用框架的各个方面来展示该提案的可行性。该系统将具有长期预测、短期预测、异常检测，并考虑在任何能源管理决策中使用的定性数据。

Result: 在功耗时间序列数据上评估了性能，以显示框架的直接应用。

Conclusion: 一个具有准确性和互连发展的物联网系统的结构化过程将支持降低功耗并支持电网稳定性。

Abstract: Power consumption has become a critical aspect of modern life due to the consistent reliance on technological advancements. Reducing power consumption or following power usage predictions can lead to lower monthly costs and improved electrical reliability. The proposal of a holistic framework to establish a foundation for IoT systems with a focus on contextual decision making, proactive adaptation, and scalable structure. A structured process for IoT systems with accuracy and interconnected development would support reducing power consumption and support grid stability. This study presents the feasibility of this proposal through the application of each aspect of the framework. This system would have long term forecasting, short term forecasting, anomaly detection, and consideration of qualitative data with any energy management decisions taken. Performance was evaluated on Power Consumption Time Series data to display the direct application of the framework.

</details>


### [180] [Adaptive prediction theory combining offline and online learning](https://arxiv.org/abs/2512.00342)
*Haizheng Li,Lei Guo*

Main category: cs.LG

TL;DR: 研究了离线学习和在线适应相结合的两阶段学习框架的预测性能，该框架适用于一类非线性随机动态系统。


<details>
  <summary>Details</summary>
Motivation: 现实世界的智能系统通常通过结合离线学习和在线适应来运行，但现有文献中很少对其进行理论研究。

Method: 利用Kullback-Leibler散度来量化分布差异，建立了在强相关和分布偏移的一般数据集下，近似非线性最小二乘估计的泛化误差上界。提出了基于离线训练模型的元LMS预测算法，以解决实际目标系统中可能存在的不确定参数漂移问题。

Result: 与纯离线或在线方法相比，集成了离线学习和在线适应的两阶段框架表现出卓越的预测性能。

Conclusion: 该论文提供了理论保证和实证研究。

Abstract: Real-world intelligence systems usually operate by combining offline learning and online adaptation with highly correlated and non-stationary system data or signals, which, however, has rarely been investigated theoretically in the literature. This paper initiates a theoretical investigation on the prediction performance of a two-stage learning framework combining offline and online algorithms for a class of nonlinear stochastic dynamical systems. For the offline-learning phase, we establish an upper bound on the generalization error for approximate nonlinear-least-squares estimation under general datasets with strong correlation and distribution shift, leveraging the Kullback-Leibler divergence to quantify the distributional discrepancies. For the online-adaptation phase, we address, on the basis of the offline-trained model, the possible uncertain parameter drift in real-world target systems by proposing a meta-LMS prediction algorithm. This two-stage framework, integrating offline learning with online adaptation, demonstrates superior prediction performances compared with either purely offline or online methods. Both theoretical guarantees and empirical studies are provided.

</details>


### [181] [Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning](https://arxiv.org/abs/2512.00351)
*Na Li,Yuchen Jiao,Hangguan Shan,Shefeng Yan*

Main category: cs.LG

TL;DR: 这篇论文提出了一种用于双人零和马尔可夫博弈的无模型自博弈算法，称为 Memory-Efficient Nash Q-Learning (ME-Nash-QL)。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习 (MARL) 理论研究存在内存效率低、样本复杂度对长时程和大型状态空间的依赖性强、计算复杂度高、非马尔可夫策略、非纳什策略以及高预热成本等问题。

Method: 该论文设计了一种模型无关的自博弈算法 ME-Nash-QL，用于解决双人零和马尔可夫博弈问题。

Result: ME-Nash-QL 算法在空间复杂度、样本复杂度和计算复杂度方面优于现有算法，并且实现了最佳的预热成本。

Conclusion: ME-Nash-QL 算法在双人零和马尔可夫博弈中具有显著的优势，为解决多智能体强化学习问题迈出了重要一步。

Abstract: The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\widetilde{O}(H^4SAB/\varepsilon^2)$, where $S$ is the number of states, $\{A, B\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\min\{A, B\}\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\,\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\,\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.

</details>


### [182] [Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning](https://arxiv.org/abs/2512.00352)
*Na Li,Zewu Zheng,Wei Ni,Hangguan Shan,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 本文研究了离线鲁棒双人零和马尔可夫博弈（RTZMGs），提出了一种名为RTZ-VI-LCB的基于模型的算法，该算法在部分覆盖和环境不确定性下，实现了接近最优的样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）需要在共享动态环境中处理环境不确定性，策略必须保持鲁棒性以应对sim-to-real的差距。本文关注离线设置中的鲁棒双人零和马尔可夫博弈。

Method: 提出了一种名为RTZ-VI-LCB的算法，该算法是乐观鲁棒值迭代与数据驱动的Bernstein-style惩罚项相结合，用于鲁棒值估计。

Result: 该算法在部分覆盖和环境不确定性下，建立了接近最优的样本复杂度保证。开发了一个信息论下界，以确认算法样本复杂度的紧密性，该复杂度在状态和行动空间方面都是最优的。

Conclusion: RTZ-VI-LCB是第一个达到这种最优性的算法，为离线RTZMGs设置了一个新的基准，并通过实验验证了其有效性。

Abstract: Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.

</details>


### [183] [Learning Causal States Under Partial Observability and Perturbation](https://arxiv.org/abs/2512.00357)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li,Yamin Wang*

Main category: cs.LG

TL;DR: 提出了一种名为CaDiff的框架，通过结合异步扩散模型（ADM）和新的双模拟指标，来增强RL算法，以解决扰动和部分可观察马尔可夫决策过程（P^2OMDPs）中的决策问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在解决部分可观察性的同时减轻扰动。在不完整和嘈杂的观察基础上做出决策是强化学习（RL）的一个关键挑战，尤其是在扰动和部分可观察的马尔可夫决策过程（P^2OMDPs）中。

Method: 通过结合新颖的异步扩散模型（ADM）和新的双模拟指标来揭示P^2OMDPs的潜在因果结构。ADM 能够以不同数量的步骤实现前向和反向过程，从而将 P^2OMDP 的扰动解释为通过扩散抑制的噪声的一部分。双模拟指标量化了部分可观察环境与其因果对应物之间的相似性。

Result: 在Roboschool任务上的实验表明，与基线相比，CaDiff 至少提高了 14.18% 的回报。

Conclusion: CaDiff 是第一个使用具有理论严谨性和实用性的扩散模型来近似因果状态的框架。

Abstract: A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.

</details>


### [184] [S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting](https://arxiv.org/abs/2512.00366)
*Wenshuo Wang,Yaomin Shen,Yingjie Tan,Yihao Chen*

Main category: cs.LG

TL;DR: 提出了一种新的知识蒸馏框架S^2-KD，它结合了语义先验和频谱表示来进行蒸馏，以提高时空预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法主要在像素层面操作，忽略了视觉模式背后的丰富语义和因果关系。

Method: 通过训练一个特权的多模态教师模型，该模型利用大型多模态模型（LMM）的文本叙述来推断事件的根本原因，同时在潜在空间中解耦频谱分量。然后，通过新的蒸馏目标将统一的语义-频谱知识转移到轻量级的、仅视觉的学生模型中。

Result: 在WeatherBench和TaxiBJ+等基准测试中，S^2-KD显著提高了简单学生模型的性能，使其优于最先进的方法，尤其是在长程和复杂的非平稳场景中。

Conclusion: S^2-KD框架能够使学生模型做出不仅在频谱上准确，而且在语义上连贯的预测，而无需任何文本输入或推理时的架构开销。

Abstract: Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.

</details>


### [185] [An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines](https://arxiv.org/abs/2512.00383)
*Jianhai Su,Jinzhu Luo,Qi Zhang*

Main category: cs.LG

TL;DR: 将离线强化学习算法整合为从零开始的在线强化学习的子程序。


<details>
  <summary>Details</summary>
Motivation: 在线学习智能体可以将其历史交互重新用作离线数据集。

Method: 形式化为一个框架，该框架可容纳离线强化学习合并的几种变体，例如最终策略推荐和在线微调。引入便捷技术以提高其在提高在线学习效率方面的有效性。

Result: 1) 所提出框架的有效性在很大程度上取决于任务的性质，2) 我们提出的技术极大地提高了其有效性，3) 现有的在线微调方法总体上是无效的，需要更多的研究。

Conclusion: 离线强化学习算法可以有效地整合为在线强化学习的子程序，但其有效性在很大程度上取决于任务的性质。

Abstract: We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.

</details>


### [186] [Efficient and Programmable Exploration of Synthesizable Chemical Space](https://arxiv.org/abs/2512.00384)
*Shitong Luo,Connor W. Coley*

Main category: cs.LG

TL;DR: PrexSyn是一种用于在可合成化学空间中进行分子发现的高效且可编程的模型，它基于decoder-only transformer，通过大规模的合成路径和分子属性数据进行训练。


<details>
  <summary>Details</summary>
Motivation: 在可合成化学空间中采样分子，使其既具有合成可及性又具有所需性质，这是一个巨大的挑战。

Method: PrexSyn基于decoder-only transformer，并使用C++数据生成引擎生成的大规模合成路径数据进行训练。

Result: PrexSyn能够近乎完美地重建可合成化学空间，并学习属性和可合成分子之间的关联。它可以通过迭代查询优化分子，实现比非合成感知基线更高的采样效率。

Conclusion: PrexSyn通过在可合成化学空间覆盖率、分子采样效率和推理速度方面设定新的技术水平，推动了可合成分子设计的前沿。

Abstract: The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.

</details>


### [187] [Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics](https://arxiv.org/abs/2512.00389)
*Deep Patel,Emmanouil-Vasileios Vlatakis-Gkaragkounis*

Main category: cs.LG

TL;DR: 本研究致力于解释在神经网络零和博弈中，尽管目标函数非凸非凹，梯度方法却能收敛的现象，通过隐藏凸性和过度参数化的视角进行分析。


<details>
  <summary>Details</summary>
Motivation: 许多新兴应用，如对抗训练、AI对齐和鲁棒优化，可以被构建为神经网络之间的零和博弈，而冯·诺依曼-纳什均衡(NE)捕捉了理想的系统行为。虽然这些博弈通常涉及非凸非凹的目标，但经验证据表明，简单的梯度方法经常收敛，这表明存在一个隐藏的几何结构。

Method: 本研究提供了一个理论框架，通过隐藏凸性和过度参数化的视角来解释这种现象。该研究确定了充分条件，包括初始化、训练动态和网络宽度，以保证在广泛的非凸min-max博弈中全局收敛到NE。该方法主要包括：(a)推导了min-max博弈中交替梯度下降-上升方案的新路径长度界限；(b)使用随机矩阵理论的工具，证明了在过度参数化下，从隐藏的凸凹几何到双边Polyak-Łojasiewicz (PŁ) min-max条件的简化以高概率成立。

Result: 对于涉及两层神经网络的博弈，这是第一个此类结果。

Conclusion: 该研究为一类非凸min-max博弈提供了全局收敛到纳什均衡的理论保证，并揭示了隐藏的凸性和过度参数化在实现收敛中的作用。

Abstract: Many emerging applications - such as adversarial training, AI alignment, and robust optimization - can be framed as zero-sum games between neural nets, with von Neumann-Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of hidden convexity and overparameterization. We identify sufficient conditions - spanning initialization, training dynamics, and network width - that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for the alternating gradient descent-ascent scheme in min-max games; and (b) we show that the reduction from a hidden convex-concave geometry to two-sided Polyak-Łojasiewicz (PŁ) min-max condition hold with high probability under overparameterization, using tools from random matrix theory.

</details>


### [188] [From Coefficients to Directions: Rethinking Model Merging with Directional Alignment](https://arxiv.org/abs/2512.00391)
*Zhikang Chen,Sen Cui,Deheng Ye,Min Zhang,Gang Niu,Yu Zhang,Masashi Sugiyama,Tingting Zhu*

Main category: cs.LG

TL;DR: 模型合并是一种将多个独立训练的模型集成到单个模型中的实用方法，无需联合重新训练。


<details>
  <summary>Details</summary>
Motivation: 先前的方法主要将合并视为参数空间分解或融合系数优化问题，而忽略了方向信息在参数和特征空间中的关键作用。简单的合并会引入参数方向的不一致性，并破坏模型之间的结构一致性，从而降低性能。此外，基于系数的优化方法隐含地假设了模型之间兼容的特征空间方向，这使得单独的系数优化是不够的。

Method: 我们强调了方向对齐的重要性，并引入了一个统一的几何框架，即具有方向对齐的合并（Merging with Directional Alignment）。

Result: 我们的分析表明，方向对齐提高了结构一致性。

Conclusion: 在基准测试、模型规模和任务配置上的大量实验进一步验证了我们方法的有效性。

Abstract: Model merging has emerged as a practical paradigm for integrating multiple independently trained models into a single model without joint retraining. Previous studies have demonstrated the effectiveness of combining parameters through strategies such as parameter decomposition, coefficient optimization, and subspace learning, significantly reducing the need for expensive joint training and achieving strong empirical performance across diverse tasks. However, these approaches predominantly treat merging as a problem of parameter space decomposition or fusion coefficient optimization, while overlooking the critical role of directional information in both parameter and feature spaces. In practice, naïve merging introduces inconsistencies in dominant parameter directions and disrupts structural coherence across models, which can degrade performance. Moreover, coefficient-based optimization methods implicitly assume compatible feature-space directions across models. However, Neural Collapse indicates that class features follow structured directional patterns, which may differ across independently trained models, making coefficient optimization alone insufficient. In this work, we emphasize the importance of \emph{directional alignment} and introduce a unified geometric framework, \emph{Merging with Directional Alignment} (\method{}), which aligns directional structures consistently in both the parameter and feature spaces. Our analysis shows that directional alignment improves structural coherence, and extensive experiments across benchmarks, model scales, and task configurations further validate the effectiveness of our approach.

</details>


### [189] [Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement](https://arxiv.org/abs/2512.00396)
*Andrea Procopio,Marco Esposito,Sara Raggiunto,Andrey Gizdov,Alberto Belli,Paola Pierleoni*

Main category: cs.LG

TL;DR: 本研究探索了使用超轻量级可分离卷积神经网络（CNN）在资源受限的可穿戴设备上进行帕金森病（PD）步态检测，实验结果表明，所提出的模型在精度和效率上都优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备和边缘节点上，使用短窗口三轴加速度进行帕金森病(PD)步态检测的时间序列分析研究。

Method: 比较幅度阈值法和三种用于时间序列分析的一维卷积神经网络：一个文献基线（可分离卷积）和两个超轻量级模型——一个纯粹可分离，一个带有残差连接。

Result: 残差可分离模型（Model 2, 533 个参数）达到 PR-AUC = 94.5%，F1 = 91.2%，MCC = 89.4%，在参数减少约 10 倍的情况下，匹配或超过了基线（5,552 个参数；PR-AUC = 93.7%，F1 = 90.5%，MCC = 88.5%）。最小的模型（Model 1，305 个参数）达到 PR-AUC = 94.0%，F1 = 91.0%，MCC = 89.1%。

Conclusion: 超轻量级可分离卷积神经网络为可穿戴PD步态检测提供了优于固定阈值的精度-效率-泛化折衷方案，并强调了为边缘部署定制的时间序列模型的价值。

Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.

</details>


### [190] [SelfAI: Building a Self-Training AI System with LLM Agents](https://arxiv.org/abs/2512.00403)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Xiaobing Yu,Yu Zhong,Shangqi Deng,Ufaq Khan,Jianghao Wu,Xiaofeng Liu,Imran Razzak,Xiaojun Chang,Yutong Xie*

Main category: cs.LG

TL;DR: SelfAI is a multi-agent platform that improves autonomous scientific discovery by addressing limitations of existing LLM-based agents, such as narrow application domains and lack of real-time interaction. It introduces a User Agent, Cognitive Agent, and Experiment Manager for translating research objectives, refining hyperparameter searches, and orchestrating parallel training workflows. The platform also includes two new evaluation metrics, Score and AUP_D, to quantify discovery efficiency and search diversity. SelfAI achieves strong performance across various benchmarks, reduces redundant trials, and enables seamless interaction with human researchers.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents for autonomous scientific discovery have limitations in application domains, real-time interaction, and mechanisms for halting exploration, leading to inefficiencies and reproducibility challenges.

Method: The paper proposes SelfAI, a multi-agent platform with a User Agent, Cognitive Agent, and Experiment Manager. The Cognitive Agent uses LLMs with optimal stopping criteria to refine hyperparameter searches. The platform also introduces two new evaluation metrics, Score and AUP_D.

Result: SelfAI consistently achieves strong performance across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks. It also reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines.

Conclusion: SelfAI addresses the limitations of existing LLM-based agents for autonomous scientific discovery by providing a general multi-agent platform with improved efficiency, reproducibility, and human interaction. The platform's performance is validated across various benchmarks, demonstrating its effectiveness in reducing redundant trials and enabling seamless interaction with human researchers.

Abstract: Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.

</details>


### [191] [TrendGNN: Towards Understanding of Epidemics, Beliefs, and Behaviors](https://arxiv.org/abs/2512.00421)
*Mulin Tian,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 提出了一种基于图的预测框架，用于预测与信念和行为相关的信号。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解机制并预测干预措施的影响，我们需要以可解释的方式预测与信念和行为相关的信号。

Method: 首先构建一个基于趋势相似性的相互关联信号图，然后应用图神经网络 (GNN) 进行预测。

Result: 通过揭示哪些信号更可预测以及哪些关系对预测准确性贡献最大，实现可解释的分析。

Conclusion: 我们的方法为在具有多个潜在相互依赖信号的领域中进行可解释建模的框架提供了早期步骤，对构建整合行为、信念和观察的未来模拟模型具有影响。

Abstract: Epidemic outcomes have a complex interplay with human behavior and beliefs. Most of the forecasting literature has focused on the task of predicting epidemic signals using simple mechanistic models or black-box models, such as deep transformers, that ingest all available signals without offering interpretability. However, to better understand the mechanisms and predict the impact of interventions, we need the ability to forecast signals associated with beliefs and behaviors in an interpretable manner. In this work, we propose a graph-based forecasting framework that first constructs a graph of interrelated signals based on trend similarity, and then applies graph neural networks (GNNs) for prediction. This approach enables interpretable analysis by revealing which signals are more predictable and which relationships contribute most to forecasting accuracy. We believe our method provides early steps towards a framework for interpretable modeling in domains with multiple potentially interdependent signals, with implications for building future simulation models that integrate behavior, beliefs, and observations.

</details>


### [192] [Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease](https://arxiv.org/abs/2512.00434)
*Benjamin D. Ballyk,Ankit Gupta,Sujay Konda,Kavitha Subramanian,Chris Landon,Ahmed Ammar Naseer,Georg Maierhofer,Sumanth Swaminathan,Vasudevan Venkateshwaran*

Main category: cs.LG

TL;DR: 本研究提出了一种新的时间序列生成模型，该模型在处理纵向临床数据的能力上有所增强，并纳入了可量化的隐私保护措施。该模型在慢性肾病和ICU患者的真实数据上进行了评估，结果表明，该模型在不损害数据隐私的情况下，能够为开发复杂慢性疾病模型提供稳健的输入数据。


<details>
  <summary>Details</summary>
Motivation: 在现代医疗工作流程中，数据隐私是一个关键挑战。由于电子病历的迅速普及，严格的数据保护法规限制了对临床记录的访问，这阻碍了机器学习模型在提高诊断准确性和个性化护理方面的应用。合成数据提供了一个有希望的替代方案；然而，当前的生成模型要么难以处理时间序列数据，要么缺乏正式的隐私保证。

Method: 该研究增强了一种最先进的时间序列生成模型，以更好地处理纵向临床数据，同时结合可量化的隐私保护措施。具体来说，该研究提出了两种模型：Augmented TimeGAN（非私有模型）和DP-TimeGAN（私有模型）。

Result: 非私有模型（Augmented TimeGAN）在多个数据集的统计指标上优于基于Transformer和Flow的模型，而私有模型（DP-TimeGAN）在CKD数据集上保持了0.778的平均真实性，优于现有最先进的隐私-效用前沿模型。在临床医生评估中，两种模型都取得了与真实数据相当的性能。

Conclusion: 该研究提出的模型能够在不损害数据隐私的情况下，为开发复杂慢性疾病模型提供稳健的输入数据。

Abstract: Data privacy is a critical challenge in modern medical workflows as the adoption of electronic patient records has grown rapidly. Stringent data protection regulations limit access to clinical records for training and integrating machine learning models that have shown promise in improving diagnostic accuracy and personalized care outcomes. Synthetic data offers a promising alternative; however, current generative models either struggle with time-series data or lack formal privacy guaranties. In this paper, we enhance a state-of-the-art time-series generative model to better handle longitudinal clinical data while incorporating quantifiable privacy safeguards. Using real data from chronic kidney disease and ICU patients, we evaluate our method through statistical tests, a Train-on-Synthetic-Test-on-Real (TSTR) setup, and expert clinical review. Our non-private model (Augmented TimeGAN) outperforms transformer- and flow-based models on statistical metrics in several datasets, while our private model (DP-TimeGAN) maintains a mean authenticity of 0.778 on the CKD dataset, outperforming existing state-of-the-art models on the privacy-utility frontier. Both models achieve performance comparable to real data in clinician evaluations, providing robust input data necessary for developing models for complex chronic conditions without compromising data privacy.

</details>


### [193] [FairMT: Fairness for Heterogeneous Multi-Task Learning](https://arxiv.org/abs/2512.00469)
*Guanyu Hu,Tangzheng Lian,Na Yan,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan,Siyang Song,Zeyu Fu*

Main category: cs.LG

TL;DR: 提出了一种名为FairMT的统一的公平感知多任务学习框架，该框架适用于不完全监督下的所有三种任务类型。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性方法大多以分类为导向，无法扩展到连续输出，使得难以制定统一的公平性目标。现有的MTL优化在结构上与公平性不一致：仅约束共享表示，允许任务负责人吸收偏差，并导致无法控制的特定于任务的差异。大多数工作将公平性视为与效用的零和博弈，强制执行对称约束，通过降低服务良好的群体来实现均等。

Method: 一种非对称异构公平约束聚合机制，将任务相关的非对称违规合并为统一的公平约束。通过原始-对偶公式共同优化效用和公平性，而head-aware多目标优化代理提供了一个易于处理的下降几何，该几何明确考虑了头部引起的各向异性。

Result: 在包含不同模态和监督机制的三个同构和异构MTL基准测试中，FairMT始终在保持卓越的任务效用的同时，实现了巨大的公平性收益。

Conclusion: FairMT在多任务学习中实现了显著的公平性提升，同时保持了任务效用。

Abstract: Fairness in machine learning has been extensively studied in single-task settings, while fair multi-task learning (MTL), especially with heterogeneous tasks (classification, detection, regression) and partially missing labels, remains largely unexplored. Existing fairness methods are predominantly classification-oriented and fail to extend to continuous outputs, making a unified fairness objective difficult to formulate. Further, existing MTL optimization is structurally misaligned with fairness: constraining only the shared representation, allowing task heads to absorb bias and leading to uncontrolled task-specific disparities. Finally, most work treats fairness as a zero-sum trade-off with utility, enforcing symmetric constraints that achieve parity by degrading well-served groups. We introduce FairMT, a unified fairness-aware MTL framework that accommodates all three task types under incomplete supervision. At its core is an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism, which consolidates task-dependent asymmetric violations into a unified fairness constraint. Utility and fairness are jointly optimized via a primal--dual formulation, while a head-aware multi-objective optimization proxy provides a tractable descent geometry that explicitly accounts for head-induced anisotropy. Across three homogeneous and heterogeneous MTL benchmarks encompassing diverse modalities and supervision regimes, FairMT consistently achieves substantial fairness gains while maintaining superior task utility. Code will be released upon paper acceptance.

</details>


### [194] [ESPO: Entropy Importance Sampling Policy Optimization](https://arxiv.org/abs/2512.00499)
*Yuepeng Sheng,Yuwei Huang,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.LG

TL;DR: 提出了一种新的框架，可以协调细粒度控制和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于群体的策略优化框架在优化粒度和训练稳定性之间存在根本的权衡。

Method: 提出了一种新的框架，该框架将序列分解为基于预测熵的组，从而能够 (1) 熵驱动的重要性抽样来捕获序列内的异质性，以及 (2) 熵自适应剪裁以基于模型不确定性动态分配信任区域。

Result: ESPO 不仅加速了收敛，而且实现了最先进的性能，尤其是在具有挑战性的 HMMT 基准测试中，准确率从 4.4% 提高到 13.13%。

Conclusion: ESPO 是一种很有前途的 LLM 强化学习方法。

Abstract: Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.

</details>


### [195] [Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction](https://arxiv.org/abs/2512.00521)
*Sabrina Islam,Md. Atiqur Rahman,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.LG

TL;DR: 提出了Rep3Net，一个统一的深度学习架构，用于预测分子对靶蛋白的生物活性。


<details>
  <summary>Details</summary>
Motivation: 传统的QSAR模型难以有效预测分子的生物活性，因为它在捕获化合物中嵌入的结构和上下文信息方面存在局限性。

Method: 该模型不仅包含描述符数据，还包括基于图的化合物表示的空间和关系信息，以及通过 ChemBERTa 从 SMILES 字符串生成的嵌入的上下文信息。该模型采用多模态连接特征。

Result: 在Poly [ADP-ribose] polymerase 1 (PARP-1) 数据集上产生了可靠的生物活性预测。与包括GCN、GAT、XGBoost等在内的传统独立模型进行全面分析和比较，表明该架构实现了最高的预测性能。

Conclusion: 该架构为药物发现中的化合物计算筛选提供了一个可扩展的生物活性预测框架。

Abstract: In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.

</details>


### [196] [Hyperbolic Continuous Structural Entropy for Hierarchical Clustering](https://arxiv.org/abs/2512.00524)
*Guangjie Zeng,Hao Peng,Angsheng Li,Li Sun,Chunyang Liu,Shengze Li,Yicheng Pan,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种用于结构增强连续层次聚类的双曲连续结构熵神经网络（HypCSE）。


<details>
  <summary>Details</summary>
Motivation: 现有的层次聚类方法存在两个主要挑战：1) 大多数方法在没有全局目标的情况下指定树状图。2) 基于图的方法通常忽略图结构的重要性，在完整或静态的预定义图上优化目标。

Method: 该方法将数据点映射到双曲空间，并最小化结构增强图上的松弛连续结构熵（SE）。具体来说，使用双曲图神经网络在双曲空间中编码图顶点，并最小化图嵌入上定义的近似SE。为了使SE目标可微以进行优化，使用树上的最低公共祖先（LCA）将其重新表述为一个函数，然后通过双曲图嵌入和划分树的类比将其放宽为连续SE（CSE）。为了确保图结构有效地捕获数据点的层次结构以进行CSE计算，采用了一种图结构学习（GSL）策略，该策略在训练期间更新图结构。

Result: 在七个数据集上的大量实验表明了HypCSE的卓越性能。

Conclusion: HypCSE在层次聚类任务中表现出色，通过在双曲空间中嵌入数据并优化连续结构熵，有效地学习数据的层次结构。

Abstract: Hierarchical clustering is a fundamental machine-learning technique for grouping data points into dendrograms. However, existing hierarchical clustering methods encounter two primary challenges: 1) Most methods specify dendrograms without a global objective. 2) Graph-based methods often neglect the significance of graph structure, optimizing objectives on complete or static predefined graphs. In this work, we propose Hyperbolic Continuous Structural Entropy neural networks, namely HypCSE, for structure-enhanced continuous hierarchical clustering. Our key idea is to map data points in the hyperbolic space and minimize the relaxed continuous structural entropy (SE) on structure-enhanced graphs. Specifically, we encode graph vertices in hyperbolic space using hyperbolic graph neural networks and minimize approximate SE defined on graph embeddings. To make the SE objective differentiable for optimization, we reformulate it into a function using the lowest common ancestor (LCA) on trees and then relax it into continuous SE (CSE) by the analogy of hyperbolic graph embeddings and partitioning trees. To ensure a graph structure that effectively captures the hierarchy of data points for CSE calculation, we employ a graph structure learning (GSL) strategy that updates the graph structure during training. Extensive experiments on seven datasets demonstrate the superior performance of HypCSE.

</details>


### [197] [Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine](https://arxiv.org/abs/2512.00528)
*Isara Liyanage,Uthayasanker Thayasivam*

Main category: cs.LG

TL;DR: 本文旨在改进可解释的Boosting Machine (EBM)，这是一种最先进的glassbox模型，它既能提供高精度又能提供完全透明度。


<details>
  <summary>Details</summary>
Motivation: 高风险领域中复杂机器学习模型的广泛采用将“黑盒”问题带到了负责任的AI研究的前沿。

Method: 本文概述了三种不同的增强方法：使用贝叶斯方法进行有针对性的超参数优化，为超参数优化实施自定义多目标公平性函数，以及用于冷启动场景的新型自监督预训练管道。

Result: 分析表明，虽然调整过程在主要的ROC AUC指标中产生了微小的改进，但它导致了模型决策行为中微妙但重要的转变，证明了多方面评估的价值。

Conclusion: 这项工作被定位为开发不仅准确而且稳健、公平和透明的机器学习系统的关键一步，满足了监管和道德合规性日益增长的需求。

Abstract: The widespread adoption of complex machine learning models in high-stakes domains has brought the "black-box" problem to the forefront of responsible AI research. This paper aims at addressing this issue by improving the Explainable Boosting Machine (EBM), a state-of-the-art glassbox model that delivers both high accuracy and complete transparency. The paper outlines three distinct enhancement methodologies: targeted hyperparameter optimization with Bayesian methods, the implementation of a custom multi-objective function for fairness for hyperparameter optimization, and a novel self-supervised pre-training pipeline for cold-start scenarios. All three methodologies are evaluated across standard benchmark datasets, including the Adult Income, Credit Card Fraud Detection, and UCI Heart Disease datasets. The analysis indicates that while the tuning process yielded marginal improvements in the primary ROC AUC metric, it led to a subtle but important shift in the model's decision-making behavior, demonstrating the value of a multi-faceted evaluation beyond a single performance score. This work is positioned as a critical step toward developing machine learning systems that are not only accurate but also robust, equitable, and transparent, meeting the growing demands of regulatory and ethical compliance.

</details>


### [198] [Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets](https://arxiv.org/abs/2512.00536)
*Aaryan Gupta,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 本文提出了一种用于监督学习的有效数据集精馏算法，该算法通过匹配训练和合成数据集上的损失来实现，而无需任何模型训练。


<details>
  <summary>Details</summary>
Motivation: 数据集精馏旨在从训练数据集中导出一个合成数据集，使得在后者上训练的模型表现与在训练数据集上训练的模型一样好。

Method: 该算法基于匹配训练和合成数据集上的损失，使用一组固定的随机抽样的回归器，无需任何模型训练。此外，该算法还通过匹配贝尔曼损失扩展到离线RL数据集精馏。

Result: 该算法只需要$\\tilde{O}(d^2)$个抽样回归器就可以导出一个合成数据集，在该数据集上，任何有界线性模型的MSE损失与其在给定训练数据上的MSE损失几乎相同。实验结果验证了理论保证，并观察到性能提升。

Conclusion: 该研究提出了一种有效的数据集精馏算法，并提供了理论保证和实验验证。

Abstract: Given a training dataset, the goal of dataset distillation is to derive a synthetic dataset such that models trained on the latter perform as well as those trained on the training dataset. In this work, we develop and analyze an efficient dataset distillation algorithm for supervised learning, specifically regression in $\mathbb{R}^d$, based on matching the losses on the training and synthetic datasets with respect to a fixed set of randomly sampled regressors without any model training. Our first key contribution is a novel performance guarantee proving that our algorithm needs only $\tilde{O}(d^2)$ sampled regressors to derive a synthetic dataset on which the MSE loss of any bounded linear model is nearly the same as its MSE loss on the given training data. In particular, the model optimized on the synthetic data has close to minimum loss on the training data, thus performing nearly as well as the model optimized on the latter. Complementing this, we also prove a matching lower bound of $Ω(d^2)$ for the number of sampled regressors showing the tightness of our analysis.
  Our second contribution is to extend our algorithm to offline RL dataset distillation by matching the Bellman loss, unlike previous works which used a behavioral cloning objective. This is the first such method which leverages both, the rewards and the next state information, available in offline RL datasets, without any policy model optimization. Our algorithm generates a synthetic dataset whose Bellman loss with respect to any linear action-value predictor is close to the latter's Bellman loss on the offline RL training dataset. Therefore, a policy associated with an action-value predictor optimized on the synthetic dataset performs nearly as well as that derived from the one optimized on the training data. We conduct experiments to validate our theoretical guarantees and observe performance gains.

</details>


### [199] [DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning](https://arxiv.org/abs/2512.00545)
*Akrati Saxena,Harshith Kumar Yadav,Bart Rutten,Shashi Shekhar Jha*

Main category: cs.LG

TL;DR: 本文提出了一种新的公平感知的影响力最大化（IM）方法，使用强化学习（RL）来确保所有社区的公平影响力覆盖，无论其受保护属性如何。


<details>
  <summary>Details</summary>
Motivation: 现实世界的社交网络存在结构性不平等，如果设计IM算法时不考虑这些不平等，可能会使多数群体不成比例地受益，同时使少数群体边缘化。

Method: 使用强化学习（RL）设计了一种公平感知IM方法，该方法结合了Structure2Vec网络嵌入的深度Q学习，将IM问题构建为马尔可夫决策过程（MDP）。

Result: 在合成基准和真实网络上进行了大量实验，结果表明，该方法在保持比基线更好的公平性-性能权衡的同时，实现了更高的公平性水平。

Conclusion: 该方法学习了有效的种子策略，这些策略可以在不同的问题实例中推广，而无需重新训练，例如改变网络大小或种子节点数量。

Abstract: The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.

</details>


### [200] [A Graph Neural Network Approach for Localized and High-Resolution Temperature Forecasting](https://arxiv.org/abs/2512.00546)
*Joud El-Shawa,Elham Bagheri,Sedef Akinli Kocak,Yalda Mohsenzadeh*

Main category: cs.LG

TL;DR: 提出了一个用于局部、高分辨率温度预测的图神经网络框架。


<details>
  <summary>Details</summary>
Motivation: 全球热浪正在加剧，对边缘化人群和全球南方地区造成不成比例的影响，而目前的数值天气预报模型通常无法捕捉到微观尺度的极端情况，导致最脆弱的群体无法及时获得预警。

Method: 利用空间学习和高效计算，生成多个时程的预测，最长可达 48 小时。

Result: 在加拿大西南安大略省，该模型捕捉到的温度模式在 1-48 小时预测中的平均 MAE 为 1.93$^{\circ}$C，在使用 24 小时输入窗口的最大区域评估中，MAE@48 小时为 2.93$^{\circ}$C。

Conclusion: 这项工作为迁移学习方法奠定了基础，可以在数据有限的全球南方地区实现本地化的、公平的预测。

Abstract: Heatwaves are intensifying worldwide and are among the deadliest weather disasters. The burden falls disproportionately on marginalized populations and the Global South, where under-resourced health systems, exposure to urban heat islands, and the lack of adaptive infrastructure amplify risks. Yet current numerical weather prediction models often fail to capture micro-scale extremes, leaving the most vulnerable excluded from timely early warnings. We present a Graph Neural Network framework for localized, high-resolution temperature forecasting. By leveraging spatial learning and efficient computation, our approach generates forecasts at multiple horizons, up to 48 hours. For Southwestern Ontario, Canada, the model captures temperature patterns with a mean MAE of 1.93$^{\circ}$C across 1-48h forecasts and MAE@48h of 2.93$^{\circ}$C, evaluated using 24h input windows on the largest region. While demonstrated here in a data-rich context, this work lays the foundation for transfer learning approaches that could enable localized, equitable forecasts in data-limited regions of the Global South.

</details>


### [201] [List Replicable Reinforcement Learning](https://arxiv.org/abs/2512.00553)
*Bohan Zhang,Michael Chen,A. Pavan,N. V. Vinodchandran,Lin F. Yang,Ruosong Wang*

Main category: cs.LG

TL;DR: 本文研究了强化学习(RL)中的可重复性问题，并提出了列表可重复性的概念，要求算法在不同运行中返回一个接近最优策略的小列表。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法不稳定且对训练条件的变化敏感，因此可重复性是一个根本挑战。

Method: 本文提出了一个可证明有效的表格RL算法，通过确保列表复杂度在状态、动作和范围长度中保持多项式来保证列表可重复性。该算法采用了一种新颖的规划策略，该策略基于随机选择的容差阈值内接近最优选择的词典顺序选择动作，以及一种在保持可重复性的同时测试随机环境中状态可达性的机制。

Result: 本文提出的规划策略可以被纳入到实际的RL框架中，以提高它们的稳定性。

Conclusion: 本文的理论研究阐明了解决实际中使用的RL算法的不稳定性问题。

Abstract: Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \emph{small list} of policies across different runs, with high probability. The size of this list defines the \emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.

</details>


### [202] [Pre-Generating Multi-Difficulty PDE Data for Few-Shot Neural PDE Solvers](https://arxiv.org/abs/2512.00564)
*Naman Choudhary,Vedant Singh,Ameet Talwalkar,Nicholas Matthew Boffi,Mikhail Khodak,Tanya Marwah*

Main category: cs.LG

TL;DR: 这篇论文研究了如何通过在低难度样本上训练来提高神经偏微分方程求解器在高难度问题上的性能，从而减少对高难度数据的需求。


<details>
  <summary>Details</summary>
Motivation: 传统偏微分方程求解器的主要成本在于生成训练数据，尤其是在复杂几何形状和高雷诺数的情况下。这使得神经求解器难以有效训练。

Method: 该研究系统地改变了二维不可压缩Navier-Stokes方程的几何形状（障碍物的数量和位置）、物理特性（雷诺数）及其组合，从而控制任务的难度。

Result: 通过包含大量低、中等难度的样本进行训练，可以用更少的样本学习高难度的物理问题。结合低难度和高难度数据，与仅使用高难度样本相比，生成数据集的计算成本可以降低8.9倍，同时达到相同的误差。

Conclusion: 在神经求解器的预生成偏微分方程数据中，如何分配跨难度级别的经典求解器计算与总分配量同样重要，并且可以通过有原则的组织获得显著收益。

Abstract: A key aspect of learned partial differential equation (PDE) solvers is that the main cost often comes from generating training data with classical solvers rather than learning the model itself. Another is that there are clear axes of difficulty--e.g., more complex geometries and higher Reynolds numbers--along which problems become (1) harder for classical solvers and thus (2) more likely to benefit from neural speedups. Towards addressing this chicken-and-egg challenge, we study difficulty transfer on 2D incompressible Navier-Stokes, systematically varying task complexity along geometry (number and placement of obstacles), physics (Reynolds number), and their combination. Similar to how it is possible to spend compute to pre-train foundation models and improve their performance on downstream tasks, we find that by classically solving (analogously pre-generating) many low and medium difficulty examples and including them in the training set, it is possible to learn high-difficulty physics from far fewer samples. Furthermore, we show that by combining low and high difficulty data, we can spend 8.9x less compute on pre-generating a dataset to achieve the same error as using only high difficulty examples. Our results highlight that how we allocate classical-solver compute across difficulty levels is as important as how much we allocate overall, and suggest substantial gains from principled curation of pre-generated PDE data for neural solvers. Our code is available at https://github.com/Naman-Choudhary-AI-ML/pregenerating-pde

</details>
