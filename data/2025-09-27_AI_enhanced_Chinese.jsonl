{"id": "2509.20617", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20617", "abs": "https://arxiv.org/abs/2509.20617", "authors": ["Eric Fithian", "Kirill Skobelev"], "title": "DELM: a Python toolkit for Data Extraction with Language Models", "comment": null, "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.", "AI": {"tldr": "DELM\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u6570\u636e\u63d0\u53d6\u7684Python\u5de5\u5177\u5305\uff0c\u65e8\u5728\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3001\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4f9d\u8d56\u4e8e\u4e34\u65f6\u811a\u672c\uff0c\u5bfc\u81f4\u53ef\u91cd\u590d\u6027\u3001\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "DELM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5177\u6709\u7ed3\u6784\u5316\u8f93\u51fa\u3001\u5185\u7f6e\u9a8c\u8bc1\u3001\u7075\u6d3b\u7684\u6570\u636e\u52a0\u8f7d\u548c\u8bc4\u5206\u7b56\u7565\u4ee5\u53ca\u9ad8\u6548\u7684\u6279\u5904\u7406\u3002", "result": "DELM\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u80fd\u529b\uff0c\u5305\u62ec\u63d0\u793a\u4f18\u5316\u7b97\u6cd5\u548c\u6210\u672c\u4e0e\u8986\u76d6\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "DELM\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u53ef\u7528\u4e8e\u5feb\u901f\u5b9e\u9a8cLLM\u6570\u636e\u63d0\u53d6\u7ba1\u9053\uff0c\u5e76\u91cf\u5316\u5b83\u4eec\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2509.20769", "categories": ["cs.IR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20769", "abs": "https://arxiv.org/abs/2509.20769", "authors": ["Tuo Zhang", "Yuechun Sun", "Ruiliang Liu"], "title": "Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems", "comment": null, "summary": "In this work, we present a retrieval-augmented generation (RAG)-based system\nfor provenance analysis of archaeological artifacts, designed to support expert\nreasoning by integrating multimodal retrieval and large vision-language models\n(VLMs). The system constructs a dual-modal knowledge base from reference texts\nand images, enabling raw visual, edge-enhanced, and semantic retrieval to\nidentify stylistically similar objects. Retrieved candidates are synthesized by\nthe VLM to generate structured inferences, including chronological,\ngeographical, and cultural attributions, alongside interpretive justifications.\nWe evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from\nthe British Museum. Expert evaluation demonstrates that the system produces\nmeaningful and interpretable outputs, offering scholars concrete starting\npoints for analysis and significantly alleviating the cognitive burden of\nnavigating vast comparative corpora.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u8003\u53e4\u6587\u7269\u7684\u6eaf\u6e90\u5206\u6790\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u68c0\u7d22\u548c\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6765\u652f\u6301\u4e13\u5bb6\u63a8\u7406\uff0c\u51cf\u8f7b\u5b66\u8005\u6d4f\u89c8\u5927\u91cf\u6bd4\u8f83\u8bed\u6599\u5e93\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u8be5\u7cfb\u7edf\u6784\u5efa\u4e86\u4e00\u4e2a\u6765\u81ea\u53c2\u8003\u6587\u672c\u548c\u56fe\u50cf\u7684\u53cc\u6a21\u6001\u77e5\u8bc6\u5e93\uff0c\u652f\u6301\u539f\u59cb\u89c6\u89c9\u3001\u8fb9\u7f18\u589e\u5f3a\u548c\u8bed\u4e49\u68c0\u7d22\u4ee5\u8bc6\u522b\u98ce\u683c\u76f8\u4f3c\u7684\u7269\u4f53\u3002\u68c0\u7d22\u5230\u7684\u5019\u9009\u5bf9\u8c61\u7531 VLM \u5408\u6210\uff0c\u4ee5\u751f\u6210\u7ed3\u6784\u5316\u7684\u63a8\u8bba\uff0c\u5305\u62ec\u5e74\u4ee3\u3001\u5730\u7406\u548c\u6587\u5316\u5f52\u5c5e\uff0c\u4ee5\u53ca\u89e3\u91ca\u6027\u7406\u7531\u3002", "result": "\u5728\u6765\u81ea\u5927\u82f1\u535a\u7269\u9986\u7684\u6b27\u4e9a\u9752\u94dc\u65f6\u4ee3\u6587\u7269\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u4ea7\u751f\u4e86\u6709\u610f\u4e49\u4e14\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5b66\u8005\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u5206\u6790\u8d77\u70b9\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2509.20804", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20804", "abs": "https://arxiv.org/abs/2509.20804", "authors": ["Meng Yuan", "Justin Zobel"], "title": "Performance Consistency of Learning Methods for Information Retrieval Tasks", "comment": null, "summary": "A range of approaches have been proposed for estimating the accuracy or\nrobustness of the measured performance of IR methods. One is to use\nbootstrapping of test sets, which, as we confirm, provides an estimate of\nvariation in performance. For IR methods that rely on a seed, such as those\nthat involve machine learning, another approach is to use a random set of seeds\nto examine performance variation. Using three different IR tasks we have used\nsuch randomness to examine a range of traditional statistical learning models\nand transformer-based learning models. While the statistical models are stable,\nthe transformer models show huge variation as seeds are changed. In 9 of 11\ncases the F1-scores (in the range 0.0--1.0) had a standard deviation of over\n0.075; while 7 of 11 precision values (also in the range 0.0--1.0) had a\nstandard deviation of over 0.125. This is in a context where differences of\nless than 0.02 have been used as evidence of method improvement. Our findings\nhighlight the vulnerability of transformer models to training instabilities and\nmoreover raise questions about the reliability of previous results, thus\nunderscoring the need for rigorous evaluation practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u65b9\u6cd5\u6027\u80fd\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u5bf9\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u975e\u5e38\u654f\u611f\uff0c\u8d28\u7591\u4e86\u4ee5\u5f80\u7ed3\u679c\u7684\u53ef\u9760\u6027\uff0c\u5f3a\u8c03\u4e86\u4e25\u683c\u8bc4\u4f30\u5b9e\u8df5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u65b9\u6cd5\u6d4b\u91cf\u6027\u80fd\u7684\u51c6\u786e\u6027\u6216\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u968f\u673a\u79cd\u5b50\u68c0\u67e5\u5404\u79cd\u4f20\u7edf\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u548c\u57fa\u4e8e Transformer \u7684\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u53d8\u5316\u3002", "result": "\u7edf\u8ba1\u6a21\u578b\u7a33\u5b9a\uff0c\u800c Transformer \u6a21\u578b\u968f\u7740\u79cd\u5b50\u7684\u53d8\u5316\u8868\u73b0\u51fa\u5de8\u5927\u5dee\u5f02\u3002\u5728 9/11 \u7684\u60c5\u51b5\u4e0b\uff0cF1 \u5206\u6570\uff08\u8303\u56f4 0.0--1.0\uff09\u7684\u6807\u51c6\u504f\u5dee\u8d85\u8fc7 0.075\uff1b\u800c 7/11 \u7684\u7cbe\u786e\u5ea6\u503c\uff08\u4e5f\u5728 0.0--1.0 \u8303\u56f4\u5185\uff09\u7684\u6807\u51c6\u504f\u5dee\u8d85\u8fc7 0.125\u3002", "conclusion": "Transformer \u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u5bf9\u5148\u524d\u7ed3\u679c\u7684\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u56e0\u6b64\u5f3a\u8c03\u9700\u8981\u4e25\u683c\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002"}}
{"id": "2509.20883", "categories": ["cs.IR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20883", "abs": "https://arxiv.org/abs/2509.20883", "authors": ["Hua Zong", "Qingtao Zeng", "Zhengxiong Zhou", "Zhihua Han", "Zhensong Yan", "Mingjie Liu", "Hechen Sun", "Jiawei Liu", "Yiwen Hu", "Qi Wang", "YiHan Xian", "Wenjie Guo", "Houyuan Xiang", "Zhiyuan Zeng", "Xiangrong Sheng", "Bencheng Yan", "Nan Hu", "Yuheng Huang", "Jinqing Lian", "Ziru Xu", "Yan Zhang", "Ju Huang", "Siran Yang", "Huimin Yi", "Jiamang Wang", "Pengjie Wang", "Han Zhu", "Jian Wu", "Dan Ou", "Jian Xu", "Haihong Tang", "Yuning Jiang", "Bo Zheng", "Lin Qu"], "title": "RecIS: Sparse to Dense, A Unified Training Framework for Recommendation Models", "comment": null, "summary": "In this paper, we propose RecIS, a unified Sparse-Dense training framework\ndesigned to achieve two primary goals: 1. Unified Framework To create a Unified\nsparse-dense training framework based on the PyTorch ecosystem that meets the\ntraining needs of industrial-grade recommendation models that integrated with\nlarge models. 2.System Optimization To optimize the sparse component, offering\nsuperior efficiency over the TensorFlow-based recommendation models. The dense\ncomponent, meanwhile, leverages existing optimization technologies within the\nPyTorch ecosystem. Currently, RecIS is being used in Alibaba for numerous\nlarge-model enhanced recommendation training tasks, and some traditional sparse\nmodels have also begun training in it.", "AI": {"tldr": "RecIS: A unified sparse-dense training framework.", "motivation": "To create a unified sparse-dense training framework based on the PyTorch ecosystem that meets the training needs of industrial-grade recommendation models that integrated with large models and to optimize the sparse component, offering superior efficiency over the TensorFlow-based recommendation models.", "method": "A unified Sparse-Dense training framework", "result": "RecIS is being used in Alibaba for numerous large-model enhanced recommendation training tasks, and some traditional sparse models have also begun training in it.", "conclusion": "RecIS is a unified sparse-dense training framework designed to achieve two primary goals: 1. Unified Framework 2.System Optimization"}}
{"id": "2509.20364", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20364", "abs": "https://arxiv.org/abs/2509.20364", "authors": ["Thomas J Sheffler"], "title": "An Approach to Checking Correctness for Agentic Systems", "comment": "15 pages, 5 figures", "summary": "This paper presents a temporal expression language for monitoring AI agent\nbehavior, enabling systematic error-detection of LLM-based agentic systems that\nexhibit variable outputs due to stochastic generation processes. Drawing from\ntemporal logic techniques used in hardware verification, this approach monitors\nexecution traces of agent tool calls and state transitions to detect deviations\nfrom expected behavioral patterns. Current error-detection approaches rely\nprimarily on text matching of inputs and outputs, which proves fragile due to\nthe natural language variability inherent in LLM responses. The proposed method\ninstead focuses on the sequence of agent actions -- such as tool invocations\nand inter-agent communications -- allowing verification of system behavior\nindependent of specific textual outputs. The temporal expression language\nprovides assertions that capture correct behavioral patterns across multiple\nexecution scenarios. These assertions serve dual purposes: validating prompt\nengineering and guardrail effectiveness during development, and providing\nregression testing when agents are updated with new LLMs or modified logic. The\napproach is demonstrated using a three-agent system, where agents coordinate to\nsolve multi-step reasoning tasks. When powered by large, capable models, all\ntemporal assertions were satisfied across many test runs. However, when smaller\nmodels were substituted in two of the three agents, executions violated\nbehavioral assertions, primarily due to improper tool sequencing and failed\ncoordination handoffs. The temporal expressions successfully flagged these\nanomalies, demonstrating the method's effectiveness for detecting behavioral\nregressions in production agentic systems. This approach provides a foundation\nfor systematic monitoring of AI agent reliability as these systems become\nincreasingly deployed in critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u8868\u8fbe\u5f0f\u8bed\u8a00\uff0c\u7528\u4e8e\u76d1\u63a7AI\u4ee3\u7406\u884c\u4e3a\uff0c\u5b9e\u73b0\u5bf9\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u9519\u8bef\u68c0\u6d4b\u3002", "motivation": "\u5f53\u524d\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8f93\u5165\u548c\u8f93\u51fa\u7684\u6587\u672c\u5339\u914d\uff0c\u7531\u4e8eLLM\u54cd\u5e94\u4e2d\u56fa\u6709\u7684\u81ea\u7136\u8bed\u8a00\u53ef\u53d8\u6027\uff0c\u8fd9\u79cd\u65b9\u6cd5\u663e\u5f97\u8106\u5f31\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4ee3\u7406\u884c\u4e3a\u7684\u5e8f\u5217\uff0c\u5141\u8bb8\u72ec\u7acb\u4e8e\u7279\u5b9a\u6587\u672c\u8f93\u51fa\u9a8c\u8bc1\u7cfb\u7edf\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u903b\u8f91\u6280\u672f\uff0c\u76d1\u63a7\u4ee3\u7406\u5de5\u5177\u8c03\u7528\u548c\u72b6\u6001\u8f6c\u6362\u7684\u6267\u884c\u8f68\u8ff9\uff0c\u4ee5\u68c0\u6d4b\u4e0e\u9884\u671f\u884c\u4e3a\u6a21\u5f0f\u7684\u504f\u5dee\u3002", "result": "\u5f53\u4f7f\u7528\u5927\u578b\u6a21\u578b\u65f6\uff0c\u6240\u6709\u65f6\u95f4\u65ad\u8a00\u90fd\u6ee1\u8db3\u3002\u5f53\u8f83\u5c0f\u6a21\u578b\u88ab\u66ff\u6362\u65f6\uff0c\u6267\u884c\u8fdd\u53cd\u4e86\u884c\u4e3a\u65ad\u8a00\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u4e0d\u6b63\u786e\u7684\u5de5\u5177\u6392\u5e8f\u548c\u5931\u8d25\u7684\u534f\u8c03\u5207\u6362\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cfb\u7edf\u6027\u76d1\u63a7AI\u4ee3\u7406\u7684\u53ef\u9760\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u5173\u952e\u5e94\u7528\u4e2d\u3002"}}
{"id": "2509.20367", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.20367", "abs": "https://arxiv.org/abs/2509.20367", "authors": ["Leyi Ouyang"], "title": "Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models", "comment": "2 Figures, 7 Tables, 1 Algorithm", "summary": "Diplomatic events consistently prompt widespread public discussion and\ndebate. Public sentiment plays a critical role in diplomacy, as a good\nsentiment provides vital support for policy implementation, helps resolve\ninternational issues, and shapes a nation's international image. Traditional\nmethods for gauging public sentiment, such as large-scale surveys or manual\ncontent analysis of media, are typically time-consuming, labor-intensive, and\nlack the capacity for forward-looking analysis. We propose a novel framework\nthat identifies specific modifications for diplomatic event narratives to shift\npublic sentiment from negative to neutral or positive. First, we train a\nlanguage model to predict public reaction towards diplomatic events. To this\nend, we construct a dataset comprising descriptions of diplomatic events and\ntheir associated public discussions. Second, guided by communication theories\nand in collaboration with domain experts, we predetermined several textual\nfeatures for modification, ensuring that any alterations changed the event's\nnarrative framing while preserving its core facts.We develop a counterfactual\ngeneration algorithm that employs a large language model to systematically\nproduce modified versions of an original text. The results show that this\nframework successfully shifted public sentiment to a more favorable state with\na 70\\% success rate. This framework can therefore serve as a practical tool for\ndiplomats, policymakers, and communication specialists, offering data-driven\ninsights on how to frame diplomatic initiatives or report on events to foster a\nmore desirable public sentiment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u5916\u4ea4\u4e8b\u4ef6\u53d9\u4e8b\u4ee5\u8f6c\u53d8\u516c\u4f17\u60c5\u7eea\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u516c\u4f17\u53cd\u5e94\uff0c\u5e76\u751f\u6210\u53cd\u4e8b\u5b9e\u6587\u672c\u3002", "motivation": "\u516c\u4f17\u60c5\u7eea\u5728\u5916\u4ea4\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u4e14\u7f3a\u4e4f\u524d\u77bb\u6027\u3002", "method": "\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u516c\u4f17\u53cd\u5e94\uff0c\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u786e\u5b9a\u6587\u672c\u7279\u5f81\u8fdb\u884c\u4fee\u6539\uff0c\u4f7f\u7528\u53cd\u4e8b\u5b9e\u751f\u6210\u7b97\u6cd5\u7cfb\u7edf\u5730\u4ea7\u751f\u4fee\u6539\u7248\u672c\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u516c\u4f17\u60c5\u7eea\u8f6c\u53d8\u4e3a\u66f4\u6709\u5229\u7684\u72b6\u6001\uff0c\u6210\u529f\u7387\u8fbe70%\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4e3a\u5916\u4ea4\u5b98\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u4f20\u64ad\u4e13\u5bb6\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u7406\u60f3\u7684\u516c\u4f17\u60c5\u7eea\u3002"}}
{"id": "2509.20379", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20379", "abs": "https://arxiv.org/abs/2509.20379", "authors": ["Ofir Azachi", "Kfir Eliyahu", "Eyal El Ani", "Rom Himelstein", "Roi Reichart", "Yuval Pinter", "Nitay Calderon"], "title": "Leveraging NTPs for Efficient Hallucination Detection in VLMs", "comment": null, "summary": "Hallucinations of vision-language models (VLMs), which are misalignments\nbetween visual content and generated text, undermine the reliability of VLMs.\nOne common approach for detecting them employs the same VLM, or a different\none, to assess generated outputs. This process is computationally intensive and\nincreases model latency. In this paper, we explore an efficient on-the-fly\nmethod for hallucination detection by training traditional ML models over\nsignals based on the VLM's next-token probabilities (NTPs). NTPs provide a\ndirect quantification of model uncertainty. We hypothesize that high\nuncertainty (i.e., a low NTP value) is strongly associated with hallucinations.\nTo test this, we introduce a dataset of 1,400 human-annotated statements\nderived from VLM-generated content, each labeled as hallucinated or not, and\nuse it to test our NTP-based lightweight method. Our results demonstrate that\nNTP-based features are valuable predictors of hallucinations, enabling fast and\nsimple ML models to achieve performance comparable to that of strong VLMs.\nFurthermore, augmenting these NTPs with linguistic NTPs, computed by feeding\nonly the generated text back into the VLM, enhances hallucination detection\nperformance. Finally, integrating hallucination prediction scores from VLMs\ninto the NTP-based models led to better performance than using either VLMs or\nNTPs alone. We hope this study paves the way for simple, lightweight solutions\nthat enhance the reliability of VLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684 next-token probabilities (NTPs) \u7684\u8f7b\u91cf\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3 VLM \u4e2d\u89c6\u89c9\u5185\u5bb9\u548c\u751f\u6210\u6587\u672c\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "VLM \u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\uff0c\u800c\u4f7f\u7528 VLM \u68c0\u6d4b\u5e7b\u89c9\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528 VLM \u7684 NTPs \u4f5c\u4e3a\u4fe1\u53f7\uff0c\u5b9e\u73b0\u5bf9\u5e7b\u89c9\u7684\u5feb\u901f\u68c0\u6d4b\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b 1400 \u4e2a\u4eba\u5de5\u6807\u6ce8\u8bed\u53e5\u7684\u6570\u636e\u96c6\u6765\u6d4b\u8bd5\u8be5\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e NTP \u7684\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u9884\u6d4b\u5e7b\u89c9\uff0c\u7b80\u5355 ML \u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u4e0e\u5f3a\u5927\u7684 VLM \u76f8\u5f53\u7684\u6027\u80fd\u3002\u7ed3\u5408\u8bed\u8a00 NTPs \u548c\u5c06 VLM \u7684\u5e7b\u89c9\u9884\u6d4b\u5206\u6570\u6574\u5408\u5230\u57fa\u4e8e NTP \u7684\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u9ad8 VLM \u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.20408", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20408", "abs": "https://arxiv.org/abs/2509.20408", "authors": ["Leo Maxime Brunswic", "Haozhi Wang", "Shuang Luo", "Jianye Hao", "Amir Rasouli", "Yinchuan Li"], "title": "A Theory of Multi-Agent Generative Flow Networks", "comment": "Accepted at SPIGM Workshop NeurIPS 2025", "summary": "Generative flow networks utilize a flow-matching loss to learn a stochastic\npolicy for generating objects from a sequence of actions, such that the\nprobability of generating a pattern can be proportional to the corresponding\ngiven reward. However, a theoretical framework for multi-agent generative flow\nnetworks (MA-GFlowNets) has not yet been proposed. In this paper, we propose\nthe theory framework of MA-GFlowNets, which can be applied to multiple agents\nto generate objects collaboratively through a series of joint actions. We\nfurther propose four algorithms: a centralized flow network for centralized\ntraining of MA-GFlowNets, an independent flow network for decentralized\nexecution, a joint flow network for achieving centralized training with\ndecentralized execution, and its updated conditional version. Joint Flow\ntraining is based on a local-global principle allowing to train a collection of\n(local) GFN as a unique (global) GFN. This principle provides a loss of\nreasonable complexity and allows to leverage usual results on GFN to provide\ntheoretical guarantees that the independent policies generate samples with\nprobability proportional to the reward function. Experimental results\ndemonstrate the superiority of the proposed framework compared to reinforcement\nlearning and MCMC-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u751f\u6210\u6d41\u7f51\u7edc\uff08MA-GFlowNets\uff09\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4e2a\u667a\u80fd\u4f53\u901a\u8fc7\u4e00\u7cfb\u5217\u8054\u5408\u52a8\u4f5c\u534f\u540c\u751f\u6210\u5bf9\u8c61\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u751f\u6210\u6d41\u7f51\u7edc\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u7b97\u6cd5\uff1a\u96c6\u4e2d\u5f0f\u6d41\u7f51\u7edc\u3001\u72ec\u7acb\u6d41\u7f51\u7edc\u3001\u8054\u5408\u6d41\u7f51\u7edc\u53ca\u5176\u6761\u4ef6\u7248\u672c\u3002\u8054\u5408\u6d41\u8bad\u7ec3\u57fa\u4e8e\u5c40\u90e8-\u5168\u5c40\u539f\u5219\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8eMCMC\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MA-GFlowNets\u6846\u67b6\u6709\u6548\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.20781", "categories": ["cs.LG", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.20781", "abs": "https://arxiv.org/abs/2509.20781", "authors": ["Alireza Heidari", "Amirhossein Ahmad", "Wei Zhang", "Ying Xiong"], "title": "Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes", "comment": "22 pages, 11 figures", "summary": "Learned Indexes (LIs) represent a paradigm shift from traditional index\nstructures by employing machine learning models to approximate the cumulative\ndistribution function (CDF) of sorted data. While LIs achieve remarkable\nefficiency for static datasets, their performance degrades under dynamic\nupdates: maintaining the CDF invariant (sum of F(k) equals 1) requires global\nmodel retraining, which blocks queries and limits the queries-per-second (QPS)\nmetric. Current approaches fail to address these retraining costs effectively,\nrendering them unsuitable for real-world workloads with frequent updates. In\nthis paper, we present Sig2Model, an efficient and adaptive learned index that\nminimizes retraining cost through three key techniques: (1) a sigmoid boosting\napproximation technique that dynamically adjusts the index model by\napproximating update-induced shifts in data distribution with localized sigmoid\nfunctions while preserving bounded error guarantees and deferring full\nretraining; (2) proactive update training via Gaussian mixture models (GMMs)\nthat identifies high-update-probability regions for strategic placeholder\nallocation to speed up updates; and (3) a neural joint optimization framework\nthat continuously refines both the sigmoid ensemble and GMM parameters via\ngradient-based learning. We evaluate Sig2Model against state-of-the-art\nupdatable learned indexes on real-world and synthetic workloads, and show that\nSig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS,\nand uses up to 1000x less memory.", "AI": {"tldr": "Sig2Model: An efficient and adaptive learned index that minimizes retraining cost through sigmoid boosting, proactive update training, and neural joint optimization.", "motivation": "Current learned indexes degrade under dynamic updates because maintaining the CDF invariant requires global model retraining, which blocks queries and limits the queries-per-second (QPS) metric. Current approaches fail to address these retraining costs effectively.", "method": "The paper presents Sig2Model, which uses three key techniques: sigmoid boosting approximation, proactive update training via Gaussian mixture models (GMMs), and a neural joint optimization framework.", "result": "Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS, and uses up to 1000x less memory compared to state-of-the-art updatable learned indexes.", "conclusion": "Sig2Model is an efficient and adaptive learned index that minimizes retraining cost and outperforms existing approaches in dynamic update scenarios."}}
{"id": "2509.20904", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20904", "abs": "https://arxiv.org/abs/2509.20904", "authors": ["Kairui Fu", "Tao Zhang", "Shuwen Xiao", "Ziyang Wang", "Xinming Zhang", "Chenchi Zhang", "Yuliang Yan", "Junjun Zheng", "Yu Li", "Zhihong Chen", "Jian Wu", "Xiangheng Kong", "Shengyu Zhang", "Kun Kuang", "Yuning Jiang", "Bo Zheng"], "title": "FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets", "comment": null, "summary": "Semantic identifiers (SIDs) have gained increasing attention in generative\nretrieval (GR) due to their meaningful semantic discriminability. However,\ncurrent research on SIDs faces three main challenges: (1) the absence of\nlarge-scale public datasets with multimodal features, (2) limited investigation\ninto optimization strategies for SID generation, which typically rely on costly\nGR training for evaluation, and (3) slow online convergence in industrial\ndeployment. To address these challenges, we propose FORGE, a comprehensive\nbenchmark for FOrming semantic identifieR in Generative rEtrieval with\nindustrial datasets. Specifically, FORGE is equipped with a dataset comprising\n14 billion user interactions and multimodal features of 250 million items\nsampled from Taobao, one of the biggest e-commerce platforms in China.\nLeveraging this dataset, FORGE explores several optimizations to enhance the\nSID construction and validates their effectiveness via offline experiments\nacross different settings and tasks. Further online analysis conducted on our\nplatform, which serves over 300 million users daily, reveals a 0.35% increase\nin transaction count, highlighting the practical impact of our method.\nRegarding the expensive SID validation accompanied by the full training of GRs,\nwe propose two novel metrics of SID that correlate positively with\nrecommendation performance, enabling convenient evaluations without any GR\ntraining. For real-world applications, FORGE introduces an offline pretraining\nschema that reduces online convergence by half. The code and data are available\nat https://github.com/selous123/al_sid.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86FORGE\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5f0f\u68c0\u7d22\u4e2d\u8bed\u4e49\u6807\u8bc6\u7b26\uff08SID\uff09\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u5f53\u524dSID\u7814\u7a76\u9762\u4e34\u7f3a\u4e4f\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u3001SID\u751f\u6210\u4f18\u5316\u7b56\u7565\u7814\u7a76\u4e0d\u8db3\u4ee5\u53ca\u5728\u7ebf\u6536\u655b\u901f\u5ea6\u6162\u7b49\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b140\u4ebf\u7528\u6237\u4ea4\u4e92\u548c2.5\u4ebf\u5546\u54c1\u591a\u6a21\u6001\u7279\u5f81\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u5347SID\u6784\u5efa\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u7ebf\u5206\u6790\u663e\u793a\u4ea4\u6613\u6570\u91cf\u589e\u52a0\u4e860.35%\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0e\u63a8\u8350\u6027\u80fd\u6b63\u76f8\u5173\u7684SID\u65b0\u6307\u6807\uff0c\u65e0\u9700GR\u8bad\u7ec3\u5373\u53ef\u8fdb\u884c\u8bc4\u4f30\u3002\u79bb\u7ebf\u9884\u8bad\u7ec3\u6a21\u5f0f\u5c06\u5728\u7ebf\u6536\u655b\u901f\u5ea6\u7f29\u77ed\u4e86\u4e00\u534a\u3002", "conclusion": "FORGE\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u4f18\u5316\u7b56\u7565\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u68c0\u7d22\u4e2d\u8bed\u4e49\u6807\u8bc6\u7b26\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2509.20368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20368", "abs": "https://arxiv.org/abs/2509.20368", "authors": ["Theo Uscidda", "Matthew Trager", "Michael Kleinman", "Aditya Chattopadhyay", "Wei Xia", "Stefano Soatto"], "title": "LATTS: Locally Adaptive Test-Time Scaling", "comment": null, "summary": "One common strategy for improving the performance of Large Language Models\n(LLMs) on downstream tasks involves using a \\emph{verifier model} to either\nselect the best answer from a pool of candidates or to steer the\nauto-regressive generation process towards better outputs. This class of\nmethods typically results in improved accuracy at the cost of increased\ncomputation at test-time, a paradigm known as \\emph{test-time scaling}.\nHowever, most existing approaches increase computation uniformly across all\nsamples and generation steps, without considering the complexity of individual\ninstances, leading to inefficient resource use. We address this limitation by\nproposing an approach, called \\emph{Locally Adaptive Test-Time Scaling\n(LATTS)}, that allocates variable compute across generation steps.\nSpecifically, at each generation step, LATTS employs a verifier-based\nacceptance criterion to decide whether to resample, backtrack, restart, or stop\nthe generation process. This criterion effectively adjusts the per-step\ncomputational effort based on a precise notion of \\emph{local difficulty}\nderived from the verifier model. Empirical results show that LATTS achieves\nsignificantly superior accuracy--compute tradeoffs compared to standard\nverifier-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Locally Adaptive Test-Time Scaling (LATTS) \u7684\u65b9\u6cd5\uff0c\u5b83\u6839\u636e\u9a8c\u8bc1\u5668\u6a21\u578b\u884d\u751f\u7684\u5c40\u90e8\u96be\u5ea6\u6765\u8c03\u6574\u6bcf\u6b65\u7684\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6240\u6709\u6837\u672c\u548c\u751f\u6210\u6b65\u9aa4\u4e2d\u7edf\u4e00\u589e\u52a0\u8ba1\u7b97\u91cf\uff0c\u800c\u6ca1\u6709\u8003\u8651\u4e2a\u4f53\u5b9e\u4f8b\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5728\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u4e2d\uff0cLATTS \u91c7\u7528\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u63a5\u53d7\u6807\u51c6\u6765\u51b3\u5b9a\u662f\u5426\u91cd\u65b0\u91c7\u6837\u3001\u56de\u6eaf\u3001\u91cd\u65b0\u542f\u52a8\u6216\u505c\u6b62\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u7684\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cLATTS \u5b9e\u73b0\u4e86\u660e\u663e\u66f4\u9ad8\u7684\u7cbe\u5ea6-\u8ba1\u7b97\u6743\u8861\u3002", "conclusion": "LATTS \u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e76\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.20373", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20373", "abs": "https://arxiv.org/abs/2509.20373", "authors": ["Shreya G. Upadhyay", "Carlos Busso", "Chi-Chun Lee"], "title": "Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition", "comment": null, "summary": "Cross-lingual speech emotion recognition (SER) remains a challenging task due\nto differences in phonetic variability and speaker-specific expressive styles\nacross languages. Effectively capturing emotion under such diverse conditions\nrequires a framework that can align the externalization of emotions across\ndifferent speakers and languages. To address this problem, we propose a\nspeaker-style aware phoneme anchoring framework that aligns emotional\nexpression at the phonetic and speaker levels. Our method builds\nemotion-specific speaker communities via graph-based clustering to capture\nshared speaker traits. Using these groups, we apply dual-space anchoring in\nspeaker and phonetic spaces to enable better emotion transfer across languages.\nEvaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)\ncorpora demonstrate improved generalization over competitive baselines and\nprovide valuable insights into the commonalities in cross-lingual emotion\nrepresentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u97f3\u6807\u548c\u8bf4\u8bdd\u4eba\u5c42\u9762\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ece\u800c\u6709\u6548\u6355\u6349\u4e0d\u540c\u8bed\u8a00\u548c\u8bf4\u8bdd\u4eba\u4e4b\u95f4\u7684\u60c5\u611f\u3002", "motivation": "\u8de8\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7531\u4e8e\u4e0d\u540c\u8bed\u8a00\u5728\u8bed\u97f3\u53d8\u5f02\u6027\u548c\u8bf4\u8bdd\u4eba\u7279\u5b9a\u8868\u8fbe\u98ce\u683c\u4e0a\u7684\u5dee\u5f02\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u6709\u6548\u6355\u6349\u60c5\u611f\uff0c\u8fd9\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5bf9\u9f50\u4e0d\u540c\u8bf4\u8bdd\u4eba\u548c\u8bed\u8a00\u60c5\u611f\u5916\u5316\u7684\u6846\u67b6\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u805a\u7c7b\u6784\u5efa\u7279\u5b9a\u4e8e\u60c5\u611f\u7684\u8bf4\u8bdd\u4eba\u793e\u533a\uff0c\u4ee5\u6355\u83b7\u5171\u4eab\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\u3002\u5229\u7528\u8fd9\u4e9b\u7ec4\uff0c\u6211\u4eec\u5728\u8bf4\u8bdd\u4eba\u548c\u8bed\u97f3\u7a7a\u95f4\u4e2d\u5e94\u7528\u53cc\u7a7a\u95f4\u951a\u5b9a\uff0c\u4ee5\u5b9e\u73b0\u8de8\u8bed\u8a00\u7684\u66f4\u597d\u60c5\u611f\u8f6c\u79fb\u3002", "result": "\u5728 MSP-Podcast (\u82f1\u8bed) \u548c BIIC-Podcast (\u53f0\u6e7e\u666e\u901a\u8bdd) \u8bed\u6599\u5e93\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u7ade\u4e89\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e3a\u8de8\u8bed\u8a00\u60c5\u611f\u8868\u5f81\u7684\u5171\u6027\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bf4\u8bdd\u4eba\u98ce\u683c\u611f\u77e5\u97f3\u7d20\u951a\u5b9a\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u8de8\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.20420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20420", "abs": "https://arxiv.org/abs/2509.20420", "authors": ["Elias N. Zois", "Moises Diaz", "Salem Said", "Miguel A. Ferrer"], "title": "Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification", "comment": "9 pages, 3 figures", "summary": "Offline handwritten signature verification remains a challenging task,\nparticularly in writer-independent settings where models must generalize across\nunseen individuals. Recent developments have highlighted the advantage of\ngeometrically inspired representations, such as covariance descriptors on\nRiemannian manifolds. However, past or present, handcrafted or data-driven\nmethods usually depend on real-world signature datasets for classifier\ntraining. We introduce a quasi-synthetic data generation framework leveraging\nthe Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small\nset of genuine samples in the SPD space is the seed to a Riemannian Gaussian\nMixture which identifies Riemannian centers as synthetic writers and variances\nas their properties. Riemannian Gaussian sampling on each center generates\npositive as well as negative synthetic SPD populations. A metric learning\nframework utilizes pairs of similar and dissimilar SPD points, subsequently\ntesting it over on real-world datasets. Experiments conducted on two popular\nsignature datasets, encompassing Western and Asian writing styles, demonstrate\nthe efficacy of the proposed approach under both intra- and cross- dataset\nevaluation protocols. The results indicate that our quasi-synthetic approach\nachieves low error rates, highlighting the potential of generating synthetic\ndata in Riemannian spaces for writer-independent signature verification\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51c6\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635 (SPD) \u7684\u9ece\u66fc\u51e0\u4f55\uff0c\u5728\u9ece\u66fc\u7a7a\u95f4\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u79bb\u7ebf\u624b\u5199\u7b7e\u540d\u9a8c\u8bc1\u3002", "motivation": "\u79bb\u7ebf\u624b\u5199\u7b7e\u540d\u9a8c\u8bc1\u5728\u4f5c\u8005\u72ec\u7acb\u7684\u73af\u5883\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6a21\u578b\u5fc5\u987b\u5728\u672a\u89c1\u8fc7\u7684\u4e2a\u4f53\u4e4b\u95f4\u8fdb\u884c\u6cdb\u5316\u3002\u8fc7\u53bb\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u7b7e\u540d\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\u5668\u8bad\u7ec3\u3002", "method": "\u5229\u7528 SPD \u7a7a\u95f4\u7684\u9ece\u66fc\u51e0\u4f55\uff0c\u4ee5 SPD \u7a7a\u95f4\u4e2d\u7684\u4e00\u5c0f\u7ec4\u771f\u5b9e\u6837\u672c\u4f5c\u4e3a\u79cd\u5b50\uff0c\u6784\u5efa\u9ece\u66fc\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u9ece\u66fc\u4e2d\u5fc3\u8bc6\u522b\u4e3a\u5408\u6210\u4f5c\u8005\uff0c\u5e76\u5c06\u65b9\u5dee\u8bc6\u522b\u4e3a\u5176\u5c5e\u6027\u3002\u5728\u6bcf\u4e2a\u4e2d\u5fc3\u4e0a\u8fdb\u884c\u9ece\u66fc\u9ad8\u65af\u62bd\u6837\uff0c\u751f\u6210\u6b63\u7684\u548c\u8d1f\u7684\u5408\u6210 SPD \u6837\u672c\u3002\u5229\u7528\u5ea6\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u6210\u5bf9\u7684\u76f8\u4f3c\u548c\u4e0d\u76f8\u4f3c\u7684 SPD \u70b9\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u7684\u7b7e\u540d\u6570\u636e\u96c6\uff08\u5305\u62ec\u897f\u65b9\u548c\u4e9a\u6d32\u4e66\u5199\u98ce\u683c\uff09\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5185\u90e8\u548c\u4ea4\u53c9\u6570\u636e\u96c6\u8bc4\u4f30\u534f\u8bae\u4e0b\u5747\u6709\u6548\u3002\u51c6\u5408\u6210\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f4e\u9519\u8bef\u7387\u3002", "conclusion": "\u5728\u9ece\u66fc\u7a7a\u95f4\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\u5bf9\u4e8e\u4f5c\u8005\u72ec\u7acb\u7684\u7b7e\u540d\u9a8c\u8bc1\u7cfb\u7edf\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.20416", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20416", "abs": "https://arxiv.org/abs/2509.20416", "authors": ["Haiduo Huang", "Jiangcheng Song", "Wenzhe Zhao", "Pengju Ren"], "title": "FastEagle: Cascaded Drafting for Accelerating Speculative Decoding", "comment": null, "summary": "Speculative decoding accelerates generation by drafting candidates and\nverifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still\nrequire N sequential passes to propose N tokens. We present FastEagle, a\nnon-autoregressive cascaded drafter that emits an entire draft in a single\nforward pass. FastEagle replaces temporal steps with a lightweight layer\ncascade and trains with layer-wise supervision to mitigate error accumulation.\nCoupled with a constrained draft tree that preserves lossless verification\ncost, FastEagle delivers substantial wall-clock speedups over strong\nautoregressive drafters while maintaining competitive acceptance behavior.\nAcross multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and\nDeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM,\nAlpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both\ngreedy and stochastic decoding, with comparable average acceptance lengths.\nThese results indicate that removing sequential dependencies in drafting is a\npractical path toward lossless LLM inference acceleration.", "AI": {"tldr": "FastEagle\u662f\u4e00\u79cd\u975e\u81ea\u56de\u5f52\u7684\u7ea7\u8054drafter\uff0c\u5b83\u53ef\u4ee5\u5728\u5355\u4e2a\u524d\u5411\u4f20\u64ad\u4e2d\u751f\u6210\u6574\u4e2a\u8349\u7a3f\uff0c\u4ece\u800c\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u76ee\u524d\u6700\u597d\u7684drafter\uff08\u4f8b\u5982EAGLE\uff09\u4ecd\u7136\u9700\u8981N\u4e2a\u8fde\u7eed\u7684pass\u6765\u63d0\u8baeN\u4e2atoken\uff0c\u8fd9\u9650\u5236\u4e86\u751f\u6210\u901f\u5ea6\u3002", "method": "FastEagle\u7528\u8f7b\u91cf\u7ea7\u7684\u5c42\u7ea7\u8054\u66ff\u6362\u4e86\u65f6\u95f4\u6b65\uff0c\u5e76\u4f7f\u7528\u5c42\u7ea7\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002\u6b64\u5916\uff0cFastEagle\u8fd8\u91c7\u7528\u4e86\u7ea6\u675f\u8349\u7a3f\u6811\uff0c\u4ee5\u4fdd\u6301\u65e0\u635f\u7684\u9a8c\u8bc1\u6210\u672c\u3002", "result": "FastEagle\u5728\u591a\u4e2aLLM\u548c\u4efb\u52a1\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8eEAGLE-3\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u5f53\u7684\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u3002", "conclusion": "\u5728drafting\u4e2d\u79fb\u9664\u987a\u5e8f\u4f9d\u8d56\u6027\u662f\u5b9e\u73b0\u65e0\u635fLLM\u63a8\u7406\u52a0\u901f\u7684\u4e00\u79cd\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2509.20940", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20940", "abs": "https://arxiv.org/abs/2509.20940", "authors": ["Su Liu", "Bin Bi", "Jan Bakus", "Paritosh Kumar Velalam", "Vijay Yella", "Vinod Hegde"], "title": "Markup Language Modeling for Web Document Understanding", "comment": null, "summary": "Web information extraction (WIE) is an important part of many e-commerce\nsystems, supporting tasks like customer analysis and product recommendation. In\nthis work, we look at the problem of building up-to-date product databases by\nextracting detailed information from shopping review websites. We fine-tuned\nMarkupLM on product data gathered from review sites of different sizes and then\ndeveloped a variant we call MarkupLM++, which extends predictions to internal\nnodes of the DOM tree. Our experiments show that using larger and more diverse\ntraining sets improves extraction accuracy overall. We also find that including\ninternal nodes helps with some product attributes, although it leads to a\nslight drop in overall performance. The final model reached a precision of\n0.906, recall of 0.724, and an F1 score of 0.805.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u8d2d\u7269\u8bc4\u8bba\u7f51\u7ad9\u63d0\u53d6\u8be6\u7ec6\u4ea7\u54c1\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u6700\u65b0\u7684\u4ea7\u54c1\u6570\u636e\u5e93\u3002", "motivation": "\u6784\u5efa\u6700\u65b0\u7684\u4ea7\u54c1\u6570\u636e\u5e93\uff0c\u652f\u6301\u5ba2\u6237\u5206\u6790\u548c\u4ea7\u54c1\u63a8\u8350\u7b49\u4efb\u52a1\u3002", "method": "\u5728\u4ece\u4e0d\u540c\u5927\u5c0f\u7684\u8bc4\u8bba\u7f51\u7ad9\u6536\u96c6\u7684\u4ea7\u54c1\u6570\u636e\u4e0a\u5fae\u8c03 MarkupLM\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a MarkupLM++ \u7684\u53d8\u4f53\uff0c\u8be5\u53d8\u4f53\u5c06\u9884\u6d4b\u6269\u5c55\u5230 DOM \u6811\u7684\u5185\u90e8\u8282\u70b9\u3002", "result": "\u4f7f\u7528\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u8bad\u7ec3\u96c6\u53ef\u4ee5\u63d0\u9ad8\u6574\u4f53\u63d0\u53d6\u51c6\u786e\u7387\u3002\u5305\u542b\u5185\u90e8\u8282\u70b9\u6709\u52a9\u4e8e\u67d0\u4e9b\u4ea7\u54c1\u5c5e\u6027\u7684\u63d0\u53d6\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6574\u4f53\u6027\u80fd\u7565\u6709\u4e0b\u964d\u3002\u6700\u7ec8\u6a21\u578b\u7684\u7cbe\u786e\u7387\u4e3a 0.906\uff0c\u53ec\u56de\u7387\u4e3a 0.724\uff0cF1 \u503c\u4e3a 0.805\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4ea7\u54c1\u4fe1\u606f\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2509.20370", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20370", "abs": "https://arxiv.org/abs/2509.20370", "authors": ["MZ Naser"], "title": "Philosophy-informed Machine Learning", "comment": null, "summary": "Philosophy-informed machine learning (PhIML) directly infuses core ideas from\nanalytic philosophy into ML model architectures, objectives, and evaluation\nprotocols. Therefore, PhIML promises new capabilities through models that\nrespect philosophical concepts and values by design. From this lens, this paper\nreviews conceptual foundations to demonstrate philosophical gains and\nalignment. In addition, we present case studies on how ML users/designers can\nadopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML\nmodel architectures. Finally, this paper sheds light on open technical barriers\nalongside philosophical, practical, and governance challenges and outlines a\nresearch roadmap toward safe, philosophy-aware, and ethically responsible\nPhIML.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u54f2\u5b66\u6307\u5bfc\u7684\u673a\u5668\u5b66\u4e60(PhIML)\uff0c\u5b83\u5c06\u5206\u6790\u54f2\u5b66\u7684\u6838\u5fc3\u601d\u60f3\u76f4\u63a5\u6ce8\u5165\u5230ML\u6a21\u578b\u67b6\u6784\u3001\u76ee\u6807\u548c\u8bc4\u4f30\u534f\u8bae\u4e2d\u3002", "motivation": "\u901a\u8fc7\u5c0a\u91cd\u54f2\u5b66\u6982\u5ff5\u548c\u4ef7\u503c\u7684\u6a21\u578b\uff0cPhIML\u6709\u671b\u5e26\u6765\u65b0\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u4ee5\u5c55\u793a\u54f2\u5b66\u4e0a\u7684\u6536\u76ca\u548c\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86ML\u7528\u6237/\u8bbe\u8ba1\u8005\u5982\u4f55\u91c7\u7528PhIML\u4f5c\u4e3a\u4e0d\u53ef\u77e5\u7684\u4e8b\u540e\u5de5\u5177\u6216\u5c06\u5176\u5185\u5728\u6784\u5efa\u5230ML\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u672c\u6587\u63ed\u793a\u4e86\u5f00\u653e\u7684\u6280\u672f\u969c\u788d\u4ee5\u53ca\u54f2\u5b66\u3001\u5b9e\u8df5\u548c\u6cbb\u7406\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u6982\u8ff0\u4e86\u8fc8\u5411\u5b89\u5168\u3001\u5177\u6709\u54f2\u5b66\u610f\u8bc6\u548c\u5bf9\u9053\u5fb7\u8d1f\u8d23\u7684PhIML\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002"}}
{"id": "2509.20374", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20374", "abs": "https://arxiv.org/abs/2509.20374", "authors": ["Nithin Somasekharan", "Ling Yue", "Yadi Cao", "Weichao Li", "Patrick Emami", "Pochinapeddi Sai Bhargav", "Anurag Acharya", "Xingyu Xie", "Shaowu Pan"], "title": "CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance across\ngeneral NLP tasks, but their utility in automating numerical experiments of\ncomplex physical system -- a critical and labor-intensive component -- remains\nunderexplored. As the major workhorse of computational science over the past\ndecades, Computational Fluid Dynamics (CFD) offers a uniquely challenging\ntestbed for evaluating the scientific capabilities of LLMs. We introduce\nCFDLLMBench, a benchmark suite comprising three complementary components --\nCFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM\nperformance across three key competencies: graduate-level CFD knowledge,\nnumerical and physical reasoning of CFD, and context-dependent implementation\nof CFD workflows. Grounded in real-world CFD practices, our benchmark combines\na detailed task taxonomy with a rigorous evaluation framework to deliver\nreproducible results and quantify LLM performance across code executability,\nsolution accuracy, and numerical convergence behavior. CFDLLMBench establishes\na solid foundation for the development and evaluation of LLM-driven automation\nof numerical experiments for complex physical systems. Code and data are\navailable at https://github.com/NREL-Theseus/cfdllmbench/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCFDLLMBench\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u9886\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u52a8\u5316\u590d\u6742\u7269\u7406\u7cfb\u7edf\u6570\u503c\u5b9e\u9a8c\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u8fd9\u4e00\u5173\u952e\u4e14\u52b3\u52a8\u5bc6\u96c6\u578b\u9886\u57df\u3002", "method": "\u8bbe\u8ba1\u4e86CFDLLMBench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542bCFDQuery\u3001CFDCodeBench\u548cFoamBench\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff0c\u5168\u9762\u8bc4\u4f30LLM\u5728CFD\u77e5\u8bc6\u3001\u6570\u503c\u548c\u7269\u7406\u63a8\u7406\u4ee5\u53ca\u5de5\u4f5c\u6d41\u7a0b\u5b9e\u73b0\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "CFDLLMBench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u80fd\u591f\u63d0\u4f9b\u53ef\u91cd\u73b0\u7684\u7ed3\u679c\uff0c\u5e76\u91cf\u5316LLM\u5728\u4ee3\u7801\u53ef\u6267\u884c\u6027\u3001\u89e3\u51b3\u65b9\u6848\u51c6\u786e\u6027\u548c\u6570\u503c\u6536\u655b\u884c\u4e3a\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "CFDLLMBench\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u590d\u6742\u7269\u7406\u7cfb\u7edf\u6570\u503c\u5b9e\u9a8c\u81ea\u52a8\u5316\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2509.20427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20427", "abs": "https://arxiv.org/abs/2509.20427", "authors": ["Team Seedream", "Yunpeng Chen", "Yu Gao", "Lixue Gong", "Meng Guo", "Qiushan Guo", "Zhiyao Guo", "Xiaoxia Hou", "Weilin Huang", "Yixuan Huang", "Xiaowen Jian", "Huafeng Kuang", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yanzuo Lu", "Zhengxiong Luo", "Tongtong Ou", "Guang Shi", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Wenxu Wu", "Yonghui Wu", "Xin Xia", "Xuefeng Xiao", "Shuang Xu", "Xin Yan", "Ceyuan Yang", "Jianchao Yang", "Zhonghua Zhai", "Chenlin Zhang", "Heng Zhang", "Qi Zhang", "Xinyu Zhang", "Yuwei Zhang", "Shijia Zhao", "Wenliang Zhao", "Wenjia Zhu"], "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "comment": "Seedream 4.0 Technical Report", "summary": "We introduce Seedream 4.0, an efficient and high-performance multimodal image\ngeneration system that unifies text-to-image (T2I) synthesis, image editing,\nand multi-image composition within a single framework. We develop a highly\nefficient diffusion transformer with a powerful VAE which also can reduce the\nnumber of image tokens considerably. This allows for efficient training of our\nmodel, and enables it to fast generate native high-resolution images (e.g.,\n1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning\ndiverse taxonomies and knowledge-centric concepts. Comprehensive data\ncollection across hundreds of vertical scenarios, coupled with optimized\nstrategies, ensures stable and large-scale training, with strong\ngeneralization. By incorporating a carefully fine-tuned VLM model, we perform\nmulti-modal post-training for training both T2I and image editing tasks\njointly. For inference acceleration, we integrate adversarial distillation,\ndistribution matching, and quantization, as well as speculative decoding. It\nachieves an inference time of up to 1.8 seconds for generating a 2K image\n(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream\n4.0 can achieve state-of-the-art results on both T2I and multimodal image\nediting. In particular, it demonstrates exceptional multimodal capabilities in\ncomplex tasks, including precise image editing and in-context reasoning, and\nalso allows for multi-image reference, and can generate multiple output images.\nThis extends traditional T2I systems into an more interactive and\nmultidimensional creative tool, pushing the boundary of generative AI for both\ncreativity and professional applications. Seedream 4.0 is now accessible on\nhttps://www.volcengine.com/experience/ark?launch=seedream.", "AI": {"tldr": "Seedream 4.0\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\uff0c\u7edf\u4e00\u4e86\u6587\u672c\u5230\u56fe\u50cf\u7684\u5408\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u591a\u56fe\u50cf\u7ec4\u5408\u3002", "motivation": "\u4e3a\u4e86\u521b\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\uff0c\u56fe\u50cf\u7f16\u8f91\u548c\u591a\u56fe\u50cf\u7ec4\u5408\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6269\u6563transformer\u548c\u4e00\u4e2a\u5f3a\u5927\u7684VAE\uff0c\u5e76\u7ed3\u5408\u591a\u6a21\u6001\u540e\u8bad\u7ec3\u3001\u5bf9\u6297\u84b8\u998f\u3001\u5206\u5e03\u5339\u914d\u3001\u91cf\u5316\u548c\u63a8\u6d4b\u89e3\u7801\u7b49\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u4e86\u5feb\u901f\u751f\u6210\u539f\u751f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff08\u59821K-4K\uff09\uff0c\u5e76\u5728T2I\u548c\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u751f\u62102K\u56fe\u50cf\u7684\u63a8\u7406\u65f6\u95f4\u4ec5\u4e3a1.8\u79d2\u3002", "conclusion": "Seedream 4.0\u5c06\u4f20\u7edf\u7684T2I\u7cfb\u7edf\u6269\u5c55\u4e3a\u4e00\u4e2a\u66f4\u5177\u4e92\u52a8\u6027\u548c\u591a\u7ef4\u5ea6\u7684\u521b\u610f\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u4eba\u5de5\u667a\u80fd\u5728\u521b\u610f\u548c\u4e13\u4e1a\u5e94\u7528\u65b9\u9762\u7684\u8fb9\u754c\u3002"}}
{"id": "2509.20422", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.20422", "abs": "https://arxiv.org/abs/2509.20422", "authors": ["Yiling Ma", "Nathan Luke Abraham", "Stefan Versick", "Roland Ruhnke", "Andrea Schneidereit", "Ulrike Niemeier", "Felix Back", "Peter Braesicke", "Peer Nowack"], "title": "mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations", "comment": null, "summary": "Atmospheric ozone is a crucial absorber of solar radiation and an important\ngreenhouse gas. However, most climate models participating in the Coupled Model\nIntercomparison Project (CMIP) still lack an interactive representation of\nozone due to the high computational costs of atmospheric chemistry schemes.\nHere, we introduce a machine learning parameterization (mloz) to interactively\nmodel daily ozone variability and trends across the troposphere and\nstratosphere in standard climate sensitivity simulations, including two-way\ninteractions of ozone with the Quasi-Biennial Oscillation. We demonstrate its\nhigh fidelity on decadal timescales and its flexible use online across two\ndifferent climate models -- the UK Earth System Model (UKESM) and the German\nICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile\ninformation as the only input, mloz produces stable ozone predictions around 31\ntimes faster than the chemistry scheme in UKESM, contributing less than 4\npercent of the respective total climate model runtimes. In particular, we also\ndemonstrate its transferability to different climate models without chemistry\nschemes by transferring the parameterization from UKESM to ICON. This\nhighlights the potential for widespread adoption in CMIP-level climate models\nthat lack interactive chemistry for future climate change assessments,\nparticularly when focusing on climate sensitivity simulations, where ozone\ntrends and variability are known to significantly modulate atmospheric feedback\nprocesses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u53c2\u6570\u5316\u65b9\u6cd5\uff08mloz\uff09\u6765\u6a21\u62df\u5bf9\u6d41\u5c42\u548c\u5e73\u6d41\u5c42\u7684\u6bcf\u65e5\u81ed\u6c27\u53d8\u5316\u548c\u8d8b\u52bf\u3002", "motivation": "\u7531\u4e8e\u5927\u6c14\u5316\u5b66\u65b9\u6848\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u8bb8\u591a\u6c14\u5019\u6a21\u578b\u7f3a\u4e4f\u4ea4\u4e92\u5f0f\u81ed\u6c27\u8868\u793a\u3002", "method": "\u4f7f\u7528\u5927\u6c14\u6e29\u5ea6\u5256\u9762\u4fe1\u606f\u4f5c\u4e3a\u8f93\u5165\uff0cmloz \u5728\u7ebf\u6a21\u62df\u81ed\u6c27\uff0c\u5e76\u4e0e UKESM \u548c ICON \u6a21\u578b\u96c6\u6210\u3002", "result": "mloz \u7684\u901f\u5ea6\u6bd4 UKESM \u4e2d\u7684\u5316\u5b66\u65b9\u6848\u5feb 31 \u500d\uff0c\u4e14\u5728\u6c14\u5019\u6a21\u578b\u8fd0\u884c\u65f6\u95f4\u4e2d\u5360\u6bd4\u4e0d\u5230 4%\u3002", "conclusion": "mloz \u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u3001\u53ef\u79fb\u690d\u6027\uff0c\u5e76\u6709\u6f5c\u529b\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7f3a\u4e4f\u4ea4\u4e92\u5f0f\u5316\u5b66\u8fc7\u7a0b\u7684 CMIP \u7ea7\u522b\u6c14\u5019\u6a21\u578b\u4e2d\u3002"}}
