{"id": "2510.16388", "categories": ["cs.DB", "H.2; H.4.0"], "pdf": "https://arxiv.org/pdf/2510.16388", "abs": "https://arxiv.org/abs/2510.16388", "authors": ["Doriana Armenise", "Ginevra Battello", "Andrea Brunello", "Lorenza Driul", "Angelo Montanari", "Elisa Rizzante", "Nicola Saccomanno", "Andrea Salvador", "Serena Xodo", "Silvia Zermano"], "title": "Unified Peripartum Database with Natural-Language-to-SQL Capabilities at Udine University Hospital: Design and Prototype", "comment": null, "summary": "The fragmentation of obstetric information across electronic health record\nmodules, device repositories, and laboratory systems, as it is common in\nhospitals, hinders both intrapartum care and reproducible research. In this\nwork, we present a practical blueprint for transforming heterogeneous\nperipartum records into computable, queryable assets by designing and\nprototyping a unified peripartum relational database with\nnatural-language-to-SQL (NL2SQL) capabilities at the Obstetrics Clinic of Udine\nUniversity Hospital. Requirements were co-defined with clinicians and\nformalized as an Entity-Relationship diagram, from which the logical schema and\nSQL implementation of the database were then derived. The latter integrates\nheterogeneous sources to connect maternal anamnestic and longitudinal history,\ncurrent-pregnancy findings, intrapartum course, and delivery and neonatal\noutcomes. The NL2SQL layer enables clinicians to pose natural-language queries\nto the system, lowering barriers to audit and exploratory analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u84dd\u56fe\uff0c\u7528\u4e8e\u5c06\u5f02\u6784\u7684\u56f4\u4ea7\u671f\u8bb0\u5f55\u8f6c\u6362\u4e3a\u53ef\u8ba1\u7b97\u3001\u53ef\u67e5\u8be2\u7684\u8d44\u4ea7\u3002", "motivation": "\u533b\u9662\u4e2d\u4ea7\u79d1\u4fe1\u606f\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6a21\u5757\u3001\u8bbe\u5907\u5b58\u50a8\u5e93\u548c\u5b9e\u9a8c\u5ba4\u7cfb\u7edf\u4e2d\u7684\u788e\u7247\u5316\u963b\u788d\u4e86\u5206\u5a29\u8fc7\u7a0b\u4e2d\u7684\u62a4\u7406\u548c\u53ef\u91cd\u590d\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5728\u4e4c\u8fea\u5185\u5927\u5b66\u533b\u9662\u5987\u4ea7\u79d1\u8bca\u6240\u8bbe\u8ba1\u548c\u539f\u578b\u5316\u4e00\u4e2a\u5177\u6709\u81ea\u7136\u8bed\u8a00\u5230SQL (NL2SQL)\u80fd\u529b\u7684\u7edf\u4e00\u56f4\u4ea7\u671f\u5173\u7cfb\u6570\u636e\u5e93\u3002", "result": "\u8be5\u6570\u636e\u5e93\u96c6\u6210\u4e86\u5f02\u6784\u6765\u6e90\uff0c\u4ee5\u8fde\u63a5\u6bcd\u4eb2\u7684\u65e2\u5f80\u53f2\u548c\u7eb5\u5411\u75c5\u53f2\u3001\u5f53\u524d\u598a\u5a20\u7ed3\u679c\u3001\u5206\u5a29\u8fc7\u7a0b\u4ee5\u53ca\u5206\u5a29\u548c\u65b0\u751f\u513f\u7ed3\u5c40\u3002", "conclusion": "NL2SQL\u5c42\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u5411\u7cfb\u7edf\u63d0\u51fa\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u964d\u4f4e\u4e86\u5ba1\u8ba1\u548c\u63a2\u7d22\u6027\u5206\u6790\u7684\u969c\u788d\u3002"}}
{"id": "2510.16470", "categories": ["cs.DB", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16470", "abs": "https://arxiv.org/abs/2510.16470", "authors": ["Elham Khabiri", "Jeffrey O. Kephart", "Fenno F. Heath III", "Srideepika Jayaraman", "Fateh A. Tipu", "Yingjie Li", "Dhruv Shah", "Achille Fokoue", "Anu Bhamidipaty"], "title": "Declarative Techniques for NL Queries over Heterogeneous Data", "comment": null, "summary": "In many industrial settings, users wish to ask questions in natural language,\nthe answers to which require assembling information from diverse structured\ndata sources. With the advent of Large Language Models (LLMs), applications can\nnow translate natural language questions into a set of API calls or database\ncalls, execute them, and combine the results into an appropriate natural\nlanguage response. However, these applications remain impractical in realistic\nindustrial settings because they do not cope with the data source heterogeneity\nthat typifies such environments. In this work, we simulate the heterogeneity of\nreal industry settings by introducing two extensions of the popular Spider\nbenchmark dataset that require a combination of database and API calls. Then,\nwe introduce a declarative approach to handling such data heterogeneity and\ndemonstrate that it copes with data source heterogeneity significantly better\nthan state-of-the-art LLM-based agentic or imperative code generation systems.\nOur augmented benchmarks are available to the research community.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u6210API\u6216\u6570\u636e\u5e93\u8c03\u7528\uff0c\u7136\u540e\u6267\u884c\u5b83\u4eec\uff0c\u5e76\u5c06\u7ed3\u679c\u7ec4\u5408\u6210\u5408\u9002\u7684\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\u3002\u4f46\u662f\uff0c\u8fd9\u4e9b\u5e94\u7528\u7a0b\u5e8f\u5728\u5b9e\u9645\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u4ecd\u7136\u4e0d\u5b9e\u7528\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u5904\u7406\u6570\u636e\u6e90\u5f02\u6784\u6027\u3002\u672c\u6587\u901a\u8fc7\u5f15\u5165\u9700\u8981\u7ec4\u5408\u6570\u636e\u5e93\u548cAPI\u8c03\u7528\u7684Spider\u57fa\u51c6\u6570\u636e\u96c6\u7684\u4e24\u4e2a\u6269\u5c55\u6765\u6a21\u62df\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u7684\u5f02\u6784\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u58f0\u660e\u5f0f\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u6570\u636e\u5f02\u6784\u6027\uff0c\u5e76\u8868\u660e\u5b83\u6bd4\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6216\u547d\u4ee4\u5f0f\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u66f4\u597d\u5730\u5904\u7406\u6570\u636e\u6e90\u5f02\u6784\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u6570\u636e\u6e90\u5f02\u6784\u6027", "method": "\u5f15\u5165Spider\u57fa\u51c6\u6570\u636e\u96c6\u7684\u4e24\u4e2a\u6269\u5c55\u6765\u6a21\u62df\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u7684\u5f02\u6784\u6027\uff0c\u5e76\u5f15\u5165\u58f0\u660e\u5f0f\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u6570\u636e\u5f02\u6784\u6027", "result": "\u58f0\u660e\u5f0f\u65b9\u6cd5\u6bd4\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6216\u547d\u4ee4\u5f0f\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u66f4\u597d\u5730\u5904\u7406\u6570\u636e\u6e90\u5f02\u6784\u6027", "conclusion": "\u589e\u5f3a\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ef\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528"}}
{"id": "2510.17089", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17089", "abs": "https://arxiv.org/abs/2510.17089", "authors": ["Christian Imenkamp", "Andrea Maldonado", "Hendrik Reiter", "Martin Werner", "Wilhelm Hasselbring", "Agnes Koschmider", "Andrea Burattin"], "title": "AVOCADO: The Streaming Process Mining Challenge", "comment": "12 pages, 4 figures", "summary": "Streaming process mining deals with the real-time analysis of streaming data.\nEvent streams require algorithms capable of processing data incrementally. To\nsystematically address the complexities of this domain, we propose AVOCADO, a\nstandardized challenge framework that provides clear structural divisions:\nseparating the concept and instantiation layers of challenges in streaming\nprocess mining for algorithm evaluation. The AVOCADO evaluates algorithms on\nstreaming-specific metrics like accuracy, Mean Absolute Error (MAE), Root Mean\nSquare Error (RMSE), Processing Latency, and robustness. This initiative seeks\nto foster innovation and community-driven discussions to advance the field of\nstreaming process mining. We present this framework as a foundation and invite\nthe community to contribute to its evolution by suggesting new challenges, such\nas integrating metrics for system throughput and memory consumption, and\nexpanding the scope to address real-world stream complexities like out-of-order\nevent arrival.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d41\u5f0f\u8fc7\u7a0b\u6316\u6398\u7684\u6807\u51c6\u6311\u6218\u6846\u67b6AVOCADO\uff0c\u7528\u4e8e\u7b97\u6cd5\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u4e86\u7cfb\u7edf\u5730\u89e3\u51b3\u6d41\u5f0f\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u7684\u590d\u6742\u6027\uff0c\u63d0\u51faAVOCADO\u6846\u67b6\u3002", "method": "AVOCADO\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u6982\u5ff5\u5c42\u548c\u5b9e\u4f8b\u5316\u5c42\u6765\u6784\u5efa\u6311\u6218\uff0c\u5e76\u4f7f\u7528\u6d41\u5f0f\u7279\u5b9a\u7684\u6307\u6807\uff08\u5982\u51c6\u786e\u6027\u3001MAE\u3001RMSE\u3001\u5904\u7406\u5ef6\u8fdf\u548c\u9c81\u68d2\u6027\uff09\u8bc4\u4f30\u7b97\u6cd5\u3002", "result": "AVOCADO\u6846\u67b6\u65e8\u5728\u4fc3\u8fdb\u521b\u65b0\u548c\u793e\u533a\u9a71\u52a8\u7684\u8ba8\u8bba\uff0c\u4ee5\u63a8\u8fdb\u6d41\u5f0f\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "AVOCADO\u6846\u67b6\u4e3a\u6d41\u5f0f\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u9080\u8bf7\u793e\u533a\u901a\u8fc7\u5efa\u8bae\u65b0\u7684\u6311\u6218\u6765\u8d21\u732e\u5176\u53d1\u5c55\uff0c\u4f8b\u5982\u96c6\u6210\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u5185\u5b58\u6d88\u8017\u7684\u6307\u6807\uff0c\u5e76\u6269\u5927\u8303\u56f4\u4ee5\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u7684\u6d41\u590d\u6742\u6027\uff08\u5982\u4e71\u5e8f\u4e8b\u4ef6\u5230\u8fbe\uff09\u3002"}}
{"id": "2510.17301", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17301", "abs": "https://arxiv.org/abs/2510.17301", "authors": ["Panos Kalnis. Shuo Shang", "Christian S. Jensen"], "title": "Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models", "comment": "5 pages", "summary": "Spatio-temporal data captures complex dynamics across both space and time,\nyet traditional visualizations are complex, require domain expertise and often\nfail to resonate with broader audiences. Here, we propose MapMuse, a\nstorytelling-based framework for interpreting spatio-temporal datasets,\ntransforming them into compelling, narrative-driven experiences. We utilize\nlarge language models and employ retrieval augmented generation (RAG) and\nagent-based techniques to generate comprehensive stories. Drawing on principles\ncommon in cinematic storytelling, we emphasize clarity, emotional connection,\nand audience-centric design. As a case study, we analyze a dataset of taxi\ntrajectories. Two perspectives are presented: a captivating story based on a\nheat map that visualizes millions of taxi trip endpoints to uncover urban\nmobility patterns; and a detailed narrative following a single long taxi\njourney, enriched with city landmarks and temporal shifts. By portraying\nlocations as characters and movement as plot, we argue that data storytelling\ndrives insight, engagement, and action from spatio-temporal information. The\ncase study illustrates how MapMuse can bridge the gap between data complexity\nand human understanding. The aim of this short paper is to provide a glimpse to\nthe potential of the cinematic storytelling technique as an effective\ncommunication tool for spatio-temporal data, as well as to describe open\nproblems and opportunities for future research.", "AI": {"tldr": "MapMuse\u662f\u4e00\u4e2a\u57fa\u4e8e\u6545\u4e8b\u8bb2\u8ff0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5f15\u4eba\u5165\u80dc\u7684\u53d9\u4e8b\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u590d\u6742\uff0c\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u4e14\u901a\u5e38\u4e0d\u80fd\u5f15\u8d77\u66f4\u5e7f\u6cdb\u53d7\u4f17\u7684\u5171\u9e23\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u65f6\u7a7a\u6570\u636e\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6280\u672f\u6765\u751f\u6210\u5168\u9762\u7684\u6545\u4e8b\u3002\u501f\u9274\u7535\u5f71\u6545\u4e8b\u8bb2\u8ff0\u4e2d\u5e38\u89c1\u7684\u539f\u5219\uff0c\u5f3a\u8c03\u6e05\u6670\u5ea6\u3001\u60c5\u611f\u8fde\u63a5\u548c\u4ee5\u53d7\u4f17\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u5c06\u5730\u70b9\u63cf\u7ed8\u6210\u89d2\u8272\uff0c\u5c06\u8fd0\u52a8\u63cf\u7ed8\u6210\u60c5\u8282\uff0c\u6570\u636e\u6545\u4e8b\u8bb2\u8ff0\u80fd\u591f\u9a71\u52a8\u5bf9\u65f6\u7a7a\u4fe1\u606f\u7684\u6d1e\u5bdf\u3001\u53c2\u4e0e\u548c\u884c\u52a8\u3002", "conclusion": "\u7535\u5f71\u6545\u4e8b\u8bb2\u8ff0\u6280\u672f\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u65f6\u7a7a\u6570\u636e\u4ea4\u6d41\u5de5\u5177\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u63cf\u8ff0\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u95ee\u9898\u548c\u673a\u4f1a\u3002"}}
{"id": "2510.16334", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16334", "abs": "https://arxiv.org/abs/2510.16334", "authors": ["Eden Shaveet", "Crystal Su", "Daniel Hsu", "Luis Gravano"], "title": "Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)", "comment": "Presented as a poster at Data Science Day 2024", "summary": "Foodborne illnesses are gastrointestinal conditions caused by consuming\ncontaminated food. Restaurants are critical venues to investigate outbreaks\nbecause they share sourcing, preparation, and distribution of foods. Public\nreporting of illness via formal channels is limited, whereas social media\nplatforms host abundant user-generated content that can provide timely public\nhealth signals. This paper analyzes signals from Yelp reviews produced by a\nHierarchical Sigmoid Attention Network (HSAN) classifier and compares them with\nofficial restaurant inspection outcomes issued by the New York City Department\nof Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at\nthe Census tract level, compare distributions of HSAN scores by prevalence of\nC-graded restaurants, and map spatial patterns across NYC. We find minimal\ncorrelation between HSAN signals and inspection scores at the tract level and\nno significant differences by number of C-graded restaurants. We discuss\nimplications and outline next steps toward address-level analyses.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528Yelp\u8bc4\u8bba\u4e2d\u7684\u4fe1\u53f7\u6765\u5206\u6790\u98df\u7269\u4f20\u64ad\u75be\u75c5\u4e0e\u9910\u9986\u536b\u751f\u68c0\u67e5\u7ed3\u679c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f7f\u7528\u4e86\u5c42\u7ea7Sigmoid\u6ce8\u610f\u529b\u7f51\u7edc\uff08HSAN\uff09\u5206\u7c7b\u5668\u3002", "motivation": "\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u53ef\u4ee5\u83b7\u53d6\u53ca\u65f6\u7684\u516c\u4f17\u5065\u5eb7\u4fe1\u53f7\uff0c\u5c24\u5176\u662f\u5728\u9910\u9986\u98df\u7269\u4f20\u64ad\u75be\u75c5\u7206\u53d1\u7684\u60c5\u51b5\u4e0b\uff0c\u4f46\u6b63\u5f0f\u6e20\u9053\u7684\u516c\u5171\u62a5\u544a\u6709\u9650\u3002", "method": "\u4f7f\u7528\u5c42\u7ea7Sigmoid\u6ce8\u610f\u529b\u7f51\u7edc\uff08HSAN\uff09\u5206\u7c7b\u5668\u5206\u6790Yelp\u8bc4\u8bba\u4e2d\u7684\u4fe1\u53f7\uff0c\u5e76\u5c06\u5176\u4e0e\u7ebd\u7ea6\u5e02\u536b\u751f\u4e0e\u7cbe\u795e\u536b\u751f\u5c40\uff08NYC DOHMH\uff09\u53d1\u5e03\u7684\u5b98\u65b9\u9910\u9986\u68c0\u67e5\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u4eba\u53e3\u666e\u67e5\u533a\u7ea7\u522b\u4e0a\uff0cHSAN\u4fe1\u53f7\u4e0e\u68c0\u67e5\u5206\u6570\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6781\u5c0f\uff0c\u5e76\u4e14C\u7b49\u7ea7\u9910\u9986\u6570\u91cf\u6ca1\u6709\u663e\u7740\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cYelp\u8bc4\u8bba\u4fe1\u53f7\u4e0e\u9910\u9986\u68c0\u67e5\u7ed3\u679c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6709\u9650\uff0c\u9700\u8981\u5728\u5730\u5740\u7ea7\u522b\u4e0a\u8fdb\u884c\u8fdb\u4e00\u6b65\u5206\u6790\u3002"}}
{"id": "2510.15963", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15963", "abs": "https://arxiv.org/abs/2510.15963", "authors": ["Jiani Huang", "Amish Sethi", "Matthew Kuo", "Mayank Keoliya", "Neelay Velingker", "JungHo Jung", "Ser-Nam Lim", "Ziyang Li", "Mayur Naik"], "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "comment": "Accepted as a Spotlight Paper at NeurIPS 2025", "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, current training pipelines primarily\nrely on high-level vision-sound-text pairs and lack fine-grained, structured\nalignment between pixel-level visual content and textual semantics. To overcome\nthis challenge, we propose ESCA, a new framework for contextualizing embodied\nagents through structured spatial-temporal understanding. At its core is\nSGClip, a novel CLIP-based, open-domain, and promptable model for generating\nscene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic\nlearning pipeline, which harnesses model-driven self-supervision from\nvideo-caption pairs and structured reasoning, thereby eliminating the need for\nhuman-labeled scene graph annotations. We demonstrate that SGClip supports both\nprompt-based inference and task-specific fine-tuning, excelling in scene graph\ngeneration and action localization benchmarks. ESCA with SGClip consistently\nimproves both open-source and commercial MLLMs, achieving state-of-the-art\nperformance across two embodied environments. Notably, it significantly reduces\nagent perception errors and enables open-source models to surpass proprietary\nbaselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ESCA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u65f6\u7a7a\u7406\u89e3\u6765\u589e\u5f3a\u5177\u8eab\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3pipeline\u7f3a\u4e4f\u50cf\u7d20\u7ea7\u89c6\u89c9\u5185\u5bb9\u548c\u6587\u672c\u8bed\u4e49\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSGClip\u7684CLIP\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60pipeline\u572887K+\u5f00\u653e\u57df\u89c6\u9891\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u573a\u666f\u56fe\u3002", "result": "SGClip\u5728\u573a\u666f\u56fe\u751f\u6210\u548c\u52a8\u4f5c\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002ESCA\u4e0eSGClip\u7ed3\u5408\uff0c\u6301\u7eed\u6539\u8fdb\u4e86\u5f00\u6e90\u548c\u5546\u4e1aMLLM\uff0c\u5e76\u5728\u4e24\u4e2a\u5177\u8eab\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ESCA\u663e\u8457\u51cf\u5c11\u4e86\u4ee3\u7406\u611f\u77e5\u9519\u8bef\uff0c\u5e76\u4f7f\u5f00\u6e90\u6a21\u578b\u8d85\u8d8a\u4e86\u4e13\u6709\u57fa\u7ebf\u3002"}}
{"id": "2510.15972", "categories": ["cs.CL", "cs.AI", "81P68 (Primary), 68T50, 68T07 (Secondary)", "I.2.7; F.1.2"], "pdf": "https://arxiv.org/pdf/2510.15972", "abs": "https://arxiv.org/abs/2510.15972", "authors": ["Ling Sun", "Peter Sullivan", "Michael Martin", "Yun Zhou"], "title": "Quantum NLP models on Natural Language Inference", "comment": "Accepted, presented, and to appear in the Proceedings of the Quantum\n  AI and NLP 2025 Conference", "summary": "Quantum natural language processing (QNLP) offers a novel approach to\nsemantic modeling by embedding compositional structure directly into quantum\ncircuits. This paper investigates the application of QNLP models to the task of\nNatural Language Inference (NLI), comparing quantum, hybrid, and classical\ntransformer-based models under a constrained few-shot setting. Using the lambeq\nlibrary and the DisCoCat framework, we construct parameterized quantum circuits\nfor sentence pairs and train them for both semantic relatedness and inference\nclassification. To assess efficiency, we introduce a novel\ninformation-theoretic metric, Information Gain per Parameter (IGPP), which\nquantifies learning dynamics independent of model size. Our results demonstrate\nthat quantum models achieve performance comparable to classical baselines while\noperating with dramatically fewer parameters. The Quantum-based models\noutperform randomly initialized transformers in inference and achieve lower\ntest error on relatedness tasks. Moreover, quantum models exhibit significantly\nhigher per-parameter learning efficiency (up to five orders of magnitude more\nthan classical counterparts), highlighting the promise of QNLP in low-resource,\nstructure-sensitive settings. To address circuit-level isolation and promote\nparameter sharing, we also propose a novel cluster-based architecture that\nimproves generalization by tying gate parameters to learned word clusters\nrather than individual tokens.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08QNLP\uff09\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6bd4\u8f83\u4e86\u91cf\u5b50\u3001\u6df7\u5408\u548c\u57fa\u4e8e\u7ecf\u5178Transformer\u7684\u6a21\u578b\u5728\u53d7\u9650\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22QNLP\u5728\u8bed\u4e49\u5efa\u6a21\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528lambeq\u5e93\u548cDisCoCat\u6846\u67b6\u6784\u5efa\u53c2\u6570\u5316\u7684\u91cf\u5b50\u7535\u8def\uff0c\u7528\u4e8e\u53e5\u5b50\u5bf9\u7684\u8bed\u4e49\u76f8\u5173\u6027\u548c\u63a8\u7406\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u805a\u7c7b\u7684\u67b6\u6784\uff0c\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u6a21\u578b\u5728\u53c2\u6570\u6570\u91cf\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4e0e\u7ecf\u5178\u57fa\u7ebf\u76f8\u5f53\u3002\u91cf\u5b50\u6a21\u578b\u5728\u63a8\u7406\u65b9\u9762\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684Transformer\uff0c\u5e76\u5728\u76f8\u5173\u6027\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u6d4b\u8bd5\u8bef\u5dee\u3002\u91cf\u5b50\u6a21\u578b\u8fd8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5355\u53c2\u6570\u5b66\u4e60\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u662fQNLP\u5728\u4f4e\u8d44\u6e90\u3001\u7ed3\u6784\u654f\u611f\u7684\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u5176\u66f4\u9ad8\u7684\u5355\u53c2\u6570\u5b66\u4e60\u6548\u7387\u3002\u63d0\u51fa\u7684\u57fa\u4e8e\u805a\u7c7b\u7684\u67b6\u6784\u901a\u8fc7\u5c06\u95e8\u53c2\u6570\u4e0e\u5b66\u4e60\u7684\u8bcd\u7c07\u8054\u7cfb\u8d77\u6765\uff0c\u6539\u5584\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a VisuoAlign \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\uff0c\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u7684\u6811\u641c\u7d22\u6765\u63d0\u9ad8 LVLMs \u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709 LVLMs \u5728\u591a\u6a21\u6001\u611f\u77e5\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5b89\u5168\u6027\u5bf9\u9f50\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u89c6\u89c9\u8f93\u5165\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u63a8\u7406\u94fe\u7f3a\u4e4f\u5b89\u5168\u76d1\u7763\uff0c\u5e76\u4e14\u5bf9\u9f50\u901a\u5e38\u5728\u6a21\u6001\u878d\u5408\u4e0b\u4f1a\u964d\u4f4e\u3002", "method": "VisuoAlign \u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u4ea4\u4e92\u63d0\u793a\u5c06\u5b89\u5168\u7ea6\u675f\u5d4c\u5165\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22 (MCTS) \u6765\u7cfb\u7edf\u5730\u6784\u5efa\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u4ee5\u786e\u4fdd\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u548c\u5408\u89c4\u54cd\u5e94\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVisuoAlign \u4e3b\u52a8\u66b4\u9732\u98ce\u9669\uff0c\u5b9e\u73b0\u5168\u9762\u7684\u6570\u636e\u96c6\u751f\u6210\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86 LVLMs \u62b5\u6297\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VisuoAlign \u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u9ad8 LVLMs \u7684\u5b89\u5168\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.15940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15940", "abs": "https://arxiv.org/abs/2510.15940", "authors": ["Jialin Lu", "Kye Emond", "Kaiyu Yang", "Swarat Chaudhuri", "Weiran Sun", "Wuyang Chen"], "title": "Lean Finder: Semantic Search for Mathlib That Understands User Intents", "comment": null, "summary": "We present Lean Finder, a semantic search engine for Lean and mathlib that\nunderstands and aligns with the intents of mathematicians. Progress in formal\ntheorem proving is often hindered by the difficulty of locating relevant\ntheorems and the steep learning curve of the Lean 4 language, making\nadvancement slow and labor-intensive. Existing Lean search engines, though\nhelpful, rely primarily on informalizations (natural language translation of\nthe formal statements), while largely overlooking the mismatch with real-world\nuser queries. In contrast, we propose a user-centered semantic search tailored\nto the needs of mathematicians. Our approach begins by analyzing and clustering\nthe semantics of public Lean discussions, then fine-tuning text embeddings on\nsynthesized queries that emulate user intents. We further align Lean Finder\nwith mathematicians' preferences using diverse feedback signals, encoding it\nwith a rich awareness of their goals from multiple perspectives. Evaluations on\nreal-world queries, informalized statements, and proof states demonstrate that\nour Lean Finder achieves over $30\\%$ relative improvement compared to previous\nsearch engines and GPT-4o. In addition, Lean Finder is compatible with\nLLM-based theorem provers, bridging retrieval with formal reasoning. Lean\nFinder is available at: https://leanfinder.github.io", "AI": {"tldr": "Lean Finder\u662f\u4e00\u4e2a\u8bed\u4e49\u641c\u7d22\u5f15\u64ce\uff0c\u4e13\u4e3aLean\u548cmathlib\u8bbe\u8ba1\uff0c\u80fd\u591f\u7406\u89e3\u6570\u5b66\u5bb6\u7684\u610f\u56fe\u3002", "motivation": "\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u8fdb\u5c55\u7f13\u6162\uff0c\u539f\u56e0\u5728\u4e8e\u96be\u4ee5\u627e\u5230\u76f8\u5173\u5b9a\u7406\u4ee5\u53caLean 4\u8bed\u8a00\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3002\u73b0\u6709\u7684Lean\u641c\u7d22\u5f15\u64ce\u4e3b\u8981\u4f9d\u8d56\u4e8e\u975e\u5f62\u5f0f\u5316\uff0c\u5ffd\u7565\u4e86\u4e0e\u5b9e\u9645\u7528\u6237\u67e5\u8be2\u7684\u4e0d\u5339\u914d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u548c\u805a\u7c7b\u516c\u5f00\u7684Lean\u8ba8\u8bba\u7684\u8bed\u4e49\uff0c\u7136\u540e\u5728\u6a21\u62df\u7528\u6237\u610f\u56fe\u7684\u5408\u6210\u67e5\u8be2\u4e0a\u5fae\u8c03\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\u641c\u7d22\u3002", "result": "\u5728\u771f\u5b9e\u67e5\u8be2\u3001\u975e\u5f62\u5f0f\u5316\u9648\u8ff0\u548c\u8bc1\u660e\u72b6\u6001\u7684\u8bc4\u4f30\u4e2d\uff0cLean Finder\u6bd4\u4ee5\u524d\u7684\u641c\u7d22\u5f15\u64ce\u548cGPT-4o\u63d0\u9ad8\u4e8630%\u4ee5\u4e0a\u3002", "conclusion": "Lean Finder\u4e0e\u57fa\u4e8eLLM\u7684\u5b9a\u7406\u8bc1\u660e\u5668\u517c\u5bb9\uff0c\u5c06\u68c0\u7d22\u4e0e\u5f62\u5f0f\u63a8\u7406\u8fde\u63a5\u8d77\u6765\u3002"}}
{"id": "2510.17326", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17326", "abs": "https://arxiv.org/abs/2510.17326", "authors": ["Kun Yu", "Jiabao Jin", "Xiaoyao Zhong", "Peng Cheng", "Lei Chen", "Zhitao Shen", "Jingkuan Song", "Hengtao Shen", "Xuemin Lin"], "title": "Approximate Nearest Neighbor Search of Large Scale Vectors on Distributed Storage", "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS) in high-dimensional space is an\nessential operator in many online services, such as information retrieval and\nrecommendation. Indices constructed by the state-of-the-art ANNS algorithms\nmust be stored in single machine's memory or disk for high recall rate and\nthroughput, suffering from substantial storage cost, constraint of limited\nscale and single point of failure. While distributed storage can provide a\ncost-effective and robust solution, there is no efficient and effective\nalgorithms for indexing vectors in distributed storage scenarios. In this\npaper, we present a new graph-cluster hybrid indexing and search system which\nsupports Distributed Storage Approximate Nearest Neighbor Search, called DSANN.\nDSANN can efficiently index, store, search billion-scale vector database in\ndistributed storage and guarantee the high availability of index service. DSANN\nemploys the concurrent index construction method to significantly reduces the\ncomplexity of index building. Then, DSANN applies Point Aggregation Graph to\nleverage the structural information of graph to aggregate similar vectors,\noptimizing storage efficiency and improving query throughput via asynchronous\nI/O in distributed storage. Through extensive experiments, we demonstrate DSANN\ncan efficiently and effectively index, store and search large-scale vector\ndatasets in distributed storage scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DSANN \u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709 ANNS \u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u5411\u91cf\u6570\u636e\u65f6\u9762\u4e34\u7684\u5b58\u50a8\u6210\u672c\u9ad8\u3001\u89c4\u6a21\u53d7\u9650\u548c\u5355\u70b9\u6545\u969c\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709 ANNS \u7b97\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65f6\uff0c\u9762\u4e34\u5b58\u50a8\u6210\u672c\u9ad8\u3001\u89c4\u6a21\u53d7\u9650\u548c\u5355\u70b9\u6545\u969c\u7b49\u95ee\u9898\uff1b\u5206\u5e03\u5f0f\u5b58\u50a8\u53ef\u4ee5\u63d0\u4f9b\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u7d22\u5f15\u7b97\u6cd5\u3002", "method": "DSANN \u91c7\u7528\u56fe\u805a\u7c7b\u6df7\u5408\u7d22\u5f15\u548c\u641c\u7d22\u7cfb\u7edf\uff0c\u5229\u7528\u5e76\u53d1\u7d22\u5f15\u6784\u5efa\u65b9\u6cd5\u964d\u4f4e\u7d22\u5f15\u6784\u5efa\u7684\u590d\u6742\u6027\uff0c\u5e76\u5e94\u7528\u70b9\u805a\u5408\u56fe\u6765\u4f18\u5316\u5b58\u50a8\u6548\u7387\u548c\u63d0\u9ad8\u67e5\u8be2\u541e\u5410\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDSANN \u80fd\u591f\u9ad8\u6548\u5730\u7d22\u5f15\u3001\u5b58\u50a8\u548c\u641c\u7d22\u5206\u5e03\u5f0f\u5b58\u50a8\u573a\u666f\u4e2d\u7684\u5927\u89c4\u6a21\u5411\u91cf\u6570\u636e\u96c6\u3002", "conclusion": "DSANN \u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u5411\u91cf\u6570\u636e\uff0c\u5e76\u4fdd\u8bc1\u7d22\u5f15\u670d\u52a1\u7684\u9ad8\u53ef\u7528\u6027\u3002"}}
{"id": "2510.16393", "categories": ["cs.IR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.16393", "abs": "https://arxiv.org/abs/2510.16393", "authors": ["Franco Maria Nardini", "Raffaele Perego", "Nicola Tonellotto", "Salvatore Trani"], "title": "Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades", "comment": null, "summary": "We investigate the exploitation of both lexical and neural relevance signals\nfor ad-hoc passage retrieval. Our exploration involves a large-scale training\ndataset in which dense neural representations of MS-MARCO queries and passages\nare complemented and integrated with 253 hand-crafted lexical features\nextracted from the same corpus. Blending of the relevance signals from the two\ndifferent groups of features is learned by a classical Learning-to-Rank (LTR)\nmodel based on a forest of decision trees. To evaluate our solution, we employ\na pipelined architecture where a dense neural retriever serves as the first\nstage and performs a nearest-neighbor search over the neural representations of\nthe documents. Our LTR model acts instead as the second stage that re-ranks the\nset of candidates retrieved by the first stage to enhance effectiveness. The\nresults of reproducible experiments conducted with state-of-the-art dense\nretrievers on publicly available resources show that the proposed solution\nsignificantly enhances the end-to-end ranking performance while relatively\nminimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of\nup to 11% with an increase in average query latency of only 4.3%. This confirms\nthe advantage of seamlessly combining two distinct families of signals that\nmutually contribute to retrieval effectiveness.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u8bcd\u6c47\u548c\u795e\u7ecf\u76f8\u5173\u6027\u4fe1\u53f7\u8fdb\u884c ad-hoc \u6bb5\u843d\u68c0\u7d22\u3002", "motivation": "\u63a2\u7d22\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5176\u4e2d MS-MARCO \u67e5\u8be2\u548c\u6bb5\u843d\u7684\u5bc6\u96c6\u795e\u7ecf\u8868\u793a\u4e0e\u4ece\u540c\u4e00\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6\u7684 253 \u4e2a\u624b\u5de5\u5236\u4f5c\u7684\u8bcd\u6c47\u7279\u5f81\u8fdb\u884c\u8865\u5145\u548c\u96c6\u6210\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u51b3\u7b56\u6811\u68ee\u6797\u7684\u7ecf\u5178 Learning-to-Rank (LTR) \u6a21\u578b\u5b66\u4e60\u6765\u81ea\u4e24\u7ec4\u4e0d\u540c\u7279\u5f81\u7684\u76f8\u5173\u6027\u4fe1\u53f7\u7684\u6df7\u5408\u3002\u91c7\u7528\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u5176\u4e2d\u5bc6\u96c6\u795e\u7ecf\u68c0\u7d22\u5668\u4f5c\u4e3a\u7b2c\u4e00\u9636\u6bb5\uff0c\u5e76\u5bf9\u6587\u6863\u7684\u795e\u7ecf\u8868\u793a\u6267\u884c\u6700\u8fd1\u90bb\u641c\u7d22\u3002LTR \u6a21\u578b\u5145\u5f53\u7b2c\u4e8c\u9636\u6bb5\uff0c\u91cd\u65b0\u6392\u5217\u7b2c\u4e00\u9636\u6bb5\u68c0\u7d22\u5230\u7684\u5019\u9009\u96c6\u4ee5\u63d0\u9ad8\u6709\u6548\u6027\u3002", "result": "\u5728\u516c\u5f00\u53ef\u7528\u7684\u8d44\u6e90\u4e0a\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5bc6\u96c6\u68c0\u7d22\u5668\u8fdb\u884c\u7684\u53ef\u91cd\u590d\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u663e\u7740\u63d0\u9ad8\u4e86\u7aef\u5230\u7aef\u6392\u540d\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u6548\u7387\u7684\u5f71\u54cd\u76f8\u5bf9\u6700\u5c0f\u3002nDCG@10 \u63d0\u9ad8\u4e86 11%\uff0c\u5e73\u5747\u67e5\u8be2\u5ef6\u8fdf\u4ec5\u589e\u52a0\u4e86 4.3%\u3002", "conclusion": "\u8bc1\u5b9e\u4e86\u65e0\u7f1d\u7ed3\u5408\u4e24\u4e2a\u4e0d\u540c\u4fe1\u53f7\u7cfb\u5217\u7684\u4f18\u52bf\uff0c\u8fd9\u4e24\u4e2a\u4fe1\u53f7\u7cfb\u5217\u76f8\u4e92\u4fc3\u8fdb\u4e86\u68c0\u7d22\u6548\u679c\u3002"}}
{"id": "2510.15991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15991", "abs": "https://arxiv.org/abs/2510.15991", "authors": ["Huiming Yang"], "title": "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection", "comment": "13 pages", "summary": "The sparse cross-modality detector offers more advantages than its\ncounterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of\nadaptability for downstream tasks and computational cost savings. However,\nexisting sparse detectors overlook the quality of token representation, leaving\nit with a sub-optimal foreground quality and limited performance. In this\npaper, we identify that the geometric structure preserved and the class\ndistribution are the key to improving the performance of the sparse detector,\nand propose a Sparse Selector (SS). The core module of SS is Ray-Aware\nSupervision (RAS), which preserves rich geometric information during the\ntraining stage, and Class-Balanced Supervision, which adaptively reweights the\nsalience of class semantics, ensuring that tokens associated with small objects\nare retained during token sampling. Thereby, outperforming other sparse\nmulti-modal detectors in the representation of tokens. Additionally, we design\nRay Positional Encoding (Ray PE) to address the distribution differences\nbetween the LiDAR modality and the image. Finally, we integrate the\naforementioned module into an end-to-end sparse multi-modality detector, dubbed\nCrossRay3D. Experiments show that, on the challenging nuScenes benchmark,\nCrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,\nwhile running 1.84 faster than other leading methods. Moreover, CrossRay3D\ndemonstrates strong robustness even in scenarios where LiDAR or camera data are\npartially or entirely missing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrossRay3D\u7684\u7a00\u758f\u8de8\u6a21\u6001\u68c0\u6d4b\u5668\uff0c\u5b83\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6bd4\u5176\u4ed6\u9886\u5148\u65b9\u6cd5\u8fd0\u884c\u901f\u5ea6\u5feb1.84\u500d\u3002\u5373\u4f7f\u5728LiDAR\u6216\u76f8\u673a\u6570\u636e\u90e8\u5206\u6216\u5b8c\u5168\u4e22\u5931\u7684\u60c5\u51b5\u4e0b\uff0cCrossRay3D\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u68c0\u6d4b\u5668\u5ffd\u7565\u4e86token\u8868\u793a\u7684\u8d28\u91cf\uff0c\u5bfc\u81f4\u524d\u666f\u8d28\u91cf\u6b20\u4f73\u4e14\u6027\u80fd\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a00\u758f\u9009\u62e9\u5668\uff08SS\uff09\uff0c\u5176\u6838\u5fc3\u6a21\u5757\u662f\u5c04\u7ebf\u611f\u77e5\u76d1\u7763\uff08RAS\uff09\u548c\u7c7b\u5e73\u8861\u76d1\u7763\uff0c\u4ee5\u53ca\u5c04\u7ebf\u4f4d\u7f6e\u7f16\u7801\uff08Ray PE\uff09\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCrossRay3D\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cmAP\u4e3a72.4\uff0cNDS\u4e3a74.7\u3002", "conclusion": "CrossRay3D\u5728\u7a00\u758f\u8de8\u6a21\u6001\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16057", "abs": "https://arxiv.org/abs/2510.16057", "authors": ["Md Kamrul Siam", "Md Jobair Hossain Faruk", "Jerry Q. Cheng", "Huanying Gu"], "title": "Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus", "comment": "7 pages (Accepted to IEEE BHI 2025)", "summary": "This study presents a novel multi-model fusion framework leveraging two\nstate-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance\nthe reliability of chest X-ray interpretation on the CheXpert dataset. From the\nfull CheXpert corpus of 224,316 chest radiographs, we randomly selected 234\nradiologist-annotated studies to evaluate unimodal performance using image-only\nprompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of\n62.8% and 76.9%, respectively. A similarity-based consensus approach, using a\n95% output similarity threshold, improved accuracy to 77.6%. To assess the\nimpact of multimodal inputs, we then generated synthetic clinical notes\nfollowing the MIMIC-CXR template and evaluated a separate subset of 50 randomly\nselected cases paired with both images and synthetic text. On this multimodal\ncohort, performance improved to 84% for ChatGPT and 76% for Claude, while\nconsensus accuracy reached 91.3%. Across both experimental conditions,\nagreement-based fusion consistently outperformed individual models. These\nfindings highlight the utility of integrating complementary modalities and\nusing output-level consensus to improve the trustworthiness and clinical\nutility of AI-assisted radiological diagnosis, offering a practical path to\nreduce diagnostic errors with minimal computational overhead.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u5229\u7528 ChatGPT \u548c Claude \u4e24\u79cd\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u63d0\u9ad8 CheXpert \u6570\u636e\u96c6\u4e0a\u80f8\u90e8 X \u5149\u7247\u5224\u8bfb\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u9ad8\u80f8\u90e8X\u5149\u7247\u5224\u8bfb\u7684\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u63d0\u793a\u8bc4\u4f30 ChatGPT \u548c Claude \u7684\u5355\u6a21\u6001\u6027\u80fd\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u5171\u8bc6\u65b9\u6cd5\u3002\u901a\u8fc7\u914d\u5bf9\u56fe\u50cf\u548c\u5408\u6210\u6587\u672c\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u8f93\u5165\u7684\u5f71\u54cd\u3002", "result": "ChatGPT \u548c Claude \u5728\u5355\u6a21\u6001\u8bbe\u7f6e\u4e0b\u7684\u8bca\u65ad\u51c6\u786e\u7387\u5206\u522b\u4e3a 62.8% \u548c 76.9%\u3002\u5728\u591a\u6a21\u6001\u961f\u5217\u4e2d\uff0cChatGPT \u7684\u6027\u80fd\u63d0\u9ad8\u5230 84%\uff0cClaude \u7684\u6027\u80fd\u63d0\u9ad8\u5230 76%\uff0c\u5171\u8bc6\u51c6\u786e\u7387\u8fbe\u5230 91.3%\u3002", "conclusion": "\u6574\u5408\u4e92\u8865\u6a21\u6001\u548c\u4f7f\u7528\u8f93\u51fa\u7ea7\u5171\u8bc6\u53ef\u4ee5\u63d0\u9ad8 AI \u8f85\u52a9\u653e\u5c04\u8bca\u65ad\u7684\u53ef\u4fe1\u5ea6\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u5e76\u4ee5\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\u63d0\u4f9b\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u7684\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u7684\u8ba4\u77e5\u7406\u89e3\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u6846\u67b6\uff0c\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u8868\u73b0\u51fa\u667a\u80fd\uff0c\u4f46\u7f3a\u4e4f\u771f\u6b63\u7684\u8ba4\u77e5\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u8ba4\u77e5\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u6587\u57fa\u4e8e\u5fc3\u7075\u54f2\u5b66\u548c\u8ba4\u77e5\u73b0\u8c61\u5b66\uff0c\u7ed3\u5408\u8fc7\u7a0b\u54f2\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u6269\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u667a\u80fd\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u6301\u7eed\u7684\u5224\u65ad\u3001\u8bb0\u5fc6\u3001\u63a7\u5236\u3001\u884c\u52a8\u548c\u8c03\u8282\u7684\u5faa\u73af\u8fc7\u7a0b\u3002", "result": "SCL\u6846\u67b6\u901a\u8fc7\u529f\u80fd\u5206\u79bb\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u57fa\u4e8eprompt\u7684\u7cfb\u7edf\u66f4\u8fde\u8d2f\u548c\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\u3002\u5b83\u5c06\u77e5\u8bc6\u5b9a\u4e49\u4e3a\u5728\u73b0\u8c61\u5b66\u8fde\u8d2f\u7684\u5faa\u73af\u4e2d\u6301\u7eed\u91cd\u5efa\u7684\u8fc7\u7a0b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SCL\u6846\u67b6\u4e3a\u5fc3\u7075\u54f2\u5b66\u3001\u8ba4\u8bc6\u8bba\u548c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u5e26\u6765\u4e86\u5f71\u54cd\u3002\u5b83\u5f3a\u8c03\u8ba4\u77e5\u67b6\u6784\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8ba4\u4e3a\u771f\u6b63\u7684\u8fdb\u5c55\u9700\u8981\u5b9e\u73b0\u8ba4\u77e5\u539f\u5219\u7684\u7ed3\u6784\uff0c\u800c\u4e0d\u662f\u66f4\u5927\u7684\u6a21\u578b\u3002"}}
{"id": "2510.15944", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15944", "abs": "https://arxiv.org/abs/2510.15944", "authors": ["Tianyu Bell Pan", "Mengdi Zhu", "Alexa Jordyn Cole", "Ronald Wilson", "Damon L. Woodard"], "title": "Lyapunov-Stable Adaptive Control for Multimodal Concept Drift", "comment": null, "summary": "Multimodal learning systems often struggle in non-stationary environments due\nto concept drift, where changing data distributions can degrade performance.\nModality-specific drifts and the lack of mechanisms for continuous, stable\nadaptation compound this challenge. This paper introduces LS-OGD, a novel\nadaptive control framework for robust multimodal learning in the presence of\nconcept drift. LS-OGD uses an online controller that dynamically adjusts the\nmodel's learning rate and the fusion weights between different data modalities\nin response to detected drift and evolving prediction errors. We prove that\nunder bounded drift conditions, the LS-OGD system's prediction error is\nuniformly ultimately bounded and converges to zero if the drift ceases.\nAdditionally, we demonstrate that the adaptive fusion strategy effectively\nisolates and mitigates the impact of severe modality-specific drift, thereby\nensuring system resilience and fault tolerance. These theoretical guarantees\nestablish a principled foundation for developing reliable and continuously\nadapting multimodal learning systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u63a7\u5236\u6846\u67b6LS-OGD\uff0c\u7528\u4e8e\u5728\u6982\u5ff5\u6f02\u79fb\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u6a21\u6001\u5b66\u4e60\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7531\u4e8e\u6982\u5ff5\u6f02\u79fb\u800c\u96be\u4ee5\u8868\u73b0\u826f\u597d\uff0c\u6a21\u6001\u7279\u5b9a\u7684\u6f02\u79fb\u548c\u7f3a\u4e4f\u6301\u7eed\u7a33\u5b9a\u9002\u5e94\u7684\u673a\u5236\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "method": "LS-OGD\u4f7f\u7528\u5728\u7ebf\u63a7\u5236\u5668\uff0c\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7684\u5b66\u4e60\u7387\u548c\u4e0d\u540c\u6570\u636e\u6a21\u6001\u4e4b\u95f4\u7684\u878d\u5408\u6743\u91cd\uff0c\u4ee5\u54cd\u5e94\u68c0\u6d4b\u5230\u7684\u6f02\u79fb\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6709\u754c\u6f02\u79fb\u6761\u4ef6\u4e0b\uff0cLS-OGD\u7cfb\u7edf\u7684\u9884\u6d4b\u8bef\u5dee\u662f\u4e00\u81f4\u6700\u7ec8\u6709\u754c\u7684\uff0c\u5982\u679c\u6f02\u79fb\u505c\u6b62\uff0c\u5219\u6536\u655b\u5230\u96f6\u3002\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u6709\u6548\u5730\u9694\u79bb\u548c\u51cf\u8f7b\u4e86\u4e25\u91cd\u6a21\u6001\u7279\u5b9a\u6f02\u79fb\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u5f39\u6027\u548c\u5bb9\u9519\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u7406\u8bba\u4fdd\u8bc1\u4e3a\u5f00\u53d1\u53ef\u9760\u548c\u6301\u7eed\u9002\u5e94\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2510.17586", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17586", "abs": "https://arxiv.org/abs/2510.17586", "authors": ["Boyan Li", "Chong Chen", "Zhujun Xue", "Yinan Mei", "Yuyu Luo"], "title": "DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework", "comment": null, "summary": "Large language models (LLMs) have advanced Text-to-SQL, yet existing\nsolutions still fall short of system-level reliability. The limitation is not\nmerely in individual modules - e.g., schema linking, reasoning, and\nverification - but more critically in the lack of structured orchestration that\nenforces correctness across the entire workflow. This gap motivates a paradigm\nshift: treating Text-to-SQL not as free-form language generation but as a\nsoftware-engineering problem that demands structured, verifiable orchestration.\nWe present DeepEye-SQL, a software-engineering-inspired framework that reframes\nText-to-SQL as the development of a small software program, executed through a\nverifiable process guided by the Software Development Life Cycle (SDLC).\nDeepEye-SQL integrates four synergistic stages: it grounds ambiguous user\nintent through semantic value retrieval and robust schema linking; enhances\nfault tolerance with N-version SQL generation using diverse reasoning\nparadigms; ensures deterministic verification via a tool-chain of unit tests\nand targeted LLM-guided revision; and introduces confidence-aware selection\nthat clusters execution results to estimate confidence and then takes a\nhigh-confidence shortcut or runs unbalanced pairwise adjudication in\nlow-confidence cases, yielding a calibrated, quality-gated output. This\nSDLC-aligned workflow transforms ad hoc query generation into a disciplined\nengineering process. Using ~30B open-source LLMs without any fine-tuning,\nDeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on\nSpider-Test, outperforming state-of-the-art solutions. This highlights that\nprincipled orchestration, rather than LLM scaling alone, is key to achieving\nsystem-level reliability in Text-to-SQL.", "AI": {"tldr": "DeepEye-SQL: a software-engineering-inspired framework that reframes Text-to-SQL as the development of a small software program, executed through a verifiable process guided by the Software Development Life Cycle (SDLC).", "motivation": "Existing Text-to-SQL solutions lack structured orchestration, hindering system-level reliability.", "method": "Integrates semantic value retrieval, robust schema linking, N-version SQL generation, deterministic verification via unit tests and LLM-guided revision, and confidence-aware selection.", "result": "Achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on Spider-Test using ~30B open-source LLMs without fine-tuning, outperforming state-of-the-art solutions.", "conclusion": "Principled orchestration, rather than LLM scaling alone, is key to achieving system-level reliability in Text-to-SQL."}}
{"id": "2510.16597", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16597", "abs": "https://arxiv.org/abs/2510.16597", "authors": ["Qiyao Peng", "Chen Wang", "Yinghui Wang", "Hongtao Liu", "Xuan Guo", "Wenjun Wang"], "title": "FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation", "comment": null, "summary": "Reviewer recommendation is a critical task for enhancing the efficiency of\nacademic publishing workflows. However, research in this area has been\npersistently hindered by the lack of high-quality benchmark datasets, which are\noften limited in scale, disciplinary scope, and comparative analyses of\ndifferent methodologies. To address this gap, we introduce FRONTIER-RevRec, a\nlarge-scale dataset constructed from authentic peer review records (2007-2025)\nfrom the Frontiers open-access publishing platform\nhttps://www.frontiersin.org/. The dataset contains 177941 distinct reviewers\nand 478379 papers across 209 journals spanning multiple disciplines including\nclinical medicine, biology, psychology, engineering, and social sciences. Our\ncomprehensive evaluation on this dataset reveals that content-based methods\nsignificantly outperform collaborative filtering. This finding is explained by\nour structural analysis, which uncovers fundamental differences between\nacademic recommendation and commercial domains. Notably, approaches leveraging\nlanguage models are particularly effective at capturing the semantic alignment\nbetween a paper's content and a reviewer's expertise. Furthermore, our\nexperiments identify optimal aggregation strategies to enhance the\nrecommendation pipeline. FRONTIER-RevRec is intended to serve as a\ncomprehensive benchmark to advance research in reviewer recommendation and\nfacilitate the development of more effective academic peer review systems. The\nFRONTIER-RevRec dataset is available at:\nhttps://anonymous.4open.science/r/FRONTIER-RevRec-5D05.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5ba1\u7a3f\u4eba\u63a8\u8350\u6570\u636e\u96c6FRONTIER-RevRec\uff0c\u65e8\u5728\u89e3\u51b3\u8be5\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5ba1\u7a3f\u4eba\u63a8\u8350\u7814\u7a76\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u8de8\u5b66\u79d1\u3001\u5305\u542b\u5bf9\u6bd4\u5206\u6790\u7684\u4f18\u8d28\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2007-2025\u5e74Frontiers\u5f00\u653e\u83b7\u53d6\u51fa\u7248\u5e73\u53f0\u6570\u636e\u7684FRONTIER-RevRec\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5185\u5bb9\u65b9\u6cd5\u548c\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u5185\u5bb9\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u534f\u540c\u8fc7\u6ee4\uff0c\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u8bba\u6587\u5185\u5bb9\u548c\u5ba1\u7a3f\u4eba\u4e13\u4e1a\u77e5\u8bc6\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "conclusion": "FRONTIER-RevRec\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u4ee5\u4fc3\u8fdb\u5ba1\u7a3f\u4eba\u63a8\u8350\u7684\u7814\u7a76\uff0c\u5e76\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16017", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16017", "abs": "https://arxiv.org/abs/2510.16017", "authors": ["Ibrahim Sheikh Mohamed", "Abdullah Yahya Abdullah Omaisan"], "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects", "comment": null, "summary": "Infrastructure in smart cities is increasingly monitored by networks of\nclosed circuit television (CCTV) cameras. Roads, bridges and tunnels develop\ncracks, potholes, and fluid leaks that threaten public safety and require\ntimely repair. Manual inspection is costly and hazardous, and existing\nautomatic systems typically address individual defect types or provide\nunstructured outputs that cannot directly guide maintenance crews. This paper\nproposes a comprehensive pipeline that leverages street CCTV streams for multi\ndefect detection and segmentation using the YOLO family of object detectors and\npasses the detections to a vision language model (VLM) for scene aware\nsummarization. The VLM generates a structured action plan in JSON format that\nincludes incident descriptions, recommended tools, dimensions, repair plans,\nand urgent alerts. We review literature on pothole, crack and leak detection,\nhighlight recent advances in large vision language models such as QwenVL and\nLLaVA, and describe the design of our early prototype. Experimental evaluation\non public datasets and captured CCTV clips demonstrates that the system\naccurately identifies diverse defects and produces coherent summaries. We\nconclude by discussing challenges and directions for scaling the system to city\nwide deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6027\u7684\u7ba1\u9053\uff0c\u5229\u7528\u8857\u9053\u95ed\u8def\u7535\u89c6\u6d41\u8fdb\u884c\u591a\u91cd\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u7ed3\u6784\u5316\u7684\u884c\u52a8\u8ba1\u5212\u3002", "motivation": "\u4eba\u5de5\u68c0\u6d4b\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u73b0\u6709\u7684\u81ea\u52a8\u7cfb\u7edf\u901a\u5e38\u53ea\u5904\u7406\u4e2a\u522b\u7f3a\u9677\u7c7b\u578b\u6216\u63d0\u4f9b\u975e\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u65e0\u6cd5\u76f4\u63a5\u6307\u5bfc\u7ef4\u62a4\u4eba\u5458\u3002", "method": "\u4f7f\u7528YOLO\u7cfb\u5217\u76ee\u6807\u68c0\u6d4b\u5668\u8fdb\u884c\u591a\u91cd\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u5e76\u5c06\u68c0\u6d4b\u7ed3\u679c\u4f20\u9012\u7ed9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u573a\u666f\u611f\u77e5\u603b\u7ed3\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u6355\u83b7\u7684\u95ed\u8def\u7535\u89c6\u7247\u6bb5\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u5404\u79cd\u7f3a\u9677\u5e76\u751f\u6210\u8fde\u8d2f\u7684\u6458\u8981\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5c06\u7cfb\u7edf\u6269\u5c55\u5230\u5168\u5e02\u8303\u56f4\u90e8\u7f72\u7684\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2510.16062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16062", "abs": "https://arxiv.org/abs/2510.16062", "authors": ["Guiyao Tie", "Zenghui Yuan", "Zeli Zhao", "Chaoran Hu", "Tianhe Gu", "Ruihang Zhang", "Sizhe Zhang", "Junran Wu", "Xiaoyue Tu", "Ming Jin", "Qingsong Wen", "Lixing Chen", "Pan Zhou", "Lichao Sun"], "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "comment": "38 pages, 25 figures, 8 tables", "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u7ea0\u6b63\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCorrectBench\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u4e0d\u540c\u81ea\u7ea0\u6b63\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8bc4\u4f30\u73b0\u6709\u81ea\u7ea0\u6b63\u65b9\u6cd5\uff0c\u5e76\u63a2\u7a76LLM\u662f\u5426\u80fd\u771f\u6b63\u7ea0\u6b63\u81ea\u8eab\u9519\u8bef\u3002", "method": "\u901a\u8fc7CorrectBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u5185\u5728\u3001\u5916\u5728\u548c\u5fae\u8c03\u7b49\u81ea\u7ea0\u6b63\u7b56\u7565\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u81ea\u7ea0\u6b63\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff1b\u6df7\u5408\u4e0d\u540c\u7684\u81ea\u7ea0\u6b63\u7b56\u7565\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u4f1a\u964d\u4f4e\u6548\u7387\uff1b\u63a8\u7406LLM\u5728\u989d\u5916\u7684\u81ea\u7ea0\u6b63\u65b9\u6cd5\u4e0b\u4f18\u5316\u6709\u9650\uff0c\u4e14\u65f6\u95f4\u6210\u672c\u9ad8\uff1b\u7b80\u5355\u7684CoT\u57fa\u7ebf\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u81ea\u7ea0\u6b63\u5177\u6709\u589e\u5f3aLLM\u63a8\u7406\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u4f46\u63d0\u9ad8\u6548\u7387\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u4fa7\u91cd\u4e8e\u4f18\u5316\u63a8\u7406\u80fd\u529b\u548c\u8fd0\u8425\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u3002"}}
{"id": "2510.15959", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.15959", "abs": "https://arxiv.org/abs/2510.15959", "authors": ["Isabelle Hupont", "Marisa Ponti", "Sven Schade"], "title": "Exploring the Potential of Citiverses for Regulatory Learning", "comment": "26 pages", "summary": "Citiverses hold the potential to support regulatory learning by offering\nimmersive, virtual environments for experimenting with policy scenarios and\ntechnologies. This paper proposes a science-for-policy agenda to explore the\npotential of citiverses as experimentation spaces for regulatory learning,\ngrounded in a consultation with a high-level panel of experts, including\npolicymakers from the European Commission, national government science advisers\nand leading researchers in digital regulation and virtual worlds. It identifies\nkey research areas, including scalability, real-time feedback, complexity\nmodelling, cross-border collaboration, risk reduction, citizen participation,\nethical considerations and the integration of emerging technologies. In\naddition, the paper analyses a set of experimental topics, spanning\ntransportation, urban planning and the environment/climate crisis, that could\nbe tested in citiverse platforms to advance regulatory learning in these areas.\nThe proposed work is designed to inform future research for policy and\nemphasizes a responsible approach to developing and using citiverses. It\nprioritizes careful consideration of the ethical, economic, ecological and\nsocial dimensions of different regulations. The paper also explores essential\npreliminary steps necessary for integrating citiverses into the broader\necosystems of experimentation spaces, including test beds, living labs and\nregulatory sandboxes", "AI": {"tldr": "\u63a2\u7d22citiverse\u4f5c\u4e3a\u76d1\u7ba1\u5b66\u4e60\u7684\u5b9e\u9a8c\u7a7a\u95f4", "motivation": "Citiverse\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u6765\u652f\u6301\u76d1\u7ba1\u5b66\u4e60\uff0c\u4ece\u800c\u53ef\u4ee5\u8fdb\u884c\u7b56\u7565\u573a\u666f\u548c\u6280\u672f\u5b9e\u9a8c\u3002", "method": "\u901a\u8fc7\u4e0e\u9ad8\u7ea7\u4e13\u5bb6\u5c0f\u7ec4\uff08\u5305\u62ec\u6b27\u76df\u59d4\u5458\u4f1a\u7684\u653f\u7b56\u5236\u5b9a\u8005\u3001\u56fd\u5bb6\u653f\u5e9c\u79d1\u5b66\u987e\u95ee\u4ee5\u53ca\u6570\u5b57\u76d1\u7ba1\u548c\u865a\u62df\u4e16\u754c\u9886\u57df\u7684\u9886\u5148\u7814\u7a76\u4eba\u5458\uff09\u534f\u5546\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u79d1\u5b66\u653f\u7b56\u8bae\u7a0b\u3002", "result": "\u786e\u5b9a\u4e86\u5173\u952e\u7814\u7a76\u9886\u57df\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u53cd\u9988\u3001\u590d\u6742\u6027\u5efa\u6a21\u3001\u8de8\u5883\u5408\u4f5c\u3001\u964d\u4f4e\u98ce\u9669\u3001\u516c\u6c11\u53c2\u4e0e\u3001\u4f26\u7406\u8003\u91cf\u4ee5\u53ca\u65b0\u5174\u6280\u672f\u7684\u6574\u5408\u3002\u5206\u6790\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\u4e3b\u9898\uff0c\u6db5\u76d6\u4ea4\u901a\u8fd0\u8f93\u3001\u57ce\u5e02\u89c4\u5212\u548c\u73af\u5883/\u6c14\u5019\u5371\u673a\uff0c\u8fd9\u4e9b\u4e3b\u9898\u53ef\u4ee5\u5728citiverse\u5e73\u53f0\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4ee5\u63a8\u8fdb\u8fd9\u4e9b\u9886\u57df\u7684\u76d1\u7ba1\u5b66\u4e60\u3002", "conclusion": "\u4e3a\u4e86\u5c06citiverse\u6574\u5408\u5230\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7a7a\u95f4\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u5305\u62ec\u8bd5\u9a8c\u53f0\u3001\u751f\u6d3b\u5b9e\u9a8c\u5ba4\u548c\u76d1\u7ba1\u6c99\u7bb1\uff0c\u63a2\u7d22\u4e86\u5fc5\u8981\u7684\u521d\u6b65\u6b65\u9aa4\uff0c\u5e76\u5f3a\u8c03\u4e86\u4ee5\u8d1f\u8d23\u4efb\u7684\u65b9\u5f0f\u5f00\u53d1\u548c\u4f7f\u7528citiverse\u3002"}}
{"id": "2510.15945", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15945", "abs": "https://arxiv.org/abs/2510.15945", "authors": ["Guangya Wan", "Zixin Stephen Xu", "Sasa Zorc", "Manel Baucells", "Mengxuan Hu", "Hao Wang", "Sheng Li"], "title": "BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling", "comment": "Under review on ARR", "summary": "Sampling multiple responses is a common way to improve LLM output quality,\nbut it comes at the cost of additional computation. The key challenge is\ndeciding when to stop generating new samples to balance accuracy gains against\nefficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive\nCriterion for Optimal N-stopping), a principled adaptive sampling framework\ngrounded in Sequential Search with Bayesian Learning. BEACON sequentially\ngenerates responses from the policy LLM, updates posterior belief over reward\ndistributions in real time without further training, and determines when to\nstop by weighing expected gains against computational cost. Sampling terminates\nonce the marginal utility of further exploration no longer justifies the\nexpense. We establish both theoretical optimality guarantees and practical\ntractability, and show empirically that BEACON reduces average sampling by up\nto 80% while maintaining response quality. We further demonstrate BEACON's\nutility for cost-efficient preference data generation and outline practical\nextensions, offering actionable insights for future researchers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6BEACON\uff0c\u4ee5\u5728\u4fdd\u8bc1\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u964d\u4f4eLLM\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5728\u63d0\u9ad8LLM\u8f93\u51fa\u8d28\u91cf\u65f6\uff0c\u591a\u91cd\u91c7\u6837\u4f1a\u5e26\u6765\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5173\u952e\u6311\u6218\u5728\u4e8e\u5982\u4f55\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u70b9\uff0c\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u751f\u6210\u65b0\u7684\u6837\u672c\u3002", "method": "BEACON\u6846\u67b6\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u5e8f\u5217\u641c\u7d22\uff0c\u901a\u8fc7\u4ece\u7b56\u7565LLM\u4e2d\u5e8f\u5217\u5730\u751f\u6210\u54cd\u5e94\uff0c\u5b9e\u65f6\u66f4\u65b0\u5956\u52b1\u5206\u5e03\u7684\u540e\u9a8c\u4fe1\u5ff5\uff0c\u5e76\u6839\u636e\u9884\u671f\u6536\u76ca\u4e0e\u8ba1\u7b97\u6210\u672c\u7684\u6743\u8861\u6765\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBEACON\u5728\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5e73\u5747\u51cf\u5c11\u4e86\u9ad8\u8fbe80%\u7684\u91c7\u6837\u3002", "conclusion": "BEACON\u4e3a\u964d\u4f4e\u91c7\u6837\u6210\u672c\u3001\u540c\u65f6\u4fdd\u6301LLM\u8f93\u51fa\u8d28\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6210\u672c\u6548\u76ca\u9ad8\u7684\u504f\u597d\u6570\u636e\u751f\u6210\u65b9\u9762\u7684\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.17748", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.17748", "abs": "https://arxiv.org/abs/2510.17748", "authors": ["William Zhang", "Wan Shen Lim", "Andrew Pavlo"], "title": "This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!", "comment": "Accepted to SIGMOD2026", "summary": "Tuning database management systems (DBMSs) is challenging due to trillions of\npossible configurations and evolving workloads. Recent advances in tuning have\nled to breakthroughs in optimizing over the possible configurations. However,\ndue to their design and inability to leverage query-level historical insights,\nexisting automated tuners struggle to adapt and re-optimize the DBMS when the\nenvironment changes (e.g., workload drift, schema transfer).\n  This paper presents the Booster framework that assists existing tuners in\nadapting to environment changes (e.g., drift, cross-schema transfer). Booster\nstructures historical artifacts into query-configuration contexts, prompts\nlarge language models (LLMs) to suggest configurations for each query based on\nrelevant contexts, and then composes the query-level suggestions into a\nholistic configuration with beam search. With multiple OLAP workloads, we\nevaluate Booster's ability to assist different state-of-the-art tuners (e.g.,\ncost-/machine learning-/LLM-based) in adapting to environment changes. By\ncomposing recommendations derived from query-level insights, Booster assists\ntuners in discovering configurations that are up to 74% better and in up to\n4.7x less time than the alternative approach of continuing to tune from\nhistorical configurations.", "AI": {"tldr": "Booster\u6846\u67b6\u901a\u8fc7\u5229\u7528\u67e5\u8be2\u7ea7\u522b\u7684\u5386\u53f2\u4fe1\u606f\uff0c\u8f85\u52a9\u73b0\u6709\u8c03\u4f18\u5668\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u4ece\u800c\u63d0\u5347\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf(DBMS)\u7684\u6027\u80fd\u548c\u8c03\u4f18\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u8c03\u4f18\u5668\u5728\u73af\u5883\u53d8\u5316\u65f6\u96be\u4ee5\u9002\u5e94\u548c\u91cd\u65b0\u4f18\u5316\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf(DBMS)\u3002", "method": "Booster\u6846\u67b6\u5c06\u5386\u53f2\u4fe1\u606f\u6784\u5efa\u6210\u67e5\u8be2-\u914d\u7f6e\u4e0a\u4e0b\u6587\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u5efa\u8bae\u914d\u7f6e\uff0c\u5e76\u901a\u8fc7beam search\u5c06\u67e5\u8be2\u7ea7\u522b\u7684\u5efa\u8bae\u7ec4\u5408\u6210\u6574\u4f53\u914d\u7f6e\u3002", "result": "Booster\u6846\u67b6\u8f85\u52a9\u8c03\u4f18\u5668\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe74%\u7684\u914d\u7f6e\uff0c\u5e76\u4e14\u6bd4\u4ece\u5386\u53f2\u914d\u7f6e\u7ee7\u7eed\u8c03\u4f18\u7684\u65b9\u6cd5\u8282\u7701\u9ad8\u8fbe4.7\u500d\u7684\u65f6\u95f4\u3002", "conclusion": "Booster\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8f85\u52a9\u73b0\u6709\u8c03\u4f18\u5668\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u63d0\u9ad8\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u8c03\u4f18\u6548\u7387\u3002"}}
{"id": "2510.16715", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16715", "abs": "https://arxiv.org/abs/2510.16715", "authors": ["Zulun Zhu", "Haoyu Liu", "Mengke He", "Siqiang Luo"], "title": "Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization", "comment": null, "summary": "Question answering in temporal knowledge graphs requires retrieval that is\nboth time-consistent and efficient. Existing RAG methods are largely semantic\nand typically neglect explicit temporal constraints, which leads to\ntime-inconsistent answers and inflated token usage. We propose STAR-RAG, a\ntemporal GraphRAG framework that relies on two key ideas: building a\ntime-aligned rule graph and conducting propagation on this graph to narrow the\nsearch space and prioritize semantically relevant, time-consistent evidence.\nThis design enforces temporal proximity during retrieval, reduces the candidate\nset of retrieval results, and lowers token consumption without sacrificing\naccuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates\nthe need for heavy model training and fine-tuning, thereby reducing\ncomputational cost and significantly simplifying deployment.Extensive\nexperiments on real-world temporal KG datasets show that our method achieves\nimproved answer accuracy while consuming fewer tokens than strong GraphRAG\nbaselines.", "AI": {"tldr": "STAR-RAG: A temporal GraphRAG framework for question answering in temporal knowledge graphs, improving time-consistent retrieval.", "motivation": "Existing RAG methods neglect explicit temporal constraints, leading to time-inconsistent answers and inflated token usage.", "method": "Building a time-aligned rule graph and conducting propagation to narrow the search space and prioritize semantically relevant, time-consistent evidence.", "result": "Improved answer accuracy and reduced token consumption compared to strong GraphRAG baselines.", "conclusion": "STAR-RAG eliminates the need for heavy model training and fine-tuning, reducing computational cost and simplifying deployment."}}
{"id": "2510.16036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16036", "abs": "https://arxiv.org/abs/2510.16036", "authors": ["Zewen Li", "Zitong Yu", "Qilang Ye", "Weicheng Xie", "Wei Zhuo", "Linlin Shen"], "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection", "comment": "Accepted by IEEE Transactions on Instrumentation and Measurement\n  (TIM)", "summary": "The robust causal capability of Multimodal Large Language Models (MLLMs) hold\nthe potential of detecting defective objects in Industrial Anomaly Detection\n(IAD). However, most traditional IAD methods lack the ability to provide\nmulti-turn human-machine dialogues and detailed descriptions, such as the color\nof objects, the shape of an anomaly, or specific types of anomalies. At the\nsame time, methods based on large pre-trained models have not fully stimulated\nthe ability of large models in anomaly detection tasks. In this paper, we\nexplore the combination of rich text semantics with both image-level and\npixel-level information from images and propose IAD-GPT, a novel paradigm based\non MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate\ndetailed anomaly prompts for specific objects. These specific prompts from the\nlarge language model (LLM) are used to activate the detection and segmentation\nfunctions of the pre-trained visual-language model (i.e., CLIP). To enhance the\nvisual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein\nimage features interact with normal and abnormal text prompts to dynamically\nselect enhancement pathways, which enables language models to focus on specific\naspects of visual data, enhancing their ability to accurately interpret and\nrespond to anomalies within images. Moreover, we design a Multi-Mask Fusion\nmodule to incorporate mask as expert knowledge, which enhances the LLM's\nperception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA\ndatasets demonstrate our state-of-the-art performance on self-supervised and\nfew-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA\ndatasets. The codes are available at\n\\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.", "AI": {"tldr": "IAD-GPT: A new MLLM-based paradigm for industrial anomaly detection (IAD) that leverages detailed anomaly prompts, text-guided enhancement, and multi-mask fusion.", "motivation": "Traditional IAD methods lack multi-turn dialogue and detailed descriptions. Existing large pre-trained models haven't fully utilized their anomaly detection capabilities.", "method": "Proposes IAD-GPT with Abnormal Prompt Generator (APG), Text-Guided Enhancer, and Multi-Mask Fusion module to enhance anomaly detection and segmentation.", "result": "Achieves state-of-the-art performance on MVTec-AD and VisA datasets for self-supervised and few-shot anomaly detection and segmentation.", "conclusion": "IAD-GPT effectively combines text semantics with image information for improved industrial anomaly detection."}}
{"id": "2510.16079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16079", "abs": "https://arxiv.org/abs/2510.16079", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "comment": null, "summary": "Current Large Language Model (LLM) agents show strong performance in tool\nuse, but lack the crucial capability to systematically learn from their own\nexperiences. While existing frameworks mainly focus on mitigating external\nknowledge gaps, they fail to address a more fundamental limitation: the\ninability to iteratively refine problem-solving strategies. In this work, we\nintroduce EvolveR, a framework designed to enable agent to self-improve through\na complete, closed-loop experience lifecycle. This lifecycle comprises two key\nstages: (1) Offline Self-Distillation, where the agent's interaction\ntrajectories are synthesized into a structured repository of abstract, reusable\nstrategic principles; (2) Online Interaction, where the agent interacts with\ntasks and actively retrieves distilled principles to guide its decision-making,\naccumulating a diverse set of behavioral trajectories. This loop employs a\npolicy reinforcement mechanism to iteratively update the agent based on its\nperformance. We demonstrate the effectiveness of EvolveR on complex multi-hop\nquestion-answering benchmarks, where it achieves superior performance over\nstrong agentic baselines. Our work presents a comprehensive blueprint for\nagents that learn not only from external data but also from the consequences of\ntheir own actions, paving the way for more autonomous and continuously\nimproving systems. Code is available at https://github.com/Edaizi/EvolveR.", "AI": {"tldr": "EvolveR\u6846\u67b6\u901a\u8fc7\u79bb\u7ebf\u81ea\u6211\u63d0\u70bc\u548c\u5728\u7ebf\u4e92\u52a8\uff0c\u4f7fLLM Agent\u80fd\u591f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u7cfb\u7edf\u5730\u5b66\u4e60\u548c\u6539\u8fdb\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709LLM Agent\u5728\u5de5\u5177\u4f7f\u7528\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u7cfb\u7edf\u5b66\u4e60\u7684\u5173\u952e\u80fd\u529b\uff0c\u65e0\u6cd5\u8fed\u4ee3\u4f18\u5316\u89e3\u51b3\u95ee\u9898\u7684\u7b56\u7565\u3002", "method": "\u5f15\u5165EvolveR\u6846\u67b6\uff0c\u5305\u542b\u79bb\u7ebf\u81ea\u6211\u63d0\u70bc\uff08\u5c06\u4ea4\u4e92\u8f68\u8ff9\u5408\u6210\u4e3a\u53ef\u91cd\u7528\u7684\u7b56\u7565\u539f\u5219\uff09\u548c\u5728\u7ebf\u4e92\u52a8\uff08\u68c0\u7d22\u63d0\u70bc\u7684\u539f\u5219\u6765\u6307\u5bfc\u51b3\u7b56\uff09\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u5f3a\u5316\u673a\u5236\u8fed\u4ee3\u66f4\u65b0Agent\u3002", "result": "\u5728\u590d\u6742\u7684\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvolveR\u4f18\u4e8e\u5f3a\u5927\u7684Agent\u57fa\u7ebf\u3002", "conclusion": "EvolveR\u4e3aAgent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u84dd\u56fe\uff0c\u4f7f\u5176\u4e0d\u4ec5\u53ef\u4ee5\u4ece\u5916\u90e8\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u8fd8\u53ef\u4ee5\u4ece\u81ea\u8eab\u884c\u4e3a\u7684\u540e\u679c\u4e2d\u5b66\u4e60\uff0c\u4e3a\u66f4\u81ea\u4e3b\u548c\u6301\u7eed\u6539\u8fdb\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.15966", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15966", "abs": "https://arxiv.org/abs/2510.15966", "authors": ["Shian Jia", "Ziyang Huang", "Xinbo Wang", "Haofei Zhang", "Mingli Song"], "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency", "comment": null, "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks\nadaptability to diverse tasks and overlooks the constructive and task-oriented\nrole of AI agent memory. Drawing from Piaget's theory of cognitive development,\nwe propose PISA, a pragmatic, psych-inspired unified memory system that\naddresses these limitations by treating memory as a constructive and adaptive\nprocess. To enable continuous learning and adaptability, PISA introduces a\ntrimodal adaptation mechanism (i.e., schema updation, schema evolution, and\nschema creation) that preserves coherent organization while supporting flexible\nmemory updates. Building on these schema-grounded structures, we further design\na hybrid memory access architecture that seamlessly integrates symbolic\nreasoning with neural retrieval, significantly improving retrieval accuracy and\nefficiency. Our empirical evaluation, conducted on the existing LOCOMO\nbenchmark and our newly proposed AggQA benchmark for data analysis tasks,\nconfirms that PISA sets a new state-of-the-art by significantly enhancing\nadaptability and long-term knowledge retention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bb0\u5fc6\u7cfb\u7edfPISA\uff0c\u5b83\u5177\u6709\u9002\u5e94\u6027\u548c\u5efa\u6784\u6027\uff0c\u53ef\u4ee5\u63d0\u9ad8\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5728LOCOMO\u548cAggQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684state-of-the-art\u3002", "motivation": "\u73b0\u6709\u7684\u8bb0\u5fc6\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86AI agent\u8bb0\u5fc6\u7684\u5efa\u6784\u6027\u548c\u9762\u5411\u4efb\u52a1\u7684\u89d2\u8272\u3002", "method": "PISA\u5f15\u5165\u4e86\u4e00\u79cd\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\uff08\u5373\u6a21\u5f0f\u66f4\u65b0\u3001\u6a21\u5f0f\u6f14\u5316\u548c\u6a21\u5f0f\u521b\u5efa\uff09\uff0c\u8be5\u673a\u5236\u5728\u652f\u6301\u7075\u6d3b\u7684\u8bb0\u5fc6\u66f4\u65b0\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u8fde\u8d2f\u7684\u7ec4\u7ec7\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5c06\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u68c0\u7d22\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728LOCOMO\u548c\u65b0\u63d0\u51fa\u7684AggQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPISA\u90fd\u8fbe\u5230\u4e86\u65b0\u7684state-of-the-art\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "PISA\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5b83\u53ef\u4ee5\u63d0\u9ad8AI agent\u7684\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002"}}
{"id": "2510.15946", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15946", "abs": "https://arxiv.org/abs/2510.15946", "authors": ["Wenshuo Wang", "Ziyou Jiang", "Junjie Wang", "Mingyang Li", "Jie Huang", "Yuekai Huang", "Zhiyuan Chang", "Feiyan Duan", "Qing Wang"], "title": "Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns", "comment": "12 Pages, Submitted to WWW'26", "summary": "Internet memes have emerged as a popular multimodal medium, yet they are\nincreasingly weaponized to convey harmful opinions through subtle rhetorical\ndevices like irony and metaphor. Existing detection approaches, including\nMLLM-based techniques, struggle with these implicit expressions, leading to\nfrequent misjudgments. This paper introduces PatMD, a novel approach that\nimproves harmful meme detection by learning from and proactively mitigating\nthese potential misjudgment risks. Our core idea is to move beyond superficial\ncontent-level matching and instead identify the underlying misjudgment risk\npatterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We\nfirst construct a knowledge base where each meme is deconstructed into a\nmisjudgment risk pattern explaining why it might be misjudged, either\noverlooking harmful undertones (false negative) or overinterpreting benign\ncontent (false positive). For a given target meme, PatMD retrieves relevant\npatterns and utilizes them to dynamically guide the MLLM's reasoning.\nExperiments on a benchmark of 6,626 memes across 5 harmful detection tasks show\nthat PatMD outperforms state-of-the-art baselines, achieving an average of\n8.30\\% improvement in F1-score and 7.71\\% improvement in accuracy,\ndemonstrating strong generalizability and improved detection capability of\nharmful memes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPatMD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6f5c\u5728\u7684\u8bef\u5224\u98ce\u9669\u6a21\u5f0f\u6765\u6539\u8fdb\u6709\u5bb3\u6a21\u56e0\u7684\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u901a\u8fc7\u8bbd\u523a\u548c\u9690\u55bb\u7b49\u5fae\u5999\u4fee\u8f9e\u624b\u6bb5\u4f20\u8fbe\u6709\u5bb3\u89c2\u70b9\u7684\u4e92\u8054\u7f51\u6a21\u56e0\uff0c\u5bfc\u81f4\u9891\u7e41\u7684\u8bef\u5224\u3002", "method": "PatMD\u6784\u5efa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u5e93\uff0c\u5c06\u6bcf\u4e2a\u6a21\u56e0\u5206\u89e3\u4e3a\u8bef\u5224\u98ce\u9669\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6a21\u5f0f\u52a8\u6001\u5730\u6307\u5bfcMLLM\u7684\u63a8\u7406\u3002", "result": "\u5728\u5305\u542b6,626\u4e2a\u6a21\u56e0\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPatMD\u57285\u4e2a\u6709\u5bb3\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cF1-score\u5e73\u5747\u63d0\u9ad8\u4e868.30%\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.71%\u3002", "conclusion": "PatMD\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u4e86\u6709\u5bb3\u6a21\u56e0\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.", "AI": {"tldr": "DeepAnalyze-8B\uff0c\u9996\u4e2a\u4e3a\u81ea\u52a8\u6570\u636e\u79d1\u5b66\u8bbe\u8ba1\u7684Agentic LLM\uff0c\u80fd\u591f\u81ea\u52a8\u5b8c\u6210\u4ece\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u522b\u7684\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u4ee3\u7406\u5728\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5b8c\u6210\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684Agentic LLM\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u7684Agentic\u8bad\u7ec3\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u6a21\u4eff\u4e86\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u4f7fLLM\u80fd\u591f\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9010\u6b65\u83b7\u53d6\u548c\u6574\u5408\u591a\u79cd\u80fd\u529b\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "DeepAnalyze\u4ec5\u75288B\u53c2\u6570\uff0c\u5c31\u80dc\u8fc7\u4e86\u4e4b\u524d\u6784\u5efa\u5728\u6700\u5148\u8fdb\u7684\u4e13\u6709LLM\u4e4b\u4e0a\u7684\uff0c\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u7684\u4ee3\u7406\u3002", "conclusion": "DeepAnalyze\u7684\u6a21\u578b\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6570\u636e\u5df2\u5f00\u6e90\uff0c\u4e3a\u5b9e\u73b0\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.16736", "categories": ["cs.IR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16736", "abs": "https://arxiv.org/abs/2510.16736", "authors": ["Patrizio Dazzi", "William Guglielmo", "Franco Maria Nardini", "Raffaele Perego", "Salvatore Trani"], "title": "Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices", "comment": null, "summary": "This paper investigates the usage of FPGA devices for energy-efficient exact\nkNN search in high-dimension latent spaces. This work intercepts a relevant\ntrend that tries to support the increasing popularity of learned\nrepresentations based on neural encoder models by making their large-scale\nadoption greener and more inclusive. The paper proposes two different\nenergy-efficient solutions adopting the same FPGA low-level configuration. The\nfirst solution maximizes system throughput by processing the queries of a batch\nin parallel over a streamed dataset not fitting into the FPGA memory. The\nsecond minimizes latency by processing each kNN incoming query in parallel over\nan in-memory dataset. Reproducible experiments on publicly available image and\ntext datasets show that our solution outperforms state-of-the-art CPU-based\ncompetitors regarding throughput, latency, and energy consumption.\nSpecifically, experiments show that the proposed FPGA solutions achieve the\nbest throughput in terms of queries per second and the best-observed latency\nwith scale-up factors of up to 16.6X. Similar considerations can be made\nregarding energy efficiency, where results show that our solutions can achieve\nup to 11.9X energy saving w.r.t. strong CPU-based competitors.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528 FPGA \u8bbe\u5907\u5728\u9ad8\u5ea6\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8282\u80fd\u7cbe\u786e kNN \u641c\u7d22\u3002", "motivation": "\u901a\u8fc7\u4f7f\u57fa\u4e8e\u795e\u7ecf\u7f16\u7801\u5668\u6a21\u578b\u7684\u5927\u89c4\u6a21\u5b66\u4e60\u8868\u793a\u7684\u91c7\u7528\u66f4\u73af\u4fdd\u548c\u66f4\u5177\u5305\u5bb9\u6027\uff0c\u6765\u652f\u6301\u8fd9\u79cd\u8868\u793a\u65b9\u6cd5\u65e5\u76ca\u666e\u53ca\u7684\u8d8b\u52bf\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u8282\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u4eec\u91c7\u7528\u76f8\u540c\u7684 FPGA \u5e95\u5c42\u914d\u7f6e\u3002\u7b2c\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u5728\u4e0d\u9002\u5408 FPGA \u5185\u5b58\u7684\u6d41\u5f0f\u6570\u636e\u96c6\u4e0a\u5e76\u884c\u5904\u7406\u6279\u5904\u7406\u67e5\u8be2\u6765\u6700\u5927\u5316\u7cfb\u7edf\u541e\u5410\u91cf\u3002\u7b2c\u4e8c\u79cd\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u5728\u5185\u5b58\u6570\u636e\u96c6\u4e0a\u5e76\u884c\u5904\u7406\u6bcf\u4e2a kNN \u4f20\u5165\u67e5\u8be2\u6765\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "\u5728\u516c\u5f00\u53ef\u7528\u7684\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u53ef\u91cd\u590d\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e CPU \u7684\u7ade\u4e89\u5bf9\u624b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 FPGA \u89e3\u51b3\u65b9\u6848\u5728\u6bcf\u79d2\u67e5\u8be2\u6b21\u6570\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u4f73\u541e\u5410\u91cf\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe 16.6 \u500d\u7684\u653e\u5927\u500d\u6570\u7684\u6700\u4f73\u89c2\u5bdf\u5ef6\u8fdf\u3002\u5728\u80fd\u6e90\u6548\u7387\u65b9\u9762\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f3a\u5927\u7684\u57fa\u4e8e CPU \u7684\u7ade\u4e89\u5bf9\u624b\u76f8\u6bd4\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe 11.9 \u500d\u7684\u8282\u80fd\u6548\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u8868\u660e\uff0cFPGA \u89e3\u51b3\u65b9\u6848\u5728 kNN \u641c\u7d22\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002"}}
{"id": "2510.16070", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.IV", "H.5.5; H.1.2; I.4.0"], "pdf": "https://arxiv.org/pdf/2510.16070", "abs": "https://arxiv.org/abs/2510.16070", "authors": ["Mahta Khoobi", "Marc Sebastian von der Stueck", "Felix Barajas Ordonez", "Anca-Maria Iancu", "Eric Corban", "Julia Nowak", "Aleksandar Kargaliev", "Valeria Perelygina", "Anna-Sophie Schott", "Daniel Pinto dos Santos", "Christiane Kuhl", "Daniel Truhn", "Sven Nebelung", "Robert Siepmann"], "title": "Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography", "comment": "Preprint version - Under second revision at Radiology (manuscript\n  RAD-25-1348)", "summary": "Structured reporting (SR) and artificial intelligence (AI) may transform how\nradiologists interact with imaging studies. This prospective study (July to\nDecember 2024) evaluated the impact of three reporting modes: free-text (FT),\nstructured reporting (SR), and AI-assisted structured reporting (AI-SR), on\nimage analysis behavior, diagnostic accuracy, efficiency, and user experience.\nFour novice and four non-novice readers (radiologists and medical students)\neach analyzed 35 bedside chest radiographs per session using a customized\nviewer and an eye-tracking system. Outcomes included diagnostic accuracy\n(compared with expert consensus using Cohen's $\\kappa$), reporting time per\nradiograph, eye-tracking metrics, and questionnaire-based user experience.\nStatistical analysis used generalized linear mixed models with Bonferroni\npost-hoc tests with a significance level of ($P \\le .01$). Diagnostic accuracy\nwas similar in FT ($\\kappa = 0.58$) and SR ($\\kappa = 0.60$) but higher in\nAI-SR ($\\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \\pm 38$\ns (FT) to $37 \\pm 18$ s (SR) and $25 \\pm 9$ s (AI-SR) ($P < .001$). Saccade\ncounts for the radiograph field ($205 \\pm 135$ (FT), $123 \\pm 88$ (SR), $97 \\pm\n58$ (AI-SR)) and total fixation duration for the report field ($11 \\pm 5$ s\n(FT), $5 \\pm 3$ s (SR), $4 \\pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <\n.001$ each). Novice readers shifted gaze towards the radiograph in SR, while\nnon-novice readers maintained their focus on the radiograph. AI-SR was the\npreferred mode. In conclusion, SR improves efficiency by guiding visual\nattention toward the image, and AI-prefilled SR further enhances diagnostic\naccuracy and user satisfaction.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u62a5\u544a\u6a21\u5f0f\uff08\u81ea\u7531\u6587\u672c FT\u3001\u7ed3\u6784\u5316\u62a5\u544a SR \u548c AI \u8f85\u52a9\u7ed3\u6784\u5316\u62a5\u544a AI-SR\uff09\u5bf9\u56fe\u50cf\u5206\u6790\u884c\u4e3a\u3001\u8bca\u65ad\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "motivation": "\u65e8\u5728\u7814\u7a76\u7ed3\u6784\u5316\u62a5\u544a (SR) \u548c\u4eba\u5de5\u667a\u80fd (AI) \u5982\u4f55\u6539\u53d8\u653e\u5c04\u79d1\u533b\u751f\u4e0e\u5f71\u50cf\u5b66\u7814\u7a76\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u62db\u52df\u4e86\u56db\u540d\u65b0\u624b\u548c\u56db\u540d\u975e\u65b0\u624b\u8bfb\u8005\uff08\u653e\u5c04\u79d1\u533b\u751f\u548c\u533b\u5b66\u751f\uff09\uff0c\u6bcf\u4e2a\u4eba\u4f7f\u7528\u5b9a\u5236\u7684\u9605\u8bfb\u5668\u548c\u773c\u52a8\u8ffd\u8e2a\u7cfb\u7edf\u5206\u6790 35 \u5f20\u5e8a\u8fb9\u80f8\u7247\u3002\u901a\u8fc7 Cohen's Kappa \u503c\u3001\u62a5\u544a\u65f6\u95f4\u3001\u773c\u52a8\u8ffd\u8e2a\u6307\u6807\u548c\u95ee\u5377\u8c03\u67e5\u6765\u8bc4\u4f30\u8bca\u65ad\u51c6\u786e\u6027\u3001\u62a5\u544a\u65f6\u95f4\u548c\u7528\u6237\u4f53\u9a8c\u3002", "result": "AI-SR \u7684\u8bca\u65ad\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u62a5\u544a\u65f6\u95f4\u66f4\u77ed\uff0c\u5e76\u4e14\u66f4\u53d7\u7528\u6237\u6b22\u8fce\u3002SR \u901a\u8fc7\u5f15\u5bfc\u89c6\u89c9\u6ce8\u610f\u529b\u6765\u63d0\u9ad8\u6548\u7387\u3002", "conclusion": "SR \u63d0\u9ad8\u4e86\u6548\u7387\uff0cAI \u9884\u586b\u5145\u7684 SR \u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cf\u5316\u4e86\u63d0\u793a\u7b56\u7565\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u5b9e\u73b0\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u7b5b\u9009\u9636\u6bb5\u7684\u81ea\u52a8\u5316\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u4e0d\u540cLLM\u548c\u63d0\u793a\u7b56\u7565\u5728\u6587\u732e\u7b5b\u9009\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u4e2aLLM\u5728\u4e94\u79cd\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e86\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u7b49\u6307\u6807\u3002", "result": "CoT-few-shot\u63d0\u793a\u7b56\u7565\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u53ef\u9760\u7684\u5e73\u8861\uff1bzero-shot\u63d0\u793a\u7b56\u7565\u6700\u5927\u5316\u4e86\u53ec\u56de\u7387\uff1bself-reflection\u63d0\u793a\u7b56\u7565\u8868\u73b0\u4e0d\u4f73\u3002GPT-4o\u548cDeepSeek\u8868\u73b0\u7a33\u5065\uff0cGPT-4o-mini\u5728\u8f83\u4f4e\u6210\u672c\u4e0b\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eLLM\u5728\u6587\u732e\u7b5b\u9009\u81ea\u52a8\u5316\u65b9\u9762\u5177\u6709\u4e0d\u5747\u8861\u4f46\u6709\u524d\u666f\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u4efb\u52a1\u81ea\u9002\u5e94\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6bd4\u8f83\u57fa\u51c6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u8c1c\u9898\u65f6\uff0c\u6027\u80fd\u4f1a\u968f\u7740\u56f0\u60d1\u5ea6\u7684\u589e\u52a0\u800c\u4e0b\u964d\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u5176\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u79fb\u52a8\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u7ed3\u679c\u72b6\u6001\u7a7a\u95f4\u5e76\u91cd\u65b0\u63d0\u793a\u81ea\u5df1\u4e0b\u4e00\u6b65\u884c\u52a8\uff0c\u6027\u80fd\u5d29\u6e83\u7684\u73b0\u8c61\u4ecd\u7136\u5b58\u5728\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u8c1c\u9898\u65f6\u6027\u80fd\u5d29\u6e83\u7684\u539f\u56e0\uff0c\u5e76\u7814\u7a76\u73af\u5883\u63a5\u53e3\u662f\u5426\u80fd\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u5176\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u79fb\u52a8\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u7ed3\u679c\u72b6\u6001\u7a7a\u95f4\u5e76\u91cd\u65b0\u63d0\u793a\u81ea\u5df1\u4e0b\u4e00\u6b65\u884c\u52a8\u3002\u901a\u8fc7\u5206\u6790\u6a21\u578b\u53c2\u6570\u5316\u7684\u7b56\u7565\uff0c\u7814\u7a76\u5176\u4e0e\u6700\u4f18\u7b56\u7565\u548c\u5747\u5300\u968f\u673a\u7b56\u7565\u7684\u5dee\u5f02\u3002", "result": "\u5373\u4f7f\u63d0\u4f9b\u73af\u5883\u63a5\u53e3\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u5d29\u6e83\u73b0\u8c61\u4ecd\u7136\u5b58\u5728\u3002\u6a21\u578b\u53c2\u6570\u5316\u7684\u7b56\u7565\u5206\u6790\u663e\u793a\uff0c\u6a21\u578b\u5728\u6bcf\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u4e0a\u90fd\u8868\u73b0\u51fa\u7c7b\u4f3c\u6a21\u5f0f\u7684\u5d29\u6e83\uff0c\u5e76\u4e14\u6027\u80fd\u53d6\u51b3\u4e8e\u8be5\u6a21\u5f0f\u662f\u5426\u53cd\u6620\u4e86\u95ee\u9898\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u8c1c\u9898\u65f6\u5b58\u5728\u6027\u80fd\u5d29\u6e83\u73b0\u8c61\uff0c\u8fd9\u53ef\u80fd\u4e0e\u6a21\u578b\u5728\u6bcf\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u4e0a\u90fd\u8868\u73b0\u51fa\u7c7b\u4f3c\u6a21\u5f0f\u7684\u5d29\u6e83\u6709\u5173\u3002"}}
{"id": "2510.15947", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.NC", "68T07, 92C55, 62M10", "I.2.6; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.15947", "abs": "https://arxiv.org/abs/2510.15947", "authors": ["Casper van Laar", "Khubaib Ahmed"], "title": "WaveNet's Precision in EEG Classification", "comment": "6 pages, 5 figures and 3 tables. Includes main text and bibliography", "summary": "This study introduces a WaveNet-based deep learning model designed to\nautomate the classification of EEG signals into physiological, pathological,\nartifact, and noise categories. Traditional methods for EEG signal\nclassification, which rely on expert visual review, are becoming increasingly\nimpractical due to the growing complexity and volume of EEG recordings.\nLeveraging a publicly available annotated dataset from Mayo Clinic and St.\nAnne's University Hospital, the WaveNet model was trained, validated, and\ntested on 209,232 samples with a 70/20/10 percent split. The model achieved a\nclassification accuracy exceeding previous CNN and LSTM-based approaches, and\nwas benchmarked against a Temporal Convolutional Network (TCN) baseline.\nNotably, the model distinguishes noise and artifacts with high precision,\nalthough it reveals a modest but explainable degree of misclassification\nbetween physiological and pathological signals, reflecting inherent clinical\noverlap. WaveNet's architecture, originally developed for raw audio synthesis,\nis well suited for EEG data due to its use of dilated causal convolutions and\nresidual connections, enabling it to capture both fine-grained and long-range\ntemporal dependencies. The research also details the preprocessing pipeline,\nincluding dynamic dataset partitioning and normalization steps that support\nmodel generalization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWaveNet\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5c06\u8111\u7535\u4fe1\u53f7\u5206\u7c7b\u4e3a\u751f\u7406\u6027\u3001\u75c5\u7406\u6027\u3001\u4f2a\u8ff9\u548c\u566a\u58f0\u7c7b\u522b\u3002", "motivation": "\u7531\u4e8e\u8111\u7535\u56fe\u8bb0\u5f55\u7684\u590d\u6742\u6027\u548c\u6570\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u4f9d\u8d56\u4e13\u5bb6\u89c6\u89c9\u68c0\u67e5\u7684\u4f20\u7edf\u8111\u7535\u56fe\u4fe1\u53f7\u5206\u7c7b\u65b9\u6cd5\u6b63\u53d8\u5f97\u8d8a\u6765\u8d8a\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u5229\u7528\u6765\u81ea Mayo Clinic \u548c St. Anne's University Hospital \u7684\u516c\u5f00\u5e26\u6ce8\u91ca\u6570\u636e\u96c6\uff0cWaveNet \u6a21\u578b\u5728 209,232 \u4e2a\u6837\u672c\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\uff0c\u5206\u5272\u6bd4\u4f8b\u4e3a 70/20/10\u3002", "result": "\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u8d85\u8fc7\u4ee5\u524d\u57fa\u4e8e CNN \u548c LSTM \u7684\u65b9\u6cd5\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u4ee5\u65f6\u95f4\u5377\u79ef\u7f51\u7edc (TCN) \u57fa\u7ebf\u4e3a\u57fa\u51c6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u6a21\u578b\u4ee5\u9ad8\u7cbe\u5ea6\u533a\u5206\u566a\u58f0\u548c\u4f2a\u8ff9\uff0c\u5c3d\u7ba1\u5b83\u63ed\u793a\u4e86\u751f\u7406\u548c\u75c5\u7406\u4fe1\u53f7\u4e4b\u95f4\u5b58\u5728\u9002\u5ea6\u4f46\u53ef\u89e3\u91ca\u7684\u9519\u8bef\u5206\u7c7b\uff0c\u53cd\u6620\u4e86\u56fa\u6709\u7684\u4e34\u5e8a\u91cd\u53e0\u3002", "conclusion": "WaveNet \u7684\u67b6\u6784\u6700\u521d\u662f\u4e3a\u539f\u59cb\u97f3\u9891\u5408\u6210\u800c\u5f00\u53d1\u7684\uff0c\u7531\u4e8e\u5176\u4f7f\u7528\u6269\u5f20\u7684\u56e0\u679c\u5377\u79ef\u548c\u6b8b\u5dee\u8fde\u63a5\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u8111\u7535\u56fe\u6570\u636e\uff0c\u4ece\u800c\u80fd\u591f\u6355\u83b7\u7ec6\u7c92\u5ea6\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u8be5\u7814\u7a76\u8fd8\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u652f\u6301\u6a21\u578b\u6cdb\u5316\u7684\u52a8\u6001\u6570\u636e\u96c6\u5206\u533a\u548c\u6807\u51c6\u5316\u6b65\u9aa4\u3002"}}
{"id": "2510.16803", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16803", "abs": "https://arxiv.org/abs/2510.16803", "authors": ["Zishuai Zhang", "Sihao Yu", "Wenyi Xie", "Ying Nie", "Junfeng Wang", "Zhiming Zheng", "Dawei Yin", "Hainan Zhang"], "title": "An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision", "comment": null, "summary": "The whole-page reranking plays a critical role in shaping the user experience\nof search engines, which integrates retrieval results from multiple modalities,\nsuch as documents, images, videos, and LLM outputs. Existing methods mainly\nrely on large-scale human-annotated data, which is costly to obtain and\ntime-consuming. This is because whole-page annotation is far more complex than\nsingle-modal: it requires assessing the entire result page while accounting for\ncross-modal relevance differences. Thus, how to improve whole-page reranking\nperformance while reducing annotation costs is still a key challenge in\noptimizing search engine result pages(SERP). In this paper, we propose SMAR, a\nnovel whole-page reranking framework that leverages strong Single-modal rankers\nto guide Modal-wise relevance Alignment for effective Reranking, using only\nlimited whole-page annotation to outperform fully-annotated reranking models.\nSpecifically, high-quality single-modal rankers are first trained on data\nspecific to their respective modalities. Then, for each query, we select a\nsubset of their outputs to construct candidate pages and perform human\nannotation at the page level. Finally, we train the whole-page reranker using\nthese limited annotations and enforcing consistency with single-modal\npreferences to maintain ranking quality within each modality. Experiments on\nthe Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by\nabout 70-90\\% while achieving significant ranking improvements compared to\nbaselines. Further offline and online A/B testing on Baidu APPs also shows\nnotable gains in standard ranking metrics as well as user experience\nindicators, fully validating the effectiveness and practical value of our\napproach in real-world search scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5168\u9875\u9762\u91cd\u6392\u5e8f\u6846\u67b6SMAR\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5f3a\u5927\u7684\u5355\u6a21\u6001\u6392\u5e8f\u5668\u6765\u6307\u5bfc\u6a21\u6001\u76f8\u5173\u6027\u5bf9\u9f50\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u91cd\u6392\u5e8f\uff0c\u4ec5\u4f7f\u7528\u6709\u9650\u7684\u5168\u9875\u9762\u6ce8\u91ca\u5373\u53ef\u80dc\u8fc7\u5b8c\u5168\u6ce8\u91ca\u7684\u91cd\u6392\u5e8f\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u9875\u9762\u91cd\u6392\u5e8f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u5168\u9875\u9762\u6807\u6ce8\u6bd4\u5355\u6a21\u6001\u6807\u6ce8\u590d\u6742\u5f97\u591a\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u8bc4\u4f30\u6574\u4e2a\u7ed3\u679c\u9875\u9762\uff0c\u540c\u65f6\u8003\u8651\u8de8\u6a21\u6001\u76f8\u5173\u6027\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5728\u63d0\u9ad8\u5168\u9875\u9762\u91cd\u6392\u5e8f\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u4ecd\u7136\u662f\u4f18\u5316\u641c\u7d22\u5f15\u64ce\u7ed3\u679c\u9875\u9762\uff08SERP\uff09\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u9996\u5148\uff0c\u5728\u7279\u5b9a\u4e8e\u5404\u81ea\u6a21\u6001\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u9ad8\u8d28\u91cf\u7684\u5355\u6a21\u6001\u6392\u5e8f\u5668\u3002\u7136\u540e\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u67e5\u8be2\uff0c\u6211\u4eec\u9009\u62e9\u5176\u8f93\u51fa\u7684\u5b50\u96c6\u4ee5\u6784\u5efa\u5019\u9009\u9875\u9762\uff0c\u5e76\u5728\u9875\u9762\u7ea7\u522b\u6267\u884c\u4eba\u5de5\u6ce8\u91ca\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u6709\u9650\u7684\u6ce8\u91ca\u6765\u8bad\u7ec3\u6574\u4e2a\u9875\u9762\u91cd\u6392\u5e8f\u5668\uff0c\u5e76\u5f3a\u5236\u4e0e\u5355\u6a21\u6001\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u4ee5\u7ef4\u6301\u6bcf\u4e2a\u6a21\u6001\u5185\u7684\u6392\u5e8f\u8d28\u91cf\u3002", "result": "\u5728\u9e92\u9e9f\u548c\u767e\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cSMAR\u964d\u4f4e\u4e86\u7ea670-90\uff05\u7684\u6ce8\u91ca\u6210\u672c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u6392\u540d\u6539\u8fdb\u3002\u5728\u767e\u5ea6\u5e94\u7528\u7a0b\u5e8f\u4e0a\u8fdb\u884c\u7684\u8fdb\u4e00\u6b65\u79bb\u7ebf\u548c\u5728\u7ebfA / B\u6d4b\u8bd5\u4e5f\u8868\u660e\uff0c\u6807\u51c6\u6392\u540d\u6307\u6807\u548c\u7528\u6237\u4f53\u9a8c\u6307\u6807\u5747\u83b7\u5f97\u4e86\u663e\u7740\u63d0\u5347\u3002", "conclusion": "SMAR\u65b9\u6cd5\u5728\u5b9e\u9645\u641c\u7d22\u573a\u666f\u4e2d\u5177\u6709\u6709\u6548\u6027\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2510.16072", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16072", "abs": "https://arxiv.org/abs/2510.16072", "authors": ["Farjana Yesmin"], "title": "Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation", "comment": "18 pages", "summary": "Machine learning models trained on imbalanced datasets often exhibit\nintersectional biases-systematic errors arising from the interaction of\nmultiple attributes such as object class and environmental conditions. This\npaper presents a data-driven framework for analyzing and mitigating such biases\nin image classification. We introduce the Intersectional Fairness Evaluation\nFramework (IFEF), which combines quantitative fairness metrics with\ninterpretability tools to systematically identify bias patterns in model\npredictions. Building on this analysis, we propose Bias-Weighted Augmentation\n(BWA), a novel data augmentation strategy that adapts transformation\nintensities based on subgroup distribution statistics. Experiments on the Open\nImages V7 dataset with five object classes demonstrate that BWA improves\naccuracy for underrepresented class-environment intersections by up to 24\npercentage points while reducing fairness metric disparities by 35%.\nStatistical analysis across multiple independent runs confirms the significance\nof improvements (p < 0.05). Our methodology provides a replicable approach for\nanalyzing and addressing intersectional biases in image classification systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u548c\u51cf\u8f7b\u56fe\u50cf\u5206\u7c7b\u4e2d\u4ea4\u53c9\u504f\u5dee\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5b9a\u91cf\u516c\u5e73\u6027\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u6765\u8bc6\u522b\u6a21\u578b\u9884\u6d4b\u4e2d\u7684\u504f\u5dee\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53ef\u4ee5\u6839\u636e\u5b50\u7ec4\u5206\u5e03\u7edf\u8ba1\u4fe1\u606f\u8c03\u6574\u8f6c\u6362\u5f3a\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u5f31\u52bf\u7fa4\u4f53\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u516c\u5e73\u6027\u6307\u6807\u7684\u5dee\u8ddd\u3002", "motivation": "\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u8868\u73b0\u51fa\u4ea4\u53c9\u504f\u5dee\uff0c\u5373\u7531\u5bf9\u8c61\u7c7b\u522b\u548c\u73af\u5883\u6761\u4ef6\u7b49\u591a\u4e2a\u5c5e\u6027\u76f8\u4e92\u4f5c\u7528\u5f15\u8d77\u7684\u7cfb\u7edf\u6027\u9519\u8bef\u3002", "method": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4ea4\u53c9\u516c\u5e73\u6027\u8bc4\u4f30\u6846\u67b6 (IFEF)\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5b9a\u91cf\u516c\u5e73\u6027\u6307\u6807\u4e0e\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u4ee5\u7cfb\u7edf\u5730\u8bc6\u522b\u6a21\u578b\u9884\u6d4b\u4e2d\u7684\u504f\u5dee\u6a21\u5f0f\u3002\u5728\u6b64\u5206\u6790\u7684\u57fa\u7840\u4e0a\uff0c\u8be5\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u5dee\u52a0\u6743\u589e\u5f3a (BWA)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u53ef\u6839\u636e\u5b50\u7ec4\u5206\u5e03\u7edf\u8ba1\u4fe1\u606f\u8c03\u6574\u8f6c\u6362\u5f3a\u5ea6\u3002", "result": "\u5728\u5177\u6709\u4e94\u4e2a\u5bf9\u8c61\u7c7b\u522b\u7684 Open Images V7 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBWA \u5c06\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u73af\u5883\u4ea4\u53c9\u70b9\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe 24 \u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5c06\u516c\u5e73\u6027\u6307\u6807\u7684\u5dee\u8ddd\u51cf\u5c11\u4e86 35%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u590d\u5236\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u548c\u89e3\u51b3\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf\u4e2d\u7684\u4ea4\u53c9\u504f\u5dee\u3002"}}
{"id": "2510.16096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16096", "abs": "https://arxiv.org/abs/2510.16096", "authors": ["Tina Behnia", "Puneesh Deora", "Christos Thrampoulidis"], "title": "Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization", "comment": "28 pages, 15 figures", "summary": "Language models are pretrained on sequences that blend statistical\nregularities (making text fluent) with factual associations between specific\ntokens (knowledge of facts). While recent work suggests that the variability of\ntheir interaction, such as paraphrases of factual associations, critically\ndetermines generalization ability, we lack a systematic analysis of these\nimpacts. This paper introduces a flexible synthetic testbed that combines a\nstatistical stream of generic tokens with an abstract factual stream of\nsource-target token pairs, enabling fine-grained control over their\ninteraction. The design enables the independent control of diversity nature by\nmanipulating stream composition (contextual structure) and the diversity level\nby varying which statistical streams each fact appears in. Through controlled\nexperiments, we find that while higher contextual diversity delays\nin-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)\nfactual generalization depends critically on contextual structure. In some\ncases, OOD performance follows the same trend as ID, but in others, diversity\nbecomes essential for non-trivial factual recall. Even when low diversity\nprohibits factual recall, optimal diversity levels depend on training duration.\nBeyond factual recall failures, we identify structures where statistical\ngeneralization fails independently, and others where both capabilities degrade.\nThis shows how the interplay between contextual design and diversity level\nimpacts different generalization aspects. Further, through a series of\ncontrolled interventions on the model components, we trace the OOD failures to\ndistinct optimization bottlenecks, highlighting the importance of the embedding\nand unembedding layers. Our synthetic framework allows us to isolate effects\nthat would be confounded in large-scale studies, offering a controlled testbed\nfor future investigations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u548c\u4e8b\u5b9e\u5173\u8054\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u8ba1\u89c4\u5f8b\u548c\u4e8b\u5b9e\u5173\u8054\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u800c\u8fd9\u79cd\u76f8\u4e92\u4f5c\u7528\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u7ed3\u5408\u4e86\u901a\u7528token\u7684\u7edf\u8ba1\u6d41\u548c\u6e90-\u76ee\u6807token\u5bf9\u7684\u62bd\u8c61\u4e8b\u5b9e\u6d41\uff0c\u4ece\u800c\u53ef\u4ee5\u7cbe\u7ec6\u5730\u63a7\u5236\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u901a\u8fc7\u63a7\u5236\u6d41\u7684\u7ec4\u6210\uff08\u4e0a\u4e0b\u6587\u7ed3\u6784\uff09\u6765\u63a7\u5236\u591a\u6837\u6027\u7684\u6027\u8d28\uff0c\u5e76\u901a\u8fc7\u6539\u53d8\u6bcf\u4e2a\u4e8b\u5b9e\u51fa\u73b0\u7684\u7edf\u8ba1\u6d41\u6765\u63a7\u5236\u591a\u6837\u6027\u6c34\u5e73\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u8f83\u9ad8\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u4f1a\u5ef6\u8fdf\u5206\u5e03\u5185 (ID) \u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u5bf9\u5206\u5e03\u5916 (OOD) \u4e8b\u5b9e\u6cdb\u5316\u7684\u5f71\u54cd\u4e3b\u8981\u53d6\u51b3\u4e8e\u4e0a\u4e0b\u6587\u7ed3\u6784\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0cOOD \u6027\u80fd\u4e0e ID \u9075\u5faa\u76f8\u540c\u7684\u8d8b\u52bf\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u591a\u6837\u6027\u5bf9\u4e8e\u975e\u5e73\u51e1\u7684\u4e8b\u5b9e\u53ec\u56de\u81f3\u5173\u91cd\u8981\u3002\u5373\u4f7f\u4f4e\u591a\u6837\u6027\u7981\u6b62\u4e8b\u5b9e\u53ec\u56de\uff0c\u6700\u4f73\u591a\u6837\u6027\u6c34\u5e73\u4e5f\u53d6\u51b3\u4e8e\u8bad\u7ec3\u6301\u7eed\u65f6\u95f4\u3002\u9664\u4e86\u4e8b\u5b9e\u53ec\u56de\u5931\u8d25\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u73b0\u4e86\u7edf\u8ba1\u6cdb\u5316\u72ec\u7acb\u5931\u8d25\u7684\u7ed3\u6784\uff0c\u4ee5\u53ca\u4e24\u79cd\u80fd\u529b\u90fd\u4f1a\u4e0b\u964d\u7684\u5176\u4ed6\u7ed3\u6784\u3002\u901a\u8fc7\u5bf9\u6a21\u578b\u7ec4\u4ef6\u8fdb\u884c\u4e00\u7cfb\u5217\u53d7\u63a7\u5e72\u9884\uff0c\u6211\u4eec\u5c06 OOD \u5931\u8d25\u8ffd\u6eaf\u5230\u4e0d\u540c\u7684\u4f18\u5316\u74f6\u9888\uff0c\u7a81\u51fa\u4e86\u5d4c\u5165\u548c\u975e\u5d4c\u5165\u5c42\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u548c\u591a\u6837\u6027\u6c34\u5e73\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u4f1a\u5f71\u54cd\u4e0d\u540c\u7684\u6cdb\u5316\u65b9\u9762\u3002\u8be5\u5408\u6210\u6846\u67b6\u5141\u8bb8\u6211\u4eec\u5206\u79bb\u5728\u5927\u578b\u7814\u7a76\u4e2d\u4f1a\u6df7\u6dc6\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u53d7\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2510.15980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15980", "abs": "https://arxiv.org/abs/2510.15980", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition", "comment": null, "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8ba4\u77e5\u8d1f\u8377\u8ffd\u8e2a(CLT)\uff0c\u4e00\u4e2a\u53d7\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u542f\u53d1\u7684\u6df1\u5ea6\u6a21\u578b\u7684\u4e2d\u7ea7\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u3002", "motivation": "\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\u3002", "method": "CLT\u88ab\u8868\u793a\u4e3a\u4e00\u4e2a\u4e09\u7ec4\u5206\u7684\u968f\u673a\u8fc7\u7a0b(IL_t, EL_t, GL_t)\uff0c\u5bf9\u5e94\u4e8e\u5185\u5728\u3001\u5916\u5728\u548c\u76f8\u5173\u8d1f\u8377\u3002\u6bcf\u4e2a\u7ec4\u4ef6\u901a\u8fc7\u53ef\u6d4b\u91cf\u7684\u4ee3\u7406\u5b9e\u4f8b\u5316\uff0c\u4f8b\u5982\u6ce8\u610f\u529b\u71b5\uff0cKV-cache\u672a\u547d\u4e2d\u7387\uff0c\u8868\u793a\u5206\u6563\u6027\u548c\u89e3\u7801\u7a33\u5b9a\u6027\u3002", "result": "CLT\u9884\u6d4b\u8bef\u5dee\u5f00\u59cb\uff0c\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u5b9e\u73b0\u8d1f\u8f7d\u5f15\u5bfc\u5e72\u9884\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad815-30%\u3002", "conclusion": "CLT\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2510.15950", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15950", "abs": "https://arxiv.org/abs/2510.15950", "authors": ["Arianna Francesconi", "Donato Cappetta", "Fabio Rebecchi", "Paolo Soda", "Valerio Guarrasi", "Rosa Sicilia"], "title": "Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics", "comment": "Proceedings of the Workshop on Artificial Intelligence for Biomedical\n  Data (AIBio 2025), 28th European Conference on Artificial Intelligence 2025,\n  Springer CCIS", "summary": "Parkinson's disease (PD) presents a growing global challenge, affecting over\n10 million individuals, with prevalence expected to double by 2040. Early\ndiagnosis remains difficult due to the late emergence of motor symptoms and\nlimitations of traditional clinical assessments. In this study, we propose a\nnovel pipeline that leverages keystroke dynamics as a non-invasive and scalable\nbiomarker for remote PD screening and telemonitoring. Our methodology involves\nthree main stages: (i) preprocessing of data from four distinct datasets,\nextracting four temporal signals and addressing class imbalance through the\ncomparison of three methods; (ii) pre-training eight state-of-the-art\ndeep-learning architectures on the two largest datasets, optimizing temporal\nwindowing, stride, and other hyperparameters; (iii) fine-tuning on an\nintermediate-sized dataset and performing external validation on a fourth,\nindependent cohort. Our results demonstrate that hybrid convolutional-recurrent\nand transformer-based models achieve strong external validation performance,\nwith AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal\nconvolutional model attains an AUC-ROC of 91.14% in external validation,\noutperforming existing methods that rely solely on internal validation. These\nfindings underscore the potential of keystroke dynamics as a reliable digital\nbiomarker for PD, offering a promising avenue for early detection and\ncontinuous monitoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u51fb\u952e\u52a8\u529b\u5b66\u4f5c\u4e3a\u751f\u7269\u6807\u5fd7\u7269\u8fdb\u884c\u5e15\u91d1\u68ee\u75c5\u8fdc\u7a0b\u7b5b\u67e5\u548c\u8fdc\u7a0b\u76d1\u63a7\u7684\u65b0\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u4e34\u5e8a\u8bc4\u4f30\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u8bca\u65ad\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u8fd0\u52a8\u75c7\u72b6\u51fa\u73b0\u8f83\u665a\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\u6570\u636e\u9884\u5904\u7406\u3001\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u5fae\u8c03\u3002", "result": "\u6df7\u5408\u5377\u79ef\u5faa\u73af\u6a21\u578b\u548c\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5916\u90e8\u9a8c\u8bc1\u6027\u80fd\uff0cAUC-ROC \u5206\u6570\u8d85\u8fc7 90%\uff0cF1-Score \u8d85\u8fc7 70%\u3002", "conclusion": "\u51fb\u952e\u52a8\u529b\u5b66\u6709\u6f5c\u529b\u6210\u4e3a\u5e15\u91d1\u68ee\u75c5\u53ef\u9760\u7684\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u65e9\u671f\u68c0\u6d4b\u548c\u6301\u7eed\u76d1\u6d4b\u63d0\u4f9b\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002"}}
{"id": "2510.16804", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16804", "abs": "https://arxiv.org/abs/2510.16804", "authors": ["Xiaokai Wei", "Jiajun Wu", "Daiyao Yi", "Reza Shirkavand", "Michelle Gong"], "title": "The Layout Is the Model: On Action-Item Coupling in Generative Recommendation", "comment": null, "summary": "Generative Recommendation (GR) models treat a user's interaction history as a\nsequence to be autoregressively predicted. When both items and actions (e.g.,\nwatch time, purchase, comment) are modeled, the layout-the ordering and\nvisibility of item/action tokens-critically determines what information the\nmodel can use and how it generalizes. We present a unified study of token\nlayouts for GR grounded in first principles: (P1) maximize item/action signal\nin both input/output space, (P2) preserve the conditioning relationship \"action\ngiven item\" and (P3) no information leakage.\n  While interleaved layout (where item and action occupy separate tokens)\nnaturally satisfies these principles, it also bloats sequence length with\nlarger training/inference cost. On the non-interleaved front, we design a novel\nand effective approach, Lagged Action Conditioning (LAC), which appears strange\non the surface but aligns well with the design principles to yield strong\naccuracy. Comprehensive experiments on public datasets and large-scale\nproduction logs evaluate different layout options and empirically verifies the\ndesign principles. Our proposed non-interleaved method, LAC, achieves\ncompetitive or superior quality at substantially lower FLOPs than interleaving.\nOur findings offer actionable guidance for assembling GR systems that are both\naccurate and efficient.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0f\u63a8\u8350 (GR) \u6a21\u578b\u4e2d token layout \u7684\u8bbe\u8ba1\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u63a2\u7d22 token layout \u5bf9 GR \u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u73b0\u6709 interleaved layout \u5b58\u5728\u5e8f\u5217\u8fc7\u957f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u975e interleaved \u65b9\u6cd5\uff0cLagged Action Conditioning (LAC)\uff0c\u5e76\u4ece\u6700\u5927\u5316\u4fe1\u53f7\u3001\u4fdd\u6301\u6761\u4ef6\u5173\u7cfb\u548c\u9632\u6b62\u4fe1\u606f\u6cc4\u6f0f\u4e09\u4e2a\u539f\u5219\u8fdb\u884c\u5206\u6790\u3002", "result": "LAC \u65b9\u6cd5\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u5927\u89c4\u6a21\u751f\u4ea7\u65e5\u5fd7\u4e0a\u8868\u73b0\u51fa\u4e0e interleaved \u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86 FLOPs\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u51c6\u786e\u9ad8\u6548\u7684 GR \u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.16088", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16088", "abs": "https://arxiv.org/abs/2510.16088", "authors": ["Zia Badar"], "title": "Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch", "comment": null, "summary": "Quantization of neural networks provides benefits of inference in less\ncompute and memory requirements. Previous work in quantization lack two\nimportant aspects which this work provides. First almost all previous work in\nquantization used a non-differentiable approach and for learning; the\nderivative is usually set manually in backpropogation which make the learning\nability of algorithm questionable, our approach is not just differentiable, we\nalso provide proof of convergence of our approach to the optimal neural\nnetwork. Second previous work in shift/logrithmic quantization either have\navoided activation quantization along with weight quantization or achieved less\naccuracy. Learning logrithmic quantize values of form $2^n$ requires the\nquantization function can scale to more than 1 bit quantization which is\nanother benifit of our quantization that it provides $n$ bits quantization as\nwell. Our approach when tested with image classification task using imagenet\ndataset, resnet18 and weight quantization only achieves less than 1 percent\naccuracy compared to full precision accuracy while taking only 15 epochs to\ntrain using shift bit quantization and achieves comparable to SOTA approaches\naccuracy in both weight and activation quantization using shift bit\nquantization in 15 training epochs with slightly higher(only higher cpu\ninstructions) inference cost compared to 1 bit quantization(without logrithmic\nquantization) and not requiring any higher precision multiplication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u5bf9\u6570\u91cf\u5316\uff0c\u5e76\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5f88\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u4e4b\u524d\u7684\u91cf\u5316\u65b9\u6cd5\u4e0d\u53ef\u5fae\uff0c\u5b66\u4e60\u80fd\u529b\u53d7\u9650\uff0c\u4e14\u5728\u6743\u91cd\u548c\u6fc0\u6d3b\u540c\u65f6\u91cf\u5316\u65f6\u7cbe\u5ea6\u8f83\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u652f\u6301n\u6bd4\u7279\u91cf\u5316\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528ResNet18\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u4ec5\u4f7f\u7528\u6743\u91cd\u8fdb\u884c\u91cf\u5316\u65f6\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e1%\u3002\u5728\u6743\u91cd\u548c\u6fc0\u6d3b\u540c\u65f6\u91cf\u5316\u65f6\uff0c\u7cbe\u5ea6\u4e0eSOTA\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2510.16173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16173", "abs": "https://arxiv.org/abs/2510.16173", "authors": ["Aria Pessianzadeh", "Naima Sultana", "Hildegarde Van den Bulck", "David Gefen", "Shahin Jabari", "Rezvaneh Rezapour"], "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions", "comment": null, "summary": "The rise of generative AI (GenAI) has impacted many aspects of human life. As\nthese systems become embedded in everyday practices, understanding public trust\nin them also becomes essential for responsible adoption and governance. Prior\nwork on trust in AI has largely drawn from psychology and human-computer\ninteraction, but there is a lack of computational, large-scale, and\nlongitudinal approaches to measuring trust and distrust in GenAI and large\nlanguage models (LLMs). This paper presents the first computational study of\nTrust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)\nspanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a\nrepresentative sample were combined with classification models to scale\nanalysis. We find that Trust and Distrust are nearly balanced over time, with\nshifts around major model releases. Technical performance and usability\ndominate as dimensions, while personal experience is the most frequent reason\nshaping attitudes. Distinct patterns also emerge across trustors (e.g.,\nexperts, ethicists, general users). Our results provide a methodological\nframework for large-scale Trust analysis and insights into evolving public\nperceptions of GenAI.", "AI": {"tldr": "\u672c\u6587\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd (GenAI) \u7684\u4fe1\u4efb\u548c\u4e0d\u4fe1\u4efb\u8fdb\u884c\u4e86\u9996\u6b21\u8ba1\u7b97\u7814\u7a76\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a\u591a\u5e74\u7684 Reddit \u6570\u636e\u96c6\u3002", "motivation": "\u8d1f\u8d23\u4efb\u5730\u91c7\u7528\u548c\u7ba1\u7406\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u7406\u89e3\u516c\u4f17\u5bf9\u5b83\u4eec\u7684\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u5f80\u5bf9\u4eba\u5de5\u667a\u80fd\u4fe1\u4efb\u7684\u7814\u7a76\u5927\u591a\u6765\u81ea\u5fc3\u7406\u5b66\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\uff0c\u4f46\u7f3a\u4e4f\u5bf9 GenAI \u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u884c\u5927\u89c4\u6a21\u3001\u957f\u671f\u4fe1\u4efb\u548c\u4e0d\u4fe1\u4efb\u6d4b\u91cf\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4f17\u5305\u6ce8\u91ca\u548c\u4e00\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u6837\u672c\u7684\u5206\u7c7b\u6a21\u578b\u6765\u6269\u5c55\u5206\u6790\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a\u591a\u5e74\u7684 Reddit \u6570\u636e\u96c6 (2022--2025)\uff0c\u8de8\u8d8a 39 \u4e2a subreddit \u548c 197,618 \u4e2a\u5e16\u5b50\u3002", "result": "\u4fe1\u4efb\u548c\u4e0d\u4fe1\u4efb\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u51e0\u4e4e\u662f\u5e73\u8861\u7684\uff0c\u5e76\u4e14\u5728\u4e3b\u8981\u6a21\u578b\u53d1\u5e03\u524d\u540e\u53d1\u751f\u4e86\u53d8\u5316\u3002\u6280\u672f\u6027\u80fd\u548c\u53ef\u7528\u6027\u662f\u4e3b\u8981\u7684\u7ef4\u5ea6\uff0c\u800c\u4e2a\u4eba\u7ecf\u9a8c\u662f\u5f71\u54cd\u6001\u5ea6\u7684\u6700\u5e38\u89c1\u539f\u56e0\u3002\u5728\u4fe1\u4efb\u8005 (\u4f8b\u5982\uff0c\u4e13\u5bb6\u3001\u4f26\u7406\u5b66\u5bb6\u3001\u666e\u901a\u7528\u6237) \u4e4b\u95f4\u4e5f\u51fa\u73b0\u4e86\u4e0d\u540c\u7684\u6a21\u5f0f\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u4e3a\u5927\u89c4\u6a21\u4fe1\u4efb\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b9\u6cd5\u6846\u67b6\uff0c\u5e76\u6df1\u5165\u4e86\u89e3\u4e86\u516c\u4f17\u5bf9 GenAI \u4e0d\u65ad\u53d8\u5316\u7684\u770b\u6cd5\u3002"}}
{"id": "2510.15981", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.15981", "abs": "https://arxiv.org/abs/2510.15981", "authors": ["Rafael Cabral", "Tuan Manh Do", "Xuejun Yu", "Wai Ming Tai", "Zijin Feng", "Xin Shen"], "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization", "comment": null, "summary": "Proof autoformalization, the task of translating natural language theorems\nand proofs into machine-verifiable code, is a critical step for integrating\nlarge language models into rigorous mathematical workflows. Current approaches\nfocus on producing executable code, but they frequently fail to preserve the\nsemantic meaning and logical structure of the original human-written argument.\nTo address this, we introduce ProofFlow, a novel pipeline that treats\nstructural fidelity as a primary objective. ProofFlow first constructs a\ndirected acyclic graph (DAG) to map the logical dependencies between proof\nsteps. Then, it employs a novel lemma-based approach to systematically\nformalize each step as an intermediate lemma, preserving the logical structure\nof the original argument. To facilitate evaluation, we present a new benchmark\nof 184 undergraduate-level problems, manually annotated with step-by-step\nsolutions and logical dependency graphs, and introduce ProofScore, a new\ncomposite metric to evaluate syntactic correctness, semantic faithfulness, and\nstructural fidelity. Experimental results show our pipeline sets a new\nstate-of-the-art for autoformalization, achieving a ProofScore of 0.545,\nsubstantially exceeding baselines like full-proof formalization (0.123), which\nprocesses the entire proof at once, and step-proof formalization (0.072), which\nhandles each step independently. Our pipeline, benchmark, and score metric are\nopen-sourced to encourage further progress at\nhttps://github.com/Huawei-AI4Math/ProofFlow.", "AI": {"tldr": "ProofFlow\u662f\u4e00\u4e2a\u65b0\u7684\u81ea\u52a8\u5f62\u5f0f\u5316pipeline\uff0c\u5b83\u4f18\u5148\u8003\u8651\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u5e76\u4f7f\u7528DAG\u56fe\u548c\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u6765\u5f62\u5f0f\u5316\u8bc1\u660e\u6b65\u9aa4\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u6301\u8bed\u4e49\u610f\u4e49\u548c\u539f\u59cb\u8bba\u8bc1\u7684\u903b\u8f91\u7ed3\u6784\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e00\u4e2a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u6765\u6620\u5c04\u8bc1\u660e\u6b65\u9aa4\u4e4b\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\u3002\u7136\u540e\uff0c\u5b83\u91c7\u7528\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u6765\u7cfb\u7edf\u5730\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5f62\u5f0f\u5316\u4e3a\u4e2d\u95f4\u5f15\u7406\uff0c\u4ece\u800c\u4fdd\u7559\u539f\u59cb\u8bba\u8bc1\u7684\u903b\u8f91\u7ed3\u6784\u3002", "result": "ProofFlow\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684state-of-the-art\uff0cProofScore\u4e3a0.545\uff0c\u5927\u5927\u8d85\u8fc7\u4e86full-proof formalization (0.123) \u548c step-proof formalization (0.072)\u3002", "conclusion": "ProofFlow pipeline\uff0cbenchmark\u548c\u8bc4\u5206\u6307\u6807\u5df2\u5f00\u6e90\uff0c\u4ee5\u9f13\u52b1\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.15954", "categories": ["cs.LG", "cs.CE", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.15954", "abs": "https://arxiv.org/abs/2510.15954", "authors": ["Hongzheng Shi", "Yuhang Wang", "Xiao Liu"], "title": "Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter", "comment": null, "summary": "As wildfires become increasingly destructive and expensive to control,\neffective management of active wildfires requires accurate, real-time fire\nspread predictions. To enhance the forecasting accuracy of active fires, data\nassimilation plays a vital role by integrating observations (such as\nremote-sensing data) and fire predictions generated from numerical models. This\npaper provides a comprehensive investigation on the application of a recently\nproposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter\n(EnSF) -- to the data assimilation problem for real-time active wildfire spread\npredictions. Leveraging a score-based generative diffusion model, EnSF has been\nshown to have superior accuracy for high-dimensional nonlinear filtering\nproblems, making it an ideal candidate for the filtering problems of wildfire\nspread models. Technical details are provided, and our numerical investigations\ndemonstrate that EnSF provides superior accuracy, stability, and computational\nefficiency, establishing it as a robust and practical method for wildfire data\nassimilation. Our code has been made publicly available.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528 Ensemble Score Filter (EnSF) \u7b97\u6cd5\u8fdb\u884c\u5b9e\u65f6\u91ce\u706b\u8513\u5ef6\u9884\u6d4b\u7684\u6570\u636e\u540c\u5316\u95ee\u9898\u3002", "motivation": "\u6709\u6548\u7ba1\u7406\u91ce\u706b\u9700\u8981\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u706b\u52bf\u8513\u5ef6\u9884\u6d4b\u3002\u6570\u636e\u540c\u5316\u901a\u8fc7\u6574\u5408\u89c2\u6d4b\u6570\u636e\u548c\u6570\u503c\u6a21\u578b\u9884\u6d4b\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u5e94\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6ee4\u6ce2\u7b97\u6cd5 EnSF \u6765\u89e3\u51b3\u91ce\u706b\u8513\u5ef6\u6a21\u578b\u7684\u6570\u636e\u540c\u5316\u95ee\u9898\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0cEnSF \u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "EnSF \u662f\u4e00\u79cd\u7a33\u5065\u5b9e\u7528\u7684\u91ce\u706b\u6570\u636e\u540c\u5316\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.16925", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16925", "abs": "https://arxiv.org/abs/2510.16925", "authors": ["Zhiding Liu", "Ben Chen", "Mingyue Cheng", "Enchong Chen", "Li Li", "Chenyi Lei", "Wenwu Ou", "Han Li", "Kun Gai"], "title": "Towards Context-aware Reasoning-enhanced Generative Searching in E-commerce", "comment": null, "summary": "Search-based recommendation is one of the most critical application scenarios\nin e-commerce platforms. Users' complex search contexts--such as spatiotemporal\nfactors, historical interactions, and current query's information--constitute\nan essential part of their decision-making, reflecting implicit preferences\nthat complement explicit query terms. Modeling such rich contextual signals and\ntheir intricate associations with candidate items remains a key challenge.\nAlthough numerous efforts have been devoted to building more effective search\nmethods, existing approaches still show limitations in integrating contextual\ninformation, which hinders their ability to fully capture user intent.\n  To address these challenges, we propose a context-aware reasoning-enhanced\ngenerative search framework for better \\textbf{understanding the complicated\ncontext}. Specifically, the framework first unifies heterogeneous user and item\ncontexts into textual representations or text-based semantic identifiers and\naligns them. To overcome the lack of explicit reasoning trajectories, we\nintroduce a self-evolving post-training paradigm that iteratively combines\nsupervised fine-tuning and reinforcement learning to progressively enhance the\nmodel's reasoning capability. In addition, we identify potential biases in\nexisting RL algorithms when applied to search scenarios and present a debiased\nvariant of GRPO to improve ranking performance. Extensive experiments on search\nlog data collected from a real-world e-commerce platform demonstrate that our\napproach achieves superior performance compared with strong baselines,\nvalidating its effectiveness for search-based recommendation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63a8\u7406\u589e\u5f3a\u751f\u6210\u641c\u7d22\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u7684\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u6539\u8fdb\u57fa\u4e8e\u641c\u7d22\u7684\u63a8\u8350\u3002", "motivation": "\u73b0\u6709\u7684\u641c\u7d22\u65b9\u6cd5\u5728\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u963b\u788d\u4e86\u5b83\u4eec\u5145\u5206\u6355\u6349\u7528\u6237\u610f\u56fe\u7684\u80fd\u529b\u3002\u7528\u6237\u590d\u6742\u7684\u641c\u7d22\u4e0a\u4e0b\u6587\u662f\u4ed6\u4eec\u51b3\u7b56\u5236\u5b9a\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u53cd\u6620\u4e86\u8865\u5145\u660e\u786e\u67e5\u8be2\u8bcd\u7684\u9690\u542b\u504f\u597d\u3002\u5efa\u6a21\u8fd9\u79cd\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u53f7\u53ca\u5176\u4e0e\u5019\u9009\u9879\u76ee\u7684\u590d\u6742\u5173\u8054\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u5c06\u5f02\u6784\u7528\u6237\u548c\u9879\u76ee\u4e0a\u4e0b\u6587\u7edf\u4e00\u4e3a\u6587\u672c\u8868\u793a\u6216\u57fa\u4e8e\u6587\u672c\u7684\u8bed\u4e49\u6807\u8bc6\u7b26\uff0c\u5e76\u5c06\u5b83\u4eec\u5bf9\u9f50\u3002\u4e3a\u4e86\u514b\u670d\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u8fed\u4ee3\u5730\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u9010\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5728\u73b0\u6709RL\u7b97\u6cd5\u5e94\u7528\u4e8e\u641c\u7d22\u573a\u666f\u65f6\u8bc6\u522b\u51fa\u6f5c\u5728\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86GRPO\u7684\u53bb\u504f\u53d8\u4f53\uff0c\u4ee5\u63d0\u9ad8\u6392\u5e8f\u6027\u80fd\u3002", "result": "\u5728\u4ece\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u6536\u96c6\u7684\u641c\u7d22\u65e5\u5fd7\u6570\u636e\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u57fa\u4e8e\u641c\u7d22\u7684\u63a8\u8350\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u57fa\u4e8e\u641c\u7d22\u7684\u63a8\u8350\u4e2d\u6709\u6548\u3002"}}
{"id": "2510.16115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16115", "abs": "https://arxiv.org/abs/2510.16115", "authors": ["Jianhan Lin", "Yuchu Qin", "Shuai Gao", "Yikang Rui", "Jie Liu", "Yanjie Lv"], "title": "StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection", "comment": null, "summary": "Well-maintained road networks are crucial for achieving Sustainable\nDevelopment Goal (SDG) 11. Road surface damage not only threatens traffic\nsafety but also hinders sustainable urban development. Accurate detection,\nhowever, remains challenging due to the diverse shapes of damages, the\ndifficulty of capturing slender cracks with high aspect ratios, and the high\nerror rates in small-scale damage recognition. To address these issues, we\npropose StripRFNet, a novel deep neural network comprising three modules: (1) a\nShape Perception Module (SPM) that enhances shape discrimination via large\nseparable kernel attention (LSKA) in multi-scale feature aggregation; (2) a\nStrip Receptive Field Module (SRFM) that employs large strip convolutions and\npooling to capture features of slender cracks; and (3) a Small-Scale\nEnhancement Module (SSEM) that leverages a high-resolution P2 feature map, a\ndedicated detection head, and dynamic upsampling to improve small-object\ndetection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses\nexisting methods. On the Chinese subset, it improves F1-score, mAP50, and\nmAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,\nrespectively. On the full dataset, it achieves the highest F1-score of 80.33%\ncompared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while\nmaintaining competitive inference speed. These results demonstrate that\nStripRFNet achieves state-of-the-art accuracy and real-time efficiency,\noffering a promising tool for intelligent road maintenance and sustainable\ninfrastructure management.", "AI": {"tldr": "StripRFNet: A new deep neural network for accurate road surface damage detection.", "motivation": "Accurate road surface damage detection is crucial for traffic safety and sustainable urban development (SDG 11), but it is challenging due to diverse damage shapes, slender cracks, and high error rates in small-scale damage recognition.", "method": "The paper proposes StripRFNet, which includes a Shape Perception Module (SPM) using large separable kernel attention, a Strip Receptive Field Module (SRFM) employing large strip convolutions and pooling, and a Small-Scale Enhancement Module (SSEM) leveraging a high-resolution feature map and dynamic upsampling.", "result": "StripRFNet outperforms existing methods on the RDD2022 benchmark, improving F1-score, mAP50, and mAP50:95 on the Chinese subset. It achieves the highest F1-score of 80.33% on the full dataset while maintaining competitive inference speed.", "conclusion": "StripRFNet achieves state-of-the-art accuracy and real-time efficiency, offering a promising tool for intelligent road maintenance and sustainable infrastructure management."}}
{"id": "2510.16198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16198", "abs": "https://arxiv.org/abs/2510.16198", "authors": ["Mohamed Gamil", "Abdelrahman Elsayed", "Abdelrahman Lila", "Ahmed Gad", "Hesham Abdelgawad", "Mohamed Aref", "Ahmed Fares"], "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture", "comment": null, "summary": "Despite recent advances in AI, multimodal culturally diverse datasets are\nstill limited, particularly for regions in the Middle East and Africa. In this\npaper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian\nculture. By designing and running a new data collection pipeline, we collected\nover 3,000 images, covering 313 concepts across landmarks, food, and folklore.\nEach entry in the dataset is manually validated for cultural authenticity and\nmultimodal coherence. EgMM-Corpus aims to provide a reliable resource for\nevaluating and training vision-language models in an Egyptian cultural context.\nWe further evaluate the zero-shot performance of Contrastive Language-Image\nPre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and\n36.4% Top-5 accuracy in classification. These results underscore the existing\ncultural bias in large-scale vision-language models and demonstrate the\nimportance of EgMM-Corpus as a benchmark for developing culturally aware\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a EgMM-Corpus \u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u57c3\u53ca\u6587\u5316\uff0c\u5305\u542b 3000 \u591a\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6\u5730\u6807\u3001\u98df\u7269\u548c\u6c11\u4fd7\u7b49 313 \u4e2a\u6982\u5ff5\u3002", "motivation": "\u73b0\u6709\u7684 AI \u6280\u672f\u5728\u591a\u6a21\u6001\u6587\u5316\u591a\u6837\u6027\u6570\u636e\u96c6\u65b9\u9762\u4ecd\u7136\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u4e2d\u4e1c\u548c\u975e\u6d32\u5730\u533a\u3002", "method": "\u8bbe\u8ba1\u5e76\u8fd0\u884c\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u624b\u52a8\u9a8c\u8bc1\u4e86\u6587\u5316\u771f\u5b9e\u6027\u548c\u591a\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5728 EgMM-Corpus \u4e0a\u8bc4\u4f30\u4e86 Contrastive Language-Image Pre-training CLIP \u7684 zero-shot \u6027\u80fd\uff0cTop-1 \u51c6\u786e\u7387\u4e3a 21.2%\uff0cTop-5 \u51c6\u786e\u7387\u4e3a 36.4%\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6587\u5316\u504f\u89c1\uff0c\u5e76\u8bc1\u660e\u4e86 EgMM-Corpus \u4f5c\u4e3a\u5f00\u53d1\u5177\u6709\u6587\u5316\u610f\u8bc6\u7684\u6a21\u578b\u57fa\u51c6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.15983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15983", "abs": "https://arxiv.org/abs/2510.15983", "authors": ["Sarah Rebecca Ondraszek", "J\u00f6rg Waitelonis", "Katja Keller", "Claudia Niessner", "Anna M. Jacyszyn", "Harald Sack"], "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science", "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th\n  International Workshop on Scientific Knowledge: Representation, Discovery,\n  and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th\n  International Semantic Web Conference, ISWC 2025. To be published in CEUR\n  proceedings", "summary": "An essential component for evaluating and comparing physical and cognitive\ncapabilities between populations is the testing of various factors related to\nhuman performance. As a core part of sports science research, testing motor\nperformance enables the analysis of the physical health of different\ndemographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe\nInstitute of Technology, is an infrastructure for publishing and archiving\nresearch data in sports science, particularly in the field of motor performance\nresearch. In this paper, we present our vision for creating a knowledge graph\nfrom MO|RE data. With an ontology rooted in the Basic Formal Ontology, our\napproach centers on formally representing the interrelation of plan\nspecifications, specific processes, and related measurements. Our goal is to\ntransform how motor performance data are modeled and shared across studies,\nmaking it standardized and machine-understandable. The idea presented here is\ndeveloped within the Leibniz Science Campus ``Digital Transformation of\nResearch'' (DiTraRe).", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5361\u5c14\u65af\u9c81\u5384\u7406\u5de5\u5b66\u9662\u5f00\u53d1\u7684\u8fd0\u52a8\u7814\u7a76 (MO|RE) \u6570\u636e\u5b58\u50a8\u5e93\uff0c\u8be5\u5b58\u50a8\u5e93\u7528\u4e8e\u53d1\u5e03\u548c\u5b58\u6863\u8fd0\u52a8\u79d1\u5b66\u9886\u57df\u7684\u7814\u7a76\u6570\u636e\uff0c\u7279\u522b\u662f\u8fd0\u52a8\u8868\u73b0\u7814\u7a76\u9886\u57df\u7684\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u4eba\u7fa4\u4e4b\u95f4\u7684\u8eab\u4f53\u548c\u8ba4\u77e5\u80fd\u529b\uff0c\u6d4b\u8bd5\u4e0e\u4eba\u7c7b\u8868\u73b0\u76f8\u5173\u7684\u5404\u79cd\u56e0\u7d20\u81f3\u5173\u91cd\u8981\u3002\u4f5c\u4e3a\u8fd0\u52a8\u79d1\u5b66\u7814\u7a76\u7684\u6838\u5fc3\u90e8\u5206\uff0c\u8fd0\u52a8\u8868\u73b0\u6d4b\u8bd5\u80fd\u591f\u5206\u6790\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u8eab\u4f53\u5065\u5eb7\u72b6\u51b5\uff0c\u5e76\u4f7f\u5b83\u4eec\u5177\u6709\u53ef\u6bd4\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece MO|RE \u6570\u636e\u521b\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u613f\u666f\u3002\u8be5\u65b9\u6cd5\u4ee5\u690d\u6839\u4e8e\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\u7684\u672c\u4f53\u4e3a\u4e2d\u5fc3\uff0c\u6b63\u5f0f\u8868\u793a\u8ba1\u5212\u89c4\u8303\u3001\u7279\u5b9a\u8fc7\u7a0b\u548c\u76f8\u5173\u6d4b\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u3002", "result": "\u672c\u6587\u65e8\u5728\u6539\u53d8\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u7684\u5efa\u6a21\u548c\u8de8\u7814\u7a76\u5171\u4eab\u65b9\u5f0f\uff0c\u4f7f\u5176\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u60f3\u6cd5\u662f\u5728\u83b1\u5e03\u5c3c\u8328\u79d1\u5b66\u56ed\u533a\u201c\u7814\u7a76\u7684\u6570\u5b57\u5316\u8f6c\u578b\u201d (DiTraRe) \u4e2d\u5f00\u53d1\u7684\u3002"}}
{"id": "2510.15955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15955", "abs": "https://arxiv.org/abs/2510.15955", "authors": ["Kiran Kate", "Yara Rizk", "Poulami Ghosh", "Ashu Gulati", "Tathagata Chakraborti", "Zidane Wright", "Mayank Agarwal"], "title": "How Good Are LLMs at Processing Tool Outputs?", "comment": null, "summary": "Most realistic task automation problems require large language models (LLMs)\nto call tools, which often return complex JSON responses. These responses must\nbe further processed to derive the information necessary for task completion.\nThe ability of LLMs to do so is under-studied. In this paper, we study the tool\nresponse processing task and LLMs' abilities to process structured (JSON)\nresponses. We created a dataset for this task, and evaluated 15 open and closed\nweight models using multiple prompting approaches. Our results show that JSON\nprocessing remains a difficult task even for frontier models across multiple\nprompting strategies. The optimal response processing strategy depends on both\nthe nature and size of the tool outputs, as well as the complexity of the\nrequired reasoning. Variations in processing approaches can lead to performance\ndifferences ranging from 3\\% to 50\\%.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5de5\u5177\u8fd4\u56de\u7684\u590d\u6742JSON\u54cd\u5e94\u65f6\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u5f71\u54cd\u4e86\u4efb\u52a1\u81ea\u52a8\u5316\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u7ed3\u6784\u5316\uff08JSON\uff09\u54cd\u5e94\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u73b0\u5b9e\u4efb\u52a1\u81ea\u52a8\u5316\u95ee\u9898\u9700\u8981\u5b83\u4eec\u8c03\u7528\u5de5\u5177\u5e76\u5904\u7406\u8fd4\u56de\u7684\u590d\u6742JSON\u54cd\u5e94\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u591a\u79cdprompting\u65b9\u6cd5\u8bc4\u4f30\u4e8615\u4e2a\u5f00\u653e\u548c\u5c01\u95ed\u6743\u91cd\u6a21\u578b\u3002", "result": "\u5373\u4f7f\u5bf9\u4e8e\u524d\u6cbf\u6a21\u578b\uff0cJSON\u5904\u7406\u4ecd\u7136\u662f\u4e00\u9879\u56f0\u96be\u7684\u4efb\u52a1\uff0c\u6700\u4f73\u54cd\u5e94\u5904\u7406\u7b56\u7565\u53d6\u51b3\u4e8e\u5de5\u5177\u8f93\u51fa\u7684\u6027\u8d28\u548c\u5927\u5c0f\u4ee5\u53ca\u6240\u9700\u63a8\u7406\u7684\u590d\u6742\u6027\u3002\u5904\u7406\u65b9\u6cd5\u7684\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f43\uff05\u523050\uff05\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406JSON\u54cd\u5e94\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u4e14\u6700\u4f73\u5904\u7406\u7b56\u7565\u7684\u9009\u62e9\u53d6\u51b3\u4e8e\u591a\u79cd\u56e0\u7d20\u3002"}}
{"id": "2510.17228", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17228", "abs": "https://arxiv.org/abs/2510.17228", "authors": ["Qing Shi", "Jing He", "Qiaosheng Chen", "Gong Cheng"], "title": "DSEBench: A Test Collection for Explainable Dataset Search with Examples", "comment": "34 pages, 5 figures, submitted to Knowledge-Based Systems", "summary": "Dataset search has been an established information retrieval task. Current\nparadigms either retrieve datasets that are relevant to a keyword query or find\ndatasets that are similar to an input target dataset. To allow for their\ncombined specification of information needs, in this article, we investigate\nthe more generalized task of Dataset Search with Examples (DSE) and further\nextend it to Explainable DSE that requires identifying the metadata and content\nfields of a dataset that indicate its relevance to the query and similarity to\nthe target datasets. To facilitate this research, we construct DSEBench, a test\ncollection that provides high-quality dataset- and field-level annotations to\nenable the evaluation of explainable DSE. We also employ a large language model\nto generate numerous annotations to be used for training. We establish\nextensive baselines on DSEBench by adapting and evaluating a variety of sparse,\ndense, and LLM-based retrieval, reranking, and explanation methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6570\u636e\u96c6\u641c\u7d22\u4e0e\u793a\u4f8b\uff08DSE\uff09\u4efb\u52a1\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u53ef\u89e3\u91ca\u7684DSE\uff0c\u8981\u6c42\u8bc6\u522b\u6570\u636e\u96c6\u7684\u5143\u6570\u636e\u548c\u5185\u5bb9\u5b57\u6bb5\uff0c\u8fd9\u4e9b\u5b57\u6bb5\u8868\u660e\u5176\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u548c\u4e0e\u76ee\u6807\u6570\u636e\u96c6\u7684\u76f8\u4f3c\u6027\u3002", "motivation": "\u4e3a\u4e86\u7ed3\u5408\u4fe1\u606f\u9700\u6c42\u7684\u89c4\u8303\uff0c\u672c\u6587\u7814\u7a76\u4e86\u66f4\u5e7f\u4e49\u7684DSE\u4efb\u52a1\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u53ef\u89e3\u91ca\u7684DSE\u3002", "method": "\u6211\u4eec\u6784\u5efa\u4e86DSEBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u6d4b\u8bd5\u96c6\u5408\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u5b57\u6bb5\u7ea7\u6ce8\u91ca\uff0c\u4ee5\u652f\u6301\u53ef\u89e3\u91ca\u7684DSE\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u5927\u91cf\u6ce8\u91ca\u4ee5\u7528\u4e8e\u8bad\u7ec3\u3002\u6211\u4eec\u901a\u8fc7\u8c03\u6574\u548c\u8bc4\u4f30\u5404\u79cd\u7a00\u758f\u3001\u5bc6\u96c6\u548c\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u3001\u91cd\u65b0\u6392\u5e8f\u548c\u89e3\u91ca\u65b9\u6cd5\uff0c\u5728DSEBench\u4e0a\u5efa\u7acb\u4e86\u5e7f\u6cdb\u7684\u57fa\u7ebf\u3002", "result": "\u6211\u4eec\u901a\u8fc7\u8c03\u6574\u548c\u8bc4\u4f30\u5404\u79cd\u7a00\u758f\u3001\u5bc6\u96c6\u548c\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u3001\u91cd\u65b0\u6392\u5e8f\u548c\u89e3\u91ca\u65b9\u6cd5\uff0c\u5728DSEBench\u4e0a\u5efa\u7acb\u4e86\u5e7f\u6cdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u6570\u636e\u96c6\u641c\u7d22\u4e0e\u793a\u4f8b\uff08DSE\uff09\u4efb\u52a1\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u53ef\u89e3\u91ca\u7684DSE\uff0c\u8981\u6c42\u8bc6\u522b\u6570\u636e\u96c6\u7684\u5143\u6570\u636e\u548c\u5185\u5bb9\u5b57\u6bb5\uff0c\u8fd9\u4e9b\u5b57\u6bb5\u8868\u660e\u5176\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u548c\u4e0e\u76ee\u6807\u6570\u636e\u96c6\u7684\u76f8\u4f3c\u6027\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8fd9\u9879\u7814\u7a76\uff0c\u6211\u4eec\u6784\u5efa\u4e86DSEBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u6d4b\u8bd5\u96c6\u5408\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u5b57\u6bb5\u7ea7\u6ce8\u91ca\uff0c\u4ee5\u652f\u6301\u53ef\u89e3\u91ca\u7684DSE\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u5927\u91cf\u6ce8\u91ca\u4ee5\u7528\u4e8e\u8bad\u7ec3\u3002"}}
{"id": "2510.16118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16118", "abs": "https://arxiv.org/abs/2510.16118", "authors": ["Nishad Sahu", "Shounak Sural", "Aditya Satish Patil", "Ragunathan", "Rajkumar"], "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025\n  Workshops", "summary": "Reliable perception is fundamental for safety critical decision making in\nautonomous driving. Yet, vision based object detector neural networks remain\nvulnerable to uncertainty arising from issues such as data bias and\ndistributional shifts. In this paper, we introduce ObjectTransforms, a\ntechnique for quantifying and reducing uncertainty in vision based object\ndetection through object specific transformations at both training and\ninference times. At training time, ObjectTransforms perform color space\nperturbations on individual objects, improving robustness to lighting and color\nvariations. ObjectTransforms also uses diffusion models to generate realistic,\ndiverse pedestrian instances. At inference time, object perturbations are\napplied to detected objects and the variance of detection scores are used to\nquantify predictive uncertainty in real time. This uncertainty signal is then\nused to filter out false positives and also recover false negatives, improving\nthe overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K\ndataset demonstrate that our method yields notable accuracy improvements and\nuncertainty reduction across all object classes during training, while\npredicting desirably higher uncertainty values for false positives as compared\nto true positives during inference. Our results highlight the potential of\nObjectTransforms as a lightweight yet effective mechanism for reducing and\nquantifying uncertainty in vision-based perception during training and\ninference respectively.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a ObjectTransforms \u7684\u6280\u672f\uff0c\u7528\u4e8e\u91cf\u5316\u548c\u51cf\u5c11\u57fa\u4e8e\u89c6\u89c9\u7684\u5bf9\u8c61\u68c0\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u7684\u5bf9\u8c61\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u53d7\u5230\u6570\u636e\u504f\u5dee\u548c\u5206\u5e03\u504f\u79fb\u7b49\u95ee\u9898\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u53ef\u9760\u7684\u611f\u77e5\u662f\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u51b3\u7b56\u7684\u57fa\u7840\u3002", "method": "ObjectTransforms \u5728\u8bad\u7ec3\u65f6\u5bf9\u5355\u4e2a\u5bf9\u8c61\u6267\u884c\u989c\u8272\u7a7a\u95f4\u6270\u52a8\uff0c\u63d0\u9ad8\u5bf9\u5149\u7167\u548c\u989c\u8272\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u3001\u591a\u6837\u5316\u7684\u884c\u4eba\u5b9e\u4f8b\u3002\u5728\u63a8\u7406\u65f6\uff0c\u5bf9\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u5e94\u7528\u5bf9\u8c61\u6270\u52a8\uff0c\u5e76\u4f7f\u7528\u68c0\u6d4b\u5206\u6570\u7684\u65b9\u5dee\u6765\u5b9e\u65f6\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728 NuImages 10K \u6570\u636e\u96c6\u4e0a\u4f7f\u7528 YOLOv8 \u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u671f\u95f4\u663e\u7740\u63d0\u9ad8\u4e86\u6240\u6709\u5bf9\u8c61\u7c7b\u522b\u7684\u51c6\u786e\u6027\u548c\u51cf\u5c11\u4e86\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u5728\u63a8\u7406\u671f\u95f4\u9884\u6d4b\u7684\u5047\u9633\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u503c\u9ad8\u4e8e\u771f\u9633\u6027\u3002", "conclusion": "ObjectTransforms \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u673a\u5236\uff0c\u53ef\u5206\u522b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u51cf\u5c11\u548c\u91cf\u5316\u57fa\u4e8e\u89c6\u89c9\u7684\u611f\u77e5\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16227", "abs": "https://arxiv.org/abs/2510.16227", "authors": ["Jennifer Hu", "Ethan Gotlieb Wilcox", "Siyuan Song", "Kyle Mahowald", "Roger P. Levy"], "title": "What Can String Probability Tell Us About Grammaticality?", "comment": null, "summary": "What have language models (LMs) learned about grammar? This question remains\nhotly debated, with major ramifications for linguistic theory. However, since\nprobability and grammaticality are distinct notions in linguistics, it is not\nobvious what string probabilities can reveal about an LM's underlying\ngrammatical knowledge. We present a theoretical analysis of the relationship\nbetween grammar, meaning, and string probability, based on simple assumptions\nabout the generative process of corpus data. Our framework makes three\npredictions, which we validate empirically using 280K sentence pairs in English\nand Chinese: (1) correlation between the probability of strings within minimal\npairs, i.e., string pairs with minimal semantic differences; (2) correlation\nbetween models' and humans' deltas within minimal pairs; and (3) poor\nseparation in probability space between unpaired grammatical and ungrammatical\nstrings. Our analyses give theoretical grounding for using probability to learn\nabout LMs' structural knowledge, and suggest directions for future work in LM\ngrammatical evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b(LM)\u5728\u8bed\u6cd5\u65b9\u9762\u7684\u5b66\u4e60\u60c5\u51b5\uff0c\u5e76\u5206\u6790\u4e86\u5b57\u7b26\u4e32\u6982\u7387\u4e0eLM\u6f5c\u5728\u8bed\u6cd5\u77e5\u8bc6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u5b66\u4e60\u8bed\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u8bed\u8a00\u7406\u8bba\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u5bf9\u4e8e\u5982\u4f55\u901a\u8fc7\u5b57\u7b26\u4e32\u6982\u7387\u6765\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u6cd5\u77e5\u8bc6\u5b58\u5728\u4e89\u8bae\u3002", "method": "\u8be5\u7814\u7a76\u57fa\u4e8e\u8bed\u6599\u5e93\u6570\u636e\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u5b57\u7b26\u4e32\u6982\u7387\u4e4b\u95f4\u5173\u7cfb\u7684\u7406\u8bba\u6846\u67b6\u3002\u8be5\u6846\u67b6\u63d0\u51fa\u4e86\u4e09\u4e2a\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u82f1\u8bed\u548c\u6c49\u8bed\u768428\u4e07\u4e2a\u53e5\u5b50\u5bf9\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6846\u67b6\u7684\u4e09\u4e2a\u9884\u6d4b\uff1a(1) \u6700\u5c0f\u5bf9\u4e2d\u5b57\u7b26\u4e32\u6982\u7387\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff1b(2) \u6a21\u578b\u548c\u4eba\u7c7b\u5728\u6700\u5c0f\u5bf9\u4e2d\u7684\u5dee\u5f02\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff1b(3) \u672a\u914d\u5bf9\u7684\u8bed\u6cd5\u548c\u975e\u8bed\u6cd5\u5b57\u7b26\u4e32\u5728\u6982\u7387\u7a7a\u95f4\u4e2d\u7684\u5206\u79bb\u5ea6\u8f83\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f7f\u7528\u6982\u7387\u6765\u4e86\u89e3LM\u7684\u7ed3\u6784\u77e5\u8bc6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3aLM\u8bed\u6cd5\u8bc4\u4f30\u7684\u672a\u6765\u5de5\u4f5c\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.16001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16001", "abs": "https://arxiv.org/abs/2510.16001", "authors": ["Ruolan Cheng", "Yong Deng", "Enrique Herrera-Viedma"], "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets", "comment": null, "summary": "Random permutation set (RPS) is a new formalism for reasoning with\nuncertainty involving order information. Measuring the conflict between two\npieces of evidence represented by permutation mass functions remains an urgent\nresearch topic in order-structured uncertain information fusion. In this paper,\na detailed analysis of conflicts in RPS is carried out from two different\nperspectives: random finite set (RFS) and Dempster-Shafer theory (DST).\nStarting from the observation of permutations, we first define an inconsistency\nmeasure between permutations inspired by the rank-biased overlap(RBO) measure\nand further propose a non-overlap-based conflict measure method for RPSs. This\npaper regards RPS theory (RPST) as an extension of DST. The order information\nnewly added in focal sets indicates qualitative propensity, characterized by\ntop-ranked elements occupying a more critical position. Some numerical examples\nare used to demonstrate the behavior and properties of the proposed conflict\nmeasure. The proposed method not only has the natural top-weightedness property\nand can effectively measure the conflict between RPSs from the DST view but\nalso provides decision-makers with a flexible selection of weights, parameters,\nand truncated depths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u968f\u673a\u7f6e\u6362\u96c6\uff08RPS\uff09\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6392\u5e8f\u504f\u5dee\u91cd\u53e0\uff08RBO\uff09\u5e76\u8003\u8651\u4e86Dempster-Shafer\u7406\u8bba\uff08DST\uff09\u7684\u6269\u5c55\u3002", "motivation": "\u5728order-structured\u4e0d\u786e\u5b9a\u4fe1\u606f\u878d\u5408\u4e2d\uff0c\u8861\u91cf\u7531\u7f6e\u6362\u8d28\u91cf\u51fd\u6570\u8868\u793a\u7684\u4e24\u6761\u8bc1\u636e\u4e4b\u95f4\u7684\u51b2\u7a81\u662f\u4e00\u4e2a\u7d27\u8feb\u7684\u7814\u7a76\u95ee\u9898\u3002", "method": "1.  \u4ece\u968f\u673a\u6709\u9650\u96c6\uff08RFS\uff09\u548cDempster-Shafer\u7406\u8bba\uff08DST\uff09\u4e24\u4e2a\u4e0d\u540c\u7684\u89d2\u5ea6\uff0c\u5bf9RPS\u4e2d\u7684\u51b2\u7a81\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u5206\u6790\u3002\n2.  \u5b9a\u4e49\u4e86\u4e00\u4e2a\u53d7\u6392\u5e8f\u504f\u5dee\u91cd\u53e0\uff08RBO\uff09\u5ea6\u91cf\u542f\u53d1\u7684\u7f6e\u6362\u95f4\u4e0d\u4e00\u81f4\u6027\u5ea6\u91cf\u3002\n3.  \u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u91cd\u53e0\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8eRPS\u3002\n4.  \u5c06RPS\u7406\u8bba\uff08RPST\uff09\u89c6\u4e3aDST\u7684\u6269\u5c55\uff0c\u5176\u4e2d\u7126\u70b9\u96c6\u4e2d\u7684\u987a\u5e8f\u4fe1\u606f\u8868\u793a\u5b9a\u6027\u503e\u5411\uff0c\u5176\u7279\u5f81\u662f\u6392\u540d\u9760\u524d\u7684\u5143\u7d20\u5360\u636e\u66f4\u5173\u952e\u7684\u4f4d\u7f6e\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u81ea\u7136\u7684top-weightedness\u5c5e\u6027\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u4eceDST\u7684\u89d2\u5ea6\u8861\u91cfRPS\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u4e14\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u6743\u91cd\u3001\u53c2\u6570\u548c\u622a\u65ad\u6df1\u5ea6\u7684\u7075\u6d3b\u9009\u62e9\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684RPS\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709top-weightedness\u5c5e\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u7075\u6d3b\u5730\u9009\u62e9\u6743\u91cd\u3001\u53c2\u6570\u548c\u622a\u65ad\u6df1\u5ea6\u3002"}}
{"id": "2510.15960", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.15960", "abs": "https://arxiv.org/abs/2510.15960", "authors": ["Sana Kordoghli", "Abdelhakim Settar", "Oumayma Belaati", "Mohammad Alkhatib"], "title": "Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling", "comment": "41 pages, 21 figures", "summary": "This work contributes to advancing sustainable energy and waste management\nstrategies by investigating the thermochemical conversion of food-based biomass\nthrough pyrolysis, highlighting the role of artificial intelligence (AI) in\nenhancing process modelling accuracy and optimization efficiency. The main\nobjective is to explore the potential of underutilized biomass resources, such\nas spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen\nproduction. Specifically, it aims to optimize the pyrolysis process while\nevaluating the performance of these resources both individually and as blends.\nProximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC\nanalyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS\n- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential\nbut had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1\nexhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic\nmodelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS\nas the most accurate. These approaches provide a detailed understanding of the\npyrolysis process, with particular emphasis on the integration of artificial\nintelligence. An LSTM model trained with lignocellulosic data predicted TGA\ncurves with exceptional accuracy (R^2: 0.9996-0.9998).", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u70ed\u89e3\u5bf9\u98df\u7269\u751f\u7269\u8d28\u8fdb\u884c\u70ed\u5316\u5b66\u8f6c\u5316\uff0c\u5e76\u5f3a\u8c03\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u63d0\u9ad8\u8fc7\u7a0b\u5efa\u6a21\u51c6\u786e\u6027\u548c\u4f18\u5316\u6548\u7387\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u4ee5\u63a8\u8fdb\u53ef\u6301\u7eed\u80fd\u6e90\u548c\u5e9f\u7269\u7ba1\u7406\u6218\u7565\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5496\u5561\u6e23\u548c\u67a3\u6838\u7b49\u672a\u5145\u5206\u5229\u7528\u7684\u751f\u7269\u8d28\u8d44\u6e90\u5728\u53ef\u6301\u7eed\u5236\u6c22\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f18\u5316\u70ed\u89e3\u8fc7\u7a0b\uff0c\u5e76\u8bc4\u4f30\u8fd9\u4e9b\u8d44\u6e90\u5355\u72ec\u548c\u6df7\u5408\u65f6\u7684\u6027\u80fd\u3002", "method": "\u5bf9\u7eaf\u67a3\u6838\u3001\u5496\u5561\u6e23\u548c\u6df7\u5408\u7269\uff0875% \u67a3\u6838 - 25% \u5496\u5561\u6e23\uff0c50% \u67a3\u6838 - 50% \u5496\u5561\u6e23\uff0c25% \u67a3\u6838 - 75% \u5496\u5561\u6e23\uff09\u8fdb\u884c\u4e86\u8fd1\u7aef\u5206\u6790\u3001\u5143\u7d20\u5206\u6790\u3001\u7ea4\u7ef4\u5206\u6790\u3001TGA/DTG\u3001\u52a8\u529b\u5b66\u3001\u70ed\u529b\u5b66\u548c Py-Micro GC \u5206\u6790\u3002\u91c7\u7528\u57fa\u4e8e\u7b49\u8f6c\u5316\u7387\u65b9\u6cd5\uff08KAS\u3001FWO\u3001Friedman\uff09\u7684\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528 LSTM \u6a21\u578b\u8bad\u7ec3\u6728\u8d28\u7ea4\u7ef4\u7d20\u6570\u636e\u4ee5\u9884\u6d4b TGA \u66f2\u7ebf\u3002", "result": "\u6df7\u5408\u7269 3 \u5177\u6709\u4f18\u5f02\u7684\u6c22\u6c14\u4ea7\u91cf\u6f5c\u529b\uff0c\u4f46\u6d3b\u5316\u80fd\u6700\u9ad8\uff08Ea: 313.24 kJ/mol\uff09\uff0c\u800c\u6df7\u5408\u7269 1 \u8868\u73b0\u51fa\u6700\u4f73\u7684\u6d3b\u5316\u80fd\u503c\uff08Ea: 161.75 kJ/mol\uff09\u3002KAS \u88ab\u786e\u5b9a\u4e3a\u6700\u51c6\u786e\u7684\u6a21\u578b\u3002LSTM \u6a21\u578b\u4ee5\u6781\u9ad8\u7684\u7cbe\u5ea6\u9884\u6d4b TGA \u66f2\u7ebf (R^2: 0.9996-0.9998)\u3002", "conclusion": "\u8be5\u7814\u7a76\u8be6\u7ec6\u4e86\u89e3\u4e86\u70ed\u89e3\u8fc7\u7a0b\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u6574\u5408\u3002"}}
{"id": "2510.17245", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17245", "abs": "https://arxiv.org/abs/2510.17245", "authors": ["Wenyu Mao", "Jiancan Wu", "Guoqing Hu", "Wei Ji", "Xiang Wang"], "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders", "comment": null, "summary": "Diffusion models have emerged as a powerful paradigm for generative\nsequential recommendation, which typically generate next items to recommend\nguided by user interaction histories with a multi-step denoising process.\nHowever, the multi-step process relies on discrete approximations, introducing\ndiscretization error that creates a trade-off between computational efficiency\nand recommendation effectiveness. To address this trade-off, we propose TA-Rec,\na two-stage framework that achieves one-step generation by smoothing the\ndenoising function during pretraining while alleviating trajectory deviation by\naligning with user preferences during fine-tuning. Specifically, to improve the\nefficiency without sacrificing the recommendation performance, TA-Rec pretrains\nthe denoising model with Temporal Consistency Regularization (TCR), enforcing\nthe consistency between the denoising results across adjacent steps. Thus, we\ncan smooth the denoising function to map the noise as oracle items in one step\nwith bounded error. To further enhance effectiveness, TA-Rec introduces\nAdaptive Preference Alignment (APA) that aligns the denoising process with user\npreference adaptively based on preference pair similarity and timesteps.\nExtensive experiments prove that TA-Rec's two-stage objective effectively\nmitigates the discretization errors-induced trade-off, enhancing both\nefficiency and effectiveness of diffusion-based recommenders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTA-Rec\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5e8f\u5217\u63a8\u8350\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5e8f\u5217\u63a8\u8350\u4f9d\u8d56\u4e8e\u591a\u6b65\u53bb\u566a\u8fc7\u7a0b\uff0c\u5b58\u5728\u79bb\u6563\u5316\u8bef\u5dee\uff0c\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u8350\u6548\u679c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u65f6\u95f4\u4e00\u81f4\u6027\u6b63\u5219\u5316(TCR)\u5e73\u6ed1\u53bb\u566a\u51fd\u6570\uff0c\u5b9e\u73b0\u4e00\u6b65\u751f\u6210\uff1b\u5fae\u8c03\u9636\u6bb5\u5f15\u5165\u81ea\u9002\u5e94\u504f\u597d\u5bf9\u9f50(APA)\uff0c\u6839\u636e\u504f\u597d\u5bf9\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u6b65\u957f\u81ea\u9002\u5e94\u5730\u8c03\u6574\u53bb\u566a\u8fc7\u7a0b\u4e0e\u7528\u6237\u504f\u597d\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cTA-Rec\u7684\u4e24\u9636\u6bb5\u76ee\u6807\u6709\u6548\u5730\u7f13\u89e3\u4e86\u79bb\u6563\u5316\u8bef\u5dee\u5f15\u8d77\u7684\u6743\u8861\uff0c\u63d0\u9ad8\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u8350\u5668\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "conclusion": "TA-Rec\u901a\u8fc7\u5e73\u6ed1\u53bb\u566a\u51fd\u6570\u548c\u5bf9\u9f50\u7528\u6237\u504f\u597d\uff0c\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5e8f\u5217\u63a8\u8350\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2510.16134", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16134", "abs": "https://arxiv.org/abs/2510.16134", "authors": ["Chen Kong", "James Fort", "Aria Kang", "Jonathan Wittmer", "Simon Green", "Tianwei Shen", "Yipu Zhao", "Cheng Peng", "Gustavo Solaira", "Andrew Berkovich", "Nikhil Raina", "Vijay Baiyya", "Evgeniy Oleinik", "Eric Huang", "Fan Zhang", "Julian Straub", "Mark Schwesinger", "Luis Pesqueira", "Xiaqing Pan", "Jakob Julian Engel", "Carl Ren", "Mingfei Yan", "Richard Newcombe"], "title": "Aria Gen 2 Pilot Dataset", "comment": null, "summary": "The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset\ncaptured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely\naccess, A2PD is released incrementally with ongoing dataset enhancements. The\ninitial release features Dia'ane, our primary subject, who records her daily\nactivities alongside friends, each equipped with Aria Gen 2 glasses. It\nencompasses five primary scenarios: cleaning, cooking, eating, playing, and\noutdoor walking. In each of the scenarios, we provide comprehensive raw sensor\ndata and output data from various machine perception algorithms. These data\nillustrate the device's ability to perceive the wearer, the surrounding\nenvironment, and interactions between the wearer and the environment, while\nmaintaining robust performance across diverse users and conditions. The A2PD is\npublicly available at projectaria.com, with open-source tools and usage\nexamples provided in Project Aria Tools.", "AI": {"tldr": "Aria Gen 2 Pilot Dataset (A2PD)\u662f\u4e00\u4e2a\u4f7f\u7528Aria Gen 2\u773c\u955c\u6355\u83b7\u7684\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684Multimodal\u5f00\u653e\u6570\u636e\u96c6\uff0c\u9010\u6b65\u53d1\u5e03\u5e76\u4e0d\u65ad\u589e\u5f3a\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u53ca\u65f6\u8bbf\u95eeA2PD\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u914d\u5907Aria Gen 2\u773c\u955c\u7684Dia'ane\u53ca\u5176\u670b\u53cb\u8bb0\u5f55\u4ed6\u4eec\u7684\u65e5\u5e38\u6d3b\u52a8\uff0c\u6db5\u76d6\u6e05\u6d01\u3001\u70f9\u996a\u3001\u996e\u98df\u3001\u73a9\u800d\u548c\u6237\u5916\u884c\u8d70\u4e94\u4e2a\u4e3b\u8981\u573a\u666f\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u548c\u5404\u79cd\u673a\u5668\u611f\u77e5\u7b97\u6cd5\u7684\u8f93\u51fa\u6570\u636e\u3002", "result": "\u5c55\u793a\u4e86\u8bbe\u5907\u611f\u77e5\u4f69\u6234\u8005\u3001\u5468\u56f4\u73af\u5883\u4ee5\u53ca\u4f69\u6234\u8005\u4e0e\u73af\u5883\u4e4b\u95f4\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u7684\u7528\u6237\u548c\u6761\u4ef6\u4e0b\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "A2PD\u5728projectaria.com\u516c\u5f00\u63d0\u4f9b\uff0c\u5e76\u5728Project Aria Tools\u4e2d\u63d0\u4f9b\u5f00\u6e90\u5de5\u5177\u548c\u4f7f\u7528\u793a\u4f8b\u3002"}}
{"id": "2510.16257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16257", "abs": "https://arxiv.org/abs/2510.16257", "authors": ["Chu Fei Luo", "Samuel Dahan", "Xiaodan Zhu"], "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback", "comment": "Findings of EMNLP 2025, 5 pages", "summary": "As language models have a greater impact on society, it is important to\nensure they are aligned to a diverse range of perspectives and are able to\nreflect nuance in human values. However, the most popular training paradigms\nfor modern language models often assume there is one optimal answer for every\nquery, leading to generic responses and poor alignment. In this work, we aim to\nenhance pluralistic alignment of language models in a low-resource setting with\ntwo methods: pluralistic decoding and model steering. We empirically\ndemonstrate that model steering offers consistent improvement over zero-shot\nand few-shot baselines with only 50 annotated samples. Our proposed methods\ndecrease false positives in several high-stakes tasks such as hate speech\ndetection and misinformation detection, and improves the distributional\nalignment to human values in GlobalOpinionQA. We hope our work highlights the\nimportance of diversity and how language models can be adapted to consider\nnuanced perspectives.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u65e8\u5728\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u591a\u5143\u5316\u5bf9\u9f50\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\uff1a\u591a\u5143\u5316\u89e3\u7801\u548c\u6a21\u578b\u5f15\u5bfc\u3002", "motivation": "\u786e\u4fdd\u8bed\u8a00\u6a21\u578b\u4e0e\u4e0d\u540c\u7684\u89c2\u70b9\u5bf9\u9f50\uff0c\u5e76\u80fd\u591f\u53cd\u6620\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u7ec6\u5fae\u5dee\u522b\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u6700\u6d41\u884c\u7684\u8bad\u7ec3\u8303\u5f0f\u901a\u5e38\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u90fd\u6709\u4e00\u4e2a\u6700\u4f73\u7b54\u6848\uff0c\u4ece\u800c\u5bfc\u81f4\u6cdb\u5316\u7684\u54cd\u5e94\u548c\u8f83\u5dee\u7684\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u591a\u5143\u5316\u89e3\u7801\u548c\u6a21\u578b\u5f15\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4ec5\u4f7f\u7528 50 \u4e2a\u5e26\u6ce8\u91ca\u7684\u6837\u672c\uff0c\u6a21\u578b\u5f15\u5bfc\u5c31\u6bd4\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u57fa\u7ebf\u63d0\u4f9b\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u548c\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7b49\u51e0\u4e2a\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7684\u5047\u9633\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u5728\u5168\u7403\u89c2\u70b9\u95ee\u7b54\u4e2d\u5bf9\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u8c03\u6574\u8bed\u8a00\u6a21\u578b\u4ee5\u8003\u8651\u7ec6\u5fae\u7684\u89c2\u70b9\u3002"}}
{"id": "2510.16004", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16004", "abs": "https://arxiv.org/abs/2510.16004", "authors": ["Andreas Radler", "Vincent Seyfried", "Stefan Pirker", "Johannes Brandstetter", "Thomas Lichtenegger"], "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction", "comment": "22 pages, 16 figures", "summary": "Neural surrogates have shown great potential in simulating dynamical systems,\nwhile offering real-time capabilities. We envision Neural Twins as a\nprogression of neural surrogates, aiming to create digital replicas of real\nsystems. A neural twin consumes measurements at test time to update its state,\nthereby enabling context-specific decision-making. A critical property of\nneural twins is their ability to remain on-trajectory, i.e., to stay close to\nthe true system state over time. We introduce Parallel-in-time Neural Twins\n(PAINT), an architecture-agnostic family of methods for modeling dynamical\nsystems from measurements. PAINT trains a generative neural network to model\nthe distribution of states parallel over time. At test time, states are\npredicted from measurements in a sliding window fashion. Our theoretical\nanalysis shows that PAINT is on-trajectory, whereas autoregressive models\ngenerally are not. Empirically, we evaluate our method on a challenging\ntwo-dimensional turbulent fluid dynamics problem. The results demonstrate that\nPAINT stays on-trajectory and predicts system states from sparse measurements\nwith high fidelity. These findings underscore PAINT's potential for developing\nneural twins that stay on-trajectory, enabling more accurate state estimation\nand decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86Parallel-in-time Neural Twins (PAINT)\uff0c\u4e00\u79cd\u7528\u4e8e\u4ece\u6d4b\u91cf\u6570\u636e\u4e2d\u5efa\u6a21\u52a8\u6001\u7cfb\u7edf\u7684\u65b9\u6cd5\u3002", "motivation": "\u795e\u7ecf\u4ee3\u7406\u5728\u6a21\u62df\u52a8\u6001\u7cfb\u7edf\u65b9\u9762\u8868\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u5b9e\u65f6\u80fd\u529b\u3002\u795e\u7ecf\u5b6a\u751f\u65e8\u5728\u521b\u5efa\u771f\u5b9e\u7cfb\u7edf\u7684\u6570\u5b57\u526f\u672c\u3002\u795e\u7ecf\u5b6a\u751f\u7684\u4e00\u4e2a\u5173\u952e\u7279\u6027\u662f\u5b83\u4eec\u4fdd\u6301\u5728\u8f68\u9053\u4e0a\u7684\u80fd\u529b\u3002", "method": "PAINT\u8bad\u7ec3\u4e00\u4e2a\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6765\u6a21\u62df\u72b6\u6001\u5728\u65f6\u95f4\u4e0a\u7684\u5e76\u884c\u5206\u5e03\u3002\u5728\u6d4b\u8bd5\u65f6\uff0c\u72b6\u6001\u4ee5\u6ed1\u52a8\u7a97\u53e3\u7684\u65b9\u5f0f\u4ece\u6d4b\u91cf\u6570\u636e\u4e2d\u9884\u6d4b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cPAINT\u662f\u5728\u8f68\u9053\u4e0a\u7684\uff0c\u800c\u81ea\u56de\u5f52\u6a21\u578b\u901a\u5e38\u4e0d\u662f\u3002\u5728\u4e8c\u7ef4\u6e4d\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cPAINT\u4fdd\u6301\u5728\u8f68\u9053\u4e0a\uff0c\u5e76\u80fd\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u4ece\u7a00\u758f\u6d4b\u91cf\u6570\u636e\u4e2d\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001\u3002", "conclusion": "PAINT\u5728\u5f00\u53d1\u4fdd\u6301\u5728\u8f68\u9053\u4e0a\u7684\u795e\u7ecf\u5b6a\u751f\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4ece\u800c\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u51b3\u7b56\u3002"}}
{"id": "2510.15961", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15961", "abs": "https://arxiv.org/abs/2510.15961", "authors": ["Yiyang Li", "Zehong Wang", "Zhengqing Yuan", "Zheyuan Zhang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "title": "Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use", "comment": null, "summary": "Illicit drug use among teenagers and young adults (TYAs) remains a pressing\npublic health concern, with rising prevalence and long-term impacts on health\nand well-being. To detect illicit drug use among TYAs, researchers analyze\nlarge-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the\nNational Survey on Drug Use and Health (NSDUH), which preserve rich\ndemographic, psychological, and environmental factors related to substance use.\nHowever, existing modeling methods treat survey variables independently,\noverlooking latent and interconnected structures among them. To address this\nlimitation, we propose LAMI (LAtent relation Mining with bi-modal\nInterpretability), a novel joint graph-language modeling framework for\ndetecting illicit drug use and interpreting behavioral risk factors among TYAs.\nLAMI represents individual responses as relational graphs, learns latent\nconnections through a specialized graph structure learning layer, and\nintegrates a large language model to generate natural language explanations\ngrounded in both graph structures and survey semantics. Experiments on the YRBS\nand NSDUH datasets show that LAMI outperforms competitive baselines in\npredictive accuracy. Interpretability analyses further demonstrate that LAMI\nreveals meaningful behavioral substructures and psychosocial pathways, such as\nfamily dynamics, peer influence, and school-related distress, that align with\nestablished risk factors for substance use.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe-\u8bed\u8a00\u8054\u5408\u5efa\u6a21\u6846\u67b6LAMI\uff0c\u7528\u4e8e\u68c0\u6d4b\u9752\u5c11\u5e74\u548c\u5e74\u8f7b\u4eba\u4e2d\u7684\u975e\u6cd5\u836f\u7269\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u5efa\u6a21\u65b9\u6cd5\u5ffd\u7565\u4e86\u8c03\u67e5\u53d8\u91cf\u4e4b\u95f4\u7684\u6f5c\u5728\u548c\u76f8\u4e92\u5173\u8054\u7684\u7ed3\u6784\u3002", "method": "\u5c06\u4e2a\u4eba\u53cd\u5e94\u8868\u793a\u4e3a\u5173\u7cfb\u56fe\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u56fe\u7ed3\u6784\u5b66\u4e60\u5c42\u5b66\u4e60\u6f5c\u5728\u8fde\u63a5\uff0c\u5e76\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u751f\u6210\u57fa\u4e8e\u56fe\u7ed3\u6784\u548c\u8c03\u67e5\u8bed\u4e49\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728 YRBS \u548c NSDUH \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAMI \u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u3002", "conclusion": "LAMI \u63ed\u793a\u4e86\u6709\u610f\u4e49\u7684\u884c\u4e3a\u5b50\u7ed3\u6784\u548c\u5fc3\u7406\u793e\u4f1a\u9014\u5f84\uff0c\u4f8b\u5982\u5bb6\u5ead\u52a8\u6001\u3001\u540c\u4f34\u5f71\u54cd\u548c\u4e0e\u5b66\u6821\u76f8\u5173\u7684\u56f0\u6270\uff0c\u8fd9\u4e9b\u90fd\u4e0e\u5df2\u786e\u5b9a\u7684\u836f\u7269\u4f7f\u7528\u98ce\u9669\u56e0\u7d20\u76f8\u4e00\u81f4\u3002"}}
{"id": "2510.17535", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17535", "abs": "https://arxiv.org/abs/2510.17535", "authors": ["Yumeng Wang", "Jirui Qi", "Catherine Chen", "Panagiotis Eustratiadis", "Suzan Verberne"], "title": "How role-play shapes relevance judgment in zero-shot LLM rankers", "comment": null, "summary": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications.", "AI": {"tldr": "\u89d2\u8272\u626e\u6f14\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4f5c\u4e3a\u96f6\u6837\u672c\u6392\u5e8f\u5668\u7684\u6027\u80fd\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u7814\u7a76\u4e86\u89d2\u8272\u626e\u6f14\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u96f6\u6837\u672c LLM \u6392\u5e8f\u5668\uff0c\u5e76\u63ed\u793a\u4e86\u89d2\u8272\u626e\u6f14\u4fe1\u53f7\u4e3b\u8981\u7f16\u7801\u5728\u65e9\u671f\u5c42\u4e2d\uff0c\u5e76\u4e0e\u4e2d\u95f4\u5c42\u7684\u4efb\u52a1\u6307\u4ee4\u8fdb\u884c\u901a\u4fe1\uff0c\u540c\u65f6\u4e0e\u67e5\u8be2\u6216\u6587\u6863\u8868\u793a\u7684\u4ea4\u4e92\u6709\u9650\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u4e3a\u6709\u5e0c\u671b\u7684\u96f6\u6837\u672c\u6392\u5e8f\u5668\uff0c\u4f46\u5b83\u4eec\u7684\u6027\u80fd\u5bf9\u63d0\u793a\u516c\u5f0f\u975e\u5e38\u654f\u611f\u3002\u89d2\u8272\u626e\u6f14\u63d0\u793a\u901a\u5e38\u63d0\u4f9b\u66f4\u5f3a\u5927\u548c\u51c6\u786e\u7684\u76f8\u5173\u6027\u6392\u5e8f\u3002\u7136\u800c\uff0c\u89d2\u8272\u626e\u6f14\u6548\u679c\u7684\u673a\u5236\u548c\u591a\u6837\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9650\u5236\u4e86\u6709\u6548\u4f7f\u7528\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u6765\u81ea\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u56e0\u679c\u5e72\u9884\u6280\u672f\u6765\u8ffd\u8e2a\u89d2\u8272\u626e\u6f14\u4fe1\u606f\u5982\u4f55\u5851\u9020 LLM \u4e2d\u7684\u76f8\u5173\u6027\u5224\u65ad\u3002", "result": "(1) \u89d2\u8272\u63cf\u8ff0\u7684\u4ed4\u7ec6\u5236\u5b9a\u5bf9 LLM \u7684\u6392\u5e8f\u8d28\u91cf\u6709\u5f88\u5927\u5f71\u54cd\uff1b(2) \u89d2\u8272\u626e\u6f14\u4fe1\u53f7\u4e3b\u8981\u7f16\u7801\u5728\u65e9\u671f\u5c42\u4e2d\uff0c\u5e76\u4e0e\u4e2d\u95f4\u5c42\u7684\u4efb\u52a1\u6307\u4ee4\u8fdb\u884c\u901a\u4fe1\uff0c\u540c\u65f6\u4e0e\u67e5\u8be2\u6216\u6587\u6863\u8868\u793a\u7684\u4ea4\u4e92\u6709\u9650\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u4e00\u7ec4\u6ce8\u610f\u529b\u5934\uff0c\u5b83\u4eec\u7f16\u7801\u5bf9\u89d2\u8272\u6761\u4ef6\u76f8\u5173\u6027\u81f3\u5173\u91cd\u8981\u7684\u4fe1\u606f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u4ec5\u63ed\u793a\u4e86 LLM \u6392\u5e8f\u4e2d\u89d2\u8272\u626e\u6f14\u7684\u5185\u90e8\u8fd0\u4f5c\uff0c\u800c\u4e14\u4e3a\u5728 IR \u53ca\u5176\u4ed6\u9886\u57df\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u63d0\u793a\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4ece\u800c\u4e3a\u5728\u96f6\u6837\u672c\u5e94\u7528\u4e2d\u5229\u7528\u89d2\u8272\u626e\u6f14\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u673a\u4f1a\u3002"}}
{"id": "2510.16136", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16136", "abs": "https://arxiv.org/abs/2510.16136", "authors": ["Sayan Deb Sarkar", "Sinisa Stekovic", "Vincent Lepetit", "Iro Armeni"], "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer", "comment": "NeurIPS 2025. Project Page: https://sayands.github.io/guideflow3d/", "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5916\u89c2\u8fc1\u79fb\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u56fe\u50cf\u6216\u6587\u672c\u7684\u5916\u89c2\u8fc1\u79fb\u52303D\u8d44\u4ea7\u4e0a\uff0c\u5373\u4f7f\u8f93\u5165\u548c\u5916\u89c2\u5bf9\u8c61\u4e4b\u95f4\u7684\u51e0\u4f55\u5f62\u72b6\u5dee\u5f02\u5f88\u5927\u4e5f\u80fdwork\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8f93\u5165\u548c\u5916\u89c2\u5bf9\u8c61\u51e0\u4f55\u5f62\u72b6\u5dee\u5f02\u663e\u8457\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u76f4\u63a5\u5e94\u75283D\u751f\u6210\u6a21\u578b\u4e5f\u65e0\u6cd5\u4ea7\u751f\u5438\u5f15\u4eba\u7684\u7ed3\u679c\u3002", "method": "\u8be5\u65b9\u6cd5\u53d7\u5230\u901a\u7528\u5f15\u5bfc\u7684\u542f\u53d1\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6821\u6b63\u6d41\u6a21\u578b\uff0c\u901a\u8fc7\u5468\u671f\u6027\u5730\u6dfb\u52a0\u5f15\u5bfc\u6765\u4e0e\u91c7\u6837\u8fc7\u7a0b\u4ea4\u4e92\u3002\u5f15\u5bfc\u88ab\u5efa\u6a21\u4e3a\u53ef\u5fae\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5b9e\u9a8c\u4e86\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u5f15\u5bfc\uff0c\u5305\u62ec\u7528\u4e8e\u5916\u89c2\u548c\u81ea\u76f8\u4f3c\u6027\u7684\u90e8\u5206\u611f\u77e5\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u7eb9\u7406\u548c\u51e0\u4f55\u7ec6\u8282\u8f6c\u79fb\u5230\u8f93\u51653D\u8d44\u4ea7\uff0c\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u4f20\u7edf\u6307\u6807\u4e0d\u9002\u5408\u8bc4\u4f30\u8be5\u4efb\u52a1\uff0c\u56e0\u6b64\u91c7\u7528\u57fa\u4e8eGPT\u7684\u7cfb\u7edf\u5ba2\u89c2\u5730\u5bf9\u8f93\u51fa\u8fdb\u884c\u6392\u5e8f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u901a\u7528\u7684\uff0c\u53ef\u4ee5\u6269\u5c55\u5230\u4e0d\u540c\u7c7b\u578b\u7684\u6269\u6563\u6a21\u578b\u548c\u5f15\u5bfc\u51fd\u6570\u3002"}}
{"id": "2510.16282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16282", "abs": "https://arxiv.org/abs/2510.16282", "authors": ["Zhaoxuan Tan", "Zixuan Zhang", "Haoyang Wen", "Zheng Li", "Rongzhi Zhang", "Pei Chen", "Fengran Mo", "Zheyuan Liu", "Qingkai Zeng", "Qingyu Yin", "Meng Jiang"], "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork", "comment": null, "summary": "Personalized large language models (LLMs) tailor content to individual\npreferences using user profiles or histories. However, existing\nparameter-efficient fine-tuning (PEFT) methods, such as the\n``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for\neach user, making them computationally expensive and impractical for real-time\nupdates. We introduce Profile-to-PEFT, a scalable framework that employs a\nhypernetwork, trained end-to-end, to map a user's encoded profile directly to a\nfull set of adapter parameters (e.g., LoRA), eliminating per-user training at\ndeployment. This design enables instant adaptation, generalization to unseen\nusers, and privacy-preserving local deployment. Experimental results\ndemonstrate that our method outperforms both prompt-based personalization and\nOPPU while using substantially fewer computational resources at deployment. The\nframework exhibits strong generalization to out-of-distribution users and\nmaintains robustness across varying user activity levels and different\nembedding backbones. The proposed Profile-to-PEFT framework enables efficient,\nscalable, and adaptive LLM personalization suitable for large-scale\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u5c06\u7528\u6237profile\u6620\u5c04\u5230adapter\u53c2\u6570\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u7528\u6237\u5355\u72ec\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684PEFT\u65b9\u6cd5\uff08\u5982OPPU\uff09\u9700\u8981\u4e3a\u6bcf\u4e2a\u7528\u6237\u8bad\u7ec3\u5355\u72ec\u7684adapter\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u66f4\u65b0\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u8d85\u7f51\u7edc\uff0c\u5c06\u7528\u6237\u7684\u7f16\u7801profile\u76f4\u63a5\u6620\u5c04\u5230\u4e00\u6574\u5957adapter\u53c2\u6570\uff08\u4f8b\u5982\uff0cLoRA\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8eprompt\u7684\u4e2a\u6027\u5316\u548cOPPU\uff0c\u540c\u65f6\u5728\u90e8\u7f72\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u8be5\u6846\u67b6\u5bf9\u5206\u5e03\u5916\u7684\u7528\u6237\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u7528\u6237\u6d3b\u52a8\u6c34\u5e73\u548c\u4e0d\u540c\u7684embedding backbone\u4e0a\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Profile-to-PEFT\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684LLM\u4e2a\u6027\u5316\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2510.16033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16033", "abs": "https://arxiv.org/abs/2510.16033", "authors": ["Junyu Ren", "Wensheng Gan", "Guangyu Zhang", "Wei Zhong", "Philip S. Yu"], "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis", "comment": "Preprint. 16 figures, 12 tables", "summary": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN", "AI": {"tldr": "ISGFAN: A robust cross-domain fault diagnosis framework using information separation and global-focal adversarial learning to handle noise and domain shifts.", "motivation": "Existing transfer fault diagnosis methods are limited by clean data or sufficient domain similarity, which is not realistic in industrial environments with noise and domain shifts.", "method": "An information separation global-focal adversarial network (ISGFAN) is proposed, which decouples domain-invariant fault representation using adversarial learning and orthogonal loss. It also uses a global-focal domain-adversarial scheme to constrain conditional and marginal distributions.", "result": "Experiments on three public datasets show that ISGFAN outperforms existing methods.", "conclusion": "The proposed ISGFAN framework is superior for cross-domain fault diagnosis under noise conditions."}}
{"id": "2510.15962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15962", "abs": "https://arxiv.org/abs/2510.15962", "authors": ["Zhuxuanzi Wang", "Mingqiao Mo", "Xi Xiao", "Chen Liu", "Chenrui Ma", "Yunbei Zhang", "Xiao Wang", "Smita Krishnaswamy", "Tianyang Wang"], "title": "CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has become the standard approach for\nadapting large language models under limited compute and memory budgets.\nAlthough previous methods improve efficiency through low-rank updates,\nquantization, or heuristic budget reallocation, they often decouple the\nallocation of capacity from the way updates evolve during training. In this\nwork, we introduce CTR-LoRA, a framework guided by curvature trust region that\nintegrates rank scheduling with stability-aware optimization. CTR-LoRA\nallocates parameters based on marginal utility derived from lightweight\nsecond-order proxies and constrains updates using a Fisher/Hessian-metric trust\nregion. Experiments on multiple open-source backbones (7B-13B), evaluated on\nboth in-distribution and out-of-distribution benchmarks, show consistent\nimprovements over strong PEFT baselines. In addition to increased accuracy,\nCTR-LoRA enhances training stability, reduces memory requirements, and achieves\nhigher throughput, positioning it on the Pareto frontier of performance and\nefficiency. These results highlight a principled path toward more robust and\ndeployable PEFT.", "AI": {"tldr": "CTR-LoRA: A parameter-efficient fine-tuning framework using curvature trust region for rank scheduling and stability-aware optimization.", "motivation": "Existing PEFT methods decouple capacity allocation from update evolution during training, leading to inefficiencies.", "method": "Introduces CTR-LoRA, which integrates rank scheduling with stability-aware optimization, allocating parameters based on marginal utility and constraining updates using a Fisher/Hessian-metric trust region.", "result": "CTR-LoRA shows consistent improvements over strong PEFT baselines on multiple open-source backbones (7B-13B) in both in-distribution and out-of-distribution benchmarks. It also enhances training stability, reduces memory requirements, and achieves higher throughput.", "conclusion": "CTR-LoRA provides a more robust and deployable approach to PEFT."}}
{"id": "2510.16076", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16076", "abs": "https://arxiv.org/abs/2510.16076", "authors": ["SeongKu Kang", "Jianxun Lian", "Dongha Lee", "Wonbin Kweon", "Sanghwan Jang", "Jaehyun Lee", "Jindong Wang", "Xing Xie", "Hwanjo Yu"], "title": "BPL: Bias-adaptive Preference Distillation Learning for Recommender System", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recommender systems suffer from biases that cause the collected feedback to\nincompletely reveal user preference. While debiasing learning has been\nextensively studied, they mostly focused on the specialized (called\ncounterfactual) test environment simulated by random exposure of items,\nsignificantly degrading accuracy in the typical (called factual) test\nenvironment based on actual user-item interactions. In fact, each test\nenvironment highlights the benefit of a different aspect: the counterfactual\ntest emphasizes user satisfaction in the long-terms, while the factual test\nfocuses on predicting subsequent user behaviors on platforms. Therefore, it is\ndesirable to have a model that performs well on both tests rather than only\none. In this work, we introduce a new learning framework, called Bias-adaptive\nPreference distillation Learning (BPL), to gradually uncover user preferences\nwith dual distillation strategies. These distillation strategies are designed\nto drive high performance in both factual and counterfactual test environments.\nEmploying a specialized form of teacher-student distillation from a biased\nmodel, BPL retains accurate preference knowledge aligned with the collected\nfeedback, leading to high performance in the factual test. Furthermore, through\nself-distillation with reliability filtering, BPL iteratively refines its\nknowledge throughout the training process. This enables the model to produce\nmore accurate predictions across a broader range of user-item combinations,\nthereby improving performance in the counterfactual test. Comprehensive\nexperiments validate the effectiveness of BPL in both factual and\ncounterfactual tests. Our implementation is accessible via:\nhttps://github.com/SeongKu-Kang/BPL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a Bias-adaptive Preference distillation Learning (BPL) \u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u9010\u6b65\u63ed\u793a\u7528\u6237\u504f\u597d\uff0c\u8be5\u6846\u67b6\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u5747\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u6536\u96c6\u5230\u7684\u53cd\u9988\u4e0d\u5b8c\u5168\u53cd\u6620\u7528\u6237\u504f\u597d\u3002\u4ee5\u5f80\u7684\u53bb\u504f\u5b66\u4e60\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u8fc7\u968f\u673a\u5c55\u793a\u7269\u54c1\u6a21\u62df\u7684\u4e13\u95e8\u7684\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\uff0c\u8fd9\u4f1a\u663e\u8457\u964d\u4f4e\u5728\u57fa\u4e8e\u5b9e\u9645\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u7684\u5178\u578b\u7684\u4e8b\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u53cc\u91cd\u84b8\u998f\u7b56\u7565\uff0c\u4ece biased \u6a21\u578b\u4e2d\u8fdb\u884c teacher-student \u84b8\u998f\uff0c\u4fdd\u7559\u4e0e\u6536\u96c6\u5230\u7684\u53cd\u9988\u5bf9\u9f50\u7684\u51c6\u786e\u504f\u597d\u77e5\u8bc6\u3002\u901a\u8fc7\u5e26\u6709\u53ef\u9760\u6027\u8fc7\u6ee4\u7684\u81ea\u84b8\u998f\uff0c\u8fed\u4ee3\u5730\u7ec6\u5316\u6a21\u578b\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u77e5\u8bc6\u3002", "result": "\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u4e2d\uff0c\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 BPL \u7684\u6709\u6548\u6027\u3002", "conclusion": "BPL \u6846\u67b6\u80fd\u591f\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u5728\u4e0d\u540c\u6d4b\u8bd5\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2510.16145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16145", "abs": "https://arxiv.org/abs/2510.16145", "authors": ["Ahmad Arrabi", "Jay hwasung Jung", "J Le", "A Nguyen", "J Reed", "E Stahl", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy", "comment": null, "summary": "Thrombectomy is one of the most effective treatments for ischemic stroke, but\nit is resource and personnel-intensive. We propose employing deep learning to\nautomate critical aspects of thrombectomy, thereby enhancing efficiency and\nsafety. In this work, we introduce a self-supervised framework that classifies\nvarious skeletal landmarks using a regression-based pretext task. Our\nexperiments demonstrate that our model outperforms existing methods in both\nregression and classification tasks. Notably, our results indicate that the\npositional pretext task significantly enhances downstream classification\nperformance. Future work will focus on extending this framework toward fully\nautonomous C-arm control, aiming to optimize trajectories from the pelvis to\nthe head during stroke thrombectomy procedures. All code used is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u8840\u6813\u5207\u9664\u672f\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u8840\u6813\u5207\u9664\u672f\u662f\u6cbb\u7597\u7f3a\u8840\u6027\u5352\u4e2d\u6700\u6709\u6548\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4f46\u5b83\u9700\u8981\u5927\u91cf\u7684\u8d44\u6e90\u548c\u4eba\u5458\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56de\u5f52\u7684pretext task\u6765\u5206\u7c7b\u5404\u79cd\u9aa8\u9abclandmark\u3002", "result": "\u8be5\u6a21\u578b\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u4f4d\u7f6epretext task\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4ee5\u6269\u5c55\u5230\u5b8c\u5168\u81ea\u4e3b\u7684C\u578b\u81c2\u63a7\u5236\uff0c\u4ece\u800c\u4f18\u5316\u4e2d\u98ce\u8840\u6813\u5207\u9664\u672f\u8fc7\u7a0b\u4e2d\u4ece\u9aa8\u76c6\u5230\u5934\u90e8\u7684\u8f68\u8ff9\u3002"}}
{"id": "2510.16340", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16340", "abs": "https://arxiv.org/abs/2510.16340", "authors": ["Pratham Singla", "Shivank Garg", "Ayush Singh", "Ishan Garg", "Ketan Suhaas Saichandran"], "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models", "comment": null, "summary": "Recent advances in post-training techniques have endowed Large Language\nModels (LLMs) with enhanced capabilities for tackling complex, logic-intensive\ntasks through the generation of supplementary planning tokens. This development\nraises a fundamental question: Are these models aware of what they \"learn\" and\n\"think\"? To address this, we define three core competencies: (1) awareness of\nlearned latent policies, (2) generalization of these policies across domains,\nand (3) alignment between internal reasoning traces and final outputs. We\nempirically evaluate these abilities on several tasks, each designed to require\nlearning a distinct policy. Furthermore, we contrast the profiles of models\npost-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization\n(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate\nthat RL-trained models not only demonstrate greater awareness of their learned\nbehaviors and stronger generalizability to novel, structurally similar tasks\nthan SFT models but also often exhibit weak alignment between their reasoning\ntraces and final outputs, an effect most pronounced in GRPO-trained models.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u8865\u5145\u89c4\u5212 tokens \u6765\u589e\u5f3a\u5904\u7406\u590d\u6742\u3001\u903b\u8f91\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u80fd\u529b\u3002\u672c\u6587\u7814\u7a76\u4e86\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u4e86\u89e3\u5b83\u4eec\u201c\u5b66\u4e60\u201d\u548c\u201c\u601d\u8003\u201d\u7684\u5185\u5bb9\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4e86\u89e3\u5b83\u4eec\u901a\u8fc7\u540e\u8bad\u7ec3\u6280\u672f\u201c\u5b66\u4e60\u201d\u548c\u201c\u601d\u8003\u201d\u7684\u5185\u5bb9\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u610f\u8bc6\u5230\u5b83\u4eec\u6240\u5b66\u4e60\u7684\u6f5c\u5728\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a(1) \u5bf9\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7b56\u7565\u7684\u610f\u8bc6\uff0c(2) \u8fd9\u4e9b\u7b56\u7565\u5728\u4e0d\u540c\u9886\u57df\u7684\u6cdb\u5316\uff0c(3) \u5185\u90e8\u63a8\u7406\u8f68\u8ff9\u548c\u6700\u7ec8\u8f93\u51fa\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002\u5bf9\u6bd4\u4e86\u901a\u8fc7\u76d1\u7763\u5fae\u8c03 (SFT)\u3001\u76f4\u63a5\u7b56\u7565\u4f18\u5316 (DPO) \u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316 (GRPO) \u8fdb\u884c\u540e\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "RL \u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u4ec5\u6bd4 SFT \u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5bf9\u5b66\u4e60\u884c\u4e3a\u7684\u610f\u8bc6\u548c\u5bf9\u65b0\u7684\u3001\u7ed3\u6784\u76f8\u4f3c\u7684\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4e14\u901a\u5e38\u5728\u63a8\u7406\u8f68\u8ff9\u548c\u6700\u7ec8\u8f93\u51fa\u4e4b\u95f4\u8868\u73b0\u51fa\u8f83\u5f31\u7684\u4e00\u81f4\u6027\uff0cGRPO \u8bad\u7ec3\u7684\u6a21\u578b\u4e2d\u8fd9\u79cd\u6548\u5e94\u6700\u4e3a\u660e\u663e\u3002", "conclusion": "RL \u8bad\u7ec3\u7684\u6a21\u578b\u6bd4 SFT \u6a21\u578b\u66f4\u4e86\u89e3\u5b83\u4eec\u7684\u884c\u4e3a\uff0c\u4f46 GRPO \u8bad\u7ec3\u7684\u6a21\u578b\u5728\u63a8\u7406\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u8f83\u5dee\u3002"}}
{"id": "2510.16047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16047", "abs": "https://arxiv.org/abs/2510.16047", "authors": ["Ioan Hedea"], "title": "Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks", "comment": "8 pages 2 column, 11 figures. Bachelor's thesis", "summary": "Modern manufacturing systems must meet hard delivery deadlines while coping\nwith stochastic task durations caused by process noise, equipment variability,\nand human intervention. Traditional deterministic schedules break down when\nreality deviates from nominal plans, triggering costly last-minute repairs.\nThis thesis combines offline constraint-programming (CP) optimisation with\nonline temporal-network execution to create schedules that remain feasible\nunder worst-case uncertainty. First, we build a CP model of the flexible\njob-shop with per-job deadline tasks and insert an optimal buffer $\\Delta^*$ to\nobtain a fully pro-active baseline. We then translate the resulting plan into a\nSimple Temporal Network with Uncertainty (STNU) and verify dynamic\ncontrollability, which guarantees that a real-time dispatcher can retime\nactivities for every bounded duration realisation without violating resource or\ndeadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4\nbenchmark suite show that our hybrid approach eliminates 100\\% of deadline\nviolations observed in state-of-the-art meta-heuristic schedules, while adding\nonly 3--5\\% makespan overhead. Scalability experiments confirm that CP\nsolve-times and STNU checks remain sub-second on medium-size instances. The\nwork demonstrates how temporal-network reasoning can bridge the gap between\nproactive buffering and dynamic robustness, moving industry a step closer to\ntruly digital, self-correcting factories.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u79bb\u7ebf\u7ea6\u675f\u7f16\u7a0b\uff08CP\uff09\u4f18\u5316\u548c\u5728\u7ebf\u65f6\u95f4\u7f51\u7edc\u6267\u884c\uff0c\u4ee5\u521b\u5efa\u5728\u6700\u574f\u60c5\u51b5\u7684\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u6301\u53ef\u884c\u7684\u8c03\u5ea6\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u8c03\u5ea6\u5728\u5b9e\u9645\u60c5\u51b5\u504f\u79bb\u6807\u79f0\u8ba1\u5212\u65f6\u4f1a\u5931\u6548\uff0c\u5bfc\u81f4\u4ee3\u4ef7\u9ad8\u6602\u7684\u4e34\u65f6\u4fee\u590d\u3002\u73b0\u4ee3\u5236\u9020\u7cfb\u7edf\u5fc5\u987b\u6ee1\u8db3\u4e25\u683c\u7684\u4ea4\u8d27\u671f\u9650\uff0c\u540c\u65f6\u5e94\u5bf9\u7531\u8fc7\u7a0b\u566a\u58f0\u3001\u8bbe\u5907\u53ef\u53d8\u6027\u548c\u4eba\u4e3a\u5e72\u9884\u5f15\u8d77\u7684\u968f\u673a\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u3002", "method": "\u9996\u5148\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u6bcf\u9879\u4f5c\u4e1a\u622a\u6b62\u65e5\u671f\u4efb\u52a1\u7684\u7075\u6d3b\u4f5c\u4e1a\u8f66\u95f4\u7684CP\u6a21\u578b\uff0c\u5e76\u63d2\u5165\u4e00\u4e2a\u6700\u4f73\u7f13\u51b2\u533a \u0394* \u4ee5\u83b7\u5f97\u5b8c\u5168\u4e3b\u52a8\u7684\u57fa\u7ebf\u3002\u7136\u540e\uff0c\u5c06\u7ed3\u679c\u8ba1\u5212\u8f6c\u6362\u4e3a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7b80\u5355\u65f6\u95f4\u7f51\u7edc\uff08STNU\uff09\u5e76\u9a8c\u8bc1\u52a8\u6001\u53ef\u63a7\u6027\uff0c\u8fd9\u4fdd\u8bc1\u4e86\u5b9e\u65f6\u8c03\u5ea6\u7a0b\u5e8f\u53ef\u4ee5\u4e3a\u6bcf\u4e2a\u6709\u754c\u6301\u7eed\u65f6\u95f4\u5b9e\u73b0\u91cd\u65b0\u5b9a\u65f6\u6d3b\u52a8\uff0c\u800c\u4e0d\u4f1a\u8fdd\u53cd\u8d44\u6e90\u6216\u622a\u6b62\u65e5\u671f\u7ea6\u675f\u3002", "result": "\u5728Kacem 1-4\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u8499\u7279\u5361\u7f57\u6a21\u62df\u8868\u660e\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u6d88\u9664\u4e86\u6700\u5148\u8fdb\u7684\u5143\u542f\u53d1\u5f0f\u8c03\u5ea6\u4e2d\u89c2\u5bdf\u5230\u7684100\uff05\u7684\u622a\u6b62\u65e5\u671f\u51b2\u7a81\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u4e863-5\uff05\u7684makespan\u5f00\u9500\u3002\u53ef\u6269\u5c55\u6027\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u5728\u4e2d\u7b49\u89c4\u6a21\u7684\u5b9e\u4f8b\u4e0a\uff0cCP\u6c42\u89e3\u65f6\u95f4\u548cSTNU\u68c0\u67e5\u4fdd\u6301\u5728\u4e9a\u79d2\u7ea7\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u65f6\u95f4\u7f51\u7edc\u63a8\u7406\u5982\u4f55\u5f25\u5408\u4e3b\u52a8\u7f13\u51b2\u548c\u52a8\u6001\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u4f7f\u5de5\u4e1a\u754c\u671d\u7740\u771f\u6b63\u7684\u6570\u5b57\u5316\u3001\u81ea\u6211\u6821\u6b63\u5de5\u5382\u8fc8\u8fdb\u4e86\u4e00\u6b65\u3002"}}
{"id": "2510.15964", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15964", "abs": "https://arxiv.org/abs/2510.15964", "authors": ["Tuowei Wang", "Kun Li", "Zixu Hao", "Donglin Bai", "Ju Ren", "Yaoxue Zhang", "Ting Cao", "Mao Yang"], "title": "Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity", "comment": null, "summary": "The adaptation of pre-trained large language models (LLMs) to diverse\ndownstream tasks via fine-tuning is critical for numerous applications.\nHowever, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques\npresents significant challenges in terms of time investments and operational\ncosts. In this paper, we first introduce a nuanced form of sparsity, termed\nShadowy Sparsity, which is distinctive in fine-tuning and has not been\nadequately addressed for acceleration. Under Shadowy Sparsity, we propose Long\nExposure, an efficient system to accelerate PEFT for LLMs. Long Exposure\ncomprises three key components: Shadowy-sparsity Exposer employs a prolonged\nsensing range to capture more sparsity details under shadowy sparsity;\nSequence-oriented Predictor provides efficient yet accurate predictions to\nhandle large sequence inputs and constantly-evolving parameters; and\nDynamic-aware Operator facilitates more structured computational patterns and\ncoalesced memory accesses, addressing dynamic sparse operations. Extensive\nevaluations show that Long Exposure outperforms state-of-the-arts with up to a\n$2.49\\times$ speedup in end-to-end fine-tuning, offering promising advancements\nin accelerating PEFT for LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Long Exposure \u7684\u9ad8\u6548\u7cfb\u7edf\uff0c\u7528\u4e8e\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT)\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u6280\u672f\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u65f6\u95f4\u548c\u8fd0\u8425\u6210\u672c\u65b9\u9762\u7684\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u6587\u9996\u5148\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ec6\u81f4\u7684\u7a00\u758f\u5f62\u5f0f\uff0c\u79f0\u4e3a Shadowy Sparsity\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86 Long Exposure\uff0c\u5b83\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aShadowy-sparsity Exposer\u3001Sequence-oriented Predictor \u548c Dynamic-aware Operator\u3002", "result": "Long Exposure \u5728\u7aef\u5230\u7aef\u5fae\u8c03\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 2.49 \u500d\u7684\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Long Exposure \u5728\u52a0\u901f LLM \u7684 PEFT \u65b9\u9762\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u8fdb\u6b65\u3002"}}
{"id": "2510.16302", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16302", "abs": "https://arxiv.org/abs/2510.16302", "authors": ["Changhao Wang", "Yanfang Liu", "Xinxin Fan", "Anzhi Zhou", "Lao Tian", "Yunfeng Lu"], "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA", "comment": "13 pages, 5 figures", "summary": "Multi-hop reasoning for question answering (QA) plays a critical role in\nretrieval-augmented generation (RAG) for modern large language models (LLMs).\nThe accurate answer can be obtained through retrieving relational structure of\nentities from knowledge graph (KG). Regarding the inherent relation-dependency\nand reasoning pattern, multi-hop reasoning can be in general classified into\ntwo categories: i) parallel fact-verification multi-hop reasoning question,\ni.e., requiring simultaneous verifications of multiple independent\nsub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding\nsequential multi-step inference with intermediate conclusions serving as\nessential premises for subsequent reasoning. Currently, the multi-hop reasoning\napproaches singly employ one of two techniques: LLM response-based fact\nverification and KG path-based chain construction. Nevertheless, the former\nexcels at parallel fact-verification but underperforms on chained reasoning\ntasks, while the latter demonstrates proficiency in chained multi-hop reasoning\nbut suffers from redundant path retrieval when handling parallel\nfact-verification reasoning. These limitations deteriorate the efficiency and\naccuracy for multi-hop QA tasks. To address this challenge, we propose a novel\ndual-track KG verification and reasoning framework DTKG, which is inspired by\nthe Dual Process Theory in cognitive science. Specifically, DTKG comprises two\nmain stages: the Classification Stage and the Branch Processing Stage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u8f68\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u548c\u63a8\u7406\u6846\u67b6DTKG\uff0c\u4ee5\u89e3\u51b3\u591a\u8df3\u95ee\u7b54\u4e2d\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u63a8\u7406\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u591a\u8df3\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u8be5\u6846\u67b6\u5206\u4e3a\u5206\u7c7b\u9636\u6bb5\u548c\u5206\u652f\u5904\u7406\u9636\u6bb5\u3002", "result": "\u672a\u63d0\u53ca", "conclusion": "\u672a\u63d0\u53ca"}}
{"id": "2510.16146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16146", "abs": "https://arxiv.org/abs/2510.16146", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Vi Vu", "Ba-Thinh Lam", "Phat Huynh", "Tianyang Wang", "Xingjian Li", "Ulas Bagci", "Min Xu"], "title": "DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization", "comment": "The paper is under review at CMIG", "summary": "The limited availability of annotated data in medical imaging makes\nsemi-supervised learning increasingly appealing for its ability to learn from\nimperfect supervision. Recently, teacher-student frameworks have gained\npopularity for their training benefits and robust performance. However, jointly\noptimizing the entire network can hinder convergence and stability, especially\nin challenging scenarios. To address this for medical image segmentation, we\npropose DuetMatch, a novel dual-branch semi-supervised framework with\nasynchronous optimization, where each branch optimizes either the encoder or\ndecoder while keeping the other frozen. To improve consistency under noisy\nconditions, we introduce Decoupled Dropout Perturbation, enforcing\nregularization across branches. We also design Pair-wise CutMix Cross-Guidance\nto enhance model diversity by exchanging pseudo-labels through augmented input\npairs. To mitigate confirmation bias from noisy pseudo-labels, we propose\nConsistency Matching, refining labels using stable predictions from frozen\nteacher models. Extensive experiments on benchmark brain MRI segmentation\ndatasets, including ISLES2022 and BraTS, show that DuetMatch consistently\noutperforms state-of-the-art methods, demonstrating its effectiveness and\nrobustness across diverse semi-supervised segmentation scenarios.", "AI": {"tldr": "DuetMatch: A dual-branch semi-supervised framework with asynchronous optimization for medical image segmentation.", "motivation": "Jointly optimizing the entire network can hinder convergence and stability, especially in challenging scenarios with limited annotated data.", "method": "A dual-branch semi-supervised framework with asynchronous optimization, Decoupled Dropout Perturbation, Pair-wise CutMix Cross-Guidance, and Consistency Matching.", "result": "DuetMatch consistently outperforms state-of-the-art methods on benchmark brain MRI segmentation datasets.", "conclusion": "DuetMatch demonstrates its effectiveness and robustness across diverse semi-supervised segmentation scenarios."}}
{"id": "2510.16359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16359", "abs": "https://arxiv.org/abs/2510.16359", "authors": ["Utsav Dhanuka", "Soham Poddar", "Saptarshi Ghosh"], "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets", "comment": "14 pages, 1 figure, work done as a part of B.Tech project at IIT\n  Kharagpur", "summary": "In an era where public health is increasingly influenced by information\nshared on social media, combatting vaccine skepticism and misinformation has\nbecome a critical societal goal. Misleading narratives around vaccination have\nspread widely, creating barriers to achieving high immunisation rates and\nundermining trust in health recommendations. While efforts to detect\nmisinformation have made significant progress, the generation of real time\ncounter-arguments tailored to debunk such claims remains an insufficiently\nexplored area. In this work, we explore the capabilities of LLMs to generate\nsound counter-argument rebuttals to vaccine misinformation. Building on prior\nresearch in misinformation debunking, we experiment with various prompting\nstrategies and fine-tuning approaches to optimise counter-argument generation.\nAdditionally, we train classifiers to categorise anti-vaccine tweets into\nmulti-labeled categories such as concerns about vaccine efficacy, side effects,\nand political influences allowing for more context aware rebuttals. Our\nevaluation, conducted through human judgment, LLM based assessments, and\nautomatic metrics, reveals strong alignment across these methods. Our findings\ndemonstrate that integrating label descriptions and structured fine-tuning\nenhances counter-argument effectiveness, offering a promising approach for\nmitigating vaccine misinformation at scale.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u9488\u5bf9\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u7684\u53cd\u9a73\u8bba\u70b9\u7684\u80fd\u529b\uff0c\u65e8\u5728\u5bf9\u6297\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u75ab\u82d7\u6000\u7591\u8bba\u548c\u9519\u8bef\u4fe1\u606f\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u9519\u8bef\u75ab\u82d7\u4fe1\u606f\u5f71\u54cd\u516c\u5171\u5065\u5eb7\uff0c\u9700\u8981\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u53cd\u9a73\u8bba\u70b9\u4ee5\u6d88\u9664\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f18\u5316\u53cd\u9a73\u8bba\u70b9\u7684\u751f\u6210\u3002\u540c\u65f6\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u5c06\u53cd\u75ab\u82d7\u63a8\u6587\u5206\u7c7b\u4e3a\u591a\u4e2a\u7c7b\u522b\uff0c\u4ee5\u4fbf\u63d0\u4f9b\u66f4\u7b26\u5408\u8bed\u5883\u7684\u53cd\u9a73\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u5224\u65ad\u3001\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u6574\u5408\u6807\u7b7e\u63cf\u8ff0\u548c\u7ed3\u6784\u5316\u5fae\u8c03\u53ef\u4ee5\u63d0\u9ad8\u53cd\u9a73\u8bba\u70b9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528LLM\u751f\u6210\u53cd\u9a73\u8bba\u70b9\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5927\u89c4\u6a21\u7f13\u89e3\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.16095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16095", "abs": "https://arxiv.org/abs/2510.16095", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study", "comment": null, "summary": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for\nexplainable medical Artificial Intelligence (AI) while constrained by data\nscarcity. Although Large Language Models (LLMs) can synthesize medical data,\ntheir clinical reliability remains unverified. This study evaluates the\nreliability of LLM-generated CoTs and investigates prompting strategies to\nenhance their quality. In a blinded comparative study, senior clinicians in\nAssisted Reproductive Technology (ART) evaluated CoTs generated via three\ndistinct strategies: Zero-shot, Random Few-shot (using shallow examples), and\nSelective Few-shot (using diverse, high-quality examples). These expert ratings\nwere compared against evaluations from a state-of-the-art AI model (GPT-4o).\nThe Selective Few-shot strategy significantly outperformed other strategies\nacross all human evaluation metrics (p < .001). Critically, the Random Few-shot\nstrategy offered no significant improvement over the Zero-shot baseline,\ndemonstrating that low-quality examples are as ineffective as no examples. The\nsuccess of the Selective strategy is attributed to two principles:\n\"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\"\n(generalization). Notably, the AI evaluator failed to discern these critical\nperformance differences. The clinical reliability of synthetic CoTs is dictated\nby strategic prompt curation, not the mere presence of examples. We propose a\n\"Dual Principles\" framework as a foundational methodology to generate\ntrustworthy data at scale. This work offers a validated solution to the data\nbottleneck and confirms the indispensable role of human expertise in evaluating\nhigh-stakes clinical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u751f\u6210\u7684\u4e34\u5e8a\u601d\u7ef4\u94fe (CoT) \u7684\u53ef\u9760\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u63d0\u9ad8\u5176\u8d28\u91cf\u7684\u63d0\u793a\u7b56\u7565\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u4e34\u5e8a\u601d\u7ef4\u94fe (CoT) \u5bf9\u4e8e\u53ef\u89e3\u91ca\u7684\u533b\u7597\u4eba\u5de5\u667a\u80fd (AI) \u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u5408\u6210\u533b\u7597\u6570\u636e\uff0c\u4f46\u5176\u4e34\u5e8a\u53ef\u9760\u6027\u4ecd\u672a\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u76f2\u6cd5\u5bf9\u6bd4\u7814\u7a76\uff0c\u751f\u6b96\u8f85\u52a9\u6280\u672f (ART) \u7684\u9ad8\u7ea7\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e86\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u7684\u7b56\u7565\u751f\u6210\u7684 CoT\uff1a\u96f6\u6837\u672c\u3001\u968f\u673a\u5c11\u6837\u672c\uff08\u4f7f\u7528\u6d45\u5c42\u793a\u4f8b\uff09\u548c\u9009\u62e9\u6027\u5c11\u6837\u672c\uff08\u4f7f\u7528\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u793a\u4f8b\uff09\u3002\u8fd9\u4e9b\u4e13\u5bb6\u8bc4\u5206\u4e0e\u6765\u81ea\u6700\u5148\u8fdb\u7684 AI \u6a21\u578b (GPT-4o) \u7684\u8bc4\u4f30\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u5728\u6240\u6709\u4eba\u5de5\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u663e\u7740\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565 (p < .001)\u3002\u968f\u673a\u5c11\u6837\u672c\u7b56\u7565\u5e76\u6ca1\u6709\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u4f9b\u663e\u7740\u6539\u8fdb\uff0c\u8fd9\u8868\u660e\u4f4e\u8d28\u91cf\u7684\u793a\u4f8b\u4e0e\u6ca1\u6709\u793a\u4f8b\u4e00\u6837\u65e0\u6548\u3002\u9009\u62e9\u6027\u7b56\u7565\u7684\u6210\u529f\u5f52\u56e0\u4e8e\u4e24\u4e2a\u539f\u5219\uff1a\u201c\u91d1\u6807\u51c6\u6df1\u5ea6\u201d\uff08\u63a8\u7406\u8d28\u91cf\uff09\u548c\u201c\u4ee3\u8868\u6027\u591a\u6837\u6027\u201d\uff08\u6cdb\u5316\uff09\u3002", "conclusion": "\u5408\u6210 CoT \u7684\u4e34\u5e8a\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u7b56\u7565\u6027\u63d0\u793a\u7684\u8c03\u6574\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u793a\u4f8b\u7684\u5b58\u5728\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u201c\u53cc\u91cd\u539f\u5219\u201d\u6846\u67b6\uff0c\u4f5c\u4e3a\u5927\u89c4\u6a21\u751f\u6210\u53ef\u4fe1\u6570\u636e\u7684\u57fa\u672c\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6570\u636e\u74f6\u9888\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bc1\u5b9e\u4e86\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u5728\u8bc4\u4f30\u9ad8\u98ce\u9669\u4e34\u5e8a AI \u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.15965", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15965", "abs": "https://arxiv.org/abs/2510.15965", "authors": ["Mohan Zhang", "Yihua Zhang", "Jinghan Jia", "Zhangyang Wang", "Sijia Liu", "Tianlong Chen"], "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model", "comment": "NeurIPS 2025", "summary": "Modern large reasoning models (LRMs) exhibit impressive multi-step\nproblem-solving via chain-of-thought (CoT) reasoning. However, this iterative\nthinking mechanism introduces a new vulnerability surface. We present the\nDeadlock Attack, a resource exhaustion method that hijacks an LRM's generative\ncontrol flow by training a malicious adversarial embedding to induce perpetual\nreasoning loops. Specifically, the optimized embedding encourages transitional\ntokens (e.g., \"Wait\", \"But\") after reasoning steps, preventing the model from\nconcluding its answer. A key challenge we identify is the\ncontinuous-to-discrete projection gap: na\\\"ive projections of adversarial\nembeddings to token sequences nullify the attack. To overcome this, we\nintroduce a backdoor implantation strategy, enabling reliable activation\nthrough specific trigger tokens. Our method achieves a 100% attack success rate\nacross four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three\nmath reasoning benchmarks, forcing models to generate up to their maximum token\nlimits. The attack is also stealthy (in terms of causing negligible utility\nloss on benign user inputs) and remains robust against existing strategies\ntrying to mitigate the overthinking issue. Our findings expose a critical and\nunderexplored security vulnerability in LRMs from the perspective of reasoning\n(in)efficiency.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6b7b\u9501\u653b\u51fb\uff0c\u8be5\u653b\u51fb\u901a\u8fc7\u8bf1\u5bfc\u6a21\u578b\u8fdb\u5165\u6c38\u4e45\u63a8\u7406\u5faa\u73af\u6765\u8017\u5c3d\u8d44\u6e90\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u591a\u6b65\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u8fed\u4ee3\u601d\u7ef4\u673a\u5236\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u8106\u5f31\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u6076\u610f\u5bf9\u6297\u5d4c\u5165\u6765\u52ab\u6301LRM\u7684\u751f\u6210\u63a7\u5236\u6d41\uff0c\u8be5\u5d4c\u5165\u9f13\u52b1\u5728\u63a8\u7406\u6b65\u9aa4\u4e4b\u540e\u4f7f\u7528\u8fc7\u6e21token\uff08\u4f8b\u5982\uff0c\u201cWait\u201d\uff0c\u201cBut\u201d\uff09\uff0c\u4ece\u800c\u963b\u6b62\u6a21\u578b\u5f97\u51fa\u7b54\u6848\u3002\u4e3a\u4e86\u514b\u670d\u8fde\u7eed\u5230\u79bb\u6563\u7684\u6295\u5f71\u5dee\u8ddd\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u540e\u95e8\u690d\u5165\u7b56\u7565\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u89e6\u53d1token\u5b9e\u73b0\u53ef\u9760\u7684\u6fc0\u6d3b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u56db\u79cd\u5148\u8fdb\u7684LRM\uff08Phi-RM\u3001Nemotron-Nano\u3001R1-Qwen\u3001R1-Llama\uff09\u548c\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u8feb\u4f7f\u6a21\u578b\u751f\u6210\u9ad8\u8fbe\u5176\u6700\u5927token\u9650\u5236\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4ece\u63a8\u7406\uff08\u4e0d\uff09\u6548\u7387\u7684\u89d2\u5ea6\u6765\u770b\uff0cLRM\u4e2d\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u5b89\u5168\u6f0f\u6d1e\u3002"}}
{"id": "2510.16695", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16695", "abs": "https://arxiv.org/abs/2510.16695", "authors": ["Iman Deznabi", "Peeyush Kumar", "Madalina Fiterau"], "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting", "comment": null, "summary": "Zero-shot forecasting aims to predict outcomes for previously unseen\nconditions without direct historical data, posing a significant challenge for\ntraditional forecasting methods. We introduce a Resolution-Aware\nRetrieval-Augmented Forecasting model that enhances predictive accuracy by\nleveraging spatial correlations and temporal frequency characteristics. By\ndecomposing signals into different frequency components, our model employs\nresolution-aware retrieval, where lower-frequency components rely on broader\nspatial context, while higher-frequency components focus on local influences.\nThis allows the model to dynamically retrieve relevant data and adapt to new\nlocations with minimal historical context.\n  Applied to microclimate forecasting, our model significantly outperforms\ntraditional forecasting methods, numerical weather prediction models, and\nmodern foundation time series models, achieving 71% lower MSE than HRRR and 34%\nlower MSE than Chronos on the ERA5 dataset.\n  Our results highlight the effectiveness of retrieval-augmented and\nresolution-aware strategies, offering a scalable and data-efficient solution\nfor zero-shot forecasting in microclimate modeling and beyond.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5229\u7528\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u65f6\u95f4\u9891\u7387\u7279\u5f81\u6765\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u5728\u6ca1\u6709\u76f4\u63a5\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u9884\u6d4b\u5148\u524d\u672a\u89c1\u6761\u4ef6\u7684\u7ed3\u679c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u6a21\u578b\u5c06\u4fe1\u53f7\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u9891\u7387\u5206\u91cf\uff0c\u91c7\u7528\u5206\u8fa8\u7387\u611f\u77e5\u68c0\u7d22\uff0c\u5176\u4e2d\u8f83\u4f4e\u9891\u7387\u5206\u91cf\u4f9d\u8d56\u4e8e\u66f4\u5e7f\u6cdb\u7684\u7a7a\u95f4\u80cc\u666f\uff0c\u800c\u8f83\u9ad8\u9891\u7387\u5206\u91cf\u4fa7\u91cd\u4e8e\u5c40\u90e8\u5f71\u54cd\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u52a8\u6001\u68c0\u7d22\u76f8\u5173\u6570\u636e\u5e76\u9002\u5e94\u5177\u6709\u6700\u5c0f\u5386\u53f2\u80cc\u666f\u7684\u65b0\u4f4d\u7f6e\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5fae\u6c14\u5019\u9884\u6d4b\u4e2d\u663e\u7740\u4f18\u4e8e\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3001\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u548c\u73b0\u4ee3\u57fa\u7840\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u5728 ERA5 \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7684 MSE \u6bd4 HRRR \u4f4e 71%\uff0c\u6bd4 Chronos \u4f4e 34%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u68c0\u7d22\u589e\u5f3a\u548c\u5206\u8fa8\u7387\u611f\u77e5\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5fae\u6c14\u5019\u5efa\u6a21\u53ca\u5176\u4ed6\u9886\u57df\u7684\u96f6\u6837\u672c\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16160", "abs": "https://arxiv.org/abs/2510.16160", "authors": ["Ahmad Arrabi", "Jay Hwasung Jung", "Jax Luo", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "Automated C-Arm Positioning via Conformal Landmark Localization", "comment": null, "summary": "Accurate and reliable C-arm positioning is essential for fluoroscopy-guided\ninterventions. However, clinical workflows rely on manual alignment that\nincreases radiation exposure and procedural delays. In this work, we present a\npipeline that autonomously navigates the C-arm to predefined anatomical\nlandmarks utilizing X-ray images. Given an input X-ray image from an arbitrary\nstarting location on the operating table, the model predicts a 3D displacement\nvector toward each target landmark along the body. To ensure reliable\ndeployment, we capture both aleatoric and epistemic uncertainties in the\nmodel's predictions and further calibrate them using conformal prediction. The\nderived prediction regions are interpreted as 3D confidence regions around the\npredicted landmark locations. The training framework combines a probabilistic\nloss with skeletal pose regularization to encourage anatomically plausible\noutputs. We validate our approach on a synthetic X-ray dataset generated from\nDeepDRR. Results show not only strong localization accuracy across multiple\narchitectures but also well-calibrated prediction bounds. These findings\nhighlight the pipeline's potential as a component in safe and reliable\nautonomous C-arm systems. Code is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance_APAH", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u5bfc\u822aC\u5f62\u81c2\u7684\u7ba1\u7ebf\uff0c\u5229\u7528X\u5c04\u7ebf\u56fe\u50cf\u5c06C\u5f62\u81c2\u5bfc\u822a\u5230\u9884\u5b9a\u4e49\u7684\u89e3\u5256\u6807\u5fd7\u7269\u3002", "motivation": "\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4f9d\u8d56\u4e8e\u624b\u52a8\u5bf9\u9f50\uff0c\u8fd9\u4f1a\u589e\u52a0\u8f90\u5c04\u66b4\u9732\u548c\u624b\u672f\u5ef6\u8fdf\u3002", "method": "\u8be5\u6a21\u578b\u9884\u6d4b\u4e00\u4e2a\u671d\u5411\u8eab\u4f53\u4e0a\u6bcf\u4e2a\u76ee\u6807\u6807\u5fd7\u7269\u76843D\u4f4d\u79fb\u5411\u91cf\u3002\u4e3a\u4e86\u786e\u4fdd\u53ef\u9760\u7684\u90e8\u7f72\uff0c\u6211\u4eec\u6355\u83b7\u6a21\u578b\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u5bf9\u5176\u8fdb\u884c\u6821\u51c6\u3002\u8bad\u7ec3\u6846\u67b6\u5c06\u6982\u7387\u635f\u5931\u4e0e\u9aa8\u9abc\u59ff\u52bf\u6b63\u5219\u5316\u76f8\u7ed3\u5408\uff0c\u4ee5\u9f13\u52b1\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u7684\u8f93\u51fa\u3002", "result": "\u5728DeepDRR\u751f\u6210\u7684\u5408\u6210X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u4ec5\u5728\u591a\u79cd\u67b6\u6784\u4e0a\u5177\u6709\u5f88\u5f3a\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u800c\u4e14\u5177\u6709\u826f\u597d\u6821\u51c6\u7684\u9884\u6d4b\u754c\u9650\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u51fa\u4e86\u8be5\u7ba1\u7ebf\u4f5c\u4e3a\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3bC\u5f62\u81c2\u7cfb\u7edf\u7ec4\u4ef6\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16363", "abs": "https://arxiv.org/abs/2510.16363", "authors": ["Nilmadhab Das", "Vishal Vaibhav", "Yash Sunil Choudhary", "V. Vijaya Saradhi", "Ashish Anand"], "title": "End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction", "comment": "Accepted version. To appear in IJCNN 2025", "summary": "Argument Mining (AM) helps in automating the extraction of complex\nargumentative structures such as Argument Components (ACs) like Premise, Claim\netc. and Argumentative Relations (ARs) like Support, Attack etc. in an\nargumentative text. Due to the inherent complexity of reasoning involved with\nthis task, modelling dependencies between ACs and ARs is challenging. Most of\nthe recent approaches formulate this task through a generative paradigm by\nflattening the argumentative structures. In contrast to that, this study\njointly formulates the key tasks of AM in an end-to-end fashion using\nAutoregressive Argumentative Structure Prediction (AASP) framework. The\nproposed AASP framework is based on the autoregressive structure prediction\nframework that has given good performance for several NLP tasks. AASP framework\nmodels the argumentative structures as constrained pre-defined sets of actions\nwith the help of a conditional pre-trained language model. These actions build\nthe argumentative structures step-by-step in an autoregressive manner to\ncapture the flow of argumentative reasoning in an efficient way. Extensive\nexperiments conducted on three standard AM benchmarks demonstrate that AASP\nachieves state-of-theart (SoTA) results across all AM tasks in two benchmarks\nand delivers strong results in one benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bba\u8bc1\u6316\u6398\uff08AM\uff09\u6846\u67b6\uff0c\u5373\u81ea\u56de\u5f52\u8bba\u8bc1\u7ed3\u6784\u9884\u6d4b\uff08AASP\uff09\uff0c\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8054\u5408\u6267\u884cAM\u7684\u5173\u952e\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u8bba\u8bc1\u6316\u6398\u65b9\u6cd5\u901a\u5e38\u5c06\u8bba\u8bc1\u7ed3\u6784\u6241\u5e73\u5316\uff0c\u5ffd\u7565\u4e86\u8bba\u8bc1\u6210\u5206\uff08AC\uff09\u548c\u8bba\u8bc1\u5173\u7cfb\uff08AR\uff09\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u96be\u4ee5\u6355\u6349\u8bba\u8bc1\u63a8\u7406\u7684\u590d\u6742\u6027\u3002", "method": "AASP\u6846\u67b6\u57fa\u4e8e\u81ea\u56de\u5f52\u7ed3\u6784\u9884\u6d4b\uff0c\u5229\u7528\u6761\u4ef6\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u8bba\u8bc1\u7ed3\u6784\u5efa\u6a21\u4e3a\u53d7\u7ea6\u675f\u7684\u9884\u5b9a\u4e49\u52a8\u4f5c\u96c6\uff0c\u901a\u8fc7\u9010\u6b65\u6784\u5efa\u8bba\u8bc1\u7ed3\u6784\u6765\u6355\u6349\u8bba\u8bc1\u63a8\u7406\u7684\u6d41\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6AM\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAASP\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\u3002", "conclusion": "AASP\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u8bba\u8bc1\u63a8\u7406\u7684\u6d41\u7a0b\uff0c\u5e76\u5728\u8bba\u8bc1\u6316\u6398\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16193", "abs": "https://arxiv.org/abs/2510.16193", "authors": ["Elija Perrier"], "title": "Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability", "comment": "Under review", "summary": "Corporate responsibility turns on notions of corporate \\textit{mens rea},\ntraditionally imputed from human agents. Yet these assumptions are under\nchallenge as generative AI increasingly mediates enterprise decision-making.\nBuilding on the theory of extended cognition, we argue that in response\ncorporate knowledge may be redefined as a dynamic capability, measurable by the\nefficiency of its information-access procedures and the validated reliability\nof their outputs. We develop a formal model that captures epistemic states of\ncorporations deploying sophisticated AI or information systems, introducing a\ncontinuous organisational knowledge metric $S_S(\\varphi)$ which integrates a\npipeline's computational cost and its statistically validated error rate. We\nderive a thresholded knowledge predicate $\\mathsf{K}_S$ to impute knowledge and\na firm-wide epistemic capacity index $\\mathcal{K}_{S,t}$ to measure overall\ncapability. We then operationally map these quantitative metrics onto the legal\nstandards of actual knowledge, constructive knowledge, wilful blindness, and\nrecklessness. Our work provides a pathway towards creating measurable and\njusticiable audit artefacts, that render the corporate mind tractable and\naccountable in the algorithmic age.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u4f01\u4e1a\u77e5\u8bc6\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4f01\u4e1a\u5728\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u65f6\u7684\u8d23\u4efb\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u4f01\u4e1a\u51b3\u7b56\uff0c\u4f20\u7edf\u7684\u4f01\u4e1a\u8d23\u4efb\u6982\u5ff5\u53d7\u5230\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u8ba4\u77e5\u7406\u8bba\uff0c\u5c06\u4f01\u4e1a\u77e5\u8bc6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u79cd\u52a8\u6001\u80fd\u529b\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f62\u5f0f\u6a21\u578b\u6765\u6355\u6349\u4f01\u4e1a\u5728\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u6216\u4fe1\u606f\u7cfb\u7edf\u65f6\u7684\u8ba4\u77e5\u72b6\u6001\u3002", "result": "\u63a8\u5bfc\u51fa\u4e00\u4e2a\u9608\u503c\u77e5\u8bc6\u8c13\u8bcd\u548c\u4e00\u4e2a\u516c\u53f8\u8303\u56f4\u5185\u7684\u8ba4\u77e5\u80fd\u529b\u6307\u6807\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5b9a\u91cf\u6307\u6807\u6620\u5c04\u5230\u5b9e\u9645\u77e5\u8bc6\u3001\u63a8\u5b9a\u77e5\u8bc6\u3001\u6545\u610f\u89c6\u800c\u4e0d\u89c1\u548c\u9c81\u83bd\u7b49\u6cd5\u5f8b\u6807\u51c6\u3002", "conclusion": "\u672c\u6587\u4e3a\u521b\u5efa\u53ef\u8861\u91cf\u548c\u53ef\u5ba1\u5224\u7684\u5ba1\u8ba1\u5de5\u4ef6\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\uff0c\u4f7f\u4f01\u4e1a\u5728\u7b97\u6cd5\u65f6\u4ee3\u80fd\u591f\u88ab\u8ffd\u8e2a\u548c\u8d1f\u8d23\u3002"}}
{"id": "2510.15967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15967", "abs": "https://arxiv.org/abs/2510.15967", "authors": ["Zhengyi Zhong", "Wenzheng Jiang", "Weidong Bao", "Ji Wang", "Cheems Wang", "Guanbo Wang", "Yongheng Deng", "Ju Ren"], "title": "Gains: Fine-grained Federated Domain Adaptation in Open Set", "comment": "Accepted by NeurIPS2025", "summary": "Conventional federated learning (FL) assumes a closed world with a fixed\ntotal number of clients. In contrast, new clients continuously join the FL\nprocess in real-world scenarios, introducing new knowledge. This raises two\ncritical demands: detecting new knowledge, i.e., knowledge discovery, and\nintegrating it into the global model, i.e., knowledge adaptation. Existing\nresearch focuses on coarse-grained knowledge discovery, and often sacrifices\nsource domain performance and adaptation efficiency. To this end, we propose a\nfine-grained federated domain adaptation approach in open set (Gains). Gains\nsplits the model into an encoder and a classifier, empirically revealing\nfeatures extracted by the encoder are sensitive to domain shifts while\nclassifier parameters are sensitive to class increments. Based on this, we\ndevelop fine-grained knowledge discovery and contribution-driven aggregation\ntechniques to identify and incorporate new knowledge. Additionally, an\nanti-forgetting mechanism is designed to preserve source domain performance,\nensuring balanced adaptation. Experimental results on multi-domain datasets\nacross three typical data-shift scenarios demonstrate that Gains significantly\noutperforms other baselines in performance for both source-domain and\ntarget-domain clients. Code is available at:\nhttps://github.com/Zhong-Zhengyi/Gains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u8054\u90a6\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u653e\u96c6\u573a\u666f\uff0c\u53ef\u4ee5\u68c0\u6d4b\u65b0\u77e5\u8bc6\u5e76\u5c06\u5176\u6574\u5408\u5230\u5168\u5c40\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u57df\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5047\u8bbe\u5ba2\u6237\u7aef\u6570\u91cf\u56fa\u5b9a\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u65b0\u5ba2\u6237\u7aef\u4e0d\u65ad\u52a0\u5165\uff0c\u5f15\u5165\u65b0\u77e5\u8bc6\uff0c\u9700\u8981\u68c0\u6d4b\u65b0\u77e5\u8bc6\u5e76\u6574\u5408\u5230\u5168\u5c40\u6a21\u578b\u4e2d\u3002\u73b0\u6709\u7814\u7a76\u4fa7\u91cd\u4e8e\u7c97\u7c92\u5ea6\u7684\u77e5\u8bc6\u53d1\u73b0\uff0c\u5e76\u4e14\u5e38\u5e38\u727a\u7272\u6e90\u57df\u6027\u80fd\u548c\u9002\u5e94\u6548\u7387\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6a21\u578b\u5206\u89e3\u4e3a\u7f16\u7801\u5668\u548c\u5206\u7c7b\u5668\uff0c\u53d1\u73b0\u7f16\u7801\u5668\u5bf9\u57df\u6f02\u79fb\u654f\u611f\uff0c\u800c\u5206\u7c7b\u5668\u53c2\u6570\u5bf9\u7c7b\u522b\u589e\u91cf\u654f\u611f\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86\u7ec6\u7c92\u5ea6\u7684\u77e5\u8bc6\u53d1\u73b0\u548c\u8d21\u732e\u9a71\u52a8\u7684\u805a\u5408\u6280\u672f\u6765\u8bc6\u522b\u548c\u6574\u5408\u65b0\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cd\u9057\u5fd8\u673a\u5236\u6765\u4fdd\u6301\u6e90\u57df\u6027\u80fd\uff0c\u786e\u4fdd\u5e73\u8861\u9002\u5e94\u3002", "result": "\u5728\u8de8\u4e09\u4e2a\u5178\u578b\u6570\u636e\u6f02\u79fb\u573a\u666f\u7684\u591a\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGains \u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u5ba2\u6237\u7aef\u7684\u6027\u80fd\u65b9\u9762\u5747\u663e\u7740\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u3002", "conclusion": "Gains \u662f\u4e00\u79cd\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u8054\u90a6\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e2d\u5b9e\u73b0\u77e5\u8bc6\u53d1\u73b0\u548c\u6574\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u57df\u6027\u80fd\u3002"}}
{"id": "2510.17139", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17139", "abs": "https://arxiv.org/abs/2510.17139", "authors": ["Zhichao Xu", "Shengyao Zhuang", "Xueguang Ma", "Bingsen Chen", "Yijun Tian", "Fengran Mo", "Jie Cao", "Vivek Srikumar"], "title": "Rethinking On-policy Optimization for Query Augmentation", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.", "AI": {"tldr": "\u5bf9\u6bd4\u4e86\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u3001\u65e0\u8bad\u7ec3\u7684\u67e5\u8be2\u6269\u5c55\u901a\u5e38\u4e0e\u66f4\u6602\u8d35\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002\u63d0\u51fa\u4e86On-policy Pseudo-document Query Expansion (OPQE)\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u63d0\u793a\u7684\u7075\u6d3b\u6027\u548c\u751f\u6210\u7ed3\u6784\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u7684\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u4f18\u4e8e\u5355\u72ec\u7684\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u91cd\u5199\u3002", "motivation": "\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u5bf9\u67e5\u8be2\u6269\u5c55\u7684\u5174\u8da3\u6fc0\u589e\uff0c\u4e3b\u8981\u6709\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u4f46\u5c1a\u672a\u5728\u4e00\u81f4\u7684\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u5bf9\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67e5\u8be2\u6269\u5c55\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u6bd4\u8f83\uff0c\u5e76\u63d0\u51fa\u4e86On-policy Pseudo-document Query Expansion (OPQE)\u65b9\u6cd5\u3002", "result": "\u7b80\u5355\u7684\u3001\u65e0\u8bad\u7ec3\u7684\u67e5\u8be2\u6269\u5c55\u901a\u5e38\u4e0e\u66f4\u6602\u8d35\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002OPQE\u4f18\u4e8e\u5355\u72ec\u7684\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u91cd\u5199\u3002", "conclusion": "\u7ed3\u5408\u63d0\u793a\u7684\u7075\u6d3b\u6027\u548c\u751f\u6210\u7ed3\u6784\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u7684\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u53ef\u4ee5\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\u3002"}}
{"id": "2510.16179", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16179", "abs": "https://arxiv.org/abs/2510.16179", "authors": ["Xavier Giro-i-Nieto", "Nefeli Andreou", "Anqi Liang", "Manel Baradad", "Francesc Moreno-Noguer", "Aleix Martinez"], "title": "Cost Savings from Automatic Quality Assessment of Generated Images", "comment": null, "summary": "Deep generative models have shown impressive progress in recent years, making\nit possible to produce high quality images with a simple text prompt or a\nreference image. However, state of the art technology does not yet meet the\nquality standards offered by traditional photographic methods. For this reason,\nproduction pipelines that use generated images often include a manual stage of\nimage quality assessment (IQA). This process is slow and expensive, especially\nbecause of the low yield of automatically generated images that pass the\nquality bar. The IQA workload can be reduced by introducing an automatic\npre-filtering stage, that will increase the overall quality of the images sent\nto review and, therefore, reduce the average cost required to obtain a high\nquality image. We present a formula that estimates the cost savings depending\non the precision and pass yield of a generic IQA engine. This formula is\napplied in a use case of background inpainting, showcasing a significant cost\nsaving of 51.61% obtained with a simple AutoML solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u9884\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u7684\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u672a\u8fbe\u5230\u4f20\u7edf\u6444\u5f71\u65b9\u6cd5\u6807\u51c6\uff0c\u5bfc\u81f4IQA\u8fc7\u7a0b\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u516c\u5f0f\u6765\u4f30\u8ba1\u6210\u672c\u8282\u7701\uff0c\u8be5\u516c\u5f0f\u4f9d\u8d56\u4e8e\u901a\u7528IQA\u5f15\u64ce\u7684\u7cbe\u5ea6\u548c\u901a\u8fc7\u7387\u3002", "result": "\u5728\u4e00\u4e2a\u80cc\u666f\u4fee\u590d\u7684\u7528\u4f8b\u4e2d\uff0c\u4f7f\u7528\u7b80\u5355\u7684AutoML\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e8651.61%\u7684\u663e\u8457\u6210\u672c\u8282\u7701\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u81ea\u52a8\u9884\u8fc7\u6ee4\u9636\u6bb5\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u83b7\u5f97\u9ad8\u8d28\u91cf\u56fe\u50cf\u6240\u9700\u7684\u5e73\u5747\u6210\u672c\u3002"}}
{"id": "2510.16373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16373", "abs": "https://arxiv.org/abs/2510.16373", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira"], "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment", "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is transforming AI,\nopening new opportunities in sensitive and high-impact areas such as Mental\nHealth (MH). Yet, despite these advancements, recent evidence reveals that\nsmaller-scale models still struggle to deliver optimal performance in\ndomain-specific applications. In this study, we present a cost-efficient yet\npowerful approach to improve MH assessment capabilities of an LLM, without\nrelying on any computationally intensive techniques. Our lightweight method\nconsists of a linear transformation applied to a specific layer's activations,\nleveraging steering vectors to guide the model's output. Remarkably, this\nintervention enables the model to achieve improved results across two distinct\ntasks: (1) identifying whether a Reddit post is useful for detecting the\npresence or absence of depressive symptoms (relevance prediction task), and (2)\ncompleting a standardized psychological screening questionnaire for depression\nbased on users' Reddit post history (questionnaire completion task). Results\nhighlight the untapped potential of steering mechanisms as computationally\nefficient tools for LLMs' MH domain adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u548csteering vectors\u6765\u63d0\u5347LLM\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u65b9\u9762\u7684\u80fd\u529b\uff0c\u65e0\u9700\u5927\u91cf\u8ba1\u7b97\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u7684\u5e94\u7528\u4e2d\uff0c\u5c0f\u89c4\u6a21\u6a21\u578b\u4ecd\u7136\u96be\u4ee5\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5fc3\u7406\u5065\u5eb7(MH)\u9886\u57df\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u5bf9\u7279\u5b9a\u5c42\u7684\u6fc0\u6d3b\u5e94\u7528\u7ebf\u6027\u53d8\u6362\uff0c\u5229\u7528steering vectors\u6765\u5f15\u5bfc\u6a21\u578b\u7684\u8f93\u51fa\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u7ed3\u679c\uff1a(1)\u8bc6\u522bReddit\u5e16\u5b50\u662f\u5426\u6709\u52a9\u4e8e\u68c0\u6d4b\u6291\u90c1\u75c7\u72b6(\u76f8\u5173\u6027\u9884\u6d4b\u4efb\u52a1)\uff0c(2)\u6839\u636e\u7528\u6237\u7684Reddit\u5e16\u5b50\u5386\u53f2\u5b8c\u6210\u6807\u51c6\u5316\u5fc3\u7406\u7b5b\u67e5\u95ee\u5377(\u95ee\u5377\u5b8c\u6210\u4efb\u52a1)\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0csteering\u673a\u5236\u4f5c\u4e3a\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u5de5\u5177\uff0c\u5728llm\u7684MH\u9886\u57df\u9002\u5e94\u65b9\u9762\u5177\u6709\u672a\u5f00\u53d1\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited.", "AI": {"tldr": "TEAM-PHI: \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc4\u4f30\u548c\u9009\u62e9PHI\u53bb\u6807\u8bc6\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "motivation": "\u8bc4\u4f30\u548c\u6bd4\u8f83PHI\u53bb\u6807\u8bc6\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u5c0f\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\u3002", "method": "\u4f7f\u7528\u591a\u4ee3\u7406\u8bc4\u4f30\u6846\u67b6TEAM-PHI\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc4\u4f30\u53bb\u6807\u8bc6\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTEAM-PHI\u80fd\u591f\u4ea7\u751f\u4e00\u81f4\u4e14\u51c6\u786e\u7684\u6392\u540d\uff0c\u5e76\u4e14\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u543b\u5408\u3002", "conclusion": "TEAM-PHI\u4e3aPHI\u53bb\u6807\u8bc6\u7684\u81ea\u52a8\u8bc4\u4f30\u548c\u6700\u4f73\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u5728ground-truth\u6807\u7b7e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2510.15968", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.15968", "abs": "https://arxiv.org/abs/2510.15968", "authors": ["Zhen Huang", "Hong Wang", "Wenkai Yang", "Muxi Tang", "Depeng Xie", "Ting-Jung Lin", "Yu Zhang", "Wei W. Xing", "Lei He"], "title": "Self-Attention to Operator Learning-based 3D-IC Thermal Simulation", "comment": null, "summary": "Thermal management in 3D ICs is increasingly challenging due to higher power\ndensities. Traditional PDE-solving-based methods, while accurate, are too slow\nfor iterative design. Machine learning approaches like FNO provide faster\nalternatives but suffer from high-frequency information loss and high-fidelity\ndata dependency. We introduce Self-Attention U-Net Fourier Neural Operator\n(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to\ncapture long-range dependencies and model local high-frequency features\neffectively. Transfer learning is employed to fine-tune low-fidelity data,\nminimizing the need for extensive high-fidelity datasets and speeding up\ntraining. Experiments demonstrate that SAU-FNO achieves state-of-the-art\nthermal prediction accuracy and provides an 842x speedup over traditional FEM\nmethods, making it an efficient tool for advanced 3D IC thermal simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u70ed\u7ba1\u7406\u6846\u67b6SAU-FNO\uff0c\u7528\u4e8e\u89e3\u51b33D\u96c6\u6210\u7535\u8def\u4e2d\u4f20\u7edf\u65b9\u6cd5\u901f\u5ea6\u6162\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e22\u5931\u9ad8\u9891\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u529f\u7387\u5bc6\u5ea6\u8f83\u9ad8\uff0c3D IC \u4e2d\u7684\u70ed\u7ba1\u7406\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u7684\u57fa\u4e8e PDE \u6c42\u89e3\u7684\u65b9\u6cd5\u867d\u7136\u51c6\u786e\uff0c\u4f46\u5bf9\u4e8e\u8fed\u4ee3\u8bbe\u8ba1\u6765\u8bf4\u592a\u6162\u3002\u50cf FNO \u8fd9\u6837\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u548c\u9ad8\u4fdd\u771f\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c U-Net \u4e0e FNO\uff0c\u4ee5\u6355\u83b7\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u5e76\u6709\u6548\u5730\u5efa\u6a21\u5c40\u90e8\u9ad8\u9891\u7279\u5f81\u3002\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u6765\u5fae\u8c03\u4f4e\u4fdd\u771f\u6570\u636e\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5bf9\u5927\u91cf\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u7684\u9700\u6c42\u5e76\u52a0\u5feb\u8bad\u7ec3\u3002", "result": "SAU-FNO \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u70ed\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u4e14\u6bd4\u4f20\u7edf\u7684 FEM \u65b9\u6cd5\u63d0\u4f9b\u4e86 842 \u500d\u7684\u52a0\u901f\u3002", "conclusion": "SAU-FNO \u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u7ea7 3D IC \u70ed\u4eff\u771f\u7684\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2510.17281", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17281", "abs": "https://arxiv.org/abs/2510.17281", "authors": ["Qingyao Ai", "Yichen Tang", "Changyue Wang", "Jianming Long", "Weihang Su", "Yiqun Liu"], "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems", "comment": null, "summary": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf(LLMsys)\u901a\u8fc7\u6269\u5927\u6570\u636e\u3001\u53c2\u6570\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6765\u6539\u8fdb\uff0c\u4f46\u7531\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9010\u6e10\u8017\u5c3d\u548c\u66f4\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5e26\u6765\u7684\u8fb9\u9645\u6536\u76ca\uff0c\u5176\u4e0a\u9650\u51e0\u4e4e\u8fbe\u5230\u3002\u56e0\u6b64\uff0c\u4e3aLLMsys\u6784\u5efa\u8bb0\u5fc6\u548c\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u5df2\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709LLM\u8bb0\u5fc6\u7684\u57fa\u51c6\u901a\u5e38\u4fa7\u91cd\u4e8e\u8bc4\u4f30\u7cfb\u7edf\u5728\u5177\u6709\u957f\u683c\u5f0f\u8f93\u5165\u7684\u540c\u8d28\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u6d4b\u8bd5\u5b83\u4eec\u4ece\u670d\u52a1\u65f6\u95f4\u5185\u7d2f\u79ef\u7684\u7528\u6237\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u53cd\u9988\u6a21\u62df\u6846\u67b6\u548c\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u3001\u8bed\u8a00\u548c\u4efb\u52a1\u7c7b\u578b\uff0c\u4ee5\u8bc4\u4f30LLMsys\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u8fdc\u4e0d\u80fd\u4ee4\u4eba\u6ee1\u610f\u3002", "conclusion": "\u6211\u4eec\u5e0c\u671b\u8fd9\u4e2a\u57fa\u51c6\u53ef\u4ee5\u4e3a\u672a\u6765LLM\u8bb0\u5fc6\u548c\u4f18\u5316\u7b97\u6cd5\u7684\u7814\u7a76\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2510.16196", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16196", "abs": "https://arxiv.org/abs/2510.16196", "authors": ["Zheng Huang", "Enpei Zhang", "Yinghao Cai", "Weikang Qiu", "Carl Yang", "Elynn Chen", "Xiang Zhang", "Rex Ying", "Dawei Zhou", "Yujun Yan"], "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI", "comment": null, "summary": "Understanding how the brain encodes visual information is a central challenge\nin neuroscience and machine learning. A promising approach is to reconstruct\nvisual stimuli, essentially images, from functional Magnetic Resonance Imaging\n(fMRI) signals. This involves two stages: transforming fMRI signals into a\nlatent space and then using a pretrained generative model to reconstruct\nimages. The reconstruction quality depends on how similar the latent space is\nto the structure of neural activity and how well the generative model produces\nimages from that space. Yet, it remains unclear which type of latent space best\nsupports this transformation and how it should be organized to represent visual\nstimuli effectively. We present two key findings. First, fMRI signals are more\nsimilar to the text space of a language model than to either a vision based\nspace or a joint text image space. Second, text representations and the\ngenerative model should be adapted to capture the compositional nature of\nvisual stimuli, including objects, their detailed attributes, and\nrelationships. Building on these insights, we propose PRISM, a model that\nProjects fMRI sIgnals into a Structured text space as an interMediate\nrepresentation for visual stimuli reconstruction. It includes an object centric\ndiffusion module that generates images by composing individual objects to\nreduce object detection errors, and an attribute relationship search module\nthat automatically identifies key attributes and relationships that best align\nwith the neural activity. Extensive experiments on real world datasets\ndemonstrate that our framework outperforms existing methods, achieving up to an\n8% reduction in perceptual loss. These results highlight the importance of\nusing structured text as the intermediate space to bridge fMRI signals and\nimage reconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u523a\u6fc0\u91cd\u5efa\u6a21\u578bPRISM\uff0c\u8be5\u6a21\u578b\u5229\u7528\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5c06fMRI\u4fe1\u53f7\u8f6c\u6362\u4e3a\u56fe\u50cf\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u5982\u4f55\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\u662f\u795e\u7ecf\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\u3002\u5229\u7528fMRI\u4fe1\u53f7\u91cd\u5efa\u89c6\u89c9\u523a\u6fc0\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u91cd\u5efa\u8d28\u91cf\u53d6\u51b3\u4e8e\u6f5c\u5728\u7a7a\u95f4\u4e0e\u795e\u7ecf\u6d3b\u52a8\u7ed3\u6784\u7684\u76f8\u4f3c\u6027\u4ee5\u53ca\u751f\u6210\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u80fd\u529b\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faPRISM\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06fMRI\u4fe1\u53f7\u6295\u5f71\u5230\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6269\u6563\u6a21\u5757\u548c\u4e00\u4e2a\u5c5e\u6027\u5173\u7cfb\u641c\u7d22\u6a21\u5757\uff0c\u4ee5\u6355\u6349\u89c6\u89c9\u523a\u6fc0\u7684\u7ec4\u5408\u6027\u8d28\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u611f\u77e5\u635f\u5931\u964d\u4f4e\u4e868%\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4f7f\u7528\u7ed3\u6784\u5316\u6587\u672c\u4f5c\u4e3a\u6865\u6881fMRI\u4fe1\u53f7\u548c\u56fe\u50cf\u91cd\u5efa\u7684\u4e2d\u95f4\u7a7a\u95f4\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16380", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16380", "abs": "https://arxiv.org/abs/2510.16380", "authors": ["Yu Ying Chiu", "Michael S. Lee", "Rachel Calcott", "Brandon Handoko", "Paul de Font-Reaulx", "Paula Rodriguez", "Chen Bo Calvin Zhang", "Ziwen Han", "Udari Madhushani Sehwag", "Yash Maurya", "Christina Q Knight", "Harry R. Lloyd", "Florence Bacus", "Mantas Mazeika", "Bing Liu", "Yejin Choi", "Mitchell L Gordon", "Sydney Levine"], "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes", "comment": "46 pages, 8 figures, 10 tables. Preprint", "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86MoReBench\uff0c\u4e00\u4e2a\u5305\u542b1000\u4e2a\u9053\u5fb7\u573a\u666f\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u786e\u4fddAI\u51b3\u7b56\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\uff0c\u9700\u8981\u7406\u89e3AI\u5982\u4f55\u505a\u51fa\u51b3\u7b56\u3002\u9053\u5fb7\u56f0\u5883\u4e3a\u8fc7\u7a0b\u8bc4\u4f30\u63d0\u4f9b\u4e86 \u043e\u0442\u043b\u0438\u0447\u043d\u0443\u044e \u8bd5\u9a8c\u573a\u3002", "method": "\u521b\u5efa\u4e86MoReBench\uff0c\u5305\u542b\u9053\u5fb7\u573a\u666f\u548c\u4e13\u5bb6\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u540c\u65f6\uff0c\u521b\u5efa\u4e86MoReBench-Theory\uff0c\u7528\u4e8e\u6d4b\u8bd5AI\u5728\u4e94\u79cd\u4e3b\u8981\u89c4\u8303\u4f26\u7406\u6846\u67b6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u73b0\u6709\u57fa\u51c6\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u65e0\u6cd5\u9884\u6d4b\u6a21\u578b\u5728\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u6a21\u578b\u5bf9\u7279\u5b9a\u9053\u5fb7\u6846\u67b6\u5b58\u5728\u504f\u597d\u3002", "conclusion": "\u8fd9\u4e9b\u57fa\u51c6\u63a8\u8fdb\u4e86\u4ee5\u8fc7\u7a0b\u4e3a\u4e2d\u5fc3\u7684\u63a8\u7406\u8bc4\u4f30\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u900f\u660e\u7684AI\u3002"}}
{"id": "2510.16206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16206", "abs": "https://arxiv.org/abs/2510.16206", "authors": ["Alex Zhavoronkov", "Dominika Wilczok", "Roman Yampolskiy"], "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI", "comment": null, "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory.To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u4f1a\u653e\u5927\u504f\u5dee\u548c\u9057\u6f0f\uff0c\u4ece\u800c\u4e0d\u6210\u6bd4\u4f8b\u5730\u6291\u5236\u67d0\u4e9b\u53d9\u8ff0\u3001\u4e2a\u4eba\u6216\u7fa4\u4f53\uff0c\u540c\u65f6\u4e0d\u6210\u6bd4\u4f8b\u5730\u63d0\u5347\u5176\u4ed6\u53d9\u8ff0\u3001\u4e2a\u4eba\u6216\u7fa4\u4f53\u3002\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6570\u5b57\u5b58\u5728\u6709\u9650\u7684\u4eba\u9010\u6e10\u88ab\u62b9\u53bb\uff0c\u800c\u5df2\u7ecf prominent \u7684\u4eba\u5219\u4f1a\u88ab\u653e\u5927\uff0c\u4ece\u800c\u91cd\u5851\u96c6\u4f53\u8bb0\u5fc6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\u53ef\u80fd\u4f1a\u653e\u5927\u504f\u5dee\u548c\u9057\u6f0f\uff0c\u4ece\u800c\u4e0d\u6210\u6bd4\u4f8b\u5730\u6291\u5236\u67d0\u4e9b\u53d9\u8ff0\u3001\u4e2a\u4eba\u6216\u7fa4\u4f53\uff0c\u540c\u65f6\u4e0d\u6210\u6bd4\u4f8b\u5730\u63d0\u5347\u5176\u4ed6\u53d9\u8ff0\u3001\u4e2a\u4eba\u6216\u7fa4\u4f53\uff0c\u4ece\u800c\u91cd\u5851\u96c6\u4f53\u8bb0\u5fc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u88ab\u8bb0\u4f4f\u7684\u6743\u5229\u201d\uff08RTBR\uff09\u7684\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u5305\u62ec\u6700\u5927\u9650\u5ea6\u5730\u964d\u4f4e\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u4fe1\u606f\u9057\u6f0f\u7684\u98ce\u9669\uff0c\u62e5\u62b1\u516c\u5e73\u5bf9\u5f85\u7684\u6743\u5229\uff0c\u540c\u65f6\u786e\u4fdd\u751f\u6210\u7684\u5185\u5bb9\u6700\u5927\u9650\u5ea6\u5730\u771f\u5b9e\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5", "conclusion": "\u63d0\u51fa\u4e86\u201c\u88ab\u8bb0\u4f4f\u7684\u6743\u5229\u201d\uff08RTBR\uff09\u7684\u6982\u5ff5\uff0c\u65e8\u5728\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u4fe1\u606f\u9057\u6f0f\u548c\u504f\u5dee\u95ee\u9898\uff0c\u786e\u4fdd\u516c\u5e73\u548c\u771f\u5b9e\u7684\u751f\u6210\u5185\u5bb9\u3002"}}
