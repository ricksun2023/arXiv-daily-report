<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 45]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 45]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

TL;DR: 研究语音语言模型(SLM)中的离散单元表示，专注于优化持续预训练期间的语音建模。


<details>
  <summary>Details</summary>
Motivation: 研究动机是优化语音语言模型在持续预训练期间的语音建模。

Method: 通过实验，系统地检查模型架构、数据表示和训练鲁棒性如何影响预训练阶段，调整现有的预训练语言模型以适应语音模态。考察了不同模型规模下语音编码器和聚类粒度的作用，展示了最佳离散化策略如何随模型容量变化。通过检查聚类分布和音素对齐，研究了离散词汇的有效使用，揭示了语言和副语言模式。此外，还探讨了聚类数据选择对模型鲁棒性的影响，强调了离散化训练和目标应用之间领域匹配的重要性。

Result: 实验结果强调了语音编码器和聚类粒度的作用，展示了最佳离散化策略如何随模型容量变化。揭示了语言和副语言模式。强调了离散化训练和目标应用之间领域匹配的重要性。

Conclusion: 结论是离散单元表示在语音语言模型中起着关键作用，并且最佳离散化策略随模型容量变化，领域匹配对于模型鲁棒性至关重要。

Abstract: This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [2] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的检测大型语言模型（LLM）幻觉的方法，该方法基于 N-Gram 频率张量，旨在捕捉更丰富的语义结构，从而更好地区分事实内容和幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的幻觉问题限制了它们生成一致和真实信息的可信度。现有的幻觉检测方法，如 ROUGE、BERTScore 或 Perplexity，缺乏有效检测幻觉所需的语义深度。

Method: 该方法构建了来自 LLM 生成文本的 N-Gram 频率张量，通过对张量进行分解，提取奇异值作为特征，并使用多层感知器（MLP）二元分类器进行幻觉检测。

Result: 该方法在 HaluEval 数据集上进行了评估，结果表明，与传统基线相比，该方法有显著改进，并且与最先进的 LLM judges 相比，具有竞争力的性能。

Conclusion: 该研究提出了一种有前景的幻觉检测新方法，通过 N-Gram 频率张量捕捉更丰富的语义结构，从而提高了幻觉检测的准确性。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [3] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为SAGE的框架，用于在推理时进行自适应更新，以解决大型语言模型无法持续适应和学习新数据的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型无法在推理时持续适应和学习新数据。

Method: 将复杂的推理任务分解为原子子任务，并引入触发引导的动态微调框架SAGE，该框架包含触发模块、触发缓冲模块和Lora存储模块。

Result: SAGE在原子推理子任务上表现出卓越的准确性、鲁棒性和稳定性。

Conclusion: SAGE可以通过在测试时进行动态知识更新，从而在原子推理子任务上实现卓越的性能。

Abstract: Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [4] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

TL;DR: 多人辩论旨在提高人工智能的推理能力，但研究发现，在能力不同的模型之间进行辩论有时反而有害。即使更强大的模型数量超过较弱的模型，辩论也可能导致准确率下降。模型倾向于达成一致，而不是挑战有缺陷的推理，导致从正确答案转向错误答案。因此，在没有激励或充分准备好抵制有说服力但错误的推理时，辩论可能会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 先前的工作主要集中在同质代理群体中的辩论，而本文探讨了模型能力的多样性如何影响多代理交互的动态和结果。

Method: 通过一系列实验

Result: 辩论会导致准确率随时间下降——即使在更强大的模型数量超过其较弱的对手的情况下。模型经常从正确答案转变为不正确的答案，以响应同伴的推理，从而偏爱达成协议而不是挑战有缺陷的推理。

Conclusion: 多人辩论中理由交流存在重要的失败模式，表明在没有激励或充分准备好抵制有说服力但错误的推理时，简单地应用辩论可能会导致性能下降。

Abstract: While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [5] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 仅用少量特征（token fertility ratios, token counts, 语言元数据）就能预测翻译质量，无需运行翻译系统。


<details>
  <summary>Details</summary>
Motivation: 探索在不运行翻译系统本身的情况下预测翻译质量的可能性。

Method: 使用梯度提升模型，基于token fertility ratios, token counts和语言学元数据预测GPT-4o在FLORES-200 benchmark上的ChrF scores。

Result: 对于XX->英语的翻译，R^2=0.66；对于英语->XX的翻译，R^2=0.72。类型学因素主导了对英语的预测，而fertility在对不同目标语言的翻译中起着更大的作用。

Conclusion: 翻译质量受到token-level fertility和更广泛的语言类型学的影响，为多语言评估和质量估计提供了新的见解。

Abstract: We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [6] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

TL;DR: 本文提出了一种直接评分方法，该方法使用合成摘要作为测试时的成对机器排序。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用作评估文档摘要、对话和故事生成等自由形式内容的自动评估器，因此需要通过测量它们与人类判断的相关性来评估此类模型。

Method: 该方法使用合成摘要作为测试时的成对机器排名。

Result: 该方法在 SummEval (+0.03)、TopicalChat (-0.03) 和 HANNA (+0.05) 元评估基准测试中，在轴平均样本级别相关性方面与最先进的成对评估器相比具有竞争力。

Conclusion: 发布合成的上下文摘要作为数据，以促进未来的工作。

Abstract: As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [7] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
*Aivin V. Solatorio*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为 Proof-Carrying Numbers (PCN) 的协议，用于强制执行数字保真度，通过机械验证来确保 LLM 生成的数字的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 作为随机系统，可能会生成偏离可用数据的数字，这种失败被称为数字幻觉。现有的保障措施无法保证保真度。

Method: PCN 是一种表示层协议，它将数字范围作为绑定到结构化声明的声明绑定令牌发出，并且验证器在声明的策略下检查每个令牌。

Result: PCN 具有可靠性、在诚实令牌下的完整性、故障关闭行为以及在策略改进下的单调性。

Conclusion: PCN 是一种轻量级且模型无关的协议，可以无缝集成到现有应用程序中，并且可以通过加密承诺进行扩展。通过强制执行验证作为显示前的强制步骤，PCN 为数字敏感设置建立了一个简单的契约：信任只能通过证明来获得，而缺少标记则表示不确定性。

Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

</details>


### [8] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
*Hajar Sakai,Yi-En Tseng,Mohammadsadegh Mikaeili,Joshua Bosire,Franziska Jovin*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大型语言模型 (LLM) 的框架，用于识别医院呼叫中心员工消息的主题和分类原因，旨在提高患者体验和护理质量。


<details>
  <summary>Details</summary>
Motivation: 医院呼叫中心产生大量员工消息，这些消息包含有价值的见解，但传统方法需要大量标注和训练。大型语言模型为医疗保健分析提供了更高效的方法。

Method: 该框架使用多阶段 LLM，包括推理、通用和轻量级模型，对员工消息进行主题识别和多类分类。同时考虑了数据安全和 HIPAA 合规性。

Result: 最佳模型 o3 实现了 78.4% 的加权 F1 分数和 79.2% 的准确率，其次是 gpt-5（75.3% 的加权 F1 分数和 76.2% 的准确率）。

Conclusion: 该方法可以将员工消息转化为可操作的见解，从而更有效地利用数据，识别培训机会，并支持改善患者体验和护理质量。

Abstract: Hospital call centers serve as the primary contact point for patients within
a hospital system. They also generate substantial volumes of staff messages as
navigators process patient requests and communicate with the hospital offices
following the established protocol restrictions and guidelines. This
continuously accumulated large amount of text data can be mined and processed
to retrieve insights; however, traditional supervised learning approaches
require annotated data, extensive training, and model tuning. Large Language
Models (LLMs) offer a paradigm shift toward more computationally efficient
methodologies for healthcare analytics. This paper presents a multi-stage
LLM-based framework that identifies staff message topics and classifies
messages by their reasons in a multi-class fashion. In the process, multiple
LLM types, including reasoning, general-purpose, and lightweight models, were
evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score
and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and
76.2% accuracy). The proposed methodology incorporates data security measures
and HIPAA compliance requirements essential for healthcare environments. The
processed LLM outputs are integrated into a visualization decision support tool
that transforms the staff messages into actionable insights accessible to
healthcare professionals. This approach enables more efficient utilization of
the collected staff messaging data, identifies navigator training
opportunities, and supports improved patient experience and care quality.

</details>


### [9] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 论文探讨了分词效率低下对形态复杂、低资源语言的不利影响，以及这如何影响大型语言模型的准确性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究动机是形态复杂、低资源语言由于分词效率问题，导致计算资源膨胀和准确率下降。

Method: 通过在AfriMMLU数据集上评估10个大型语言模型，分析 fertility（tokens/word）与准确率之间的关系。

Result: 研究结果表明，较高的 fertility 预示着所有模型和科目中较低的准确率。推理模型在各种资源语言中表现优于非推理模型。token 数量翻倍会导致训练成本和时间翻四倍。

Conclusion: 研究结论强调了形态学感知分词、公平定价和多语言基准的重要性，以实现公平的自然语言处理。

Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [10] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

TL;DR: 本文介绍了一种基于检索增强生成（RAG）架构的生物医学文献问答（Q&A）系统，旨在改进对准确的、基于证据的医学信息的访问。


<details>
  <summary>Details</summary>
Motivation: 传统健康搜索引擎存在不足，且公众获取生物医学研究存在滞后性。

Method: 该系统整合了包括PubMed文章、精选的Q&A数据集和医学百科全书在内的多种来源，以检索相关信息并生成简洁、上下文相关的回答。检索流程使用基于MiniLM的语义嵌入和FAISS向量搜索，而答案生成由经过微调的Mistral-7B-v0.3语言模型执行，该模型使用QLoRA进行优化，以实现高效、低资源训练。

Result: 使用BERTScore (F1) 测量的经验结果表明，与基线模型相比，事实一致性和语义相关性有了显著提高。

Conclusion: 研究结果强调了RAG增强语言模型在弥合复杂生物医学文献和可访问的公共健康知识之间的差距的潜力，为未来在多语言适应、保护隐私的推理和个性化医疗AI系统方面的工作铺平了道路。

Abstract: This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [11] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
*Liang Zhang,Yuan Li,Shijie Zhang,Zheng Zhang,Xitong Li*

Main category: cs.CL

TL;DR: 提出了一种新的框架SAID，它以统一的方式整合了文本和关系结构信息，用于模型预训练。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中在文本数据上，忽略了有效捕获会话系统中固有的关键结构信息，例如query-query关系和query-answer关系。

Method: 在SAID框架的基础上，进一步提出了一种新的机制，即query-adaptive attention network (QueryAdapt)，它通过从良好学习的query-query和query-answer关系中显式生成特定于意图的关系tokens，从而在关系token级别上运行，从而实现更细粒度的知识转移。

Result: 在两个真实世界数据集上的大量实验结果表明，SAID明显优于最先进的方法。

Conclusion: 本文提出了一种新的框架，可以有效地提高意图检测的准确性。

Abstract: Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

</details>


### [12] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 大型语言模型是否真正理解概念还是仅仅识别模式？本文提出双向推理来测试真正的理解。发现模型在正向任务上进行微调时，其双向推理能力会显著下降。提出了对比微调（CFT）方法，通过正例、反例和正向模糊示例来训练模型，实现了双向推理。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否真正理解概念，还是仅仅识别模式。

Method: 提出双向推理的概念，并设计对比微调（CFT）方法，包含正例、反例和正向模糊示例。

Result: 发现模型在正向任务上进行微调时，双向推理能力会显著下降。对比微调（CFT）方法能够有效实现双向推理，同时保持正向任务的能力。

Conclusion: 双向推理既可以作为评估真正理解的理论框架，也可以作为开发更强大人工智能系统的实际训练方法。

Abstract: This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [13] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
*Anya Ji,Claire Augusta Bergey,Ron Eliav,Yoav Artzi,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 研究了人们如何谈论他们以前从未谈论过的事情，并检验了两种相互竞争的观点：共享命名系统与更广泛的概念对齐。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨人们在谈论新事物时，是建立任意的链接，还是进行更广泛的概念对齐。

Method: 通过双人交流研究，利用KiloGram数据集，参与者通过重复交流来协调一组图像的指称约定，并测量他们对未讨论图像的描述对齐程度。

Result: 研究发现，伙伴们相对于他们的预测试标签表现出更高的对齐度，且泛化能力随着视觉相似性的增加而非线性衰减。图像的可命名性水平对结果影响不大。

Conclusion: 研究表明，特别的约定不是随意的标签，而是反映了真正的概念协调，对指称理论和更自适应的语言代理的设计具有影响。

Abstract: How do people talk about things they've never talked about before? One view
suggests that a new shared naming system establishes an arbitrary link to a
specific target, like proper names that cannot extend beyond their bearers. An
alternative view proposes that forming a shared way of describing objects
involves broader conceptual alignment, reshaping each individual's semantic
space in ways that should generalize to new referents. We test these competing
accounts in a dyadic communication study (N=302) leveraging the
recently-released KiloGram dataset containing over 1,000 abstract tangram
images. After pairs of participants coordinated on referential conventions for
one set of images through repeated communication, we measured the extent to
which their descriptions aligned for undiscussed images. We found strong
evidence for generalization: partners showed increased alignment relative to
their pre-test labels. Generalization also decayed nonlinearly with visual
similarity (consistent with Shepard's law) and was robust across levels of the
images' nameability. These findings suggest that ad hoc conventions are not
arbitrary labels but reflect genuine conceptual coordination, with implications
for theories of reference and the design of more adaptive language agents.

</details>


### [14] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
*Hongyan Xie,Yitong Yao,Yikun Ban,Zixuan Huang,Deqing Wang,Zhenhe Wu,Haoxiang Su,Chao Wang,Shuangyong Song,Xuelong Li*

Main category: cs.CL

TL;DR: CoPeD通过关注正确的推理链来提高小语言模型(SLM)的推理能力，减少噪声数据的影响。


<details>
  <summary>Details</summary>
Motivation: 小语言模型(SLM)通过模仿大型语言模型(LLM)的CoT数据进行微调，但这些数据包含噪声，导致SLM学习到错误的关联。

Method: 提出了Chain-of-Thought Correctness Perception Distillation (CoPeD)，它从任务设置和数据利用的角度改进SLM的推理质量。引入了正确性感知任务设置，鼓励模型基于正确的理由预测答案，并在不正确时修改答案。提出了正确性感知加权损失，根据理由和答案的组合损失动态调整每个训练实例的贡献。

Result: CoPeD在同分布(IND)和异分布(OOD)基准推理数据集上均有效。

Conclusion: CoPeD有效地提高了SLM的推理能力，并减少了噪声数据的影响。

Abstract: Large language models (LLMs) excel at reasoning tasks but are expensive to
deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated
by LLMs to copy LLMs' abilities. However, these CoT data may include noisy
rationales that either fail to substantiate the answers or contribute no
additional information to support answer prediction, which leads SLMs to
capture spurious correlations between questions and answers and compromise the
quality of reasoning. In this work, we propose Chain-of-Thought Correctness
Perception Distillation (CoPeD), which aims to improve the reasoning quality of
the student model from the perspectives of task setting and data utilization.
Firstly, we introduce a correctness-aware task setting that encourages the
student model to predict answers based on correct rationales and revise them
when they are incorrect. This setting improves the faithfulness of reasoning
and allows the model to learn from its mistakes. Then, we propose a
Correctness-Aware Weighted loss, which dynamically adjusts the contribution of
each training instance based on the combined loss of the rationale and the
answer. This strategy encourages the model to focus more on samples where the
rationale offers stronger support for the correct answer. Experiments have
shown that CoPeD is effective on both in-distribution (IND) and
out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [15] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 提出了一种新的高效偏好数据集构建方法，通过利用大型语言模型的内在调控能力，减少了计算开销，并提高了模型对齐的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的偏好数据集构建方法依赖于预先收集的指令，导致与目标模型存在分布不匹配的问题，并且需要采样多个随机响应，引入了大量的计算开销。

Method: 通过提取层级的方向向量来编码人类偏好，并使用这些向量来过滤自合成的指令，基于其内在一致性。在解码过程中，应用双向内在控制来引导token表示，从而精确生成具有清晰对齐区分的响应对。

Result: 在AlpacaEval 2.0和Arena-Hard上，Llama3-8B和Qwen2-7B的平均胜率分别提高了13.89%和13.45%，同时计算成本降低了高达48.1%。

Conclusion: 该方法在对齐性能和效率方面都取得了显著的改进，为偏好数据集的构建提供了一个新的方向。

Abstract: Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [16] [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)
*Hao Lin,Peitong Xie,Jingxue Chen,Jie Lin,Qingkun Tang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoLER是一种领域感知的RAG方法，它使用MoL增强的强化学习来优化检索。


<details>
  <summary>Details</summary>
Motivation: 现有的粗排序优化方法通常难以平衡领域特定知识学习和查询增强，导致检索性能欠佳。

Method: MoLER包含两个阶段：使用混合损失的持续预训练阶段，以及利用组相对策略优化的强化学习阶段，以优化查询和段落生成，从而最大限度地提高文档召回率。关键创新是多查询单通道后期融合策略，该策略减少了RL训练期间的计算开销，同时通过多查询多通道后期融合保持了可扩展的推理。

Result: 在基准数据集上的大量实验表明，MoLER实现了最先进的性能，显著优于基线方法。

Conclusion: MoLER弥合了RAG系统中的知识差距，从而可以在专业领域中实现强大且可扩展的检索。

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval
stage, particularly the coarse-ranking process. Existing coarse-ranking
optimization approaches often struggle to balance domain-specific knowledge
learning with query enhencement, resulting in suboptimal retrieval performance.
To address this challenge, we propose MoLER, a domain-aware RAG method that
uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a
two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of
Losses (MoL) to balance domain-specific knowledge with general language
capabilities, and a reinforcement learning (RL) phase leveraging Group Relative
Policy Optimization (GRPO) to optimize query and passage generation for
maximizing document recall. A key innovation is our Multi-query Single-passage
Late Fusion (MSLF) strategy, which reduces computational overhead during RL
training while maintaining scalable inference via Multi-query Multi-passage
Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER
achieves state-of-the-art performance, significantly outperforming baseline
methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and
scalable retrieval in specialized domains.

</details>


### [17] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
*Qiyuan Chen,Jiahe Chen,Hongsen Huang,Qian Shao,Jintai Chen,Renjie Hua,Hongxia Xu,Ruijia Wu,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 本研究提出了一个用于生成式搜索引擎优化（GSEO）的综合框架。


<details>
  <summary>Details</summary>
Motivation: 传统的基于排名的搜索到生成式搜索引擎的范式转变使得传统的SEO指标过时，迫切需要理解、测量和优化内容对合成答案的影响。

Method: 构建了一个大规模的、以内容为中心的基准CC-GSEO-Bench，并提出了一个多维评估框架，该框架系统地量化影响，超越了表面层次的归因，以评估实质性的语义影响。设计了一种新颖的多智能体系统，该系统通过协作分析-修改-评估工作流程来自动化内容的战略改进。

Result: 通过使用该框架的实证分析，揭示了对内容影响动态的新颖见解，为创作者提供了可操作的策略，并为未来的GSEO研究建立了原则性基础。

Conclusion: 为生成式搜索引擎优化 (GSEO) 建立了一个全面的端到端框架

Abstract: The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

</details>


### [18] [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)
*Joe Wilder,Nikhil Kadapala,Benji Xu,Mohammed Alsaadi,Aiden Parsons,Mitchell Rogers,Palash Agarwal,Adam Hassick,Laura Dietz*

Main category: cs.CL

TL;DR: 本研究参与了CheckThat! Task 2英语任务，旨在从社交媒体文本中提取值得核实的声明。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索不同的提示方法和上下文学习技术，以提高声明提取的准确性。

Method: 研究采用了包括少样本提示和使用不同LLM家族进行微调等方法。

Result: 研究的最佳METEOR得分是通过微调FLAN-T5模型实现的。然而，其他方法有时可以提取更高质量的声明，即使它们的METEOR得分较低。

Conclusion: 结论是微调FLAN-T5模型在METEOR指标上表现最佳，但其他方法在提取高质量声明方面可能更有效。

Abstract: We participate in CheckThat! Task 2 English and explore various methods of
prompting and in-context learning, including few-shot prompting and fine-tuning
with different LLM families, with the goal of extracting check-worthy claims
from social media passages. Our best METEOR score is achieved by fine-tuning a
FLAN-T5 model. However, we observe that higher-quality claims can sometimes be
extracted using other methods, even when their METEOR scores are lower.

</details>


### [19] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

TL;DR: 该论文提出了一种新的视角，将声学和语言表示的对齐和匹配视为一个检测问题，目标是以高精度和召回率识别有意义的对应关系，确保语言标记的完全覆盖，同时灵活处理冗余或嘈杂的声学帧，以便为自动语音识别（ASR）转移语言知识。


<details>
  <summary>Details</summary>
Motivation: 弥合自动语音识别（ASR）中知识转移的预训练模型，声学和语言表示的对齐是一个核心挑战。这种对齐本质上是结构化的和非对称的：虽然多个连续的声学帧通常对应于单个语言标记（多对一），但某些声学过渡区域可能与多个相邻标记相关（一对多）。此外，声学序列通常包括没有语言对应部分的帧，例如背景噪声或静音可能导致不平衡的匹配条件。

Method: 该论文提出了一种基于不平衡最优传输的对齐模型，该模型通过声学和语言模态之间的软匹配和部分匹配，显式地处理分布不匹配和结构不对称性。该方法确保每个语言标记都以至少一个声学观察为基础，同时允许从声学单元到语言单元的灵活的概率映射。

Result: 在基于CTC的ASR系统上，利用预训练的语言模型进行知识转移的实验结果表明，该方法在灵活控制匹配程度从而提高ASR性能方面是有效的。

Conclusion: 该论文提出了一种新的对齐方法，通过将对齐视为检测问题，并采用不平衡最优传输来处理声学和语言模态之间的不匹配和不对称性，从而有效地提高了ASR性能。

Abstract: Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


### [20] [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)
*Marc Marone,Orion Weller,William Fleshman,Eugene Yang,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 介绍了mmBERT，一个在超过1800种语言的3T tokens多语文本上预训练的仅编码器语言模型。


<details>
  <summary>Details</summary>
Motivation: 缺乏对编码器模型的最新研究，尤其是在多语模型方面。

Method: 引入了几种新颖的元素，包括逆掩码比率调度和逆温度采样比率。在衰减阶段向数据组合中添加了超过1700种低资源语言。

Result: 在分类性能上与OpenAI的o3和Google的Gemini 2.5 Pro等模型相似。在分类和检索任务上，mmBERT显著优于前一代模型，无论是在高资源语言还是低资源语言上。

Conclusion: mmBERT在多语种分类和检索任务上表现出色，尤其是在低资源语言方面。

Abstract: Encoder-only languages models are frequently used for a variety of standard
machine learning tasks, including classification and retrieval. However, there
has been a lack of recent research for encoder models, especially with respect
to multilingual models. We introduce mmBERT, an encoder-only language model
pretrained on 3T tokens of multilingual text in over 1800 languages. To build
mmBERT we introduce several novel elements, including an inverse mask ratio
schedule and an inverse temperature sampling ratio. We add over 1700
low-resource languages to the data mix only during the decay phase, showing
that it boosts performance dramatically and maximizes the gains from the
relatively small amount of training data. Despite only including these
low-resource languages in the short decay phase we achieve similar
classification performance to models like OpenAI's o3 and Google's Gemini 2.5
Pro. Overall, we show that mmBERT significantly outperforms the previous
generation of models on classification and retrieval tasks -- on both high and
low-resource languages.

</details>


### [21] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
*Shay Dahary,Avi Edana,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 这篇论文研究了歌曲歌词的情感归属问题，通过预测六种基本情感的强度评分来进行多标签情感分类。


<details>
  <summary>Details</summary>
Motivation: 研究歌词情感内容对听众体验和音乐偏好的影响。

Method: 1. 构建了一个人工标注的数据集，使用平均意见得分（MOS）方法聚合多个评分者的注释。2. 在零样本场景下，评估了几个公开的大型语言模型（LLMs）。3. 针对预测多标签情感评分，对基于BERT的模型进行了微调。

Result: 实验结果揭示了零样本模型和微调模型在捕捉歌词细微情感内容方面的相对优势和局限性。

Conclusion: 研究结果强调了LLMs在创意文本情感识别方面的潜力，并为基于情感的音乐信息检索应用提供了模型选择策略的见解。

Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [22] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
*Yuxuan Hu,Jihao Liu,Ke Wang,Jinliang Zhen,Weikang Shi,Manyuan Zhang,Qi Dou,Rui Liu,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为 LM-Searcher 的新框架，该框架利用大型语言模型 (LLM) 进行跨域神经架构优化，无需进行广泛的领域特定调整。


<details>
  <summary>Details</summary>
Motivation: 现有基于 LLM 的 NAS 方法严重依赖提示工程和领域特定调整，限制了它们在不同任务中的实用性和可扩展性。

Method: 该方法的核心是 NCode，一种用于神经架构的通用数字字符串表示，它支持跨域架构编码和搜索。还将 NAS 问题重新定义为排序任务，训练 LLM 从候选池中选择高性能架构。

Result: LM-Searcher 在同域（例如，用于图像分类的 CNN）和异域（例如，用于分割和生成的 LoRA 配置）任务中都取得了具有竞争力的性能。

Conclusion: 该研究为灵活且可泛化的基于 LLM 的架构搜索建立了一个新范例。

Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [23] [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)
*Hong Su*

Main category: cs.CL

TL;DR: 本文扩展了大型语言模型（LLM）在问题解答中方法重用的范围，以解决相似度低或具有隐藏相似性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要求问题高度相似，限制了方法重用的范围。

Method: 1. 将问题和解决方案分离，引导LLM将解决方案适配到新的相关问题；2. 扩展到问题仅共享部分特征或隐藏特征的情况。

Result: 实验验证表明，该方法提高了过滤可重用解决方案的概率，从而提高了跨问题方法重用的有效性。

Conclusion: 该研究成功地扩展了LLM方法重用的范围，使其能够处理更广泛的问题类型。

Abstract: Large language models (LLMs) have been widely applied to assist in finding
solutions for diverse questions. Prior work has proposed representing a method
as a pair of a question and its corresponding solution, enabling method reuse.
However, existing approaches typically require the questions to be highly
similar. In this paper, we extend the scope of method reuse to address
questions with low similarity or with hidden similarities that are not
explicitly observable. For questions that are similar in a general-specific
sense (i.e., broader or narrower in scope), we propose to first separate the
question and solution, rather than directly feeding the pair to the LLM. The
LLM is then guided to adapt the solution to new but related questions, allowing
it to focus on solution transfer rather than question recognition. Furthermore,
we extend this approach to cases where questions only share partial features or
hidden characteristics. This enables cross-question method reuse beyond
conventional similarity constraints. Experimental verification shows that our
scope-extension approach increases the probability of filtering out reusable
solutions, thereby improving the effectiveness of cross-question method reuse.

</details>


### [24] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
*Michael Hoffmann,Jophin John,Stefan Schweter,Gokul Ramakrishnan,Hoi-Fong Mak,Alice Zhang,Dmitry Gaynullin,Nicolay J. Hammer*

Main category: cs.CL

TL;DR: Llama-GENBA-10B is a trilingual (English, German, Bavarian) language model built on Llama 3.1-8B and scaled to 10B parameters. It aims to address English-centric bias and support low-resource languages like Bavarian.


<details>
  <summary>Details</summary>
Motivation: Addressing English-centric bias in large language models and promoting low-resource languages like Bavarian.

Method: Continuous pretraining on 164B tokens (balanced across English, German, and Bavarian), unified tokenizer, architecture and language-ratio optimization for cross-lingual transfer, and a standardized trilingual evaluation suite.

Result: Achieves strong cross-lingual performance, surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and outperforming EuroLLM in English, while matching its results in German.

Conclusion: Demonstrates efficient large-scale multilingual pretraining and offers a blueprint for inclusive foundation models that integrate low-resource languages.

Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing
English-centric bias in large language models. Built on Llama 3.1-8B and scaled
to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens
(82B English, 82B German, and 80M Bavarian), balancing resources while
preventing English dominance. Targeted at the German NLP community, the model
also promotes Bavarian as a low-resource language. Development tackled four
challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)
creating a unified tokenizer for English, German, and Bavarian, (3) optimizing
architecture and language-ratio hyperparameters for cross-lingual transfer, and
(4) establishing the first standardized trilingual evaluation suite by
translating German benchmarks into Bavarian. Evaluations show that
Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned
variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing
itself as the best model in its class for this language, while also
outperforming EuroLLM in English and matching its results in German. Training
on the Cerebras CS-2 demonstrated efficient large-scale multilingual
pretraining with documented energy use, offering a blueprint for inclusive
foundation models that integrate low-resource languages.

</details>


### [25] [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)
*Ningyuan Deng,Hanyu Duan,Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 研究了文本嵌入模型是否能准确编码数字信息，发现它们通常难以捕捉数字细节。


<details>
  <summary>Details</summary>
Motivation: 当前的文本嵌入模型在需要理解细微数字信息的任务上的能力尚不清楚，这对于金融和医疗等领域至关重要。

Method: 使用金融领域的合成数据，评估了13个广泛使用的文本嵌入模型。

Result: 发现这些模型通常难以准确捕捉数字细节。

Conclusion: 该研究深入分析了嵌入的数值能力，为未来的研究提供了信息，以增强基于嵌入模型的自然语言处理系统处理数值内容的能力。

Abstract: Text embedding models are widely used in natural language processing
applications. However, their capability is often benchmarked on tasks that do
not require understanding nuanced numerical information in text. As a result,
it remains unclear whether current embedding models can precisely encode
numerical content, such as numbers, into embeddings. This question is critical
because embedding models are increasingly applied in domains where numbers
matter, such as finance and healthcare. For example, Company X's market share
grew by 2\% should be interpreted very differently from Company X's market
share grew by 20\%, even though both indicate growth in market share. This
study aims to examine whether text embedding models can capture such nuances.
Using synthetic data in a financial context, we evaluate 13 widely used text
embedding models and find that they generally struggle to capture numerical
details accurately. Our further analyses provide deeper insights into embedding
numeracy, informing future research to strengthen embedding model-based NLP
systems with improved capacity for handling numerical content.

</details>


### [26] [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Fahmida Islam,Maryam Tahermazandarani,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文全面概述了会话式问答 (ConvQA) 领域的现状，为未来发展方向提供了宝贵的见解。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理 (NLP) 领域，会话式问答 (ConvQA) 系统已成为一个关键领域，它推动了机器进行动态和上下文感知的对话。

Method: 本文考察了 ConvQA 系统的核心组成部分，即历史选择、问题理解和答案预测，强调了它们在确保多轮对话的连贯性和相关性方面的相互作用。它还进一步研究了先进的机器学习技术（包括但不限于强化学习、对比学习和迁移学习）的使用，以提高 ConvQA 的准确性和效率。探讨了大型语言模型（即 RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B 和 LLaMA 3）的关键作用，从而通过数据可扩展性和架构进步展示了它们的影响。

Result: 本文全面分析了关键的 ConvQA 数据集。

Conclusion: 本文概述了 ConvQA 领域，并为指导该领域未来的发展方向提供了宝贵的见解。

Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal
area within Natural Language Processing (NLP) by driving advancements that
enable machines to engage in dynamic and context-aware conversations. These
capabilities are increasingly being applied across various domains, i.e.,
customer support, education, legal, and healthcare where maintaining a coherent
and relevant conversation is essential. Building on recent advancements, this
survey provides a comprehensive analysis of the state-of-the-art in ConvQA.
This survey begins by examining the core components of ConvQA systems, i.e.,
history selection, question understanding, and answer prediction, highlighting
their interplay in ensuring coherence and relevance in multi-turn
conversations. It further investigates the use of advanced machine learning
techniques, including but not limited to, reinforcement learning, contrastive
learning, and transfer learning to improve ConvQA accuracy and efficiency. The
pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,
Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact
through data scalability and architectural advancements. Additionally, this
survey presents a comprehensive analysis of key ConvQA datasets and concludes
by outlining open research directions. Overall, this work offers a
comprehensive overview of the ConvQA landscape and provides valuable insights
to guide future advancements in the field.

</details>


### [27] [Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models](https://arxiv.org/abs/2509.05719)
*Donya Rooein,Flor Miriam Plaza-del-Arco,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: Farsi is considered a middle-resource language, but this label is misleading due to data availability and quality issues.


<details>
  <summary>Details</summary>
Motivation: The study focuses on the challenges in data availability and quality for subjective tasks in Farsi, despite the increasing amount of digital text.

Method: The authors reviewed 110 publications on subjective tasks in Farsi and evaluated prediction models using available datasets.

Result: The review reveals a lack of publicly available datasets and essential demographic factors in existing datasets. The evaluation of prediction models shows highly unstable results.

Conclusion: The volume of data is insufficient to significantly improve Farsi's prospects in NLP, highlighting the need for better data resources.

Abstract: Given Farsi's speaker base of over 127 million people and the growing
availability of digital text, including more than 1.3 million articles on
Wikipedia, it is considered a middle-resource language. However, this label
quickly crumbles when the situation is examined more closely. We focus on three
subjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)
and find significant challenges in data availability and quality, despite the
overall increase in data availability. We review 110 publications on subjective
tasks in Farsi and observe a lack of publicly available datasets. Furthermore,
existing datasets often lack essential demographic factors, such as age and
gender, that are crucial for accurately modeling subjectivity in language. When
evaluating prediction models using the few available datasets, the results are
highly unstable across both datasets and models. Our findings indicate that the
volume of data is insufficient to significantly improve a language's prospects
in NLP.

</details>


### [28] [QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing](https://arxiv.org/abs/2509.05729)
*Charles M. Varmantchaonala,Niclas GÖtting,Nils-Erik SchÜtte,Jean Louis E. K. Fendji,Christopher Gies*

Main category: cs.CL

TL;DR: 本研究提出了一种名为QCSE的预训练量子上下文敏感嵌入模型，该模型利用量子系统的独特性质来学习语言中的上下文关系。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的能力，为自然语言处理提供了一种新颖的方法，旨在编码和理解自然语言的复杂性。

Method: 论文提出了五种不同的方法来计算上下文矩阵，包括指数衰减、正弦调制、相移和基于哈希的转换。

Result: 在Fulani语料库和英语语料库上的评估结果表明，QCSE不仅能捕获上下文敏感性，还能利用量子系统的表达性来表示丰富的上下文感知语言信息。

Conclusion: 这项工作强调了量子计算在自然语言处理中的强大功能，并为将QNLP应用于各种任务和领域的实际语言挑战开辟了新途径。

Abstract: Quantum Natural Language Processing (QNLP) offers a novel approach to
encoding and understanding the complexity of natural languages through the
power of quantum computation. This paper presents a pretrained quantum
context-sensitive embedding model, called QCSE, that captures context-sensitive
word embeddings, leveraging the unique properties of quantum systems to learn
contextual relationships in languages. The model introduces quantum-native
context learning, enabling the utilization of quantum computers for linguistic
tasks. Central to the proposed approach are innovative context matrix
computation methods, designed to create unique, representations of words based
on their surrounding linguistic context. Five distinct methods are proposed and
tested for computing the context matrices, incorporating techniques such as
exponential decay, sinusoidal modulation, phase shifts, and hash-based
transformations. These methods ensure that the quantum embeddings retain
context sensitivity, thereby making them suitable for downstream language tasks
where the expressibility and properties of quantum systems are valuable
resources. To evaluate the effectiveness of the model and the associated
context matrix methods, evaluations are conducted on both a Fulani corpus, a
low-resource African language, dataset of small size and an English corpus of
slightly larger size. The results demonstrate that QCSE not only captures
context sensitivity but also leverages the expressibility of quantum systems
for representing rich, context-aware language information. The use of Fulani
further highlights the potential of QNLP to mitigate the problem of lack of
data for this category of languages. This work underscores the power of quantum
computation in natural language processing (NLP) and opens new avenues for
applying QNLP to real-world linguistic challenges across various tasks and
domains.

</details>


### [29] [Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification](https://arxiv.org/abs/2509.05741)
*Fernando Gabriela García,Qiyang Shi,Zilin Feng*

Main category: cs.CL

TL;DR: VeriFact-CoT通过多阶段机制解决大型语言模型在生成复杂内容时出现的幻觉和缺乏可信来源的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在生成复杂、对事实敏感的内容时，普遍存在的幻觉和缺乏可信引用的问题。

Method: 引入了一种多阶段机制，包括“事实验证-反思-引用整合”，使LLM能够批判性地自查和修改中间推理步骤和最终答案。

Result: 显著提高了生成输出的客观准确性、可信度和可追溯性。

Conclusion: VeriFact-CoT使LLM在需要高保真度的应用中更加可靠，如科学研究、新闻报道和法律咨询。

Abstract: This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a
novel method designed to address the pervasive issues of hallucination and the
absence of credible citation sources in Large Language Models (LLMs) when
generating complex, fact-sensitive content. By incorporating a multi-stage
mechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT
empowers LLMs to critically self-examine and revise their intermediate
reasoning steps and final answers. This process significantly enhances the
objective accuracy, trustworthiness, and traceability of the generated outputs,
making LLMs more reliable for applications demanding high fidelity such as
scientific research, news reporting, and legal consultation.

</details>


### [30] [LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization](https://arxiv.org/abs/2509.05863)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: LatinX是一个多语种TTS模型，用于级联语音到语音翻译，可在不同语言中保留源说话者的身份。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够跨语言保持说话人音色的语音合成模型。

Method: 该模型是一个12层decoder-only Transformer，经过三个阶段的训练：文本到音频的映射预训练、zero-shot语音克隆的监督微调，以及使用基于词错率（WER）和说话人相似度指标自动标记的配对进行的直接偏好优化（DPO）对齐。

Result: LatinX与DPO相比于微调的基线，持续降低了WER并提高了客观相似度。人工评估表明，与强大的基线（XTTSv2）相比，感知到的说话人相似度更强。

Conclusion: LatinX模型在跨语言语音合成和说话人音色保持方面表现出色，但客观指标和主观感受之间存在差距，未来工作将关注平衡偏好信号和降低延迟的架构。

Abstract: We present LatinX, a multilingual text-to-speech (TTS) model for cascaded
speech-to-speech translation that preserves the source speaker's identity
across languages. LatinX is a 12-layer decoder-only Transformer trained in
three stages: (i) pre-training for text-to-audio mapping, (ii) supervised
fine-tuning for zero-shot voice cloning, and (iii) alignment with Direct
Preference Optimization (DPO) using automatically labeled pairs based on Word
Error Rate (WER) and speaker-similarity metrics. Trained on English and Romance
languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER
and improves objective similarity over the fine-tuned baseline. Human
evaluations further indicate stronger perceived speaker similarity than a
strong baseline (XTTSv2), revealing gaps between objective and subjective
measures. We provide cross-lingual analyses and discuss balanced preference
signals and lower-latency architectures as future work.

</details>


### [31] [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)
*ZiXuan Zhang,Bowen Hao,Yingjie Li,Hongzhi Yin*

Main category: cs.CL

TL;DR: 本研究提出了一种名为ZhiFangDanTai的框架，该框架结合了基于图的检索增强生成（GraphRAG）和LLM微调，以解决现有中药方剂分析模型的不足，例如缺乏完整的方剂组成和详细解释。


<details>
  <summary>Details</summary>
Motivation: 现有中药方剂分析模型缺乏全面的结果，例如完整的方剂组成和详细的解释。现有的数据集缺乏足够的细节，例如方剂的君、臣、佐、使的作用；功效；禁忌症；舌象和脉象诊断，限制了模型输出的深度。

Method: 该研究提出ZhiFangDanTai，一个结合了基于图的检索增强生成（GraphRAG）与LLM微调的框架。ZhiFangDanTai使用GraphRAG来检索和综合结构化的中医药知识到简洁的摘要中，同时构建一个增强的指令数据集来提高LLM整合检索信息的能力。

Result: 在收集的和临床数据集上的实验结果表明，ZhiFangDanTai 比state-of-the-art模型取得了显著的改进。

Conclusion: 该研究证明了结合GraphRAG与微调技术可以减少中药方剂任务中的泛化误差和幻觉率。

Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in
treating epidemics and complex diseases. Existing models for TCM utilize
traditional algorithms or deep learning techniques to analyze formula
relationships, yet lack comprehensive results, such as complete formula
compositions and detailed explanations. Although recent efforts have used TCM
instruction datasets to fine-tune Large Language Models (LLMs) for explainable
formula generation, existing datasets lack sufficient details, such as the
roles of the formula's sovereign, minister, assistant, courier; efficacy;
contraindications; tongue and pulse diagnosis-limiting the depth of model
outputs. To address these challenges, we propose ZhiFangDanTai, a framework
combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM
fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured
TCM knowledge into concise summaries, while also constructing an enhanced
instruction dataset to improve LLMs' ability to integrate retrieved
information. Furthermore, we provide novel theoretical proofs demonstrating
that integrating GraphRAG with fine-tuning techniques can reduce generalization
error and hallucination rates in the TCM formula task. Experimental results on
both collected and clinical datasets demonstrate that ZhiFangDanTai achieves
significant improvements over state-of-the-art models. Our model is
open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

</details>


### [32] [MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries](https://arxiv.org/abs/2509.05878)
*François Grolleau,Emily Alsentzer,Timothy Keyes,Philip Chung,Akshay Swaminathan,Asad Aali,Jason Hom,Tridu Huynh,Thomas Lew,April S. Liang,Weihan Chu,Natasha Z. Steele,Christina F. Lin,Jingkun Yang,Kameron C. Black,Stephen P. Ma,Fateme N. Haredasht,Nigam H. Shah,Kevin Schulman,Jonathan H. Chen*

Main category: cs.CL

TL;DR: 本文提出了一种评估LLM生成临床文本事实准确性的框架MedFactEval和一个生成高质量、基于事实的出院总结的工作流程MedAgentBrief。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）生成的临床文本的事实准确性是应用的关键障碍，因为专家审查对于这些系统所需的持续质量保证来说是不可扩展的。

Method: 1. 引入MedFactEval框架，用于可扩展的、基于事实的评估，其中临床医生定义高显著性的关键事实，并由一个“LLM陪审团”（一个多LLM多数投票）评估这些事实是否包含在生成的总结中。
2. 提出了MedAgentBrief，一个模型无关的、多步骤的工作流程，旨在生成高质量、基于事实的出院总结。

Result: MedFactEval LLM陪审团与一个七名医生多数投票建立的金标准参考几乎完全一致（Cohen's kappa=81%），其性能在统计学上不亚于单个人类专家（kappa=67%，P < 0.001）。

Conclusion: 这项工作提供了一个强大的评估框架（MedFactEval）和一个高性能的生成工作流程（MedAgentBrief），为推进生成式人工智能在临床工作流程中的负责任部署提供了一个全面的方法。

Abstract: Evaluating factual accuracy in Large Language Model (LLM)-generated clinical
text is a critical barrier to adoption, as expert review is unscalable for the
continuous quality assurance these systems require. We address this challenge
with two complementary contributions. First, we introduce MedFactEval, a
framework for scalable, fact-grounded evaluation where clinicians define
high-salience key facts and an "LLM Jury"--a multi-LLM majority vote--assesses
their inclusion in generated summaries. Second, we present MedAgentBrief, a
model-agnostic, multi-step workflow designed to generate high-quality, factual
discharge summaries. To validate our evaluation framework, we established a
gold-standard reference using a seven-physician majority vote on
clinician-defined key facts from inpatient cases. The MedFactEval LLM Jury
achieved almost perfect agreement with this panel (Cohen's kappa=81%), a
performance statistically non-inferior to that of a single human expert
(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework
(MedFactEval) and a high-performing generation workflow (MedAgentBrief),
offering a comprehensive approach to advance the responsible deployment of
generative AI in clinical workflows.

</details>


### [33] [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)
*Abhijnan Nath,Carine Graff,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在多轮、多方协作中作为伙伴的有效性，特别关注了不同对齐方法的影响。


<details>
  <summary>Details</summary>
Motivation: 在多人协作的复杂环境中，常见对齐技术在简化单用户设置下开发，无法保证LLM行为的可预测性、验证性和可靠性。本文旨在研究不同对齐方法对LLM作为协作伙伴的影响。

Method: 通过角色扮演方法，评估不同训练的摩擦代理在协作任务对话中的干预效果。提出了一个新的反事实评估框架，用于量化摩擦干预如何改变群体协作和信念对齐的轨迹。

Result: 研究结果表明，在帮助达成共识和提高任务结果的正确性方面，具有摩擦意识的方法明显优于常见的对齐基线。

Conclusion: 摩擦意识方法能够显著提升LLM在多轮多方协作中作为伙伴的有效性，有助于群体达成共识并提高任务结果的正确性。

Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are
increasingly being considered "collaborators" with humans. If such AI
collaborators are to be reliable, their behavior over multiturn interactions
must be predictable, validated and verified before deployment. Common alignment
techniques are typically developed under simplified single-user settings and do
not account for the dynamics of long-horizon multiparty interactions. This
paper examines how different alignment methods affect LLM agents' effectiveness
as partners in multiturn, multiparty collaborations. We study this question
through the lens of friction agents that intervene in group dialogues to
encourage the collaborative group to slow down and reflect upon their reasoning
for deliberative decision-making. Using a roleplay methodology, we evaluate
interventions from differently-trained friction agents in collaborative task
conversations. We propose a novel counterfactual evaluation framework that
quantifies how friction interventions change the trajectory of group
collaboration and belief alignment. Our results show that a friction-aware
approach significantly outperforms common alignment baselines in helping both
convergence to a common ground, or agreed-upon task-relevant propositions, and
correctness of task outcomes.

</details>


### [34] [Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling](https://arxiv.org/abs/2509.05908)
*Yue Gu,Zhihao Du,Ying Shi,Shiliang Zhang,Qian Chen,Jiqing Han*

Main category: cs.CL

TL;DR: 提出了一种名为PSC-Joint的方法，用于解决上下文ASR中biasing信息量变化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的基于交叉注意力的上下文ASR模型在识别个性化biasing短语方面取得了显著进展，但其有效性受到biasing信息量变化的影响，尤其是在biasing列表长度显著增加时。

Method: 定义并计算ASR中间表示和biasing信息之间从粗到细的三个语义相关性：列表级别、短语级别和token级别。然后，联合建模这三个相关性以产生它们的交集，从而突出显示和整合跨各种粒度的最相关的biasing信息以进行上下文识别。此外，为了降低三个语义相关性联合建模带来的计算成本，还提出了一种基于分组竞争策略的净化机制，以过滤掉不相关的biasing短语。

Result: 在AISHELL-1上实现了高达21.34%的平均相对F1分数提升，在KeSpeech上实现了28.46%的提升。

Conclusion: PSC-Joint方法能够有效地提高上下文ASR在biasing信息量变化下的性能。

Abstract: Recently, cross-attention-based contextual automatic speech recognition (ASR)
models have made notable advancements in recognizing personalized biasing
phrases. However, the effectiveness of cross-attention is affected by
variations in biasing information volume, especially when the length of the
biasing list increases significantly. We find that, regardless of the length of
the biasing list, only a limited amount of biasing information is most relevant
to a specific ASR intermediate representation. Therefore, by identifying and
integrating the most relevant biasing information rather than the entire
biasing list, we can alleviate the effects of variations in biasing information
volume for contextual ASR. To this end, we propose a purified semantic
correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and
calculate three semantic correlations between the ASR intermediate
representations and biasing information from coarse to fine: list-level,
phrase-level, and token-level. Then, the three correlations are jointly modeled
to produce their intersection, so that the most relevant biasing information
across various granularities is highlighted and integrated for contextual
recognition. In addition, to reduce the computational cost introduced by the
joint modeling of three semantic correlations, we also propose a purification
mechanism based on a grouped-and-competitive strategy to filter out irrelevant
biasing phrases. Compared with baselines, our PSC-Joint approach achieves
average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%
on KeSpeech, across biasing lists of varying lengths.

</details>


### [35] [Accelerating Large Language Model Inference via Early-Exiting Algorithms](https://arxiv.org/abs/2509.05915)
*Sangmin Bae*

Main category: cs.CL

TL;DR: 这篇论文提出了一种通过协同设计自适应算法和模型架构来解决大语言模型部署中计算成本高的问题的方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算成本高，自适应计算方法（如提前退出）虽然可以降低成本，但会引入系统瓶颈，反而降低批量推理的吞吐量。

Method: 1. 提出一种高效的并行解码机制，解决传统提前退出中的关键开销来源。2. 提出深度参数共享架构，减少模型参数，缓解动态推理中的同步问题。3. 提出一个统一的框架，预训练轻量级路由器，为每个token动态分配最佳递归深度。

Result: 建立了一个新的效率和性能之间的帕累托前沿。

Conclusion: 通过在单个模型中有效地优化自适应计算和参数效率，实现了效率和性能的提升。

Abstract: Large language models have achieved remarkable capabilities, but their
practical deployment is hindered by significant computational costs. While
adaptive computation methods like early-exiting promise to reduce these costs,
they introduce a fundamental conflict: the per-token dynamism intended to save
computation often creates system-level bottlenecks that can paradoxically
reduce throughput in batched inference. This dissertation resolves this
conflict by co-designing adaptive algorithms and model architectures to strike
an optimal balance between dynamism and efficiency. To this end, our work first
addresses critical sources of overhead in conventional early-exiting by
proposing an efficient parallel decoding mechanism. We then show that deep
parameter sharing provides an architectural foundation that not only yields
compact, parameter-efficient models but also inherently mitigates the critical
synchronization issues affecting dynamic inference. Finally, this work presents
a unified framework where lightweight routers are pretrained to dynamically
assign an optimal recursion depth for each token. This approach establishes a
new Pareto frontier between efficiency and performance by effectively
optimizing for both adaptive computation and parameter efficiency within a
single model.

</details>


### [36] [KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino](https://arxiv.org/abs/2509.06065)
*Lorenzo Alfred Nery,Ronald Dawson Catignas,Thomas James Tiam-Lee*

Main category: cs.CL

TL;DR: 本研究提出了KatotohananQA，一个TruthfulQA基准的菲律宾语翻译版本，用于评估大型语言模型在低资源语言中的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM真实性的基准主要为英语，缺乏对低资源语言的评估。本研究旨在填补这一空白。

Method: 使用二元选择框架评估了七个免费专有模型在KatotohananQA上的表现。

Result: 研究发现英语和菲律宾语的真实性之间存在显著的性能差距，OpenAI的GPT-5和GPT-5 mini模型表现出较强的多语言鲁棒性。不同问题类型、类别和主题的表现也存在差异。

Conclusion: 研究结果表明，某些问题类型、类别和主题的多语言迁移能力较差，强调需要更广泛的多语言评估，以确保LLM使用的公平性和可靠性。

Abstract: Large Language Models (LLMs) achieve remarkable performance across various
tasks, but their tendency to produce hallucinations limits reliable adoption.
Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet
they are primarily available in English, leaving a gap in evaluating LLMs in
low-resource languages. To address this, we present KatotohananQA, a Filipino
translation of the TruthfulQA benchmark. Seven free-tier proprietary models
were assessed using a binary-choice framework. Findings show a significant
performance gap between English and Filipino truthfulness, with newer OpenAI
models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.
Results also reveal disparities across question characteristics, suggesting
that some question types, categories, and topics are less robust to
multilingual transfer which highlight the need for broader multilingual
evaluation to ensure fairness and reliability in LLM usage.

</details>


### [37] [Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis](https://arxiv.org/abs/2509.06074)
*Zhenqi Jia,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于多模态细粒度上下文交互图的CSS系统(MFCIG-CSS)，用于生成具有自然韵律的语音。


<details>
  <summary>Details</summary>
Motivation: 现有的对话语音合成(CSS)方法忽略了细粒度的语义和韵律交互建模。

Method: 构建了语义交互图和韵律交互图，有效地编码了词级的语义、韵律以及它们在MDH中对后续话语的影响。

Result: 在DailyTalk数据集上的实验表明，MFCIG-CSS在韵律表现力方面优于所有基线模型。

Conclusion: MFCIG-CSS能够提升合成语音的自然对话韵律。

Abstract: Conversational Speech Synthesis (CSS) aims to generate speech with natural
prosody by understanding the multimodal dialogue history (MDH). The latest work
predicts the accurate prosody expression of the target utterance by modeling
the utterance-level interaction characteristics of MDH and the target
utterance. However, MDH contains fine-grained semantic and prosody knowledge at
the word level. Existing methods overlook the fine-grained semantic and
prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a
novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our
approach constructs two specialized multimodal fine-grained dialogue
interaction graphs: a semantic interaction graph and a prosody interaction
graph. These two interaction graphs effectively encode interactions between
word-level semantics, prosody, and their influence on subsequent utterances in
MDH. The encoded interaction features are then leveraged to enhance synthesized
speech with natural conversational prosody. Experiments on the DailyTalk
dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of
prosodic expressiveness. Code and speech samples are available at
https://github.com/AI-S2-Lab/MFCIG-CSS.

</details>


### [38] [Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge](https://arxiv.org/abs/2509.06079)
*Hao Liang,Ruitao Wu,Bohan Zeng,Junbo Niu,Wentao Zhang,Bin Dong*

Main category: cs.CL

TL;DR: 提出了一种新的多模态推理框架，通过caption辅助来连接视觉和文本模态，在ICML 2025 AI for Math Workshop & Challenge 2中获得第一名，并在MathVerse基准测试中验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本推理模型在多模态场景中表现不佳。

Method: 引入caption辅助推理框架，有效桥接视觉和文本模态。

Result: 在ICML 2025 AI for Math Workshop & Challenge 2: SeePhys中获得第一名，并在MathVerse基准测试中验证了其泛化能力。

Conclusion: 该方法有效且具有鲁棒性和通用性。

Abstract: Multimodal reasoning remains a fundamental challenge in artificial
intelligence. Despite substantial advances in text-based reasoning, even
state-of-the-art models such as GPT-o3 struggle to maintain strong performance
in multimodal scenarios. To address this gap, we introduce a caption-assisted
reasoning framework that effectively bridges visual and textual modalities. Our
approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge
2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we
validate its generalization on the MathVerse benchmark for geometric reasoning,
demonstrating the versatility of our method. Our code is publicly available at
https://github.com/OpenDCAI/SciReasoner.

</details>


### [39] [Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models](https://arxiv.org/abs/2509.06100)
*Kefan Cao,Shuaicheng Wu*

Main category: cs.CL

TL;DR: 提出了一种名为OLieRA的新方法，用于解决大型语言模型在顺序多任务设置中容易出现的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统增量微调会破坏LLM参数的内在几何结构，限制性能。

Method: 将李群理论引入LLM微调，利用乘法更新来保持参数几何结构，同时对任务子空间应用正交约束。

Result: 在Standard CL基准测试中取得了最先进的结果，并且在大量任务设置中仍然是表现最佳的方法之一。

Conclusion: OLieRA方法有效地解决了灾难性遗忘问题，并在多个任务设置中表现出色。

Abstract: Large language models (LLMs) are prone to catastrophic forgetting in
sequential multi-task settings. Parameter regularization methods such as O-LoRA
and N-LoRA alleviate task interference by enforcing low-rank subspace
orthogonality, but they overlook the fact that conventional additive
fine-tuning disrupts the intrinsic geometric structure of LLM parameters,
limiting performance. Our key insight is that the parameter space of LLMs
possesses a geometric structure, which must be preserved in addition to
enforcing orthogonality. Based on this, we propose Orthogonal Low-rank
Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM
fine-tuning: leveraging multiplicative updates to preserve parameter geometry
while applying orthogonality constraints to task subspaces. Experiments
demonstrate that OLieRA achieves state-of-the-art results on the Standard CL
benchmark and remains among the top-performing methods in the Large Number of
Tasks setting.

</details>


### [40] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
*Jinrui Yang,Xudong Han,Timothy Baldwin*

Main category: cs.CL

TL;DR: EuroParlVote: A new benchmark for evaluating LLMs in political contexts, linking European Parliament speeches to vote outcomes with MEP demographic data.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs in politically sensitive contexts and identifying potential biases.

Method: Evaluating state-of-the-art LLMs on gender classification and vote prediction tasks using the EuroParlVote benchmark.

Result: LLMs frequently misclassify female MEPs as male, show reduced accuracy for female speakers' votes, and favor centrist political groups.

Conclusion: Proprietary models outperform open-weight alternatives in robustness and fairness. The EuroParlVote dataset, code, and demo are released for future research.

Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language
models (LLMs) in politically sensitive contexts. It links European Parliament
debate speeches to roll-call vote outcomes and includes rich demographic
metadata for each Member of the European Parliament (MEP), such as gender, age,
country, and political group. Using EuroParlVote, we evaluate state-of-the-art
LLMs on two tasks -- gender classification and vote prediction -- revealing
consistent patterns of bias. We find that LLMs frequently misclassify female
MEPs as male and demonstrate reduced accuracy when simulating votes for female
speakers. Politically, LLMs tend to favor centrist groups while underperforming
on both far-left and far-right ones. Proprietary models like GPT-4o outperform
open-weight alternatives in terms of both robustness and fairness. We release
the EuroParlVote dataset, code, and demo to support future research on fairness
and accountability in NLP within political contexts.

</details>


### [41] [Understanding the Influence of Synthetic Data for Text Embedders](https://arxiv.org/abs/2509.06184)
*Jacob Mitchell Springer,Vaibhav Adlakha,Siva Reddy,Aditi Raghunathan,Marius Mosbach*

Main category: cs.CL

TL;DR: 论文研究了使用大型语言模型生成的合成数据对通用文本嵌入器的影响。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开的合成数据集，无法研究其在泛化中的作用。

Method: 论文复制并公开发布了Wang等人提出的合成数据(Mistral-E5)，并分析了合成数据改进模型泛化的具体方面。

Result: 研究发现，合成数据带来的好处是稀疏的，并且高度局限于单个数据集。不同类别之间的性能存在权衡，并且有益于一项任务的数据会降低另一项任务的性能。

Conclusion: 研究结果表明，当前的合成数据方法在构建通用嵌入器方面存在局限性，并对在合成数据上进行训练可以构建更强大的跨任务嵌入模型的观点提出了挑战。

Abstract: Recent progress in developing general purpose text embedders has been driven
by training on ever-growing corpora of synthetic LLM-generated data.
Nonetheless, no publicly available synthetic dataset exists, posing a barrier
to studying its role for generalization. To address this issue, we first
reproduce and publicly release the synthetic data proposed by Wang et al.
(Mistral-E5). Our synthetic data is high quality and leads to consistent
improvements in performance. Next, we critically examine where exactly
synthetic data improves model generalization. Our analysis reveals that
benefits from synthetic data are sparse and highly localized to individual
datasets. Moreover, we observe trade-offs between the performance on different
categories and data that benefits one task, degrades performance on another.
Our findings highlight the limitations of current synthetic data approaches for
building general-purpose embedders and challenge the notion that training on
synthetic data leads to more robust embedding models across tasks.

</details>


### [42] [Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation](https://arxiv.org/abs/2509.06196)
*Mohamed T. Younes,Omar Walid,Khaled Shaban,Ali Hamdi,Mai Hassan*

Main category: cs.CL

TL;DR: 本文提出了一种新的招聘自动化方法，通过微调大型语言模型（LLM）来提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在招聘任务中存在局限性。

Method: 1. 创建一个使用标准化JSON格式的合成数据集。2. 使用DeepSeek解析简历，并将简历解析为相同的结构化JSON格式，并将其放入训练集中。3. 微调LLM。

Result: 微调后的Phi-4模型达到了90.62%的F1分数，表明在招聘任务中具有出色的精确率和召回率。

Conclusion: 微调后的LLM有潜力通过提供更准确的候选人-职位匹配来彻底改变招聘工作流程。

Abstract: This paper presents a novel approach to recruitment automation. Large
Language Models (LLMs) were fine-tuned to improve accuracy and efficiency.
Building upon our previous work on the Multilayer Large Language Model-Based
Robotic Process Automation Applicant Tracking (MLAR) system . This work
introduces a novel methodology. Training fine-tuned LLMs specifically tuned for
recruitment tasks. The proposed framework addresses the limitations of generic
LLMs by creating a synthetic dataset that uses a standardized JSON format. This
helps ensure consistency and scalability. In addition to the synthetic data
set, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes
were parsed into the same structured JSON format and placed in the training
set. This will help improve data diversity and realism. Through
experimentation, we demonstrate significant improvements in performance
metrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall
similarity compared to base models and other state-of-the-art LLMs. In
particular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,
indicating exceptional precision and recall in recruitment tasks. This study
highlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize
recruitment workflows by providing more accurate candidate-job matching.

</details>


### [43] [MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment](https://arxiv.org/abs/2509.06200)
*Omar Walid,Mohamed T. Younes,Khaled Shaban,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MSLEF is a multi-segment ensemble framework that uses LLM fine-tuning to improve resume parsing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the limitations of single-model systems in resume parsing by adapting to diverse formats and structures.

Method: The paper introduces a segment-aware architecture that leverages field-specific weighting tailored to each resume part and integrates fine-tuned LLMs using weighted voting. It incorporates Gemini-2.5-Flash LLM, Gemma 9B, LLaMA 3.1 8B, and Phi-4 14B.

Result: MSLEF achieves significant improvements in Exact Match (EM), F1 score, BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best single model by up to +7% in RS.

Conclusion: The segment-aware design enhances generalization across varied resume layouts, making it highly adaptable to real-world hiring scenarios while ensuring precise and reliable candidate representation.

Abstract: This paper presents MSLEF, a multi-segment ensemble framework that employs
LLM fine-tuning to enhance resume parsing in recruitment automation. It
integrates fine-tuned Large Language Models (LLMs) using weighted voting, with
each model specializing in a specific resume segment to boost accuracy.
Building on MLAR , MSLEF introduces a segment-aware architecture that leverages
field-specific weighting tailored to each resume part, effectively overcoming
the limitations of single-model systems by adapting to diverse formats and
structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level
aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4
14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,
BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best
single model by up to +7% in RS. Its segment-aware design enhances
generalization across varied resume layouts, making it highly adaptable to
real-world hiring scenarios while ensuring precise and reliable candidate
representation.

</details>


### [44] [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)
*Jinju Kim,Taehan Kim,Abdul Waheed,Rita Singh*

Main category: cs.CL

TL;DR: 该论文探讨了AI音乐生成中的版权问题，并尝试应用机器学习取消学习技术来避免侵权。


<details>
  <summary>Details</summary>
Motivation: AI音乐生成系统存在滥用版权内容的风险，引发伦理和法律问题。

Method: 将现有的机器学习取消学习方法应用于预训练的文本到音乐（TTM）基线模型。

Result: 分析了这些方法在取消学习预训练数据集方面的有效性，同时评估了对模型性能的影响。

Conclusion: 为未来在音乐生成模型中应用取消学习技术奠定了基础，并提供了对音乐生成中应用取消学习的挑战的见解。

Abstract: AI music generation is rapidly emerging in the creative industries, enabling
intuitive music generation from textual descriptions. However, these systems
pose risks in exploitation of copyrighted creations, raising ethical and legal
concerns. In this paper, we present preliminary results on the first
application of machine unlearning techniques from an ongoing research to
prevent inadvertent usage of creative content. Particularly, we explore
existing methods in machine unlearning to a pre-trained Text-to-Music (TTM)
baseline and analyze their efficacy in unlearning pre-trained datasets without
harming model performance. Through our experiments, we provide insights into
the challenges of applying unlearning in music generation, offering a
foundational analysis for future works on the application of unlearning for
music generative models.

</details>


### [45] [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)
*Junjie Mu,Zonghao Ying,Zhekui Fan,Zonglei Jing,Yaoyuan Zhang,Zhengmin Yu,Wenxin Zhang,Quanchen Zou,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 提出了Mask-GCG，通过可学习的token masking识别suffix中的重要tokens，减少冗余并降低计算开销，加速jailbreak攻击。


<details>
  <summary>Details</summary>
Motivation: 现有GCG方法依赖固定长度的后缀，但这些后缀中可能存在冗余，未被探索。

Method: 采用可学习的token masking来识别suffix中的重要tokens，增加高影响力位置的tokens的更新概率，并剪除低影响力位置的tokens。

Result: 实验结果表明，suffix中的大多数tokens对攻击成功有显著贡献，剪除少数低影响力tokens不影响损失值或攻击成功率。

Conclusion: 揭示了LLM prompt中的token冗余，为从jailbreak攻击的角度开发高效且可解释的LLM提供了见解。

Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various
successful methods whereby attackers manipulate models into generating harmful
responses that they are designed to avoid. Among these, Greedy Coordinate
Gradient (GCG) has emerged as a general and effective approach that optimizes
the tokens in a suffix to generate jailbreakable prompts. While several
improved variants of GCG have been proposed, they all rely on fixed-length
suffixes. However, the potential redundancy within these suffixes remains
unexplored. In this work, we propose Mask-GCG, a plug-and-play method that
employs learnable token masking to identify impactful tokens within the suffix.
Our approach increases the update probability for tokens at high-impact
positions while pruning those at low-impact positions. This pruning not only
reduces redundancy but also decreases the size of the gradient space, thereby
lowering computational overhead and shortening the time required to achieve
successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the
original GCG and several improved variants. Experimental results show that most
tokens in the suffix contribute significantly to attack success, and pruning a
minority of low-impact tokens does not affect the loss values or compromise the
attack success rate (ASR), thereby revealing token redundancy in LLM prompts.
Our findings provide insights for developing efficient and interpretable LLMs
from the perspective of jailbreak attacks.

</details>


### [46] [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)
*Ao Chang,Yubo Chen,Jun Zhao*

Main category: cs.CL

TL;DR: 提出了一种参数化的RAG (P-RAG) 框架PL-CA，通过数据增强和编码将法律知识注入到LLM中，缓解了模型上下文压力。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法直接将检索到的文档注入模型上下文，导致模型上下文窗口受限，计算开销大，影响模型性能。现有基准缺乏专家注释，且侧重于单个下游任务，无法反映模型的真实能力。

Method: 提出PL-CA框架，利用参数化RAG对语料知识进行数据增强，并将法律知识编码为参数化向量，然后通过LoRA将参数化知识集成到LLM的前馈网络（FFN）中。

Result: 实验结果表明，该方法减少了过长上下文带来的开销，同时在下游任务上保持了与传统RAG相当的性能。

Conclusion: 提出的PL-CA框架有效缓解了模型上下文压力，并在多任务法律数据集上取得了良好的实验结果。

Abstract: Conventional RAG is considered one of the most effective methods for
addressing model knowledge insufficiency and hallucination, particularly in the
judicial domain that requires high levels of knowledge rigor, logical
consistency, and content integrity. However, the conventional RAG method only
injects retrieved documents directly into the model's context, which severely
constrains models due to their limited context windows and introduces
additional computational overhead through excessively long contexts, thereby
disrupting models' attention and degrading performance on downstream tasks.
Moreover, many existing benchmarks lack expert annotation and focus solely on
individual downstream tasks while real-world legal scenarios consist of
multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for
reflecting models' true capabilities. To address these limitations, we propose
PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data
augmentation on corpus knowledge and encode this legal knowledge into
parametric vectors, and then integrates this parametric knowledge into the
LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context
pressure. Additionally, we also construct a multi-task legal dataset comprising
more than 2000 training and test instances, which are all expert-annotated and
manually verified. We conduct our experiments on our dataset, and the
experimental results demonstrate that our method reduces the overhead
associated with excessively long contexts while maintaining competitive
performance on downstream tasks compared to conventional RAG. Our code and
dataset are provided in the appendix.

</details>


### [47] [Do LLMs exhibit the same commonsense capabilities across languages?](https://arxiv.org/abs/2509.06401)
*Ivan Martínez-Murillo,Elena Lloret,Paloma Moreda,Albert Gatt*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型 (LLM) 的多语言常识生成能力，并提出了一个新的基准数据集 MULTICOM，该数据集将 COCOTEROS 数据集扩展到四种语言：英语、西班牙语、荷兰语和巴伦西亚语。任务是生成一个包含给定三个单词的常识性句子。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在多种语言中生成常识的能力。

Method: 使用 MULTICOM 基准评估了一系列开源 LLM，包括 LLaMA、Qwen、Gemma、EuroLLM 和 Salamandra。评估结合了自动指标、LLM 作为评判方法和人工注释。

Result: 结果表明，英语的表现始终优于其他语言，而资源较少的语言的表现明显较低。上下文支持产生的结果好坏参半，但往往有利于代表性不足的语言。

Conclusion: 研究结果强调了 LLM 在多语言常识生成方面的局限性。

Abstract: This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.

</details>


### [48] [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)
*Junteng Liu,Yunji Li,Chi Zhang,Jingyang Li,Aili Chen,Ke Ji,Weiyu Cheng,Zijia Wu,Chengyu Du,Qidi Xu,Jiayuan Song,Zhengmao Zhu,Wenhu Chen,Pengyu Zhao,Junxian He*

Main category: cs.CL

TL;DR: 本文介绍了一种名为 WebExplorer 的新型网络代理，它通过模型探索和迭代的长短查询演化来生成具有挑战性的信息寻求数据，从而解决了现有开源网络代理在复杂任务中信息寻求能力有限或缺乏透明实现的问题。WebExplorer-8B 在各种信息寻求基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源网络代理在复杂任务中的信息寻求能力有限，并且缺乏透明的实现，关键挑战在于缺乏具有挑战性的信息寻求数据。

Method: 提出了一种系统的数据生成方法 WebExplorer，该方法使用基于模型的探索和迭代的长短查询演化来创建具有挑战性的查询-答案对，这些查询-答案对需要多步骤推理和复杂的网络导航。通过监督微调和强化学习，开发了先进的网络代理 WebExplorer-8B。

Result: WebExplorer-8B 在其规模上实现了最先进的性能，支持 128K 的上下文长度和最多 100 个工具调用轮次，从而能够解决长时程问题。作为一个 8B 大小的模型，WebExplorer-8B 在 RL 训练后能够有效地搜索平均 16 轮，在 BrowseComp-en/zh 上实现了比 WebSailor-72B 更高的准确率，并在 WebWalkerQA 和 FRAMES 上获得了高达 100B 参数的模型中的最佳性能。该模型在 HLE 基准测试中也实现了强大的泛化能力。

Conclusion: 本文提出的方法是通往长时程网络代理的实用途径。

Abstract: The paradigm of Large Language Models (LLMs) has increasingly shifted toward
agentic applications, where web browsing capabilities are fundamental for
retrieving information from diverse online sources. However, existing
open-source web agents either demonstrate limited information-seeking abilities
on complex tasks or lack transparent implementations. In this work, we identify
that the key challenge lies in the scarcity of challenging data for information
seeking. To address this limitation, we introduce WebExplorer: a systematic
data generation approach using model-based exploration and iterative,
long-to-short query evolution. This method creates challenging query-answer
pairs that require multi-step reasoning and complex web navigation. By
leveraging our curated high-quality dataset, we successfully develop advanced
web agent WebExplorer-8B through supervised fine-tuning followed by
reinforcement learning. Our model supports 128K context length and up to 100
tool calling turns, enabling long-horizon problem solving. Across diverse
information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art
performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able
to effectively search over an average of 16 turns after RL training, achieving
higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best
performance among models up to 100B parameters on WebWalkerQA and FRAMES.
Beyond these information-seeking tasks, our model also achieves strong
generalization on the HLE benchmark even though it is only trained on
knowledge-intensive QA data. These results highlight our approach as a
practical path toward long-horizon web agents.

</details>


### [49] [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)
*Andrei Baroian,Kasper Notebomer*

Main category: cs.CL

TL;DR: 本研究探索了Transformer语言模型中不同深度层的功能角色和计算能力需求，并提出了三种新的Layer-Wise Scaling (LWS) 变体。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型忽略了不同深度层的功能差异和计算需求。

Method: 通过在预训练阶段使用两点或三点线性插值重新分配FFN宽度和注意力头，引入了Framed、Reverse和Crown三种LWS变体，并对LWS及其变体进行了系统性的消融研究。

Result: 所有模型在相似的损失下收敛，并且与同等成本的各向同性基线相比，实现了更好的性能，而训练吞吐量没有显著下降。

Conclusion: 这项工作代表了预训练的分层架构设计空间的初步探索，但未来的工作应该将实验扩展到更多数量级的tokens和参数，以充分评估它们的潜力。

Abstract: Transformer-based language models traditionally use uniform (isotropic) layer
sizes, yet they ignore the diverse functional roles that different depths can
play and their computational capacity needs. Building on Layer-Wise Scaling
(LWS) and pruning literature, we introduce three new LWS variants - Framed,
Reverse, and Crown - that redistribute FFN widths and attention heads via two
or three-point linear interpolation in the pre-training stage. We present the
first systematic ablation of LWS and its variants, on a fixed budget of 180M
parameters, trained on 5B tokens. All models converge to similar losses and
achieve better performance compared to an equal-cost isotropic baseline,
without a substantial decrease in training throughput. This work represents an
initial step into the design space of layer-wise architectures for
pre-training, but future work should scale experiments to orders of magnitude
more tokens and parameters to fully assess their potential.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [Label Smoothing++: Enhanced Label Regularization for Training Neural Networks](https://arxiv.org/abs/2509.05307)
*Sachin Chhabra,Hemanth Venkateswara,Baoxin Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Label Smoothing++ 的新型标签平滑策略，旨在解决传统标签平滑忽略类别间关系的问题。


<details>
  <summary>Details</summary>
Motivation: 传统one-hot标签训练神经网络容易导致过拟合和过度自信，标签平滑虽然可以提高泛化能力，但会破坏类别间的关系。

Method: 本文提出的 Label Smoothing++ 策略，为非目标类别分配非零概率，并考虑类别间的关系。该方法固定目标类别的标签，并使网络能够学习与非目标类别相关的标签。

Result: 在多个数据集上的大量实验表明，Label Smoothing++ 可以缓解过度自信的预测，同时促进类间关系和泛化能力。

Conclusion: Label Smoothing++ 是一种有效的标签正则化训练策略，可以在提高模型泛化能力的同时，保持类别间的关系。

Abstract: Training neural networks with one-hot target labels often results in
overconfidence and overfitting. Label smoothing addresses this issue by
perturbing the one-hot target labels by adding a uniform probability vector to
create a regularized label. Although label smoothing improves the network's
generalization ability, it assigns equal importance to all the non-target
classes, which destroys the inter-class relationships. In this paper, we
propose a novel label regularization training strategy called Label
Smoothing++, which assigns non-zero probabilities to non-target classes and
accounts for their inter-class relationships. Our approach uses a fixed label
for the target class while enabling the network to learn the labels associated
with non-target classes. Through extensive experiments on multiple datasets, we
demonstrate how Label Smoothing++ mitigates overconfident predictions while
promoting inter-class relationships and generalization capabilities.

</details>


### [51] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

TL;DR: 本研究提出了一种名为VILOD的可视交互标注工具，用于目标检测，旨在解决深度学习中数据集标注耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法缺乏透明性，限制了人类专家的策略性见解，并可能忽略与所用查询策略不一致的信息样本。为了解决这些问题，本研究着眼于人机结合的方法。

Method: 该研究开发并 исследует “VILOD：一种用于对象检测的可视交互标记工具”。VILOD利用图像特征的t-SNE投影、不确定性热图和模型状态视图等组件。

Result: 通过比较用例的实证研究表明，VILOD通过其交互式可视化，有助于通过使模型的状态和数据集特征更易于解释来实现不同的标注策略。在VILOD中采用的不同视觉引导标注策略与自动不确定性抽样AL基线相比，产生了具有竞争力的OD性能轨迹。

Conclusion: 这项工作贡献了一种新颖的工具和实证见解，使OD注释的HITL-AL工作流程更加透明、易于管理且可能更有效。

Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


### [52] [Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification](https://arxiv.org/abs/2509.05319)
*Zhengda Li*

Main category: cs.CV

TL;DR: 提出了一种自适应知识蒸馏（AKD）框架，以解决传统知识蒸馏中固定平衡因子alpha的次优问题。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏使用固定的平衡因子alpha，但静态的alpha并非最优，因为hard和soft监督之间的最佳权衡在训练过程中可能会发生变化。

Method: 1. 将alpha作为一个可学习的参数，可以在训练过程中自动学习和优化。2. 引入一个公式来反映学生和教师之间的差距，从而动态计算alpha，由学生-教师差异指导。3. 进一步引入一个上下文感知模块（CAM），使用MLP + Attention来自适应地重新加权类别的教师输出。

Result: 在CIFAR-10上，使用ResNet-50作为教师，ResNet-18作为学生，实验表明该方法比固定权重的KD基线取得了更高的准确率，并产生了更稳定的收敛。

Conclusion: 提出的AKD框架能够有效地提高知识蒸馏的性能，并通过自适应调整hard和soft监督的权重以及重新加权教师输出，实现了更优的准确率和更稳定的收敛。

Abstract: Knowledge distillation (KD) is a widely used technique to transfer knowledge
from a large teacher network to a smaller student model. Traditional KD uses a
fixed balancing factor alpha as a hyperparameter to combine the hard-label
cross-entropy loss with the soft-label distillation loss. However, a static
alpha is suboptimal because the optimal trade-off between hard and soft
supervision can vary during training.
  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.
First we try to make alpha as learnable parameter that can be automatically
learned and optimized during training. Then we introduce a formula to reflect
the gap between the student and the teacher to compute alpha dynamically,
guided by student-teacher discrepancies, and further introduce a Context-Aware
Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher
outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as
student demonstrate that our approach achieves superior accuracy compared to
fixed-weight KD baselines, and yields more stable convergence.

</details>


### [53] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

TL;DR: 本文介绍了一个开源框架 Video2EEG-SPGN-Diffusion，它利用 SEED-VD 数据集生成以视频刺激为条件的脑电信号多模态数据集。


<details>
  <summary>Details</summary>
Motivation: 公开用于对齐视频和脑电数据对的工程管道，促进具有脑电对齐能力的多模态大型模型的训练。

Method: 使用与扩散模型集成的自博弈图网络 (SPGN) 生成个性化脑电信号。

Result: 发布了一个新数据集，其中包含 1000 多个 SEED-VD 视频刺激样本，这些样本与生成的 62 通道脑电信号（200 Hz）和情感标签配对，从而实现视频-脑电对齐并推进多模态研究。

Conclusion: 该框架为情绪分析、数据增强和脑机接口应用提供了新颖的工具，具有重要的研究和工程意义。

Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,
that leverages the SEED-VD dataset to generate a multimodal dataset of EEG
signals conditioned on video stimuli. Additionally, we disclose an engineering
pipeline for aligning video and EEG data pairs, facilitating the training of
multimodal large models with EEG alignment capabilities. Personalized EEG
signals are generated using a self-play graph network (SPGN) integrated with a
diffusion model. As a major contribution, we release a new dataset comprising
over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG
signals at 200 Hz and emotion labels, enabling video-EEG alignment and
advancing multimodal research. This framework offers novel tools for emotion
analysis, data augmentation, and brain-computer interface applications, with
substantial research and engineering significance.

</details>


### [54] [Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19](https://arxiv.org/abs/2509.05322)
*Pavithra Elumalai,Sudharsan Vijayaraghavan,Madhumita Mondal,Areejit Samal*

Main category: cs.CV

TL;DR: 研究了使用 Forman-Ricci 曲率 (FRC)、Ollivier-Ricci 曲率 (ORC) 和边介数中心性 (EBC) 来压缩随机连接神经网络 (RWNN) 的方法，通过选择性地保留重要的突触（或边）同时修剪其余部分。


<details>
  <summary>Details</summary>
Motivation: 通过捕获不同的连接模式如何影响学习效率和模型性能，随机连接神经网络 (RWNN) 可以作为研究深度学习中网络拓扑影响的重要试验台。同时，它们为探索以边缘为中心的网络度量作为修剪和优化的工具提供了一个自然的框架。

Method: 在 COVID-19 胸部 X 射线图像分类上训练 RWNN，旨在降低网络复杂性，同时保持准确性、特异性和灵敏度方面的性能。通过结合另外两个以边缘为中心的度量 FRC 和 EBC，扩展了先前关于使用 ORC 修剪 RWNN 的工作，跨越三个网络生成器：Erdős-Rényi (ER) 模型、Watts-Strogatz (WS) 模型和 Barabási-Albert (BA) 模型。

Result: 结果初步证明，基于 FRC 的修剪可以有效地简化 RWNN，提供显着的计算优势，同时保持与 ORC 相当的性能。

Conclusion: FRC-based 的修剪可以有效地简化 RWNN，提供显着的计算优势，同时保持与 ORC 相当的性能。

Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for
investigating the impact of network topology in deep learning by capturing how
different connectivity patterns impact both learning efficiency and model
performance. At the same time, they provide a natural framework for exploring
edge-centric network measures as tools for pruning and optimization. In this
study, we investigate three edge-centric network measures: Forman-Ricci
curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness
centrality (EBC), to compress RWNNs by selectively retaining important synapses
(or edges) while pruning the rest. As a baseline, RWNNs are trained for
COVID-19 chest x-ray image classification, aiming to reduce network complexity
while preserving performance in terms of accuracy, specificity, and
sensitivity. We extend prior work on pruning RWNN using ORC by incorporating
two additional edge-centric measures, FRC and EBC, across three network
generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and
Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the
pruning performance of the three measures in terms of compression ratio and
theoretical speedup. A central focus of our study is to evaluate whether FRC,
which is computationally more efficient than ORC, can achieve comparable
pruning effectiveness. Along with performance evaluation, we further
investigate the structural properties of the pruned networks through modularity
and global efficiency, offering insights into the trade-off between modular
segregation and network efficiency in compressed RWNNs. Our results provide
initial evidence that FRC-based pruning can effectively simplify RWNNs,
offering significant computational advantages while maintaining performance
comparable to ORC.

</details>


### [55] [Optical Music Recognition of Jazz Lead Sheets](https://arxiv.org/abs/2509.05329)
*Juan Carlos Martinez-Sevilla,Francesco Foscarin,Patricia Garcia-Iasci,David Rizo,Jorge Calvo-Zaragoza,Gerhard Widmer*

Main category: cs.CV

TL;DR: 本文解决了手写爵士乐谱的光学音乐识别(OMR)的挑战，该乐谱类型包含旋律和和弦。


<details>
  <summary>Details</summary>
Motivation: 现有的OMR系统无法处理包含和弦的乐谱，并且手写图像具有高度的可变性和质量问题。

Method: 创建了一个包含293份手写爵士乐谱的数据集，并开发了一个用于爵士乐谱的OMR模型。

Result: 发布了代码、数据和模型。

Conclusion: 本文为爵士乐谱的OMR做出了贡献。

Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR)
for handwritten jazz lead sheets, a widely used musical score type that encodes
melody and chords. The task is challenging due to the presence of chords, a
score component not handled by existing OMR systems, and the high variability
and quality issues associated with handwritten images. Our contribution is
two-fold. We present a novel dataset consisting of 293 handwritten jazz lead
sheets of 163 unique pieces, amounting to 2021 total staves aligned with
Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score
images generated from the ground truth. The second contribution is the
development of an OMR model for jazz lead sheets. We discuss specific
tokenisation choices related to our kind of data, and the advantages of using
synthetic scores and pretrained models. We publicly release all code, data, and
models.

</details>


### [56] [RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness](https://arxiv.org/abs/2509.05333)
*Junghyun Park,Tuan Anh Nguyen,Dugki Min*

Main category: cs.CV

TL;DR: 本文介绍了一种名为 Re-Thinking Vision Language Model (RT-VLM) 的框架，旨在解决对象识别模型在实际部署中因域漂移而导致的精度下降问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的部署经常使现代对象识别模型暴露于域漂移，从而导致精度严重下降。这些变化包括：(i) 低级图像统计的变化，(ii) 对象姿势和视点的变化，(iii) 部分遮挡，以及 (iv) 相邻类之间的视觉混淆。

Method: 该框架的基础是一个独特的合成数据集生成管道，该管道生成带有“4-Clues”注释的图像：精确的边界框、类名、详细的对象级标题以及整个场景的综合上下文级标题。然后，我们在此资源上执行 Llama 3.2 11B Vision Instruct 的参数高效监督调整。在推理时，执行两阶段的“重新思考”方案：模型首先发出自己的四个线索，然后重新检查这些响应作为证据并迭代地纠正它们。

Result: 在隔离各个域漂移的鲁棒性基准测试中，RT-VLM 始终超过强大的基线。

Conclusion: 这些发现表明，将结构化多模态证据与显式的自我批评循环相结合，是实现可靠且可转移的视觉理解的有希望的途径。

Abstract: Real world deployments often expose modern object recognition models to
domain shifts that precipitate a severe drop in accuracy. Such shifts encompass
(i) variations in low level image statistics, (ii) changes in object pose and
viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent
classes. To mitigate this degradation, we introduce the Re-Thinking Vision
Language Model (RT-VLM) framework. The foundation of this framework is a unique
synthetic dataset generation pipeline that produces images annotated with
"4-Clues": precise bounding boxes, class names, detailed object-level captions,
and a comprehensive context-level caption for the entire scene. We then perform
parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this
resource. At inference time, a two stage Re-Thinking scheme is executed: the
model first emits its own four clues, then re examines these responses as
evidence and iteratively corrects them. Across robustness benchmarks that
isolate individual domain shifts, RT-VLM consistently surpasses strong
baselines. These findings indicate that the integration of structured
multimodal evidence with an explicit self critique loop constitutes a promising
route toward reliable and transferable visual understanding.

</details>


### [57] [A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices](https://arxiv.org/abs/2509.05334)
*Diwen Huang*

Main category: cs.CV

TL;DR: 本文介绍了一种使用智能手机技术测量羽毛球扣球速度的新型、经济高效且用户友好的系统。


<details>
  <summary>Details</summary>
Motivation: 传统的运动性能指标捕获技术昂贵、复杂，业余和休闲运动员难以获得。本文旨在解决羽毛球运动中这一问题。

Method: 该方法利用定制训练的 YOLOv5 模型进行羽毛球检测，结合卡尔曼滤波器进行鲁棒的轨迹跟踪。通过实施基于视频的运动学速度估计方法和时空缩放，该系统可以自动计算标准视频记录中羽毛球的速度。

Result: 该系统将整个过程打包到一个直观的移动应用程序中，普及了高级性能分析的访问，并使所有级别的玩家都能分析和改进他们的比赛。

Conclusion: 该研究 democratizing access to high-level performance analytics and empowering players at all levels to analyze and improve their game.

Abstract: Performance metrics in sports, such as shot speed and angle, provide crucial
feedback for athlete development. However, the technology to capture these
metrics has historically been expensive, complex, and largely inaccessible to
amateur and recreational players. This paper addresses this gap in the context
of badminton, one of the world's most popular sports, by introducing a novel,
cost-effective, and user-friendly system for measuring smash speed using
ubiquitous smartphone technology. Our approach leverages a custom-trained
YOLOv5 model for shuttlecock detection, combined with a Kalman filter for
robust trajectory tracking. By implementing a video-based kinematic speed
estimation method with spatiotemporal scaling, the system automatically
calculates the shuttlecock's velocity from a standard video recording. The
entire process is packaged into an intuitive mobile application, democratizing
access to high-level performance analytics and empowering players at all levels
to analyze and improve their game.

</details>


### [58] [RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation](https://arxiv.org/abs/2509.05554)
*Yihong Leng,Siming Zheng,Jinwei Chen,Bo Li,Jiaojiao Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出了一种鲁棒的事件引导去模糊（RED）网络，该网络具有特定于模态的解缠表示，以解决事件流的不完整性问题，从而提高运动先验的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的去模糊方法侧重于跨模态交互，忽略了事件流的内在不完整性，这种不完整性会损害运动先验的完整性，并限制事件引导去模糊的有效性。

Method: 1. 提出了一个面向鲁棒性的扰动策略（RPS），该策略将随机掩蔽应用于事件，使 RED 暴露于不完整的模式，从而培养针对各种未知场景条件的鲁棒性。
2. 提出了一个解缠的 OmniAttention，用于显式地对来自两个内在不同但互补的来源（模糊图像和部分中断的事件）的内运动、间运动和跨模态相关性进行建模。
3. 设计了两个交互模块，以增强模糊图像中的运动敏感区域，并将语义上下文注入到不完整的事件表示中。

Result: 在合成和真实世界数据集上的大量实验表明，RED 在准确性和鲁棒性方面始终如一地实现了最先进的性能。

Conclusion: RED网络通过模态特定的解缠表示和面向鲁棒性的扰动策略，有效地解决了事件流的不完整性问题，提高了运动先验的完整性，从而在去模糊任务中取得了优异的性能。

Abstract: Event cameras provide sparse yet temporally high-temporal-resolution motion
information, demonstrating great potential for motion deblurring. Existing
methods focus on cross-modal interaction, overlooking the inherent
incompleteness of event streams, which arises from the trade-off between
sensitivity and noise introduced by the thresholding mechanism of Dynamic
Vision Sensors (DVS). Such degradation compromises the integrity of motion
priors and limits the effectiveness of event-guided deblurring. To tackle these
challenges, we propose a Robust Event-guided Deblurring (RED) network with
modality-specific disentangled representation. First, we introduce a
Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to
events, which exposes RED to incomplete patterns and then foster robustness
against various unknown scenario conditions.Next, a disentangled OmniAttention
is presented to explicitly model intra-motion, inter-motion, and cross-modality
correlations from two inherently distinct but complementary sources: blurry
images and partially disrupted events. Building on these reliable features, two
interactive modules are designed to enhance motion-sensitive areas in blurry
images and inject semantic context into incomplete event representations.
Extensive experiments on synthetic and real-world datasets demonstrate RED
consistently achieves state-of-the-art performance in both accuracy and
robustness.

</details>


### [59] [A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research](https://arxiv.org/abs/2509.05335)
*Zebo Xu,Shaoyun Yu,Mark Torrance,Guido Nottbusch,Nan Zhao,Zhenguang Cai*

Main category: cs.CV

TL;DR: 本文构建了一个大规模的汉字手写数据库，并升级了手写工具箱，以研究语言成分如何调节汉字手写。


<details>
  <summary>Details</summary>
Motivation: 理解语音、语义和正字法系统如何调节汉字手写，以及缺乏捕获和批量处理细粒度手写数据的工具。

Method: 构建了一个大规模手写数据库，其中42名中国人在听写任务中手写了1200个汉字。增强了现有的手写包，并为升级后的OpenHandWrite_Toolbox提供了全面的文档。

Result: 正字法预测因子影响汉字、部首和笔画层面的手写准备和执行。语音因素也会影响所有三个层面的执行。这些词汇效应表现出分层衰减——在字符层面最明显，其次是部首，在笔画层面最弱。

Conclusion: 部首和笔画层面的手写准备和执行与语言成分密切相关。该数据库和工具箱为未来心理语言学和神经语言学研究提供了宝贵的资源。

Abstract: Understanding what linguistic components (e.g., phonological, semantic, and
orthographic systems) modulate Chinese handwriting at the character, radical,
and stroke levels remains an important yet understudied topic. Additionally,
there is a lack of comprehensive tools for capturing and batch-processing
fine-grained handwriting data. To address these issues, we constructed a
large-scale handwriting database in which 42 Chinese speakers for each
handwriting 1200 characters in a handwriting-to-dictation task. Additionally,
we enhanced the existing handwriting package and provided comprehensive
documentation for the upgraded OpenHandWrite_Toolbox, which can easily modify
the experimental design, capture the stroke-level handwriting trajectory, and
batch-process handwriting measurements (e.g., latency, duration, and
pen-pressure). In analysing our large-scale database, multiple regression
results show that orthographic predictors impact handwriting preparation and
execution across character, radical, and stroke levels. Phonological factors
also influence execution at all three levels. Importantly, these lexical
effects demonstrate hierarchical attenuation - they were most pronounced at the
character level, followed by the radical, and were weakest at the stroke
levels. These findings demonstrate that handwriting preparation and execution
at the radical and stroke levels are closely intertwined with linguistic
components. This database and toolbox offer valuable resources for future
psycholinguistic and neurolinguistic research on the handwriting of characters
and sub-characters across different languages.

</details>


### [60] [Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory](https://arxiv.org/abs/2509.05337)
*Younggeol Cho,Gokhan Solak,Olivia Nocentini,Marta Lorenzini,Andrea Fortuna,Arash Ajoudani*

Main category: cs.CV

TL;DR: 提出了一种混合模型，结合动态图神经网络（DGNN）和长短期记忆（LSTM）网络，用于预测跌倒。


<details>
  <summary>Details</summary>
Motivation: 在跌倒检测方面已取得 значительных 进展，但在跌倒发生前的预测以及稳定和即将发生的跌倒之间的瞬态分析仍有待探索。

Method: 利用混合模型，结合动态图神经网络（DGNN）和长短期记忆（LSTM）网络，解耦运动预测和步态分类任务，以高精度预测跌倒。DGNN 作为分类器，区分三种步态状态：稳定、瞬态和跌倒。然后，基于 LSTM 的网络预测后续时间步骤中的人体运动，从而能够及早发现跌倒。

Result: 该模型在 OUMVLP-Pose 和 URFD 数据集上进行了训练和验证，在预测误差和识别精度方面表现出优于仅依赖 DGNN 的模型和文献中的模型。

Conclusion: 解耦预测和分类提高了性能，并且该方法可以监测瞬态，为增强高级辅助系统的功能提供有价值的见解。

Abstract: Detecting and preventing falls in humans is a critical component of assistive
robotic systems. While significant progress has been made in detecting falls,
the prediction of falls before they happen, and analysis of the transient state
between stability and an impending fall remain unexplored. In this paper, we
propose a anticipatory fall detection method that utilizes a hybrid model
combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory
(LSTM) networks that decoupled the motion prediction and gait classification
tasks to anticipate falls with high accuracy. Our approach employs real-time
skeletal features extracted from video sequences as input for the proposed
model. The DGNN acts as a classifier, distinguishing between three gait states:
stable, transient, and fall. The LSTM-based network then predicts human
movement in subsequent time steps, enabling early detection of falls. The
proposed model was trained and validated using the OUMVLP-Pose and URFD
datasets, demonstrating superior performance in terms of prediction error and
recognition accuracy compared to models relying solely on DGNN and models from
literature. The results indicate that decoupling prediction and classification
improves performance compared to addressing the unified problem using only the
DGNN. Furthermore, our method allows for the monitoring of the transient state,
offering valuable insights that could enhance the functionality of advanced
assistance systems.

</details>


### [61] [Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis](https://arxiv.org/abs/2509.05703)
*Ragib Amin Nihal,Benjamin Yen,Takeshi Ashizawa,Kazuhiro Nakadai*

Main category: cs.CV

TL;DR: 研究如何使用视觉语言模型 (VLM) 分析海洋哺乳动物的声音


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型没有经过特定领域生物声学频谱图的训练

Method: 将 VLM 解释与基于 LLM 的验证相结合，以构建领域知识

Result: 无需手动注释或模型重新训练即可适应声音数据

Conclusion: 视觉语言模型可以通过视觉方式从频谱图中提取有意义的模式

Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic
spectrograms. Vision Language Models (VLMs) are not trained on these
domain-specific visualizations. We investigate whether VLMs can extract
meaningful patterns from spectrograms visually. Our framework integrates VLM
interpretation with LLM-based validation to build domain knowledge. This
enables adaptation to acoustic data without manual annotation or model
retraining.

</details>


### [62] [Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging](https://arxiv.org/abs/2509.05340)
*Dibya Jyoti Bora,Mrinal Kanti Mishra*

Main category: cs.CV

TL;DR: 本文比较了两种聚类算法（K-Means 和 FCM）在脑肿瘤MRI图像分割中的应用。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤MRI图像分割是医学图像分析中的一个关键挑战，精确的肿瘤边界 deline 对于临床决策至关重要。

Method: 使用 K-Means（硬聚类）和 FCM（软聚类）算法分割 BraTS2020 数据集中的脑肿瘤MRI图像，并使用高斯滤波和 CLAHE 进行预处理。

Result: K-Means 速度更快 (0.3s/图像)，但 FCM 精度更高 (DSC=0.67 vs 0.43)，但计算成本更高 (1.3s/图像)。

Conclusion: 结果表明计算效率和边界精度之间存在固有的权衡。

Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a
pivotal challenge in medical image analysis due to the heterogeneous nature of
tumor morphology and intensity distributions. Accurate delineation of tumor
boundaries is critical for clinical decision-making, radiotherapy planning, and
longitudinal disease monitoring. In this study, we perform a comprehensive
comparative analysis of two major clustering paradigms applied in MRI tumor
segmentation: hard clustering, exemplified by the K-Means algorithm, and soft
clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each
pixel strictly to a single cluster, FCM introduces partial memberships, meaning
each pixel can belong to multiple clusters with varying degrees of association.
Experimental validation was performed using the BraTS2020 dataset,
incorporating pre-processing through Gaussian filtering and Contrast Limited
Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice
Similarity Coefficient (DSC) and processing time, which collectively
demonstrated that K-Means achieved superior speed with an average runtime of
0.3s per image, whereas FCM attained higher segmentation accuracy with an
average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher
computational cost (1.3s per image). These results highlight the inherent
trade-off between computational efficiency and boundary precision.

</details>


### [63] [Handling imbalance and few-sample size in ML based Onion disease classification](https://arxiv.org/abs/2509.05341)
*Abhijeet Manoj Pal,Rajbabu Velmurugan*

Main category: cs.CV

TL;DR: 提出了一种用于洋葱作物病虫害多分类的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要集中于二元分类，限制了它们在需要准确识别特定疾病或害虫类型的场景中的实际应用。

Method: 通过集成基于注意力的模块并采用全面的数据增强管道来缓解类别不平衡，从而增强了预训练的卷积神经网络 (CNN) 模型。

Result: 该模型在真实世界田间图像数据集上给出了 96.90% 的总体准确率和 0.96 F1 分数。

Conclusion: 该模型在使用相同数据集的情况下给出了比其他方法更好的结果。

Abstract: Accurate classification of pests and diseases plays a vital role in precision
agriculture, enabling efficient identification, targeted interventions, and
preventing their further spread. However, current methods primarily focus on
binary classification, which limits their practical applications, especially in
scenarios where accurately identifying the specific type of disease or pest is
essential. We propose a robust deep learning based model for multi-class
classification of onion crop diseases and pests. We enhance a pre-trained
Convolutional Neural Network (CNN) model by integrating attention based modules
and employing comprehensive data augmentation pipeline to mitigate class
imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1
score on real-world field image dataset. This model gives better results than
other approaches using the same datasets.

</details>


### [64] [Delta Velocity Rectified Flow for Text-to-Image Editing](https://arxiv.org/abs/2509.05342)
*Gaspard Beaudouin,Minghan Li,Jaeyeon Kim,Sunghoon Yoon,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出了 Delta Velocity Rectified Flow (DVRF)，这是一种新颖的无反演、路径感知的编辑框架，用于文本到图像编辑中的校正流模型。


<details>
  <summary>Details</summary>
Motivation: 为了减轻先前蒸馏采样方法中普遍存在的过度平滑伪影，显式地对源速度场和目标速度场之间的差异进行建模。

Method: DVRF 是一种基于蒸馏的方法，它显式地对源速度场和目标速度场之间的差异进行建模。引入了一个时间相关的平移项，以推动噪声潜在变量更接近目标轨迹，从而增强与目标分布的对齐。

Result: 实验结果表明，DVRF 实现了卓越的编辑质量、保真度和可控性，同时不需要任何架构修改，使其高效且广泛适用于文本到图像的编辑任务。

Conclusion: 当禁用此平移时，DVRF 简化为 Delta Denoising Score，从而桥接了基于分数的扩散优化和基于速度的校正流优化。此外，当平移项遵循校正流动力学下的线性时间表时，DVRF 概括了无反演方法 FlowEdit，并为其提供了有原则的理论解释。

Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,
path-aware editing framework within rectified flow models for text-to-image
editing. DVRF is a distillation-based method that explicitly models the
discrepancy between the source and target velocity fields in order to mitigate
over-smoothing artifacts rampant in prior distillation sampling approaches. We
further introduce a time-dependent shift term to push noisy latents closer to
the target trajectory, enhancing the alignment with the target distribution. We
theoretically demonstrate that when this shift is disabled, DVRF reduces to
Delta Denoising Score, thereby bridging score-based diffusion optimization and
velocity-based rectified-flow optimization. Moreover, when the shift term
follows a linear schedule under rectified-flow dynamics, DVRF generalizes the
Inversion-free method FlowEdit and provides a principled theoretical
interpretation for it. Experimental results indicate that DVRF achieves
superior editing quality, fidelity, and controllability while requiring no
architectural modifications, making it efficient and broadly applicable to
text-to-image editing tasks. Code is available at
https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.

</details>


### [65] [Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis](https://arxiv.org/abs/2509.05343)
*Zahid Ullah,Minki Hong,Tahir Mahmood,Jihie Kim*

Main category: cs.CV

TL;DR: 该研究将注意力机制整合到五个广泛使用的CNN架构中，以提高医学图像分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN难以捕捉医学图像中细粒度和复杂的特征，影响诊断准确性。

Method: 将Squeeze and Excitation模块或混合卷积块注意力模块添加到VGG16、ResNet18、InceptionV3、DenseNet121和EfficientNetB5等模型中。

Result: 在脑肿瘤MRI数据集和妊娠组织病理学数据集上，注意力增强的CNN始终优于基线架构。EfficientNetB5与混合注意力机制实现了最高的整体性能。

Conclusion: 注意力机制不仅提高了分类精度，还增强了特征定位，从而在异构成像模式中实现了更好的泛化。

Abstract: Deep learning has become a powerful tool for medical image analysis; however,
conventional Convolutional Neural Networks (CNNs) often fail to capture the
fine-grained and complex features critical for accurate diagnosis. To address
this limitation, we systematically integrate attention mechanisms into five
widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,
DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient
regions and improve discriminative performance. Specifically, each baseline
model is augmented with either a Squeeze and Excitation block or a hybrid
Convolutional Block Attention Module, allowing adaptive recalibration of
channel and spatial feature representations. The proposed models are evaluated
on two distinct medical imaging datasets, a brain tumor MRI dataset comprising
multiple tumor subtypes, and a Products of Conception histopathological dataset
containing four tissue categories. Experimental results demonstrate that
attention augmented CNNs consistently outperform baseline architectures across
all metrics. In particular, EfficientNetB5 with hybrid attention achieves the
highest overall performance, delivering substantial gains on both datasets.
Beyond improved classification accuracy, attention mechanisms enhance feature
localization, leading to better generalization across heterogeneous imaging
modalities. This work contributes a systematic comparative framework for
embedding attention modules in diverse CNN architectures and rigorously
assesses their impact across multiple medical imaging tasks. The findings
provide practical insights for the development of robust, interpretable, and
clinically applicable deep learning based decision support systems.

</details>


### [66] [Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset](https://arxiv.org/abs/2509.05348)
*Ashen Rodrigo,Isuru Munasinghe,Asanka Perera*

Main category: cs.CV

TL;DR: 本文评估了五种目标检测模型（YOLOv3、Faster R-CNN、RetinaNet、EfficientDet 和 Swin Transformer）在识别太阳能电池板缺陷和污染物方面的性能。


<details>
  <summary>Details</summary>
Motivation: 及时准确地检测太阳能电池板的缺陷和污染物对于维持光伏系统的效率和可靠性至关重要。

Method: 使用一个定制的、以 COCO 格式注释的数据集，该数据集专门为太阳能电池板缺陷和污染物检测而设计，并开发了一个用户界面来训练和评估模型。基于平均精度均值 (mAP)、精度、召回率和推理速度评估和比较每个模型的性能。

Result: 结果表明了检测精度和计算效率之间的权衡，突出了每个模型的相对优势和局限性。

Conclusion: 这些发现为在实际太阳能电池板监测和维护场景中选择合适的检测方法提供了有价值的指导。

Abstract: Timely and accurate detection of defects and contaminants in solar panels is
critical for maintaining the efficiency and reliability of photovoltaic
systems. This study presents a comprehensive evaluation of five
state-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,
EfficientDet, and Swin Transformer, for identifying physical and electrical
defects as well as surface contaminants such as dust, dirt, and bird droppings
on solar panels. A custom dataset, annotated in the COCO format and
specifically designed for solar panel defect and contamination detection, was
developed alongside a user interface to train and evaluate the models. The
performance of each model is assessed and compared based on mean Average
Precision (mAP), precision, recall, and inference speed. The results
demonstrate the trade-offs between detection accuracy and computational
efficiency, highlighting the relative strengths and limitations of each model.
These findings provide valuable guidance for selecting appropriate detection
approaches in practical solar panel monitoring and maintenance scenarios.
  The dataset will be publicly available at
https://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.

</details>


### [67] [Unsupervised Instance Segmentation with Superpixels](https://arxiv.org/abs/2509.05352)
*Cuong Manh Hoang*

Main category: cs.CV

TL;DR: 提出了一种新的实例分割框架，该框架无需人工注释即可有效分割对象。


<details>
  <summary>Details</summary>
Motivation: 当前流行的模型通过大量人工注释进行训练，从而在实例分割中带来令人印象深刻的性能，而人工注释的收集成本很高。

Method: 首先，将 MultiCut 算法应用于自监督特征以进行粗略的掩码分割。然后，采用掩码滤波器来获得高质量的粗略掩码。为了训练分割网络，我们计算了一种新颖的超像素引导掩码损失，该损失包括硬损失和软损失，以及从低级图像特征分割的高质量粗略掩码和超像素。最后，提出了一种采用新的自适应损失的自训练过程，以提高预测掩码的质量。

Result: 在实例分割和对象检测的公共数据集上进行了实验，证明了该框架的有效性。结果表明，该框架优于以往的最新方法。

Conclusion: 该论文提出了一种无需人工标注的高效实例分割框架，并在公共数据集上取得了优于现有技术水平的结果。

Abstract: Instance segmentation is essential for numerous computer vision applications,
including robotics, human-computer interaction, and autonomous driving.
Currently, popular models bring impressive performance in instance segmentation
by training with a large number of human annotations, which are costly to
collect. For this reason, we present a new framework that efficiently and
effectively segments objects without the need for human annotations. Firstly, a
MultiCut algorithm is applied to self-supervised features for coarse mask
segmentation. Then, a mask filter is employed to obtain high-quality coarse
masks. To train the segmentation network, we compute a novel superpixel-guided
mask loss, comprising hard loss and soft loss, with high-quality coarse masks
and superpixels segmented from low-level image features. Lastly, a
self-training process with a new adaptive loss is proposed to improve the
quality of predicted masks. We conduct experiments on public datasets in
instance segmentation and object detection to demonstrate the effectiveness of
the proposed framework. The results show that the proposed framework
outperforms previous state-of-the-art methods.

</details>


### [68] [Augmented Structure Preserving Neural Networks for cell biomechanics](https://arxiv.org/abs/2509.05388)
*Juan Olalla-Pombo,Alberto Badías,Miguel Ángel Sanz-Gómez,José María Benítez,Francisco Javier Montáns*

Main category: cs.CV

TL;DR: 本文提出了一种新的细胞运动预测方法，该方法结合了结构保持神经网络和机器学习工具，可以准确预测细胞轨迹，并包含一个基于神经网络的mitosis事件预测模型。


<details>
  <summary>Details</summary>
Motivation: 理解细胞生物力学中的复杂现象对于理解生命进化和相关过程至关重要，但细胞间的相互作用及其对细胞决策的影响尚不清楚。

Method: 结合了结构保持神经网络（研究细胞运动作为纯机械系统）和其他机器学习工具（人工神经网络），后者可以考虑通过计算机视觉技术从实验中直接推断出的环境因素。

Result: 该模型在模拟和真实细胞迁移案例中进行了测试，可以高精度地预测完整的细胞轨迹。该工作还包括一个基于神经网络架构的mitosis事件预测模型，该模型使用了相同的观察特征。

Conclusion: 本文提出的新方法能够准确预测细胞轨迹，并可用于mitosis事件预测。

Abstract: Cell biomechanics involve a great number of complex phenomena that are
fundamental to the evolution of life itself and other associated processes,
ranging from the very early stages of embryo-genesis to the maintenance of
damaged structures or the growth of tumors. Given the importance of such
phenomena, increasing research has been dedicated to their understanding, but
the many interactions between them and their influence on the decisions of
cells as a collective network or cluster remain unclear. We present a new
approach that combines Structure Preserving Neural Networks, which study cell
movements as a purely mechanical system, with other Machine Learning tools
(Artificial Neural Networks), which allow taking into consideration
environmental factors that can be directly deduced from an experiment with
Computer Vision techniques. This new model, tested on simulated and real cell
migration cases, predicts complete cell trajectories following a roll-out
policy with a high level of accuracy. This work also includes a mitosis event
prediction model based on Neural Networks architectures which makes use of the
same observed features.

</details>


### [69] [Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding](https://arxiv.org/abs/2509.05431)
*GodsGift Uzor,Tania-Amanda Nkoyo Fredrick Eneye,Chukwuebuka Ijezue*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的解码器EMCAD，用于脑肿瘤分割，旨在优化性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分析中，脑肿瘤分割是一个关键的预处理步骤，尤其是在计算资源有限的情况下，需要高效的解码机制。然而，这些解码机制通常计算成本很高。

Method: 利用一种新的高效多尺度卷积注意力解码器EMCAD，在BraTs2020数据集上进行脑肿瘤分割。

Result: 该模型获得了0.31的最佳Dice评分，并在整个训练过程中保持了0.285 +/- 0.015的稳定平均Dice评分，性能适中。初始模型在验证集上保持了一致的性能，没有出现过拟合的迹象。

Conclusion: EMCAD解码器在脑肿瘤分割任务中表现出一定的效率和稳定性，但Dice评分表明仍有提升空间。

Abstract: Brain tumor segmentation is a critical pre-processing step in the medical
image analysis pipeline that involves precise delineation of tumor regions from
healthy brain tissue in medical imaging data, particularly MRI scans. An
efficient and effective decoding mechanism is crucial in brain tumor
segmentation especially in scenarios with limited computational resources.
However these decoding mechanisms usually come with high computational costs.
To address this concern EMCAD a new efficient multi-scale convolutional
attention decoder designed was utilized to optimize both performance and
computational efficiency for brain tumor segmentation on the BraTs2020 dataset
consisting of MRI scans from 369 brain tumor patients. The preliminary result
obtained by the model achieved a best Dice score of 0.31 and maintained a
stable mean Dice score of 0.285 plus/minus 0.015 throughout the training
process which is moderate. The initial model maintained consistent performance
across the validation set without showing signs of over-fitting.

</details>


### [70] [FAVAE-Effective Frequency Aware Latent Tokenizer](https://arxiv.org/abs/2509.05441)
*Tejaswini Medi,Hsien-Yi Wang,Arianna Rampini,Margret Keuper*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于小波的、频率敏感的变分自编码器 (FA-VAE) 框架，用于解决现有潜在 tokenizer 在重建图像时缺乏真实感的问题，尤其是在具有清晰过渡的纹理区域。


<details>
  <summary>Details</summary>
Motivation: 现有技术在图像合成方面取得了显著进展，但重建的图像通常缺乏真实感，尤其是在具有清晰过渡的纹理区域，这是由于高频控制的精细细节丢失所致。

Method: 本文提出了一种基于小波的、频率敏感的变分自编码器 (FA-VAE) 框架，该框架明确地解耦了低频和高频分量的优化。

Result: 该方法能够更好地重建精细纹理，同时保留全局结构，缩小了当前潜在 tokenizer 中的保真度差距。

Conclusion: 本文强调了频率感知优化对于真实图像表示的重要性，并对内容创建、神经渲染和医学成像等应用具有更广泛的意义。

Abstract: Latent generative models have shown remarkable progress in high-fidelity
image synthesis, typically using a two-stage training process that involves
compressing images into latent embeddings via learned tokenizers in the first
stage. The quality of generation strongly depends on how expressive and
well-optimized these latent embeddings are. While various methods have been
proposed to learn effective latent representations, the reconstructed images
often lack realism, particularly in textured regions with sharp transitions,
due to loss of fine details governed by high frequencies. We conduct a detailed
frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers
and show that conventional objectives inherently prioritize low-frequency
reconstruction, often at the expense of high-frequency fidelity. Our analysis
reveals these latent tokenizers exhibit a bias toward low-frequency
information, when jointly optimized, leading to over-smoothed outputs and
visual artifacts that diminish perceptual quality. To address this, we propose
a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework
that explicitly decouples the optimization of low- and high-frequency
components. This decoupling enables improved reconstruction of fine textures
while preserving global structure. Our approach bridges the fidelity gap in
current latent tokenizers and emphasizes the importance of frequency-aware
optimization for realistic image representation, with broader implications for
applications in content creation, neural rendering, and medical imaging.

</details>


### [71] [Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's](https://arxiv.org/abs/2509.05446)
*Iftekhar Haider Chowdhury,Zaed Ikbal Syed,Ahmed Faizul Haque Dhrubo,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: 提出了一种新颖的单次滤波器剪枝框架，通过融合梯度敏感性、一阶泰勒展开和激活分布的KL散度之间的差异来计算每个滤波器的微分敏感度分数。


<details>
  <summary>Details</summary>
Motivation: 深度卷积神经网络在各种计算机视觉任务中取得了最先进的性能，但它们的实际部署受到计算和内存开销的限制。

Method: 通过融合梯度敏感性、一阶泰勒展开和激活分布的KL散度之间的差异，为每个滤波器计算一个微分敏感度分数。应用指数缩放机制来强调在不同指标中重要性不一致的滤波器。

Result: 在 50% 到 70% 的不同剪枝率下进行的大量实验表明，微分敏感度融合剪枝显着降低了模型复杂性，在保持高精度的同时，实现了超过 80% 的每秒浮点运算次数的减少。例如，在 70% 的剪枝率下，我们的方法保留了高达 98.23% 的基线精度，在压缩和泛化方面都超过了传统的启发式方法。

Conclusion: 所提出的方法为可扩展和自适应的深度卷积神经网络压缩提供了一种有效的解决方案，为在边缘和移动平台上高效部署铺平了道路。

Abstract: Deep Convolutional Neural Networks have achieved state of the art performance
across various computer vision tasks, however their practical deployment is
limited by computational and memory overhead. This paper introduces
Differential Sensitivity Fusion Pruning, a novel single shot filter pruning
framework that focuses on evaluating the stability and redundancy of filter
importance scores across multiple criteria. Differential Sensitivity Fusion
Pruning computes a differential sensitivity score for each filter by fusing the
discrepancies among gradient based sensitivity, first order Taylor expansion,
and KL divergence of activation distributions. An exponential scaling mechanism
is applied to emphasize filters with inconsistent importance across metrics,
identifying candidates that are structurally unstable or less critical to the
model performance. Unlike iterative or reinforcement learning based pruning
strategies, Differential Sensitivity Fusion Pruning is efficient and
deterministic, requiring only a single forward-backward pass for scoring and
pruning. Extensive experiments across varying pruning rates between 50 to 70
percent demonstrate that Differential Sensitivity Fusion Pruning significantly
reduces model complexity, achieving over 80 percent Floating point Operations
Per Seconds reduction while maintaining high accuracy. For instance, at 70
percent pruning, our approach retains up to 98.23 percent of baseline accuracy,
surpassing traditional heuristics in both compression and generalization. The
proposed method presents an effective solution for scalable and adaptive Deep
Convolutional Neural Networks compression, paving the way for efficient
deployment on edge and mobile platforms.

</details>


### [72] [Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging](https://arxiv.org/abs/2509.05483)
*Jinhao Wang,Florian Vogl,Pascal Schütz,Saša Ćuković,William R. Taylor*

Main category: cs.CV

TL;DR: Veriserum是一个用于训练深度学习的开源数据集，用于双平面荧光镜分析，包含11万张X光图像，10个膝关节植入物组合，用于日常活动姿势。


<details>
  <summary>Details</summary>
Motivation: 旨在支持2D/3D图像配准、图像分割、X射线畸变校正和3D重建等应用的发展，通过提供可重复的基准来推进计算机视觉和医学成像研究。

Method: 包含大约11万张X射线图像，这些图像是在1600次试验中捕获的，包含与日常活动相关的姿势，例如水平步态和斜坡下降。每张图像都用自动配准的地面实况姿势进行注释，而200张图像包括手动配准的姿势用于基准测试。

Result: 包含双平面图像和校准工具等关键特征。旨在支持算法开发和评估的可重复基准。

Conclusion: Veriserum是一个免费访问的数据集，旨在通过为算法开发和评估提供可重复的基准来推进计算机视觉和医学成像研究。

Abstract: Veriserum is an open-source dataset designed to support the training of deep
learning registration for dual-plane fluoroscopic analysis. It comprises
approximately 110,000 X-ray images of 10 knee implant pair combinations (2
femur and 5 tibia implants) captured during 1,600 trials, incorporating poses
associated with daily activities such as level gait and ramp descent. Each
image is annotated with an automatically registered ground-truth pose, while
200 images include manually registered poses for benchmarking.
  Key features of Veriserum include dual-plane images and calibration tools.
The dataset aims to support the development of applications such as 2D/3D image
registration, image segmentation, X-ray distortion correction, and 3D
reconstruction. Freely accessible, Veriserum aims to advance computer vision
and medical imaging research by providing a reproducible benchmark for
algorithm development and evaluation. The Veriserum dataset used in this study
is publicly available via
https://movement.ethz.ch/data-repository/veriserum.html, with the data stored
at ETH Z\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.

</details>


### [73] [An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures](https://arxiv.org/abs/2509.05490)
*Andrzej D. Dobrzycki,Ana M. Bernardos,José R. Casar*

Main category: cs.CV

TL;DR: 该研究探索了在资源受限环境中使用YOLOv8和YOLOv10进行目标检测时，层冻结策略对迁移学习的影响。


<details>
  <summary>Details</summary>
Motivation: 在无人机等资源受限环境中部署YOLO需要高效的迁移学习，但不同冻结配置对YOLOv8和YOLOv10的影响尚不明确。

Method: 通过在YOLOv8和YOLOv10上使用多种冻结配置，并在四个具有挑战性的数据集上进行实验，结合梯度行为分析和可视化解释。

Result: 研究表明，不存在通用的最佳冻结策略，最佳策略取决于数据属性。冻结主干网络可有效保留通用特征，而较浅的冻结更适合处理极端类别不平衡。与完全微调相比，这些配置最多可减少28%的GPU内存消耗，并且在某些情况下，可以实现超过完全微调的平均精度均值（mAP@50）分数。

Conclusion: 该研究为选择冻结策略提供了经验性发现和实践指南，为资源有限场景中的目标检测提供了一种实用的、基于证据的平衡迁移学习方法。

Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object
detection. However, deploying it in resource-constrained environments such as
unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although
layer freezing is a common technique, the specific impact of various freezing
configurations on contemporary YOLOv8 and YOLOv10 architectures remains
unexplored, particularly with regard to the interplay between freezing depth,
dataset characteristics, and training dynamics. This research addresses this
gap by presenting a detailed analysis of layer-freezing strategies. We
systematically investigate multiple freezing configurations across YOLOv8 and
YOLOv10 variants using four challenging datasets that represent critical
infrastructure monitoring. Our methodology integrates a gradient behavior
analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper
insights into training dynamics under different freezing strategies. Our
results reveal that there is no universal optimal freezing strategy but,
rather, one that depends on the properties of the data. For example, freezing
the backbone is effective for preserving general-purpose features, while a
shallower freeze is better suited to handling extreme class imbalance. These
configurations reduce graphics processing unit (GPU) memory consumption by up
to 28% compared to full fine-tuning and, in some cases, achieve mean average
precision (mAP@50) scores that surpass those of full fine-tuning. Gradient
analysis corroborates these findings, showing distinct convergence patterns for
moderately frozen models. Ultimately, this work provides empirical findings and
practical guidelines for selecting freezing strategies. It offers a practical,
evidence-based approach to balanced transfer learning for object detection in
scenarios with limited resources.

</details>


### [74] [Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection](https://arxiv.org/abs/2509.05512)
*Bryce Grant,Peng Wang*

Main category: cs.CV

TL;DR: 提出了一种新的深度学习框架，利用四元数代数进行旋转等变图像分类和目标检测。


<details>
  <summary>Details</summary>
Motivation: 传统四元数神经网络试图完全在四元数域中操作，但计算效率不高。

Method: 通过使用实值运算的汉密尔顿积分解来近似四元数卷积，并引入独立四元数批归一化（IQBN）以提高训练稳定性，并将四元数运算扩展到空间注意力机制。

Result: 在图像分类任务中，QUAN以更少的参数和更快的收敛速度实现了更高的精度。对于目标检测，QUAN 比标准 CNN 表现出更高的参数效率和旋转处理能力。

Conclusion: QUAN 在资源受限的机器人系统中具有部署潜力，需要在其他领域进行具有旋转感知能力的感知和应用。

Abstract: This paper introduces Quaternion Approximate Networks (QUAN), a novel deep
learning framework that leverages quaternion algebra for rotation equivariant
image classification and object detection. Unlike conventional quaternion
neural networks attempting to operate entirely in the quaternion domain, QUAN
approximates quaternion convolution through Hamilton product decomposition
using real-valued operations. This approach preserves geometric properties
while enabling efficient implementation with custom CUDA kernels. We introduce
Independent Quaternion Batch Normalization (IQBN) for training stability and
extend quaternion operations to spatial attention mechanisms. QUAN is evaluated
on image classification (CIFAR-10/100, ImageNet), object detection (COCO,
DOTA), and robotic perception tasks. In classification tasks, QUAN achieves
higher accuracy with fewer parameters and faster convergence compared to
existing convolution and quaternion-based models. For objection detection, QUAN
demonstrates improved parameter efficiency and rotation handling over standard
Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion
CNNs in this downstream task. These results highlight its potential for
deployment in resource-constrained robotic systems requiring rotation-aware
perception and application in other domains.

</details>


### [75] [OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation](https://arxiv.org/abs/2509.05513)
*Ahad Jawaid,Yu Xiang*

Main category: cs.CV

TL;DR: OpenEgo是一个大规模的自中心操作数据集，包含标准化的手部姿势注释和与意图对齐的动作原语，旨在降低从自中心视频中学习灵巧操作的门槛。


<details>
  <summary>Details</summary>
Motivation: 现有的自中心人体视频语料库通常缺乏细粒度的、时间局部化的动作描述或灵巧的手部注释。

Method: OpenEgo统一了手部姿势布局，并提供描述性的、带有时间戳的动作原语。使用语言条件模仿学习策略来预测灵巧的手部轨迹。

Result: OpenEgo总计1107小时，跨越六个公共数据集，涵盖600多个环境中的290个操作任务。

Conclusion: OpenEgo旨在降低从自中心视频中学习灵巧操作的门槛，并支持视觉-语言-动作学习中的可重复研究。

Abstract: Egocentric human videos provide scalable demonstrations for imitation
learning, but existing corpora often lack either fine-grained, temporally
localized action descriptions or dexterous hand annotations. We introduce
OpenEgo, a multimodal egocentric manipulation dataset with standardized
hand-pose annotations and intention-aligned action primitives. OpenEgo totals
1107 hours across six public datasets, covering 290 manipulation tasks in 600+
environments. We unify hand-pose layouts and provide descriptive, timestamped
action primitives. To validate its utility, we train language-conditioned
imitation-learning policies to predict dexterous hand trajectories. OpenEgo is
designed to lower the barrier to learning dexterous manipulation from
egocentric video and to support reproducible research in vision-language-action
learning. All resources and instructions will be released at
www.openegocentric.com.

</details>


### [76] [Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2509.05515)
*Sen Wang,Kunyi Li,Siyun Liang,Elena Alegret,Jing Ma,Nassir Navab,Stefano Gasperini*

Main category: cs.CV

TL;DR: 本文提出了一种名为VALA的轻量级有效方法，用于将2D图像中的开放词汇语言特征提炼成3D高斯分布，从而改进基于语言的3D场景交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个基本问题：背景高斯对渲染像素的贡献可以忽略不计，但获得与主要前景高斯相同的特征；由于语言嵌入中存在特定于视角的噪声，导致多视角不一致。

Method: 提出了可视性感知语言聚合（VALA），该方法计算每条射线的边际贡献，并应用可视性感知门来仅保留可见的高斯分布。此外，提出了一种余弦空间中的流式加权几何中位数，以合并噪声多视角特征。

Result: VALA在参考数据集上改进了开放词汇定位和分割，始终超越现有工作。

Conclusion: VALA以快速且节省内存的方式生成了鲁棒、视角一致的语言特征嵌入。

Abstract: Recently, distilling open-vocabulary language features from 2D images into 3D
Gaussians has attracted significant attention. Although existing methods
achieve impressive language-based interactions of 3D scenes, we observe two
fundamental issues: background Gaussians contributing negligibly to a rendered
pixel get the same feature as the dominant foreground ones, and multi-view
inconsistencies due to view-specific noise in language embeddings. We introduce
Visibility-Aware Language Aggregation (VALA), a lightweight yet effective
method that computes marginal contributions for each ray and applies a
visibility-aware gate to retain only visible Gaussians. Moreover, we propose a
streaming weighted geometric median in cosine space to merge noisy multi-view
features. Our method yields a robust, view-consistent language feature
embedding in a fast and memory-efficient manner. VALA improves open-vocabulary
localization and segmentation across reference datasets, consistently
surpassing existing works.

</details>


### [77] [DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation](https://arxiv.org/abs/2509.05543)
*Haitao Tian,Pierre Payeur*

Main category: cs.CV

TL;DR: 本文提出了一种对比表示学习框架，通过使用修剪的（单一动作）骨骼序列进行预训练来增强人类动作分割。


<details>
  <summary>Details</summary>
Motivation: 与以往为动作识别量身定制并建立在孤立的序列表示基础上的表示学习工作不同，本文提出的框架侧重于结合跨序列变化来利用多尺度表示。

Method: 该方法提出了一种新的数据增强策略“Shuffle and Warp”，利用不同的多动作排列。后者有效地辅助了对比学习中引入的两个替代任务：交叉排列对比（CPC）和相对顺序推理（ROR）。

Result: DuoCLR 在一个修剪过的骨骼数据集上进行预训练，并在一个未修剪的数据集上进行评估，在多类和多标签动作分割任务中，DuoCLR 显示出比最先进的比较方法有显着提升。

Conclusion: 消融研究用于评估所提出方法的每个组成部分的有效性。

Abstract: In this paper, a contrastive representation learning framework is proposed to
enhance human action segmentation via pre-training using trimmed (single
action) skeleton sequences. Unlike previous representation learning works that
are tailored for action recognition and that build upon isolated sequence-wise
representations, the proposed framework focuses on exploiting multi-scale
representations in conjunction with cross-sequence variations. More
specifically, it proposes a novel data augmentation strategy, 'Shuffle and
Warp', which exploits diverse multi-action permutations. The latter effectively
assists two surrogate tasks that are introduced in contrastive learning: Cross
Permutation Contrasting (CPC) and Relative Order Reasoning (ROR). In
optimization, CPC learns intra-class similarities by contrasting
representations of the same action class across different permutations, while
ROR reasons about inter-class contexts by predicting relative mapping between
two permutations. Together, these tasks enable a Dual-Surrogate Contrastive
Learning (DuoCLR) network to learn multi-scale feature representations
optimized for action segmentation. In experiments, DuoCLR is pre-trained on a
trimmed skeleton dataset and evaluated on an untrimmed dataset where it
demonstrates a significant boost over state-the-art comparatives in both
multi-class and multi-label action segmentation tasks. Lastly, ablation studies
are conducted to evaluate the effectiveness of each component of the proposed
approach.

</details>


### [78] [Sensitivity-Aware Post-Training Quantization for Deep Neural Networks](https://arxiv.org/abs/2509.05576)
*Zekang Zheng,Haokun Li,Yaofo Chen,Mingkui Tan,Qing Du*

Main category: cs.CV

TL;DR: 提出了一种高效的训练后量化（PTQ）方法，该方法通过参数敏感性分析指导量化过程，在保证精度的前提下，显著提高了量化速度。


<details>
  <summary>Details</summary>
Motivation: 现有的PTQ方法为了在高压缩比下保持精度，需要进行迭代参数更新，导致计算复杂度和资源开销大，限制了其在资源受限的边缘计算和实时推理场景中的应用。

Method: 该方法优先量化高敏感度参数，并利用未量化的低敏感度参数来补偿量化误差，从而减轻精度下降。此外，通过利用参数敏感性的列向聚类，该方法引入了一种具有全局共享逆 Hessian 矩阵更新机制的行并行量化框架，从而降低了计算复杂度。

Result: 在ResNet-50和YOLOv5s上的实验结果表明，该方法比Optimal Brain Quantization基线快20-200倍，平均精度损失低于0.3%，证实了该方法在效率和精度之间取得平衡的有效性。

Conclusion: 该论文提出了一种有效的PTQ方法，可以在保证精度的前提下，显著提高量化速度，适用于资源受限的场景。

Abstract: Model quantization reduces neural network parameter precision to achieve
compression, but often compromises accuracy. Existing post-training
quantization (PTQ) methods employ iterative parameter updates to preserve
accuracy under high compression ratios, incurring significant computational
complexity and resource overhead, which limits applicability in
resource-constrained edge computing and real-time inference scenarios. This
paper proposes an efficient PTQ method guided by parameter sensitivity
analysis. The approach prioritizes quantization of high-sensitivity parameters,
leveraging unquantized low-sensitivity parameters to compensate for
quantization errors, thereby mitigating accuracy degradation. Furthermore, by
exploiting column-wise clustering of parameter sensitivity, the method
introduces a row-parallel quantization framework with a globally shared inverse
Hessian matrix update mechanism, reducing computational complexity by an order
of magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a
20-200-fold quantization speedup over the Optimal Brain Quantization baseline,
with mean accuracy loss below 0.3%, confirming the method's efficacy in
balancing efficiency and accuracy.

</details>


### [79] [Reconstruction and Reenactment Separated Method for Realistic Gaussian Head](https://arxiv.org/abs/2509.05582)
*Zhiling Ye,Cong Zhou,Xiubao Zhang,Haifeng Shen,Weihong Deng,Quan Lu*

Main category: cs.CV

TL;DR: 提出了一种新的3D高斯头部重建和重演框架，仅需单张人像即可生成可控头像。


<details>
  <summary>Details</summary>
Motivation: 旨在解决单张图像生成可控头像的问题。

Method: 构建了一个基于WebSSL的大规模单次高斯头部生成器，并采用两阶段训练方法以增强泛化能力和高频纹理重建。

Result: 实现了超轻量级高斯头像的控制信号驱动，能够在512x512分辨率下达到90 FPS的高帧率渲染。参数规模的增加能够提高性能，且驱动效率不受影响。

Conclusion: 实验结果表明该方法优于当前最佳方法。

Abstract: In this paper, we explore a reconstruction and reenactment separated
framework for 3D Gaussians head, which requires only a single portrait image as
input to generate controllable avatar. Specifically, we developed a large-scale
one-shot gaussian head generator built upon WebSSL and employed a two-stage
training approach that significantly enhances the capabilities of
generalization and high-frequency texture reconstruction. During inference, an
ultra-lightweight gaussian avatar driven by control signals enables high
frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further
demonstrate that the proposed framework follows the scaling law, whereby
increasing the parameter scale of the reconstruction module leads to improved
performance. Moreover, thanks to the separation design, driving efficiency
remains unaffected. Finally, extensive quantitative and qualitative experiments
validate that our approach outperforms current state-of-the-art methods.

</details>


### [80] [MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios](https://arxiv.org/abs/2509.05592)
*Changtao Miao,Yi Zhang,Man Luo,Weiwei Feng,Kaiyuan Zheng,Qi Chu,Tao Gong,Jianshu Li,Yunfeng Diao,Wei Zhou,Joey Tianyi Zhou,Xiaoshuai Hao*

Main category: cs.CV

TL;DR: 提出了一个更贴近现实场景的Deepfake检测数据集MFFI，它在伪造方法、面部场景、真实数据多样性和多层次降级操作四个维度上增强了真实性。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测方法受限于数据集的多样性不足，无法很好地适应真实世界的场景，尤其是在伪造技术未知、面部场景变化、真实数据丰富性以及真实世界传播的退化等方面。

Method: 提出了多维面部伪造图像（MFFI）数据集，该数据集包含50种不同的伪造方法和1024K图像样本，并在四个维度上增强了真实性：更广泛的伪造方法、不同的面部场景、多样化的真实数据和多层次的降级操作。

Result: 基准评估表明，MFFI在场景复杂性、跨域泛化能力和检测难度梯度方面优于现有的公共数据集。

Conclusion: MFFI数据集的技术进步和在模拟真实世界条件下的实用性得到了验证，该数据集已公开。

Abstract: Rapid advances in Artificial Intelligence Generated Content (AIGC) have
enabled increasingly sophisticated face forgeries, posing a significant threat
to social security. However, current Deepfake detection methods are limited by
constraints in existing datasets, which lack the diversity necessary in
real-world scenarios. Specifically, these data sets fall short in four key
areas: unknown of advanced forgery techniques, variability of facial scenes,
richness of real data, and degradation of real-world propagation. To address
these challenges, we propose the Multi-dimensional Face Forgery Image
(\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances
realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied
Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation
Operations. MFFI integrates $50$ different forgery methods and contains $1024K$
image samples. Benchmark evaluations show that MFFI outperforms existing public
datasets in terms of scene complexity, cross-domain generalization capability,
and detection difficulty gradients. These results validate the technical
advance and practical utility of MFFI in simulating real-world conditions. The
dataset and additional details are publicly available at
{https://github.com/inclusionConf/MFFI}.

</details>


### [81] [Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization](https://arxiv.org/abs/2509.05604)
*Jungin Park,Jiyoung Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: 本研究提出了一种新的视频摘要方法，它将视频摘要视为一个语言引导的时空图建模问题。该方法构建了一个递归时空图网络，称为VideoGraph，它将对象和帧分别表示为空间和时间图的节点。为了防止边被配置为视觉相似性，该方法将从视频中导出的语言查询合并到图节点表示中，使它们能够包含语义知识。此外，该方法采用了一种递归策略来细化初始图，并将每个帧节点正确分类为关键帧。


<details>
  <summary>Details</summary>
Motivation: 以前的方法侧重于通过时间建模来建立视频中帧之间的全局互连性。然而，细粒度的视觉实体，如对象，也与视频的主要内容高度相关。此外，最近研究的语言引导视频摘要需要对复杂的真实世界视频进行全面的语言理解。

Method: 该方法提出了一种递归时空图网络，称为VideoGraph，它将对象和帧分别表示为空间和时间图的节点。图中的节点通过图边连接和聚合，表示节点之间的语义关系。为了防止边被配置为视觉相似性，该方法将从视频中导出的语言查询合并到图节点表示中，使它们能够包含语义知识。此外，该方法采用了一种递归策略来细化初始图，并将每个帧节点正确分类为关键帧。

Result: 在通用和query-focused的视频摘要的多个基准测试中，VideoGraph取得了state-of-the-art的性能，包括监督和非监督的方式。

Conclusion: 本文提出了一种新的视频摘要方法，它将视频摘要视为一个语言引导的时空图建模问题。实验结果表明，该方法在通用和query-focused的视频摘要的多个基准测试中取得了state-of-the-art的性能。

Abstract: Video summarization aims to select keyframes that are visually diverse and
can represent the whole story of a given video. Previous approaches have
focused on global interlinkability between frames in a video by temporal
modeling. However, fine-grained visual entities, such as objects, are also
highly related to the main content of the video. Moreover, language-guided
video summarization, which has recently been studied, requires a comprehensive
linguistic understanding of complex real-world videos. To consider how all the
objects are semantically related to each other, this paper regards video
summarization as a language-guided spatiotemporal graph modeling problem. We
present recursive spatiotemporal graph networks, called VideoGraph, which
formulate the objects and frames as nodes of the spatial and temporal graphs,
respectively. The nodes in each graph are connected and aggregated with graph
edges, representing the semantic relationships between the nodes. To prevent
the edges from being configured with visual similarity, we incorporate language
queries derived from the video into the graph node representations, enabling
them to contain semantic knowledge. In addition, we adopt a recursive strategy
to refine initial graphs and correctly classify each frame node as a keyframe.
In our experiments, VideoGraph achieves state-of-the-art performance on several
benchmarks for generic and query-focused video summarization in both supervised
and unsupervised manners. The code is available at
https://github.com/park-jungin/videograph.

</details>


### [82] [Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning](https://arxiv.org/abs/2509.05606)
*Juan Yeo,Ijun Jang,Taesup Kim*

Main category: cs.CV

TL;DR: 本文提出了一种自监督学习框架，通过额外的自监督学习在密集特征空间中迁移现有的语义知识，从而克服了现有方法在捕获局部语义方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督表示学习方法侧重于总结图像整体的全局表示，但在捕获密集预测任务所需的局部语义方面存在不足。

Method: 该方法通过对齐教师模型和学生模型之间的密集特征分布来实现知识迁移。具体来说，引入了Patch-level Kernel Alignment (PaKA)，这是一种简单而有效的对齐目标，可以捕获统计依赖关系，从而匹配两个模型之间密集patch的结构关系。此外，还研究了专门为密集表示学习设计的增强策略。

Result: 该框架在一系列密集视觉基准测试中取得了最先进的结果。

Conclusion: 该方法是有效的。

Abstract: Dense representations are essential for vision tasks that require spatial
precision and fine-grained detail. While most self-supervised representation
learning methods focus on global representations that summarize the image as a
whole, such approaches often fall short in capturing the localized semantics
necessary for dense prediction tasks. To overcome these limitations, we propose
a framework that builds on pretrained representations through additional
self-supervised learning, aiming to transfer existing semantic knowledge into
the dense feature space. Our method aligns the distributions of dense features
between a teacher and a student model. Specifically, we introduce Patch-level
Kernel Alignment (PaKA), a simple yet effective alignment objective that
captures statistical dependencies, thereby matching the structural
relationships of dense patches across the two models. In addition, we
investigate augmentation strategies specifically designed for dense
representation learning. Our framework achieves state-of-the-art results across
a variety of dense vision benchmarks, demonstrating the effectiveness of our
approach.

</details>


### [83] [SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](https://arxiv.org/abs/2509.05614)
*Hanzhen Wang,Jiaming Xu,Jiayi Pan,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecPrune-VLA: A training-free pruning method for Vision-Language-Action (VLA) models that uses both local and global information for smarter token selection.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for VLA models only use local information, ignoring global context from prior actions, leading to performance drops and limited speedup.

Method: SpecPrune-VLA introduces a two-level pruning approach: static pruning at the action level using global history and local context, and dynamic pruning at the layer level based on layer-specific importance. It also uses a lightweight action-aware controller to adjust pruning aggressiveness based on action granularity.

Result: SpecPrune-VLA achieves 1.46x speedup on NVIDIA A800 and 1.57x speedup on NVIDIA GeForce RTX 3090 compared to OpenVLA-OFT, with negligible success rate loss on the LIBERO dataset.

Conclusion: SpecPrune-VLA effectively leverages both local and global information for token selection, achieving significant speedups with minimal performance loss in VLA models.

Abstract: Pruning accelerates compute-bound models by reducing computation. Recently
applied to Vision-Language-Action (VLA) models, existing methods prune tokens
using only local info from current action, ignoring global context from prior
actions, causing >20% success rate drop and limited speedup. We observe high
similarity across consecutive actions and propose leveraging both local
(current) and global (past) info for smarter token selection. We introduce
SpecPrune-VLA, a training-free method with two-level pruning and heuristic
control: (1) Static pruning at action level: uses global history and local
context to reduce visual tokens per action; (2) Dynamic pruning at layer level:
prunes tokens per layer based on layer-specific importance; (3) Lightweight
action-aware controller: classifies actions as coarse/fine-grained (by speed),
adjusting pruning aggressiveness since fine-grained actions are
pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times
speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.
OpenVLA-OFT, with negligible success rate loss.

</details>


### [84] [SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.05625)
*Kien Nguyen,Anh Tran,Cuong Pham*

Main category: cs.CV

TL;DR: 提出了一种名为Subspace Mapping (SuMa) 的新方法，旨在稳健且有效地消除扩散模型中的特定概念，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的概念消除方法在稳健性和有效性之间难以兼顾，尤其是在处理版权人物或名人等窄概念时效果不佳。消除这些窄概念对于解决版权和法律问题至关重要，但由于它们与非目标概念距离较近，因此需要更精细的操作。

Method: 首先推导出一个代表要擦除概念的目标子空间，然后通过将其映射到参考子空间来中和它，从而最小化两者之间的距离。

Result: 在四个任务（子类擦除、名人擦除、艺术风格擦除和实例擦除）上的大量实验表明，该方法在图像质量上与注重有效性的方法相当，同时在完整性方面也与目标方法相当。

Conclusion: SuMa 是一种有效的方法，可以在消除窄概念的同时保持图像质量。

Abstract: The rapid growth of text-to-image diffusion models has raised concerns about
their potential misuse in generating harmful or unauthorized contents. To
address these issues, several Concept Erasure methods have been proposed.
However, most of them fail to achieve both robustness, i.e., the ability to
robustly remove the target concept., and effectiveness, i.e., maintaining image
quality. While few recent techniques successfully achieve these goals for NSFW
concepts, none could handle narrow concepts such as copyrighted characters or
celebrities. Erasing these narrow concepts is critical in addressing copyright
and legal concerns. However, erasing them is challenging due to their close
distances to non-target neighboring concepts, requiring finer-grained
manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel
method specifically designed to achieve both robustness and effectiveness in
easing these narrow concepts. SuMa first derives a target subspace representing
the concept to be erased and then neutralizes it by mapping it to a reference
subspace that minimizes the distance between the two. This mapping ensures the
target concept is robustly erased while preserving image quality. We conduct
extensive experiments with SuMa across four tasks: subclass erasure, celebrity
erasure, artistic style erasure, and instance erasure and compare the results
with current state-of-the-art methods. Our method achieves image quality
comparable to approaches focused on effectiveness, while also yielding results
that are on par with methods targeting completeness.

</details>


### [85] [Self-supervised Learning for Hyperspectral Images of Trees](https://arxiv.org/abs/2509.05630)
*Moqsadur Rahman,Saurav Kumar,Santosh S. Palmate,M. Shahriar Hossain*

Main category: cs.CV

TL;DR: 本文利用高光谱图像进行精准农业研究，重点在于无标签或少标签情况下的图像分析。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分析在无标签或少标签情况下具有挑战性。

Method: 采用自监督学习创建神经网络嵌入，反映作物田地树木的植被特性。

Result: 构建的树木表征在使用植被属性相关嵌入空间时，在下游机器学习任务中表现更好。

Conclusion: 自监督学习方法能够有效提升高光谱图像在精准农业中的应用效果。

Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a
critical impetus to precision agriculture. Analysis of the hyperspectral images
with limited or no labels is challenging. This paper focuses on self-supervised
learning to create neural network embeddings reflecting vegetation properties
of trees from aerial hyperspectral images of crop fields. Experimental results
demonstrate that a constructed tree representation, using a vegetation
property-related embedding space, performs better in downstream machine
learning tasks compared to the direct use of hyperspectral vegetation
properties as tree representations.

</details>


### [86] [Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh](https://arxiv.org/abs/2509.05652)
*Ha Meem Hossain,Pritam Nath,Mahitun Nesa Mahi,Imtiaz Uddin,Ishrat Jahan Eiste,Syed Nasibur Rahman Ratul,Md Naim Uddin Mozumdar,Asif Mohammed Saad*

Main category: cs.CV

TL;DR: 针对孟加拉国独特道路环境，在非孟加拉国数据集上训练的车辆检测系统难以准确识别当地车辆类型，从而在发展中地区的自动驾驶技术中造成严重差距。本研究评估了六种 YOLO 模型变体在自定义数据集上的性能，该数据集包含 29 个不同的车辆类别，包括区域特定车辆，例如``Desi Nosimon''、``Leguna''、``Battery Rickshaw''和``CNG''。


<details>
  <summary>Details</summary>
Motivation: 在非孟加拉国数据集上训练的车辆检测系统难以准确识别孟加拉国当地车辆类型，从而在发展中地区的自动驾驶技术中造成严重差距。

Method: 使用手机摄像头在各种孟加拉国道路上捕获高分辨率图像 (1920x1080)，并使用 LabelImg 以 YOLO 格式边界框手动注释。

Result: YOLOv11x 表现最佳，达到 63.7% mAP@0.5、43.8% mAP@0.5:0.95、61.4% 召回率和 61.6% F1 分数，但每张图像的推理需要 45.8 毫秒。中型变体（YOLOv8m、YOLOv11m）实现了最佳平衡，提供了强大的检测性能，mAP@0.5 值分别为 62.5% 和 61.8%，同时保持了 14-15 毫秒左右的适度推理时间。

Conclusion: 这项研究为开发专门适用于孟加拉国交通状况的稳健对象检测系统奠定了基础，解决了在传统通用训练模型无法充分发挥作用的发展中地区自动驾驶汽车技术进步的关键需求。

Abstract: Vehicle detection systems trained on Non-Bangladeshi datasets struggle to
accurately identify local vehicle types in Bangladesh's unique road
environments, creating critical gaps in autonomous driving technology for
developing regions. This study evaluates six YOLO model variants on a custom
dataset featuring 29 distinct vehicle classes, including region-specific
vehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and
``CNG''. The dataset comprises high-resolution images (1920x1080) captured
across various Bangladeshi roads using mobile phone cameras and manually
annotated using LabelImg with YOLO format bounding boxes. Performance
evaluation revealed YOLOv11x as the top performer, achieving 63.7\% mAP@0.5,
43.8\% mAP@0.5:0.95, 61.4\% recall, and 61.6\% F1-score, though requiring 45.8
milliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m)
struck an optimal balance, delivering robust detection performance with mAP@0.5
values of 62.5\% and 61.8\% respectively, while maintaining moderate inference
times around 14-15 milliseconds. The study identified significant detection
challenges for rare vehicle classes, with Construction Vehicles and Desi
Nosimons showing near-zero accuracy due to dataset imbalances and insufficient
training samples. Confusion matrices revealed frequent misclassifications
between visually similar vehicles, particularly Mini Trucks versus Mini Covered
Vans. This research provides a foundation for developing robust object
detection systems specifically adapted to Bangladesh traffic conditions,
addressing critical needs in autonomous vehicle technology advancement for
developing regions where conventional generic-trained models fail to perform
adequately.

</details>


### [87] [EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation](https://arxiv.org/abs/2509.05659)
*Guandong Li,Zhaobin Chu*

Main category: cs.CV

TL;DR: EditIDv2 focuses on improving character editing in complex narrative scenes with long text inputs, addressing issues like degraded editing, semantic biases, and identity inconsistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with long, complex narratives, leading to poor editing, semantic misunderstanding, and identity issues.

Method: It uses PerceiverAttention decomposition, ID loss, joint dynamic training with a diffusion model, and an offline fusion strategy.

Result: EditIDv2 achieves deep, multi-level semantic editing while maintaining identity consistency in complex narrative environments with minimal data.

Conclusion: EditIDv2 effectively handles long prompts and generates high-quality images, achieving excellent results in the IBench evaluation.

Abstract: We propose EditIDv2, a tuning-free solution specifically designed for
high-complexity narrative scenes and long text inputs. Existing character
editing methods perform well under simple prompts, but often suffer from
degraded editing capabilities, semantic understanding biases, and identity
consistency breakdowns when faced with long text narratives containing multiple
semantic layers, temporal logic, and complex contextual relationships. In
EditID, we analyzed the impact of the ID integration module on editability. In
EditIDv2, we further explore and address the influence of the ID feature
integration module. The core of EditIDv2 is to discuss the issue of editability
injection under minimal data lubrication. Through a sophisticated decomposition
of PerceiverAttention, the introduction of ID loss and joint dynamic training
with the diffusion model, as well as an offline fusion strategy for the
integration module, we achieve deep, multi-level semantic editing while
maintaining identity consistency in complex narrative environments using only a
small amount of data lubrication. This meets the demands of long prompts and
high-quality image generation, and achieves excellent results in the IBench
evaluation.

</details>


### [88] [OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation](https://arxiv.org/abs/2509.05661)
*Xiaomeng Zhu,Changwei Wang,Haozhe Wang,Xinyu Liu,Fangzhen Lin*

Main category: cs.CV

TL;DR: 提出了一种新的场景图预测方法，该方法利用常识知识，通过解耦场景图捕获和文本预测两个步骤，着重研究了纯文本的场景图预测（LSGA）问题，并提出了面向对象的两阶段方法（OOTSM）。


<details>
  <summary>Details</summary>
Motivation: 现有的场景图预测方法主要依赖视觉线索，难以整合有价值的常识知识，限制了长期预测的鲁棒性。

Method: 首先使用场景图捕获模型将视频剪辑转换为场景图序列，然后使用纯文本模型预测未来帧中的场景图。重点是第二步，即语言场景图预测（LSGA），并提出了面向对象的两阶段方法（OOTSM），其中大型语言模型（LLM）首先预测对象的外观和消失，然后再生成详细的人与对象关系。

Result: 在 LSGA 任务中，对微调的开源 LLM 与零样本 API（即 GPT-4o、GPT-4o-mini 和 DeepSeek-V3）在从 Action Genome 注释构建的基准上进行了评估。对于 SGA，将 OOTSM 与 STTran++ 结合使用，实验表明有效实现了最先进的性能：短期 mean-Recall (@10) 提高了 3.4%，而长期 mean-Recall (@50) 显着提高了 21.9%。

Conclusion: 该研究提出了一种新的场景图预测方法，通过利用常识知识和解耦预测步骤，在长期预测方面取得了显著的性能提升。

Abstract: A scene graph is a structured represention of objects and their relationships
in a scene. Scene Graph Anticipation (SGA) involves predicting future scene
graphs from video clips, enabling applications as intelligent surveillance and
human-machine collaboration. Existing SGA approaches primarily leverage visual
cues, often struggling to integrate valuable commonsense knowledge, thereby
limiting long-term prediction robustness. To explicitly leverage such
commonsense knowledge, we propose a new approach to better understand the
objects, concepts, and relationships in a scene graph. Our approach decouples
the SGA task in two steps: first a scene graph capturing model is used to
convert a video clip into a sequence of scene graphs, then a pure text-based
model is used to predict scene graphs in future frames. Our focus in this work
is on the second step, and we call it Linguistic Scene Graph Anticipation
(LSGA) and believes it should have independent interest beyond the use in SGA
discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method
(OOTSM) where an Large Language Model (LLM) first forecasts object appearances
and disappearances before generating detailed human-object relations. We
conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we
evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o,
GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome
annotations. For SGA, we combine our OOTSM with STTran++ from, and our
experiments demonstrate effective state-of-the-art performance: short-term
mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves
dramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.

</details>


### [89] [WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising](https://arxiv.org/abs/2509.05662)
*Wasikul Islam*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的图像去噪方法，该方法受到高能粒子物理中pileup去除技术的启发，通过将物理先验知识嵌入到神经网络结构中，提高了在强噪声下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在高能粒子物理中，对撞机测量受到“pileup”的污染，这掩盖了人们感兴趣的硬散射信号。受此启发，本文探讨了这些原则如何通过将物理引导的归纳偏差嵌入到神经架构中来为图像去噪提供信息。

Method: 本文介绍了一系列受PU启发的去噪器：具有守恒约束的残差CNN、它的高斯噪声变体，以及加权归纳Pileup物理启发的U-Network用于去噪(WIPUNet)，它将这些思想集成到UNet主干中。

Result: 在$\\sigma\\in\\{15,25,50,75,100\\}$的高斯噪声下的CIFAR-10上，受PU启发的cnn与标准基线具有竞争力，而WIPUNet在更高的噪声下显示出更大的优势。互补的BSD500实验显示了相同的趋势，表明物理启发的先验知识提供了稳定性，而纯粹的数据驱动模型会降低性能。

Conclusion: 本文的贡献是：(i)将pileup缓解原则转化为模块化归纳偏差；(ii)将它们集成到UNet中；(iii)在不依赖重型SOTA机械的情况下，证明了在高噪声下的鲁棒性增益。

Abstract: In high-energy particle physics, collider measurements are contaminated by
"pileup", overlapping soft interactions that obscure the hard-scatter signal of
interest. Dedicated subtraction strategies exploit physical priors such as
conservation, locality, and isolation. Inspired by this analogy, we investigate
how such principles can inform image denoising by embedding physics-guided
inductive biases into neural architectures. This paper is a proof of concept:
rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether
physics-inspired priors improve robustness under strong corruption.
  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with
conservation constraints, its Gaussian-noise variants, and the Weighted
Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which
integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at
$\sigma\in\{15,25,50,75,100\}$, PU-inspired CNNs are competitive with standard
baselines, while WIPUNet shows a \emph{widening margin} at higher noise.
Complementary BSD500 experiments show the same trend, suggesting
physics-inspired priors provide stability where purely data-driven models
degrade. Our contributions are: (i) translating pileup-mitigation principles
into modular inductive biases; (ii) integrating them into UNet; and (iii)
demonstrating robustness gains at high noise without relying on heavy SOTA
machinery.

</details>


### [90] [Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance](https://arxiv.org/abs/2509.05669)
*Weijie Shen,Xinrui Wang,Yuanqi Nie,Apiradee Boonmee*

Main category: cs.CV

TL;DR: 提出了一种新的框架，旨在增强 LVLM 的多轮视觉文本推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型 (LLM) 和视觉语言大型模型 (LVLM) 在单轮任务中表现出色，但在需要深度上下文理解和复杂视觉推理的多轮交互中面临重大挑战，经常导致推理碎片化、上下文丢失和幻觉。

Method: 该框架引入了两个关键创新：视觉文本上下文记忆单元 (VCMU) 和自适应视觉焦点引导 (AVFG) 机制。

Result: 在具有挑战性的数据集（包括 VisDial、经过调整的 A-OKVQA 和我们新颖的多轮指令跟随 (MTIF) 数据集）上进行的大量实验表明，CAMVR 始终能够实现最先进的性能。

Conclusion: CAMVR 能够持续实现最佳性能

Abstract: Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)
excel in single-turn tasks but face significant challenges in multi-turn
interactions requiring deep contextual understanding and complex visual
reasoning, often leading to fragmented reasoning, context loss, and
hallucinations. To address these limitations, we propose Context-Aware
Multi-Turn Visual Reasoning (CAMVR), a novel framework designed to empower
LVLMs with robust and coherent multi-turn visual-textual inference
capabilities. CAMVR introduces two key innovations: a Visual-Textual Context
Memory Unit (VCMU), a dynamic read-write memory network that stores and manages
critical visual features, textual semantic representations, and their
cross-modal correspondences from each interaction turn; and an Adaptive Visual
Focus Guidance (AVFG) mechanism, which leverages the VCMU's context to
dynamically adjust the visual encoder's attention to contextually relevant
image regions. Our multi-level reasoning integration strategy ensures that
response generation is deeply coherent with both current inputs and accumulated
historical context. Extensive experiments on challenging datasets, including
VisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following
(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art
performance.

</details>


### [91] [MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics](https://arxiv.org/abs/2509.05670)
*Gašper Podobnik,Tomaž Vrtovec*

Main category: cs.CV

TL;DR: 图像分割研究激增，但重现性面临危机，评估指标的选择和实施至关重要。距离度量标准实施中的缺陷可能导致开源工具之间的巨大差异。为此，我们引入了 MeshMetrics，它是一个基于网格的框架，可以比传统的基于网格的方法更精确地计算基于距离的度量标准。


<details>
  <summary>Details</summary>
Motivation: 图像分割领域的研究激增，但重现性面临危机，其中评估指标的实施是主要原因。

Method: 我们提出了 MeshMetrics，一个基于网格的框架，可以更精确地计算基于距离的度量标准，优于传统的基于网格的方法。

Result: MeshMetrics 比已建立的工具实现了更高的准确性和精度，并且受离散化伪影的影响更小。

Conclusion: 我们发布了 MeshMetrics，一个开源 Python 包，它提供了一种更精确的距离度量计算方法，经验证比现有工具更准确、更精确。

Abstract: The surge of research in image segmentation has yielded remarkable
performance gains but also exposed a reproducibility crisis. A major
contributor is performance evaluation, where both selection and implementation
of metrics play critical roles. While recent efforts have improved the former,
the reliability of metric implementation has received far less attention.
Pitfalls in distance-based metric implementation can lead to considerable
discrepancies between common open-source tools, for instance, exceeding 100 mm
for the Hausdorff distance and 30%pt for the normalized surface distance for
the same pair of segmentations. To address these pitfalls, we introduce
MeshMetrics, a mesh-based framework that provides a more precise computation of
distance-based metrics than conventional grid-based approaches. Through
theoretical analysis and empirical validation, we demonstrate that MeshMetrics
achieves higher accuracy and precision than established tools, and is
substantially less affected by discretization artifacts, such as distance
quantization. We release MeshMetrics as an open-source Python package,
available at https://github.com/gasperpodobnik/MeshMetrics.

</details>


### [92] [Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization](https://arxiv.org/abs/2509.05695)
*Jingwei Peng,Zhixuan Qiu,Boyu Jin,Surasakdi Siripong*

Main category: cs.CV

TL;DR: 本文提出了一种名为LVLM-VAR的新框架，该框架利用预训练的视觉-语言大模型（LVLM）进行视频动作识别，从而提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理各种视频数据时，在深度语义理解、复杂上下文信息和细粒度区分方面存在局限性。

Method: 该方法采用视频到语义令牌（VST）模块，将原始视频序列转换为离散的、语义和时间上一致的“语义动作令牌”，从而有效地创建LVLM可以理解的“动作叙述”。然后，将这些令牌与自然语言指令相结合，由LoRA微调的LVLM（例如，LLaVA-13B）处理，以进行鲁棒的动作分类和语义推理。

Result: LVLM-VAR在具有挑战性的基准测试（如NTU RGB+D和NTU RGB+D 120）上取得了最先进或极具竞争力的性能，显示出显著的改进（例如，在NTU RGB+D X-Sub上为94.1%，在NTU RGB+D 120 X-Set上为90.0%）。

Conclusion: LVLM-VAR通过为其预测生成自然语言解释，从而大大提高了模型的可解释性。

Abstract: Human action recognition often struggles with deep semantic understanding,
complex contextual information, and fine-grained distinction, limitations that
traditional methods frequently encounter when dealing with diverse video data.
Inspired by the remarkable capabilities of large language models, this paper
introduces LVLM-VAR, a novel framework that pioneers the application of
pre-trained Vision-Language Large Models (LVLMs) to video action recognition,
emphasizing enhanced accuracy and interpretability. Our method features a
Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video
sequences into discrete, semantically and temporally consistent "semantic
action tokens," effectively crafting an "action narrative" that is
comprehensible to an LVLM. These tokens, combined with natural language
instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)
for robust action classification and semantic reasoning. LVLM-VAR not only
achieves state-of-the-art or highly competitive performance on challenging
benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant
improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),
but also substantially boosts model interpretability by generating natural
language explanations for its predictions.

</details>


### [93] [JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization](https://arxiv.org/abs/2509.05696)
*Hongyu Zhou,Yunzhou Zhang,Tingsong Huang,Fawei Ge,Man Qi,Xichen Zhang,Yizhong Zhang*

Main category: cs.CV

TL;DR: 提出了一种结合RGB图像和法线图像的跨视角地理定位方法，以解决无人机定位和导航中的视角差异和外观变化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖RGB图像的语义特征，忽略了空间结构信息在捕获视角不变特征中的重要性。

Method: 引入了一种联合感知网络，结合RGB和法线图像，并利用差异感知融合模块和联合约束交互聚合策略来实现深度融合和语义与结构信息的联合约束表示。此外，还提出了一种3D地理增强技术来生成潜在的视角变化样本。

Result: 在University-1652和SUES-200数据集上进行了大量实验，验证了该方法在复杂视角变化下的鲁棒性，并取得了最先进的性能。

Conclusion: 该方法能够有效应对跨视角地理定位中的视角差异和外观变化问题，具有较强的鲁棒性和先进性。

Abstract: Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle
(UAV) localization and navigation. However, significant challenges arise from
the drastic viewpoint differences and appearance variations between images.
Existing methods predominantly rely on semantic features from RGB images, often
neglecting the importance of spatial structural information in capturing
viewpoint-invariant features. To address this issue, we incorporate geometric
structural information from normal images and introduce a Joint perception
network to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a
dual-branch feature extraction framework, leveraging a Difference-Aware Fusion
Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to
enable deep fusion and joint-constrained semantic and structural information
representation. Furthermore, we propose a 3D geographic augmentation technique
to generate potential viewpoint variation samples, enhancing the network's
ability to learn viewpoint-invariant features. Extensive experiments on the
University-1652 and SUES-200 datasets validate the robustness of our method
against complex viewpoint ariations, achieving state-of-the-art performance.

</details>


### [94] [LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction](https://arxiv.org/abs/2509.05728)
*Niels Balemans,Ali Anwar,Jan Steckel,Siegfried Mercelis*

Main category: cs.CV

TL;DR: LiDAR-BIND-T通过增强时间一致性来改进LiDAR-BIND框架，从而提高SLAM的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高异构传感器融合框架的时间一致性。

Method: 引入了时间嵌入相似性、运动对齐变换损失和窗口时间融合等机制，并更新了模型架构以更好地保持空间结构。

Result: 在雷达/声纳到激光雷达的转换评估中，时间空间一致性得到了改善，绝对轨迹误差降低，基于Cartographer的SLAM的占用地图精度更高。

Conclusion: 提出的LiDAR-BIND-T在保持即插即用模态融合的同时，显著增强了时间稳定性，从而提高了下游SLAM的鲁棒性和性能。

Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that
binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,
with mechanisms that explicitly enforce temporal consistency. We introduce
three contributions: (i) temporal embedding similarity that aligns consecutive
latents, (ii) a motion-aligned transformation loss that matches displacement
between predictions and ground truth LiDAR, and (iii) windows temporal fusion
using a specialised temporal module. We further update the model architecture
to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR
translation demonstrate improved temporal and spatial coherence, yielding lower
absolute trajectory error and better occupancy map accuracy in
Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose
different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a
correlation-peak distance metric providing practical temporal quality
indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or
LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially
enhancing temporal stability, resulting in improved robustness and performance
for downstream SLAM.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [95] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 本文研究视频扩散Transformer中的注意力机制，通过提取和可视化生成视频模型中的互注意力图，为文本到视频生成中的注意力行为提供了一个可解释的窗口。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家启发，他们通过操纵模拟视频信号来创造新的视觉美学。研究目的是探索注意力图作为分析工具和原始艺术材料的潜力。

Method: 基于开源Wan模型，开发了一个工具，用于提取和可视化互注意力图。

Result: 通过探索性探针和艺术案例研究，检验了注意力图的潜力。

Conclusion: 这项工作有助于不断发展的艺术可解释人工智能领域（XAIxArts），邀请艺术家将人工智能的内部运作重新用作一种创造性媒介。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [96] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: AR系统易受认知攻击影响，影响用户决策。


<details>
  <summary>Details</summary>
Motivation: 解决AR系统在战术环境中易受认知攻击影响的问题。

Method: 提出感知图模型，模拟人类感知过程，并用语义结构表示结果。

Result: 模型可以计算反映感知失真程度的量化分数。

Conclusion: 提供了一种检测和分析认知攻击影响的稳健且可测量的方法。

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [97] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 本文介绍了一个名为SynDelay的合成数据集，用于预测交货延迟，旨在解决高质量公开数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 目前用于供应链管理的AI， 尤其是在预测交货延迟的任务中， 受到高质量、公开可用的数据集稀缺的限制。现有的数据集通常是专有的、小型的或维护不一致的，从而妨碍了可重复性和基准测试。

Method: 使用在真实世界数据上训练的先进生成模型生成SynDelay，该模型保留了真实的交付模式，同时确保了隐私。

Result: SynDelay提供了一个具有挑战性和实践性的测试平台，用于推进预测建模。论文提供了基线结果和评估指标作为初始基准。

Conclusion: SynDelay通过供应链数据中心公开提供，鼓励社区贡献数据集、模型和评估实践，以推进该领域的研究。

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [98] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: 本文介绍了一个新的多模态情感识别数据集 MVRS，该数据集包含来自 13 名参与者在 VR 环境中体验不同情绪刺激（放松、恐惧、压力、悲伤、快乐）时的眼动追踪、身体运动和生理信号。


<details>
  <summary>Details</summary>
Motivation: 缺乏包含身体运动和生理信号的多模态数据集限制了情感识别领域的发展。

Method: 使用眼动追踪（通过 VR 头显中的网络摄像头）、身体运动（Kinect v2）以及 EMG 和 GSR 信号（Arduino UNO）同步记录数据。参与者遵循统一的协议并填写问卷。提取每个模态的特征，使用早期和晚期融合技术进行融合，并使用分类器进行评估。

Result: 实验结果验证了数据集的质量和情绪可分离性。

Conclusion: MVRS 数据集对多模态情感计算是一个有价值的贡献。

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [99] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: 本研究对比了三种大型语言模型在模拟真实学习场景下的辅导任务表现，使用 Gemini 作为虚拟裁判进行评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被设想为个性化学习的智能助手，但在真实的学习场景中，系统性的对比评估仍然有限。

Method: 使用包含学生对十个混合形式问题的答案和正确性标签的数据集，要求每个大型语言模型分析测验以识别潜在的知识组成部分，推断学生的掌握情况，并生成有针对性的改进指导。使用 Gemini 作为虚拟裁判，沿着准确性、清晰性、可操作性和适当性等维度进行成对比较。

Result: GPT-4o 在总体上更受欢迎，产生的反馈比其他模型更具信息量和结构性，而 DeepSeek-V3 和 GLM-4.5 表现出间歇性的优势，但一致性较低。

Conclusion: 研究结果表明，部署大型语言模型作为个性化支持的高级助教是可行的，并为未来关于大型语言模型驱动的个性化学习的实证研究提供方法指导。

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [100] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: SasAgent是一个基于大型语言模型（LLM）的多智能体AI系统，它可以自动分析小角度散射（SAS）数据，并通过文本输入实现用户交互。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在利用LLM驱动的AI系统来简化科学工作流程，并加强SAS研究中的自动化。

Method: 论文介绍了SasAgent，它包含一个协调代理和三个专业代理，分别负责散射长度密度（SLD）计算、合成数据生成和实验数据拟合。这些代理利用LLM友好的工具来高效执行任务。这些工具包括模型数据工具、RAG文档工具、bump拟合工具和SLD计算器工具，它们都源自SasView Python库。

Result: 论文通过多种示例展示了SasAgent解释复杂提示、计算SLD、生成准确散射数据和高精度拟合实验数据集的能力。

Conclusion: 该研究展示了LLM驱动的AI系统在简化科学工作流程和加强SAS研究自动化方面的潜力。

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [101] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: 论文分析了提示工程中的优化前景，发现系统提示生成产生平滑衰减的自相关性，而多样化生成表现出非单调模式，在中间语义距离处具有峰值相关性，表明崎岖、分层结构的 landscape。


<details>
  <summary>Details</summary>
Motivation: 提示工程已成为优化大型语言模型性能的关键技术，但其底层的优化前景仍然知之甚少。目前的方法将提示优化视为一个黑盒问题，应用复杂的搜索算法，而没有描述它们所浏览的 landscape topology。

Method: 使用跨语义嵌入空间的自相关分析，对提示工程中的适应度 landscape 结构进行了系统的分析。通过对两种不同的提示生成策略（系统枚举（1,024 个提示）和新颖性驱动的多样化（1,000 个提示））的错误检测任务进行实验

Result: 系统提示生成产生平滑衰减的自相关性，而多样化生成表现出非单调模式，在中间语义距离处具有峰值相关性，表明崎岖、分层结构的 landscape。跨 10 个错误检测类别的特定于任务的分析揭示了不同错误类型之间不同程度的 ruggedness。

Conclusion: 我们的发现为理解提示工程 landscape 中优化的复杂性提供了经验基础。

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [102] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: 本文介绍了一种名为Code Like Humans的agentic框架，用于医学编码，它可以支持完整的ICD-10编码系统，并在罕见诊断代码上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 医学编码中，专家将非结构化的临床笔记映射到字母数字代码，以进行诊断和程序。

Method: 该框架使用大型语言模型，并实施官方编码指南。

Result: 该框架在罕见诊断代码上实现了最佳性能，但微调的判别分类器在高频代码上仍具有优势。

Conclusion: 本文分析了系统性能，并识别了其盲点（系统性编码不足的代码）。

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [103] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: 本文介绍了一种理解基于反馈对齐中反复出现的失败的统一视角，即对齐差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地通过来自人类反馈的强化学习 (RLHF) 和相关方法（例如直接偏好优化 (DPO)、宪法 AI 和 RLAIF）与人类偏好保持一致。虽然有效，但这些方法表现出反复出现的失败模式，即奖励黑客、谄媚、注释者漂移和错误泛化。

Method: 使用 KL 倾斜形式主义，说明了为什么优化压力往往会放大代理奖励和真正的人类意图之间的差异。将这些失败组织成 AI 对齐的墨菲定律目录，并提出对齐三难困境作为在优化强度、价值捕获和泛化之间进行权衡的一种方式。

Result: 小规模的实证研究作为说明性支持。

Conclusion: 我们的贡献不是一个明确的不可能性定理，而是一种重新构建围绕结构限制和权衡的对齐辩论的视角，为未来的设计提供更清晰的指导。

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [104] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种多智能体系统，可以直接在真实街景图像上编辑和重新设计自行车设施，用于主动交通规划中的公共参与。


<details>
  <summary>Details</summary>
Motivation: 传统方法劳动密集，阻碍了集体审议和协作决策。现有的生成方法通常需要大量的领域特定训练数据，并且难以在复杂的街景场景中实现设计的精确空间变化。

Method: 该框架集成了车道定位、提示优化、设计生成和自动评估，以合成逼真的、符合上下文的设计。

Result: 实验表明，该系统可以适应不同的道路几何形状和环境条件，始终产生视觉连贯且符合指令的结果。

Conclusion: 这项工作为将多智能体管道应用于交通基础设施规划和设施设计奠定了基础。

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [105] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: TreeGPT: A new neural network for program synthesis using abstract syntax trees (ASTs).


<details>
  <summary>Details</summary>
Motivation: Existing methods for program synthesis either use sequential processing or graph neural networks, which have limitations in capturing hierarchical tree structures and long-range dependencies.

Method: A hybrid architecture combining transformer-based self-attention with a novel Tree Feed-Forward Network (TreeFFN) that uses global parent-child aggregation.

Result: Achieved 96% accuracy on the ARC Prize 2025 dataset, significantly outperforming transformer baselines, large-scale models, and specialized program synthesis methods with only 1.5M parameters.

Conclusion: TreeGPT's global parent-child aggregation mechanism, particularly with edge projection and gating, is highly effective for processing ASTs and achieving state-of-the-art results in program synthesis.

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


### [106] [OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](https://arxiv.org/abs/2509.05578)
*Ruixun Liu,Lingyu Kong,Derun Li,Hang Zhao*

Main category: cs.AI

TL;DR: OccVLA：一种用于自动驾驶的新框架，它将 3D occupancy representations 集成到统一的多模态推理过程中，从而增强了视觉语言模型的 3D 空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型 (MLLM) 缺乏强大的 3D 空间理解能力，这对于自动驾驶至关重要。 这种局限性源于两个关键挑战：(1) 在没有昂贵的手动注释的情况下，难以构建可访问但有效的 3D 表示，以及 (2) 由于缺乏大规模的 3D 视觉语言预训练，VLM 中丢失了细粒度的空间细节。

Method: OccVLA 框架将密集的 3D occupancy 视为预测输出和监督信号，使模型能够直接从 2D 视觉输入中学习细粒度的空间结构。 occupancy 预测被视为隐式推理过程，可以在推理期间跳过，而不会降低性能，从而不会增加额外的计算开销。

Result: OccVLA 在 nuScenes 轨迹规划基准测试中取得了最先进的结果，并在 3D 视觉问答任务中表现出卓越的性能。

Conclusion: OccVLA 为自动驾驶提供了一种可扩展、可解释且完全基于视觉的解决方案。

Abstract: Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.

</details>


### [107] [MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions](https://arxiv.org/abs/2509.05685)
*Jian Yang,Jiahui Wu,Li Fang,Hongchao Fan,Bianying Zhang,Huijie Zhao,Guangyi Yang,Rui Xin,Xiong You*

Main category: cs.AI

TL;DR: 提出了一种新的道路网络表示学习框架MSRFormer，它集成了多尺度空间交互，通过解决它们的流异质性和长距离依赖性。


<details>
  <summary>Details</summary>
Motivation: 城市道路网络的异构性和分层性质给精确的表示学习带来了挑战。图神经网络由于其同质性假设和对单一结构尺度的关注而常常难以胜任。

Method: 使用空间流卷积从大型轨迹数据集中提取小尺度特征，并识别尺度相关的空间交互区域以捕获道路网络的空间结构和流异质性。通过使用图Transformer，MSRFormer有效地捕获跨多个尺度的复杂空间依赖性。空间交互特征通过残差连接融合，并被馈送到对比学习算法以导出最终的道路网络表示。

Result: 在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，与流量相关的任务更多地受益于整合轨迹数据，并且在复杂的道路网络结构中也获得了更大的改进，与最具竞争力的基线方法相比，改进高达16%。

Conclusion: 这项研究为开发与任务无关的道路网络表示模型提供了一个实用的框架，并强调了空间交互的尺度效应和流量异质性之间相互作用的独特关联模式。

Abstract: Transforming road network data into vector representations using deep
learning has proven effective for road network analysis. However, urban road
networks' heterogeneous and hierarchical nature poses challenges for accurate
representation learning. Graph neural networks, which aggregate features from
neighboring nodes, often struggle due to their homogeneity assumption and focus
on a single structural scale. To address these issues, this paper presents
MSRFormer, a novel road network representation learning framework that
integrates multi-scale spatial interactions by addressing their flow
heterogeneity and long-distance dependencies. It uses spatial flow convolution
to extract small-scale features from large trajectory datasets, and identifies
scale-dependent spatial interaction regions to capture the spatial structure of
road networks and flow heterogeneity. By employing a graph transformer,
MSRFormer effectively captures complex spatial dependencies across multiple
scales. The spatial interaction features are fused using residual connections,
which are fed to a contrastive learning algorithm to derive the final road
network representation. Validation on two real-world datasets demonstrates that
MSRFormer outperforms baseline methods in two road network analysis tasks. The
performance gains of MSRFormer suggest the traffic-related task benefits more
from incorporating trajectory data, also resulting in greater improvements in
complex road network structures with up to 16% improvements compared to the
most competitive baseline method. This research provides a practical framework
for developing task-agnostic road network representation models and highlights
distinct association patterns of the interplay between scale effects and flow
heterogeneity of spatial interactions.

</details>


### [108] [Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs](https://arxiv.org/abs/2509.05714)
*Zhaoyu Fan,Kaihang Pan,Mingze Zhou,Bosheng Qin,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Fei Wu,Yueting Zhuang*

Main category: cs.AI

TL;DR: 这篇论文提出了一个新的基准测试CogEdit，用于评估多模态大型语言模型（MLLMs）的元认知知识编辑能力。同时，提出了一个名为MIND的框架，该框架通过构建元知识记忆、采用博弈论交互和结合标签细化来实现元认知编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要强调认知层面的修改，缺乏对更深层次的元认知过程的关注。

Method: 提出了MIND框架，该框架构建了一个用于自我意识的元知识记忆，采用博弈论交互来监控知识激活，并结合标签细化来进行噪声鲁棒更新。

Result: MIND在传统和元认知知识编辑基准测试中均优于现有的认知编辑方法。

Conclusion: MIND框架能够显著提升多模态大型语言模型的知识编辑能力，特别是在元认知层面。

Abstract: Knowledge editing enables multimodal large language models (MLLMs) to
efficiently update outdated or incorrect information. However, existing
benchmarks primarily emphasize cognitive-level modifications while lacking a
focus on deeper meta-cognitive processes. To bridge this gap, we introduce
CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge
editing abilities across three levels: (1) Counterfactual-Driven Editing,
assessing self-awareness of knowledge correctness changes; (2) Boundary
Constraint Editing, ensuring appropriate generalization without unintended
interference; and (3) Noise-Robust Editing, promoting reflective evaluation of
uncertain information. To advance meta-cognitive editing, we propose MIND
(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that
constructs a meta-knowledge memory for self-awareness, employs game-theoretic
interactions to monitor knowledge activation, and incorporates label refinement
for noise-robust updates. Extensive experiments show that MIND significantly
outperforms existing cognitive editing approaches, achieving strong performance
on both traditional and meta-cognitive knowledge editing benchmarks.

</details>


### [109] [Hyperbolic Large Language Models](https://arxiv.org/abs/2509.05757)
*Sarang Patil,Zeyong Zhang,Yiran Huang,Tengfei Ma,Mengjia Xu*

Main category: cs.AI

TL;DR: 本文综述了利用双曲几何作为表示空间来增强语义表示学习和多尺度推理的大型语言模型 (LLM) 的最新进展。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的数据表现出高度非欧几里得潜在分层结构，例如蛋白质网络、交通运输网络、金融网络、大脑网络以及自然语言中的语言结构或句法树。使用 LLM 从这些原始的非结构化输入数据中有效学习内在的语义蕴含和分层关系仍然是一个未被充分探索的领域。

Method: 本文根据四个主要类别介绍了双曲 LLM (HypLLM) 的主要技术分类：(1) 通过 exp/log 映射的双曲 LLM；(2) 双曲微调模型；(3) 完全双曲 LLM；(4) 双曲状态空间模型。

Result: 本文探讨了关键的潜在应用，并概述了未来的研究方向。一个包含关键论文、模型、数据集和代码实现的存储库可在 https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main 获取。

Conclusion: 双曲几何作为一种非欧几里得空间，由于其在建模树状分层结构方面的有效性，已迅速普及，成为跨图、图像、语言和多模态数据等领域的复杂数据建模的富有表现力的潜在表示空间。

Abstract: Large language models (LLMs) have achieved remarkable success and
demonstrated superior performance across various tasks, including natural
language processing (NLP), weather forecasting, biological protein folding,
text generation, and solving mathematical problems. However, many real-world
data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein
networks, transportation networks, financial networks, brain networks, and
linguistic structures or syntactic trees in natural languages. Effectively
learning intrinsic semantic entailment and hierarchical relationships from
these raw, unstructured input data using LLMs remains an underexplored area.
Due to its effectiveness in modeling tree-like hierarchical structures,
hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity
as an expressive latent representation space for complex data modeling across
domains such as graphs, images, languages, and multi-modal data. Here, we
provide a comprehensive and contextual exposition of recent advancements in
LLMs that leverage hyperbolic geometry as a representation space to enhance
semantic representation learning and multi-scale reasoning. Specifically, the
paper presents a taxonomy of the principal techniques of Hyperbolic LLMs
(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log
maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)
hyperbolic state-space models. We also explore crucial potential applications
and outline future research directions. A repository of key papers, models,
datasets, and code implementations is available at
https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.

</details>


### [110] [DRF: LLM-AGENT Dynamic Reputation Filtering Framework](https://arxiv.org/abs/2509.05764)
*Yuwei Lou,Hao Hu,Shaocong Ma,Zongfei Zhang,Liang Wang,Jidong Ge,Xianping Tao*

Main category: cs.AI

TL;DR: 提出了一个动态信誉过滤框架（DRF），用于量化agent性能和评估agent可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统在量化智能体性能和评估智能体可信度方面面临挑战。

Method: 构建了一个交互式评级网络来量化智能体性能，设计了一种信誉评分机制来衡量智能体的诚实度和能力，并整合了一种基于置信上限的策略来提高智能体选择效率。

Result: 实验表明，DRF显著提高了逻辑推理和代码生成任务中的任务完成质量和协作效率。

Conclusion: DRF为多智能体系统处理大规模任务提供了一种新方法。

Abstract: With the evolution of generative AI, multi - agent systems leveraging large -
language models(LLMs) have emerged as a powerful tool for complex tasks.
However, these systems face challenges in quantifying agent performance and
lack mechanisms to assess agent credibility. To address these issues, we
introduce DRF, a dynamic reputation filtering framework. DRF constructs an
interactive rating network to quantify agent performance, designs a reputation
scoring mechanism to measure agent honesty and capability, and integrates an
Upper Confidence Bound - based strategy to enhance agent selection efficiency.
Experiments show that DRF significantly improves task completion quality and
collaboration efficiency in logical reasoning and code - generation tasks,
offering a new approach for multi - agent systems to handle large - scale
tasks.

</details>


### [111] [Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation](https://arxiv.org/abs/2509.05772)
*Nasser Alkhulaifi,Ismail Gokay Dogan,Timothy R. Cargan,Alexander L. Bowler,Direnc Pekaslan,Nicholas J. Watson,Isaac Triguero*

Main category: cs.AI

TL;DR: 本文提出了一种基于自动化特征工程（AFE）的决策重点学习（DFL）框架，用于优化电池储能系统（BESS）的运行，以降低成本。


<details>
  <summary>Details</summary>
Motivation: 在能源管理中，不确定性下的决策很复杂，尤其是在电池储能系统（BESS）的运行中，预测误差会传递到次优决策中。虽然新兴的决策重点学习（DFL）方法集成了预测和优化，但它们主要在合成数据集或小规模问题上进行测试，缺乏实际可行性的证据。

Method: 本文利用自动化特征工程（AFE）来提取更丰富的表示，并改进DFL的新兴方法。我们提出了一个适用于小数据集的AFE-DFL框架，该框架可以预测电价和需求，同时优化BESS运行以降低成本。

Result: 结果表明，平均而言，DFL产生的运营成本低于PTO，并且与没有AFE的相同模型相比，添加AFE进一步提高了DFL方法的性能22.9-56.5%。

Conclusion: 这些发现为DFL在实际环境中的实际可行性提供了经验证据，表明特定领域的AFE增强了DFL，并减少了对BESS优化领域专业知识的依赖，从而产生了经济效益，并对面临类似挑战的能源管理系统具有更广泛的意义。

Abstract: Decision-making under uncertainty in energy management is complicated by
unknown parameters hindering optimal strategies, particularly in Battery Energy
Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat
forecasting and optimisation as separate processes, allowing prediction errors
to cascade into suboptimal decisions as models minimise forecasting errors
rather than optimising downstream tasks. The emerging Decision-Focused Learning
(DFL) methods overcome this limitation by integrating prediction and
optimisation; however, they are relatively new and have been tested primarily
on synthetic datasets or small-scale problems, with limited evidence of their
practical viability. Real-world BESS applications present additional
challenges, including greater variability and data scarcity due to collection
constraints and operational limitations. Because of these challenges, this work
leverages Automated Feature Engineering (AFE) to extract richer representations
and improve the nascent approach of DFL. We propose an AFE-DFL framework
suitable for small datasets that forecasts electricity prices and demand while
optimising BESS operations to minimise costs. We validate its effectiveness on
a novel real-world UK property dataset. The evaluation compares DFL methods
against PTO, with and without AFE. The results show that, on average, DFL
yields lower operating costs than PTO and adding AFE further improves the
performance of DFL methods by 22.9-56.5% compared to the same models without
AFE. These findings provide empirical evidence for DFL's practical viability in
real-world settings, indicating that domain-specific AFE enhances DFL and
reduces reliance on domain expertise for BESS optimisation, yielding economic
benefits with broader implications for energy management systems facing similar
challenges.

</details>


### [112] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: NoteAid-Chatbot is a conversational AI that helps patients understand their care using a 'learning as conversation' framework.


<details>
  <summary>Details</summary>
Motivation: Patients need knowledge to participate in their care, and current methods may be insufficient.

Method: A multi-agent LLM and reinforcement learning setup was used to train a lightweight LLaMA 3.2 3B model. It was fine-tuned on synthetic conversational data and then trained with RL based on patient understanding assessments.

Result: NoteAid-Chatbot demonstrates clarity, relevance, and structured dialogue in patient education, even without explicit supervision. It surpasses non-expert humans in a Turing test.

Conclusion: The study demonstrates the feasibility of using low-cost, PPO-based RL for open-ended conversational domains, expanding the use of RL-based alignment methods, with a focus on healthcare.

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [113] [MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration](https://arxiv.org/abs/2509.05933)
*Md Hasebul Hasan,Mahir Labib Dihan,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: MapAgent是一个用于地理空间推理的分层多代理框架，通过定制工具集和代理支架集成地图，解决现有框架在空间推理、多跳规划和实时地图交互方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有框架在地理空间任务中表现不足，因为它们需要空间推理、多跳规划和实时地图交互。

Method: MapAgent采用分层多代理即插即用框架，将规划与执行分离。高层规划器将复杂查询分解为子目标，并将其路由到专用模块。对于工具繁重的模块，设计了一个专用的地图工具代理，以自适应地并行协调相关API，而更简单的模块则无需额外的代理开销。

Result: 在四个不同的地理空间基准测试中，MapAgent优于最先进的工具增强和代理基线。

Conclusion: MapAgent通过分层设计减少认知负荷，提高工具选择准确性，并实现跨类似API的精确协调。该框架已开源。

Abstract: Agentic AI has significantly extended the capabilities of large language
models (LLMs) by enabling complex reasoning and tool use. However, most
existing frameworks are tailored to domains such as mathematics, coding, or web
automation, and fall short on geospatial tasks that require spatial reasoning,
multi-hop planning, and real-time map interaction. To address these challenges,
we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with
customized toolsets and agentic scaffolds for map-integrated geospatial
reasoning. Unlike existing flat agent-based approaches that treat tools
uniformly-often overwhelming the LLM when handling similar but subtly different
geospatial APIs-MapAgent decouples planning from execution. A high-level
planner decomposes complex queries into subgoals, which are routed to
specialized modules. For tool-heavy modules-such as map-based services-we then
design a dedicated map-tool agent that efficiently orchestrates related APIs
adaptively in parallel to effectively fetch geospatial data relevant for the
query, while simpler modules (e.g., solution generation or answer extraction)
operate without additional agent overhead. This hierarchical design reduces
cognitive load, improves tool selection accuracy, and enables precise
coordination across similar APIs. We evaluate MapAgent on four diverse
geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and
MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented
and agentic baselines. We open-source our framwork at
https://github.com/Hasebul/MapAgent.

</details>


### [114] [Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL](https://arxiv.org/abs/2509.06024)
*Haoyang He,Zihua Rong,Kun Ji,Chenyang Li,Qing Huang,Chong Xia,Lan Yang,Honggang Zhang*

Main category: cs.AI

TL;DR: 提出了动态推理效率奖励（DRER）框架，以改进大型语言模型（LLM）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于规则的奖励函数仅评估答案格式和正确性，无法判断思维链（CoT）是否真正改进了答案，且任务特定训练对逻辑深度控制有限。

Method: DRER包含推理质量奖励（Reasoning Quality Reward）和动态长度优势（Dynamic Length Advantage）。推理质量奖励对那些能提高正确答案可能性的推理链进行细粒度奖励，动态长度优势会衰减偏离验证集长度阈值的响应优势。

Result: 实验表明，DRER有效。7B模型在Logictree上达到GPT-o3-mini水平，CoT增强答案的平均置信度提高30%。模型在不同的逻辑推理数据集和AIME24数学基准测试中表现出泛化能力。

Conclusion: 结果表明，RL可以塑造CoT行为，并为提高大型语言模型的形式推理能力提供了一条实用路径。

Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for
strengthening the reasoning abilities of large language models (LLMs). Yet the
rule-based reward functions commonly used on mathematical or programming
benchmarks assess only answer format and correctness, providing no signal as to
whether the induced Chain-of-Thought (CoT) actually improves the answer.
Furthermore, such task-specific training offers limited control over logical
depth and therefore may fail to reveal a model's genuine reasoning capacity. We
propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward
framework that reshapes both reward and advantage signals. (i) A Reasoning
Quality Reward assigns fine-grained credit to those reasoning chains that
demonstrably raise the likelihood of the correct answer, directly incentivising
the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage
decays the advantage of responses whose length deviates from a
validation-derived threshold, stabilising training. To facilitate rigorous
assessment, we also release Logictree, a dynamically constructed deductive
reasoning dataset that functions both as RL training data and as a
comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B
model attains GPT-o3-mini level performance on Logictree with 400 trianing
steps, while the average confidence of CoT-augmented answers rises by 30%. The
model further exhibits generalisation across diverse logical-reasoning
datasets, and the mathematical benchmark AIME24. These results illuminate how
RL shapes CoT behaviour and chart a practical path toward enhancing
formal-reasoning skills in large language models. All code and data are
available in repository https://github.com/Henryhe09/DRER.

</details>


### [115] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: 提出了一种新的深度推理范式，通过逆向工程从已知解决方案中发现潜在的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习和指令蒸馏方法在开放式、创造性生成任务中存在局限性，强化学习缺乏明确的奖励信号，而指令蒸馏成本高昂且受限于教师模型的能力。

Method: 提出了一种名为REER的逆向工程推理范式，它从已知良好的解决方案出发，反向计算发现潜在的、逐步的深度推理过程。同时，构建并开源了一个名为DeepWriting-20K的大规模数据集，包含20,000个开放式任务的深度推理轨迹。

Result: 训练出的DeepWriter-8B模型不仅超越了强大的开源基线模型，而且在性能上与GPT-4o和Claude 3.5等领先的专有模型竞争，甚至在某些时候优于它们。

Conclusion: REER范式和DeepWriting-20K数据集为开放式、创造性生成任务中的深度推理提供了一种新的有效方法。

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [116] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: 大型推理模型(LRM)在复杂推理任务中表现出色，但存在过度思考的问题。本文提出了一种名为EDIT的测试时缩放方法，旨在提高LRM的推理效率，使其在简洁性和正确性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: LRM虽然在复杂推理任务中表现出色，但容易过度思考，导致策略切换频繁，推理过程冗长复杂，可解释性差。

Method: 提出一种名为EDIT（Efficient Dynamic Inference Trimming）的测试时缩放方法，通过约束引导生成，并联合跟踪不同约束下的长度和答案分布，从而选择在简洁性和正确性之间取得最佳平衡的响应。

Result: 大量实验表明，EDIT显著提高了推理效率，产生了紧凑而信息丰富的输出，从而提高了可读性和用户体验。

Conclusion: EDIT方法能够有效地引导LRM找到最短的正确推理路径，提高推理效率，并在简洁性和正确性之间取得平衡。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [117] [PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments](https://arxiv.org/abs/2509.06235)
*Olivier Schipper,Yudi Zhang,Yali Du,Mykola Pechenizkiy,Meng Fang*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个名为 PillagerBench 的新框架，用于评估 Minecraft 中实时竞争性团队对抗场景中的多智能体系统。同时，作者提出了一个名为 TactiCrafter 的基于 LLM 的多智能体系统，该系统通过可读的战术促进团队合作，学习因果依赖关系，并适应对手的策略。


<details>
  <summary>Details</summary>
Motivation: 目前，基于 LLM 的智能体在各种合作和战略推理任务中显示出潜力，但它们在竞争性多智能体环境中的有效性仍有待探索。为了解决这个问题，我们推出了 PillagerBench。

Method: 我们提出了 TactiCrafter，一个基于 LLM 的多智能体系统，它通过人类可读的战术促进团队合作，学习因果依赖关系，并适应对手的策略。PillagerBench 提供了一个可扩展的 API、多轮测试和基于规则的内置对手，用于公平、可重复的比较。

Result: 我们的评估表明，TactiCrafter 优于基线方法，并通过自我对弈展示了自适应学习。此外，我们还分析了它的学习过程和多个游戏 эпизод 中的战略演变。

Conclusion: 为了鼓励进一步研究，我们已经开源了 PillagerBench，以促进竞争环境中多智能体 AI 的进步。

Abstract: LLM-based agents have shown promise in various cooperative and strategic
reasoning tasks, but their effectiveness in competitive multi-agent
environments remains underexplored. To address this gap, we introduce
PillagerBench, a novel framework for evaluating multi-agent systems in
real-time competitive team-vs-team scenarios in Minecraft. It provides an
extensible API, multi-round testing, and rule-based built-in opponents for
fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based
multi-agent system that facilitates teamwork through human-readable tactics,
learns causal dependencies, and adapts to opponent strategies. Our evaluation
demonstrates that TactiCrafter outperforms baseline approaches and showcases
adaptive learning through self-play. Additionally, we analyze its learning
process and strategic evolution over multiple game episodes. To encourage
further research, we have open-sourced PillagerBench, fostering advancements in
multi-agent AI for competitive environments.

</details>


### [118] [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)
*Manvi Jha,Jiaxin Wan,Deming Chen*

Main category: cs.AI

TL;DR: Proof2Silicon是一个端到端框架，它结合了PREFACE流程，以直接从自然语言规范生成正确构建的硬件。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动代码生成方面表现出令人印象深刻的能力，但经常生成未能通过形式验证的代码，而形式验证对于硬件和安全关键领域至关重要。为了克服这个根本限制。

Method: Proof2Silicon通过以下方式运行：（1）利用PREFACE的验证器驱动的RL代理来迭代优化prompt生成，确保Dafny代码的正确性；（2）使用Dafny的Python后端和PyLog自动将验证的Dafny程序翻译成可综合的高级C；（3）使用Vivado HLS生成RTL实现。

Result: 在具有挑战性的100任务基准上进行严格评估，PREFACE的RL引导的prompt优化始终将各种LLM的Dafny验证成功率提高了高达21%。重要的是，Proof2Silicon实现了高达72%的端到端硬件综合成功率，通过Vivado HLS综合流程生成RTL设计。

Conclusion: 这些结果展示了一个稳健、可扩展且自动化的LLM驱动的形式验证硬件综合pipeline，弥合了自然语言规范和硅实现之间的差距。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
automated code generation but frequently produce code that fails formal
verification, an essential requirement for hardware and safety-critical
domains. To overcome this fundamental limitation, we previously proposed
PREFACE, a model-agnostic framework based on reinforcement learning (RL) that
iteratively repairs the prompts provided to frozen LLMs, systematically
steering them toward generating formally verifiable Dafny code without costly
fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis
framework that embeds the previously proposed PREFACE flow to enable the
generation of correctness-by-construction hardware directly from natural
language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's
verifier-driven RL agent to optimize prompt generation iteratively, ensuring
Dafny code correctness; (2) automatically translating verified Dafny programs
into synthesizable high-level C using Dafny's Python backend and PyLog; and (3)
employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a
challenging 100-task benchmark, PREFACE's RL-guided prompt optimization
consistently improved Dafny verification success rates across diverse LLMs by
up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis
success rate of up to 72%, generating RTL designs through Vivado HLS synthesis
flows. These results demonstrate a robust, scalable, and automated pipeline for
LLM-driven, formally verified hardware synthesis, bridging natural-language
specification and silicon realization.

</details>


### [119] [REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents](https://arxiv.org/abs/2509.06269)
*Vishal Raman,Vijai Aravindh R,Abhijith Ragav*

Main category: cs.AI

TL;DR: 提出REMI，一个多模态生活方式代理的因果模式记忆架构，集成了个人因果知识图谱、因果推理引擎和基于模式的规划模块。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化AI助手难以整合复杂的个人数据和因果知识，导致建议泛化且缺乏解释力。

Method: 使用个人因果图，执行以目标为导向的因果遍历，并检索适应性计划模式以生成定制的行动计划。大型语言模型协调这些组件，产生具有透明因果解释的答案。

Result: 基于CSM的代理可以提供比基线LLM代理更符合上下文感知、更符合用户需求的推荐。

Conclusion: 证明了一种在个性化代理中进行记忆增强的因果推理的新方法，推进了透明和值得信赖的AI生活方式助手的开发。

Abstract: Personalized AI assistants often struggle to incorporate complex personal
data and causal knowledge, leading to generic advice that lacks explanatory
power. We propose REMI, a Causal Schema Memory architecture for a multimodal
lifestyle agent that integrates a personal causal knowledge graph, a causal
reasoning engine, and a schema based planning module. The idea is to deliver
explainable, personalized recommendations in domains like fashion, personal
wellness, and lifestyle planning. Our architecture uses a personal causal graph
of the user's life events and habits, performs goal directed causal traversals
enriched with external knowledge and hypothetical reasoning, and retrieves
adaptable plan schemas to generate tailored action plans. A Large Language
Model orchestrates these components, producing answers with transparent causal
explanations. We outline the CSM system design and introduce new evaluation
metrics for personalization and explainability, including Personalization
Salience Score and Causal Reasoning Accuracy, to rigorously assess its
performance. Results indicate that CSM based agents can provide more context
aware, user aligned recommendations compared to baseline LLM agents. This work
demonstrates a novel approach to memory augmented, causal reasoning in
personalized agents, advancing the development of transparent and trustworthy
AI lifestyle assistants.

</details>


### [120] [TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning](https://arxiv.org/abs/2509.06278)
*Chuang Jiang,Mingyue Cheng,Xiaoyu Tao,Qingyang Mao,Jie Ouyang,Qi Liu*

Main category: cs.AI

TL;DR: TableMind是一个LLM驱动的表格推理Agent，它自主执行多轮工具调用，在安全沙箱环境中编写和执行数据分析代码，并具有规划和自我反思等高级能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的方法在表格推理中存在数值计算复杂和操作精细的问题，而工具集成推理虽然提高了计算精度，但缺乏自主适应性。

Method: TableMind采用两阶段微调范式：首先在高质量推理轨迹上进行监督微调，建立有效的工具使用模式；然后进行强化微调，优化多目标策略。提出了Rank-Aware Policy Optimization (RAPO)算法。

Result: TableMind在多个主流基准测试中表现优于现有方法，在推理精度和计算精度方面都取得了显著提高。

Conclusion: TableMind通过自主工具调用、代码执行和策略自适应，显著提升了表格推理的性能。

Abstract: Table reasoning is crucial for leveraging structured data in domains such as
finance, healthcare, and scientific research. While large language models
(LLMs) show promise in multi-step reasoning, purely text-based methods often
struggle with the complex numerical computations and fine-grained operations
inherently required in this task. Tool-integrated reasoning improves
computational accuracy via explicit code execution, yet existing systems
frequently rely on rigid patterns, supervised imitation, and lack true
autonomous adaptability. In this paper, we present TableMind, an LLM-driven
table reasoning agent that (i) autonomously performs multi-turn tool
invocation, (ii) writes and executes data-analyzing code in a secure sandbox
environment for data analysis and precise numerical reasoning, and (iii)
exhibits high-level capabilities such as planning and self-reflection to adapt
strategies. To realize these capabilities, we adopt a two-stage fine-tuning
paradigm built on top of a powerful pre-trained language model: supervised
fine-tuning on high-quality reasoning trajectories to establish effective tool
usage patterns, followed by reinforcement fine-tuning to optimize
multi-objective strategies. In particular, we propose Rank-Aware Policy
Optimization (RAPO), which increases the update weight of high-quality
trajectories when their output probabilities are lower than those of
low-quality ones, thereby guiding the model more consistently toward better and
more accurate answers. Extensive experiments on several mainstream benchmarks
demonstrate that TableMind achieves superior performance compared to
competitive baselines, yielding substantial gains in both reasoning accuracy
and computational precision.

</details>


### [121] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文研究了用于深度研究的自主单智能体模型，该模型具有最少的网络爬取和Python工具集成。


<details>
  <summary>Details</summary>
Motivation: 为大型语言模型（LLM）配备复杂的、交错的推理和工具使用能力已成为智能体人工智能研究的关键重点，尤其是在最近以推理为导向（“思考”）的模型取得进展的情况下。此类能力是解锁许多重要应用的关键。其中一项应用是深度研究（DR），它需要在许多来源上进行广泛的搜索和推理。

Method: 我们提出了一种简单的RL方法，该方法完全使用合成数据，我们将其应用于各种开源LLM。

Result: 我们最好的变体SFR-DR-20B在Humanity's Last Exam基准测试中达到了28.7%。

Conclusion: 我们进行了关键分析实验，以提供对我们方法的更多见解。

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [122] [From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs](https://arxiv.org/abs/2509.06284)
*Jiaxiang Chen,Zhuo Wang,Mingxi Zou,Zhucong Li,Zhijian Zhou,Song Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 本研究提出了一种通过指导和改进实现结构化推理的框架，以解决大型语言模型中隐式探索的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于随机和无指导的推理路径，导致不稳定、缺乏纠错和有限的学习经验。

Method: 从成功的轨迹中提取结构化推理模式，并从失败中提取反思信号。在推理过程中，模型逐步遵循这些指导，并在每一步后进行改进以纠正错误并稳定推理过程。

Result: 在BBH和四个额外的基准测试中，该方法始终优于强大的基线。

Conclusion: 结构化推理提高了稳定性和泛化能力，指导方针在不同领域之间具有良好的迁移性，并灵活地支持跨模型协作。

Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing
strong performance across diverse tasks. However, existing methods often rely
on implicit exploration, where the model follows stochastic and unguided
reasoning paths-like walking without a map. This leads to unstable reasoning
paths, lack of error correction, and limited learning from past experience. To
address these issues, we propose a framework that shifts from implicit
exploration to structured reasoning through guideline and refinement. First, we
extract structured reasoning patterns from successful trajectories and
reflective signals from failures. During inference, the model follows these
guidelines step-by-step, with refinement applied after each step to correct
errors and stabilize the reasoning process. Experiments on BBH and four
additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method
consistently outperforms strong baselines across diverse reasoning tasks.
Structured reasoning with stepwise execution and refinement improves stability
and generalization, while guidelines transfer well across domains and flexibly
support cross-model collaboration, matching or surpassing supervised
fine-tuning in effectiveness and scalability.

</details>


### [123] [Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models](https://arxiv.org/abs/2509.06307)
*Lei Shu,Dong Zhao*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)在住宅改造决策中具有潜力，但需要提高准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统建筑节能改造决策方法泛化性有限，可解释性低，阻碍了其在不同住宅环境中的应用。大型语言模型可以通过处理情境信息并生成从业者可读的建议来提供帮助。

Method: 评估了七个大型语言模型（ChatGPT、DeepSeek、Gemini、Grok、Llama 和 Claude）在住宅改造决策中的表现，目标是最大限度地减少CO2排放（技术目标）和最小化投资回收期（社会技术目标）。使用包含美国49个州的400个家庭的数据集，从准确性、一致性、敏感性和推理四个维度评估性能。

Result: 大型语言模型在许多情况下都能生成有效的建议，在不进行微调的情况下，最高达到54.5%的top 1匹配和92.8%的top 5匹配。技术目标的性能更强，而社会技术决策受到经济权衡和当地环境的限制。模型之间的一致性较低，性能较高的模型往往与其他模型不同。大型语言模型对位置和建筑几何形状敏感，但对技术和居住者行为不太敏感。

Conclusion: 总的来说，大型语言模型是能源改造决策的有希望的助手，但为了可靠的实践，需要提高准确性、一致性和上下文处理能力。

Abstract: Conventional approaches to building energy retrofit decision making suffer
from limited generalizability and low interpretability, hindering adoption in
diverse residential contexts. With the growth of Smart and Connected
Communities, generative AI, especially large language models (LLMs), may help
by processing contextual information and producing practitioner readable
recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,
Llama, and Claude) on residential retrofit decisions under two objectives:
maximizing CO2 reduction (technical) and minimizing payback period
(sociotechnical). Performance is assessed on four dimensions: accuracy,
consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49
US states. LLMs generate effective recommendations in many cases, reaching up
to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.
Performance is stronger for the technical objective, while sociotechnical
decisions are limited by economic trade offs and local context. Agreement
across models is low, and higher performing models tend to diverge from others.
LLMs are sensitive to location and building geometry but less sensitive to
technology and occupant behavior. Most models show step by step, engineering
style reasoning, but it is often simplified and lacks deeper contextual
awareness. Overall, LLMs are promising assistants for energy retrofit decision
making, but improvements in accuracy, consistency, and context handling are
needed for reliable practice.

</details>


### [124] [Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation](https://arxiv.org/abs/2509.06337)
*Jianpeng Zhao,Chenyu Yuan,Weiming Luo,Haoling Xie,Guangwei Zhang,Steven Jige Quan,Zixuan Yuan,Pengyang Wang,Denghui Zhang*

Main category: cs.AI

TL;DR: 本文探讨了使用大型语言模型（LLM）模拟虚拟调查对象的新范式，以解决传统调查方法的成本高、耗时和规模有限等问题。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法成本高、耗时且规模有限。

Method: 引入了部分属性模拟（PAS）和完整属性模拟（FAS）两种新的模拟设置，并构建了LLM-S^3基准套件，该套件跨越四个社会学领域的11个真实世界公共数据集。

Result: 对多个主流LLM（GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B）的评估揭示了预测性能的一致趋势，突出了失败模式，并展示了上下文和提示设计如何影响模拟保真度。

Conclusion: 这项工作为LLM驱动的调查模拟奠定了严谨的基础，为社会学研究和政策评估提供了可扩展且经济高效的工具。

Abstract: Questionnaire-based surveys are foundational to social science research and
public policymaking, yet traditional survey methods remain costly,
time-consuming, and often limited in scale. This paper explores a new paradigm:
simulating virtual survey respondents using Large Language Models (LLMs). We
introduce two novel simulation settings, namely Partial Attribute Simulation
(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the
ability of LLMs to generate accurate and demographically coherent responses. In
PAS, the model predicts missing attributes based on partial respondent
profiles, whereas FAS involves generating complete synthetic datasets under
both zero-context and context-enhanced conditions. We curate a comprehensive
benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey
Simulation), that spans 11 real-world public datasets across four sociological
domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA
3.0/3.1-8B) reveals consistent trends in prediction performance, highlights
failure modes, and demonstrates how context and prompt design impact simulation
fidelity. This work establishes a rigorous foundation for LLM-driven survey
simulations, offering scalable and cost-effective tools for sociological
research and policy evaluation. Our code and dataset are available at:
https://github.com/dart-lab-research/LLM-S-Cube-Benchmark

</details>


### [125] [Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent](https://arxiv.org/abs/2509.06341)
*Issue Yishu Wang,Kakam Chong,Xiaofeng Wang,Xu Yan,DeXin Kong,Chen Ju,Ming Chen,Shuai Xiao,Shuguang Han,jufeng chen*

Main category: cs.AI

TL;DR: 本文提出了一个评估框架，用于评估在电子商务对话中，作为卖家代理的大型语言模型（LLM）的议价能力，尤其是在跟踪和解释买家意图方面的能力。


<details>
  <summary>Details</summary>
Motivation: 在线二手市场中，多轮议价是买卖双方互动的重要组成部分。大型语言模型可以充当卖家代理，代表卖家在给定的业务约束下与买家谈判。对于此类代理来说，一个关键能力是跟踪和准确解释长期谈判中累积的买家意图，这直接影响议价效率。

Method: 本文介绍了一个多轮评估框架，用于衡量电子商务对话中卖家代理的议价能力。该框架测试代理是否可以提取和跟踪买家意图。

Result: 本文构建了一个大规模的电子商务议价基准，涵盖 622 个类别、9,892 种产品和 3,014 个任务；提出了一个基于心理理论（ToM）的turn-level评估框架，带有注释的买家意图，超越了仅关注结果的指标；构建了一个自动管道，可以从大量对话数据中提取可靠的意图。

Conclusion: 本文提出了一个用于评估 LLM 议价能力的框架，并构建了相应的数据集和评估指标，为后续研究奠定了基础。

Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part
of seller-buyer interactions. Large Language Models (LLMs) can act as seller
agents, negotiating with buyers on behalf of sellers under given business
constraints. A critical ability for such agents is to track and accurately
interpret cumulative buyer intents across long negotiations, which directly
impacts bargaining effectiveness. We introduce a multi-turn evaluation
framework for measuring the bargaining ability of seller agents in e-commerce
dialogues. The framework tests whether an agent can extract and track buyer
intents. Our contributions are: (1) a large-scale e-commerce bargaining
benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a
turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated
buyer intents, moving beyond outcome-only metrics; and (3) an automated
pipeline that extracts reliable intent from massive dialogue data.

</details>


### [126] [A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research](https://arxiv.org/abs/2509.06355)
*Yunzhe Wang,Volkan Ustun,Chris McGroarty*

Main category: cs.AI

TL;DR: DECOY is a multi-agent simulator that balances high-fidelity detail with computational efficiency by abstracting strategic, long-horizon planning in 3D terrains into high-level discretized simulation while preserving low-level environmental fidelity.


<details>
  <summary>Details</summary>
Motivation: The need to balance high-fidelity detail with computational efficiency in modern simulation environments for complex multi-agent interactions.

Method: A waypoint system simplifies and discretizes continuous states and actions, paired with neural predictive and generative models trained on real CS:GO tournament data to reconstruct event outcomes.

Result: Replays generated from human data in DECOY closely match those observed in the original game.

Conclusion: DECOY provides a valuable tool for advancing research in strategic multi-agent planning and behavior generation.

Abstract: Modern simulation environments for complex multi-agent interactions must
balance high-fidelity detail with computational efficiency. We present DECOY, a
novel multi-agent simulator that abstracts strategic, long-horizon planning in
3D terrains into high-level discretized simulation while preserving low-level
environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a
testbed, our framework accurately simulates gameplay using only movement
decisions as tactical positioning -- without explicitly modeling low-level
mechanics such as aiming and shooting. Central to our approach is a waypoint
system that simplifies and discretizes continuous states and actions, paired
with neural predictive and generative models trained on real CS:GO tournament
data to reconstruct event outcomes. Extensive evaluations show that replays
generated from human data in DECOY closely match those observed in the original
game. Our publicly available simulation environment provides a valuable tool
for advancing research in strategic multi-agent planning and behavior
generation.

</details>


### [127] [Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning](https://arxiv.org/abs/2509.06409)
*Yihong Luo,Wenwu He,Zhuo-Xu Cui,Dong Liang*

Main category: cs.AI

TL;DR: DiagCoT: A multi-stage framework using VLMs to mimic radiologists' diagnostic reasoning.


<details>
  <summary>Details</summary>
Motivation: Developing interpretable and diagnostically competent AI systems for radiology is challenging.

Method: Multi-stage framework with contrastive image-report tuning, chain-of-thought supervision, and reinforcement tuning.

Result: Improved zero-shot disease classification AUC to 0.76, pathology grounding mIoU to 0.31, and report generation BLEU to 0.33 on MIMIC-CXR.

Conclusion: DiagCoT offers a scalable approach for developing AI systems for radiology by converting unstructured clinical narratives into structured supervision.

Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised
fine-tuning to general-purpose vision-language models (VLMs) to emulate
radiologists' stepwise diagnostic reasoning using only free-text reports.
DiagCoT combines contrastive image-report tuning for domain alignment,
chain-of-thought supervision to capture inferential logic, and reinforcement
tuning with clinical reward signals to enhance factual accuracy and fluency. On
the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC
from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08
to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33
(absolute gain of 0.22). It outperformed state-of-the-art models including
LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By
converting unstructured clinical narratives into structured supervision,
DiagCoT offers a scalable approach for developing interpretable and
diagnostically competent AI systems for radiology.

</details>


### [128] [Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://arxiv.org/abs/2509.06436)
*Song Yu,Xiaofei Xu,Ke Deng,Li Li,Lin Tian*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为Tree of Agents (TOA) 的多代理推理框架，旨在解决大型语言模型在处理长文本任务时遇到的“中间信息丢失”问题。TOA通过分割输入、多代理协同推理、动态信息交换和树状结构路径来缓解位置偏差和减少幻觉，同时利用前缀哈希缓存和自适应剪枝策略提高处理效率。实验表明，TOA在长文本任务上优于多个基线模型，并能与更大的商业模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长文本任务时，存在“中间信息丢失”的问题，即输入中间部分的信息未被充分利用。现有的减少输入的方法可能会丢弃关键信息，而扩展上下文窗口的方法则可能导致注意力分散。

Method: 论文提出了Tree of Agents (TOA) 框架，该框架将输入分割成块，由独立代理处理。每个代理生成局部认知，然后代理之间动态交换信息，沿着树状结构的路径进行协同推理。TOA还采用了前缀哈希缓存和自适应剪枝策略来提高处理效率。

Result: 实验结果表明，TOA在长文本任务上显著优于多个基线模型，并能与最新的、更大的商业模型（如Gemini1.5-pro）相媲美。TOA由紧凑的LLaMA3.1-8B驱动。

Conclusion: Tree of Agents (TOA) 是一种有效的多代理推理框架，可以缓解大型语言模型在处理长文本任务时遇到的“中间信息丢失”问题，并在效率和性能上都取得了显著的提升。

Abstract: Large language models (LLMs) face persistent challenges when handling
long-context tasks, most notably the lost in the middle issue, where
information located in the middle of a long input tends to be underutilized.
Some existing methods that reduce input have the risk of discarding key
information, while others that extend context windows often lead to attention
dispersion. To address these limitations, we propose Tree of Agents (TOA), a
multi-agent reasoning framework that segments the input into chunks processed
by independent agents. Each agent generates its local cognition, then agents
dynamically exchange information for collaborative reasoning along
tree-structured paths. TOA enables agents to probe different reasoning orders
for multi-perspective understanding, effectively mitigating position bias and
reducing hallucinations. To improve processing efficiency, we incorporate
prefix-hash caching and adaptive pruning strategies, achieving significant
performance improvements with comparable API overhead. Experiments show that
TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple
baselines and demonstrates comparable performance to the latest and much larger
commercial models, such as Gemini1.5-pro, on various long-context tasks. Code
is available at https://github.com/Aireduce952/Tree-of-Agents.

</details>


### [129] [HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data](https://arxiv.org/abs/2509.06444)
*Cheng Qian,Hainan Zhang,Yongxin Tong,Hong-Wei Zheng,Zhiming Zheng*

Main category: cs.AI

TL;DR: HyFedRAG: A federated RAG framework for hybrid data modalities, enabling privacy-preserving and efficient information retrieval in distributed healthcare settings.


<details>
  <summary>Details</summary>
Motivation: Challenges in retrieving rare disease cases due to data heterogeneity, privacy constraints, and limitations of centralized RAG systems in distributed healthcare.

Method: An edge-cloud collaborative RAG framework (HyFedRAG) is introduced, featuring edge-side LLMs for data conversion and server-side LLMs for integration, along with anonymization tools and a three-tier caching strategy.

Result: HyFedRAG outperforms existing baselines in retrieval quality, generation consistency, and system efficiency on PMC-Patients dataset.

Conclusion: HyFedRAG offers a scalable and privacy-compliant solution for RAG over structurally heterogeneous data, unlocking LLM potential in sensitive environments.

Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.

</details>


### [130] [Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tengfei Pan*

Main category: cs.AI

TL;DR: 本文提出了一种新的指令数据选择方法，以提高大型语言模型在下游任务中的对齐性能和效率。


<details>
  <summary>Details</summary>
Motivation: 当前指令集优化方法无法随着指令池的扩展而提高性能，因为驱动对齐模型性能的关键因素尚不清楚。

Method: 本文首先研究了影响指令数据集分布和对齐模型性能之间关系的关键因素。然后，设计了一种指令选择算法，以同时最大化所选指令的深度和语义覆盖。

Result: 实验结果表明，与最先进的基线方法相比，该方法可以更快地持续提高模型性能，从而实现“加速扩展”。

Conclusion: 指令的深度和语义空间的覆盖率是决定下游性能的关键因素，可以解释开发集中超过 70% 的模型损失。

Abstract: With the growing demand for applying large language models to downstream
tasks, improving model alignment performance and efficiency has become crucial.
Such a process involves selecting informative instructions from a candidate
pool. However, due to the complexity of instruction set distributions, the key
factors driving the performance of aligned models remain unclear. As a result,
current instruction set refinement methods fail to improve performance as the
instruction pool expands continuously. To address this issue, we first
investigate the key factors that influence the relationship between instruction
dataset distribution and aligned model performance. Based on these insights, we
propose a novel instruction data selection method. We identify that the depth
of instructions and the coverage of the semantic space are the crucial factors
determining downstream performance, which could explain over 70\% of the model
loss on the development set. We then design an instruction selection algorithm
to simultaneously maximize the depth and semantic coverage of the selected
instructions. Experimental results demonstrate that, compared to
state-of-the-art baseline methods, it can sustainably improve model performance
at a faster pace and thus achieve \emph{``Accelerated Scaling''}.

</details>


### [131] [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents](https://arxiv.org/abs/2509.06477)
*Pengxiang Zhao,Guangyi Liu,Yaozhen Liang,Weiqing He,Zhengxi Lu,Yuehao Huang,Yaxuan Guo,Kexin Zhang,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.AI

TL;DR: 提出了一个用于评估混合GUI智能代理的基准测试MAS-Bench，专注于移动领域，通过自主生成快捷方式来评估代理的能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高GUI智能代理在智能手机和计算机等平台上的效率，结合灵活的GUI操作和高效快捷方式的混合范例正在成为一个有希望的方向。然而，系统地评估这些混合代理的框架仍未被充分探索。

Method: 引入MAS-Bench基准测试，评估GUI-shortcut混合代理，包含139个跨11个真实应用程序的复杂任务，一个包含88个预定义快捷方式的知识库，以及7个评估指标。任务可以通过纯GUI操作解决，但可以通过智能嵌入快捷方式来显著加速。

Result: 混合代理比纯GUI代理实现了显著更高的成功率和效率。这一结果也证明了该方法在评估代理的快捷方式生成能力方面的有效性。

Conclusion: MAS-Bench填补了一个关键的评估空白，为未来创建更高效和强大的智能代理奠定了基础。

Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones
and computers, a hybrid paradigm that combines flexible GUI operations with
efficient shortcuts (e.g., API, deep links) is emerging as a promising
direction. However, a framework for systematically benchmarking these hybrid
agents is still underexplored. To take the first step in bridging this gap, we
introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut
hybrid agents with a specific focus on the mobile domain. Beyond merely using
predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously
generate shortcuts by discovering and creating reusable, low-cost workflows. It
features 139 complex tasks across 11 real-world applications, a knowledge base
of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation
metrics. The tasks are designed to be solvable via GUI-only operations, but can
be significantly accelerated by intelligently embedding shortcuts. Experiments
show that hybrid agents achieve significantly higher success rates and
efficiency than their GUI-only counterparts. This result also demonstrates the
effectiveness of our method for evaluating an agent's shortcut generation
capabilities. MAS-Bench fills a critical evaluation gap, providing a
foundational platform for future advancements in creating more efficient and
robust intelligent agents.

</details>


### [132] [MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization](https://arxiv.org/abs/2509.06490)
*Niki Kotecha,Ehecatl Antonio del Rio Chanona*

Main category: cs.AI

TL;DR: 提出了一种结合强化学习 (RL) 和多目标进化算法 (MOEA) 的方法，以解决不确定性下动态多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统的多目标优化方法难以实时适应供应链的动态特性。

Method: 利用 MOEA 搜索策略神经网络的参数空间，生成策略的 Pareto 前沿。

Result: 通过案例研究证明了该方法的有效性，展示了其响应供应链动态的能力，并在库存管理案例研究中优于最先进的方法。

Conclusion: 该策略不仅提高了决策效率，而且为管理不确定性和优化供应链性能提供了一个更强大的框架。

Abstract: In supply chain management, decision-making often involves balancing multiple
conflicting objectives, such as cost reduction, service level improvement, and
environmental sustainability. Traditional multi-objective optimization methods,
such as linear programming and evolutionary algorithms, struggle to adapt in
real-time to the dynamic nature of supply chains. In this paper, we propose an
approach that combines Reinforcement Learning (RL) and Multi-Objective
Evolutionary Algorithms (MOEAs) to address these challenges for dynamic
multi-objective optimization under uncertainty. Our method leverages MOEAs to
search the parameter space of policy neural networks, generating a Pareto front
of policies. This provides decision-makers with a diverse population of
policies that can be dynamically switched based on the current system
objectives, ensuring flexibility and adaptability in real-time decision-making.
We also introduce Conditional Value-at-Risk (CVaR) to incorporate
risk-sensitive decision-making, enhancing resilience in uncertain environments.
We demonstrate the effectiveness of our approach through case studies,
showcasing its ability to respond to supply chain dynamics and outperforming
state-of-the-art methods in an inventory management case study. The proposed
strategy not only improves decision-making efficiency but also offers a more
robust framework for managing uncertainty and optimizing performance in supply
chains.

</details>


### [133] [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)
*Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia Xiao*

Main category: cs.AI

TL;DR: BFS-Prover-V2通过训练时强化学习和推理时计算扩展来解决LLM在自动定理证明中的扩展问题，在MiniF2F和ProofNet测试集上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLM）集成到自动定理证明中显示出巨大的前景，但受到训练时强化学习（RL）和推理时计算扩展的挑战的根本限制。

Method: 引入了一种新颖的多回合离策略RL框架，用于在训练时不断提高LLM step-prover的性能。此外，还采用了规划器增强的多智能体搜索架构，该架构在推理时扩展了推理能力。该架构使用通用推理模型作为高级规划器，以迭代地将复杂定理分解为一系列更简单的子目标。

Result: BFS-Prover-V2在MiniF2F和ProofNet测试集上分别达到了95.08%和41.4%的state-of-the-art结果。

Conclusion: 在形式数学领域中证明，这项工作中提出的RL和推理技术具有更广泛的意义，可以应用于其他需要长期多回合推理和复杂搜索的领域。

Abstract: The integration of Large Language Models (LLMs) into automated theorem
proving has shown immense promise, yet is fundamentally constrained by
challenges in scaling up both training-time reinforcement learning (RL) and
inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system
designed to address this dual scaling problem. We present two primary
innovations. The first is a novel multi-turn off-policy RL framework for
continually improving the performance of LLM step-prover at training time. This
framework, inspired by the principles of AlphaZero, utilizes a multi-stage
expert iteration pipeline featuring adaptive tactic-level data filtering and
periodic retraining to surmount the performance plateaus that typically curtail
long-term RL in LLM-based agents. The second innovation is a planner-enhanced
multi-agent search architecture that scales reasoning capabilities at inference
time. This architecture employs a general reasoning model as a high-level
planner to iteratively decompose complex theorems into a sequence of simpler
subgoals. This hierarchical approach substantially reduces the search space,
enabling a team of parallel prover agents to collaborate efficiently by
leveraging a shared proof cache. We demonstrate that this dual approach to
scaling yields state-of-the-art results on established formal mathematics
benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F
and ProofNet test sets respectively. While demonstrated in the domain of formal
mathematics, the RL and inference techniques presented in this work are of
broader interest and may be applied to other domains requiring long-horizon
multi-turn reasoning and complex search.

</details>


### [134] [An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503)
*Eser Aygün,Anastasiya Belyaeva,Gheorghe Comanici,Marc Coram,Hao Cui,Jake Garrison,Renee Johnston Anton Kast,Cory Y. McLean,Peter Norgaard,Zahra Shamsi,David Smalling,James Thompson,Subhashini Venugopalan,Brian P. Williams,Chujun He,Sarah Martinson,Martyna Plomecka,Lai Wei,Yuchen Zhou,Qian-Ze Zhu,Matthew Abraham,Erica Brand,Anna Bulanova,Jeffrey A. Cardille,Chris Co,Scott Ellsworth,Grace Joseph,Malcolm Kane,Ryan Krueger,Johan Kartiwa,Dan Liebling,Jan-Matthis Lueckmann,Paul Raccuglia,Xuefei,Wang,Katherine Chou,James Manyika,Yossi Matias,John C. Platt,Lizzie Dorfman,Shibl Mourad,Michael P. Brenner*

Main category: cs.AI

TL;DR: AI系统自动创建高质量科研软件，加速科研进展。


<details>
  <summary>Details</summary>
Motivation: 科研软件开发缓慢，阻碍科学发现。

Method: 利用大型语言模型和树搜索，最大化质量指标。

Result: 在生物信息学、流行病学等领域超越人类专家水平。

Conclusion: 该系统是加速科学进展的重要一步。

Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

</details>


### [135] [CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning](https://arxiv.org/abs/2509.06641)
*Zhou-Peng Shou,Zhi-Qiang You,Fang Wang,Hai-Bo Liu*

Main category: cs.AI

TL;DR: 本文提出了一种零样本多模态推理组件，通过模拟人类认知策略，以“意图草图”为中心，解决多模态大模型中存在的“捷径”问题和上下文理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型在复杂跨模态推理中存在的“捷径”问题和上下文理解不足的问题。

Method: 该组件包含三个模块：意图感知器、策略生成器和策略选择器，通过构建“理解-计划-选择”的认知过程，生成和过滤“意图草图”策略来指导最终推理。

Result: 在IntentBench、WorldSense和Daily-Omni等数据集上的实验表明，该方法具有通用性和鲁棒性，与基线相比，完整的“三模块”方案在不同的推理引擎和pipeline组合中都取得了持续的改进，收益高达约9.51个百分点。

Conclusion: “意图草图”推理组件在零样本场景中具有实用价值和可移植性。

Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding
in complex cross-modal reasoning of multimodal large models, this paper
proposes a zero-shot multimodal reasoning component guided by human-like
cognitive strategies centered on an "intent sketch". The component comprises a
plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and
Strategy Selector-that explicitly constructs a "understand-plan-select"
cognitive process. By generating and filtering "intent sketch" strategies to
guide the final reasoning, it requires no parameter fine-tuning and achieves
cross-model transfer solely through in-context engineering.
Information-theoretic analysis shows that this process can reduce conditional
entropy and improve information utilization efficiency, thereby suppressing
unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and
Daily-Omni validate the method's generality and robust gains; compared with
their respective baselines, the complete "three-module" scheme yields
consistent improvements across different reasoning engines and pipeline
combinations, with gains up to approximately 9.51 percentage points,
demonstrating the practical value and portability of the "intent sketch"
reasoning component in zero-shot scenarios.

</details>


### [136] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 本研究综述了基于强化学习(RL)的深度研究系统，这类系统通过协调推理、网络搜索和工具使用来解决复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有端到端训练方法不实用，SFT存在模仿和偏差问题，DPO依赖schema和代理，且在长时信用分配和多目标权衡方面表现不佳。SFT和DPO还依赖于人类定义的决策点和子技能。

Method: 该研究系统地整理了DeepSeek-R1之后的工作，主要从三个方面进行：数据合成与管理、agentic研究的强化学习方法、agentic RL训练系统和框架。

Result: 总结了常见模式，揭示了基础设施瓶颈，并为使用RL训练稳健、透明的深度研究agent提供了实践指导。

Conclusion: 强化学习通过优化轨迹级策略，实现探索和恢复行为，减少了对人类先验和评估者偏差的依赖，更适合闭环、工具交互研究。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [137] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 提出了一种新的API智能座舱环境VehicleWorld，并发现基于状态的函数调用(SFC)优于传统函数调用(FC)。


<details>
  <summary>Details</summary>
Motivation: 传统的函数调用(FC)方法是无状态的，需要多次探索性调用来构建环境意识，导致效率低下和错误恢复能力有限。

Method: 提出了基于状态的函数调用(SFC)，它维护显式的系统状态感知，并实现直接状态转换以达到目标条件。

Result: 实验结果表明，SFC明显优于传统的FC方法，实现了卓越的执行精度和更低的延迟。

Conclusion: 直接状态预测优于函数调用进行环境控制，并且提出的SFC方法在车辆智能座舱环境中表现更好。

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [138] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: 提出了一个评估迭代改进的框架，用于衡量迭代在不同任务（构思、代码和数学）中的帮助和阻碍。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏明确的方法来衡量大型语言模型在多轮工作流程中迭代的帮助和阻碍。

Method: 通过控制12轮对话，利用各种提示（从模糊的“改进它”反馈到有针对性的引导），并记录每轮输出。使用领域适当的检查（代码的单元测试；数学的答案等效性加推理合理性；构思的原创性和可行性）对结果进行评分，并使用三类指标跟踪轮次行为：跨轮次的语义移动、轮次间变化和输出大小增长。

Result: 收益与领域相关：它们在想法和代码中出现较早，但在数学中，当以细化为指导时，后期轮次很重要。模糊的反馈通常会停滞或逆转正确性，而有针对性的提示可以可靠地改变预期的质量轴（构思中的新颖性与可行性；代码中的速度与可读性；在数学中，细化优于探索并推动后期轮次收益）。

Conclusion: 该框架和指标使迭代可衡量且可在模型之间进行比较，并表明何时引导、停止或切换策略。

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


### [139] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: 本文介绍了一种名为RAFFLES的评估架构，用于识别长程、多组件LLM代理系统的故障。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法无法有效识别长程、多组件LLM代理系统的故障原因。

Method: RAFFLES采用迭代、多组件流水线，使用中心Judge系统地调查故障，并使用一组专门的Evaluators评估系统组件和Judge本身的推理质量。

Result: 在Who&When数据集上，RAFFLES优于基线，在算法生成数据集上的agent-step故障对准确率超过43%，在手工制作数据集上的准确率超过20%。

Conclusion: RAFFLES是朝着为自主系统引入自动化故障检测的关键一步，优于人工审查。

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [140] [A Unified Framework for Cultural Heritage Data Historicity and Migration: The ARGUS Approach](https://arxiv.org/abs/2509.06044)
*Lingxiao Kong,Apostolos Sarris,Miltiadis Polidorou,Victor Klingenberg,Vasilis Sevetlidis,Vasilis Arampatzakis,George Pavlidis,Cong Yang,Zeyd Boukhers*

Main category: cs.DB

TL;DR: 本文提出了一个文化遗产数据管理框架，用于处理多样、多源和多尺度的文化遗产数据，以实现有效的监测和保护。


<details>
  <summary>Details</summary>
Motivation: 文化遗产保护在管理多样性、多来源和多尺度的文化遗产数据方面面临重大挑战。

Method: 该框架包括标准化、丰富、集成、可视化、摄取和发布策略的系统数据处理流程，利用数据插补技术、数据库集成和LLM驱动的自然语言处理。

Result: 该方法已应用于五个欧洲试点地点，结果表明数据可访问性得到改善，分析能力得到增强，并为保护工作提供了更有效的决策。

Conclusion: 该框架能够适应不同的文化遗产环境，并提高了数据管理和保护工作的效率。

Abstract: Cultural heritage preservation faces significant challenges in managing
diverse, multi-source, and multi-scale data for effective monitoring and
conservation. This paper documents a comprehensive data historicity and
migration framework implemented within the ARGUS project, which addresses the
complexities of processing heterogeneous cultural heritage data. We describe a
systematic data processing pipeline encompassing standardization, enrichment,
integration, visualization, ingestion, and publication strategies. The
framework transforms raw, disparate datasets into standardized formats
compliant with FAIR principles. It enhances sparse datasets through established
imputation techniques, ensures interoperability through database integration,
and improves querying capabilities through LLM-powered natural language
processing. This approach has been applied across five European pilot sites
with varying preservation challenges, demonstrating its adaptability to diverse
cultural heritage contexts. The implementation results show improved data
accessibility, enhanced analytical capabilities, and more effective
decision-making for conservation efforts.

</details>


### [141] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: 本研究构建了一个用于氮化硼纳米片（BNNS）聚合物导热复合材料的语言原生数据库，该数据库从论文中捕获轻结构化信息。


<details>
  <summary>Details</summary>
Motivation: 传统化学和材料研究严重依赖知识叙述，进步通常由基于语言的原理、机制和实验经验描述驱动，而非表格，限制了传统数据库和机器学习的利用。

Method: 通过复合检索（语义、关键词和数值过滤器）查询异构数据库，将文献合成为准确、可验证和专家风格的指导。

Result: 该系统能够合成文献为准确、可验证和专家风格的指导。

Conclusion: 该框架为LLM驱动的材料发现提供了丰富的语言基础，实现了高保真高效的检索增强生成（RAG）和工具增强代理，以交错检索与推理并提供可操作的标准操作程序（SOP）。

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


### [142] [MCTuner: Spatial Decomposition-Enhanced Database Tuning via LLM-Guided Exploration](https://arxiv.org/abs/2509.06298)
*Zihan Yan,Rui Xi,Mengshu Hou*

Main category: cs.DB

TL;DR: MCTuner是一个数据库旋钮调优框架，旨在通过最小化配置空间中无效区域的探索来优化数据库性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略领域知识或难以探索高维旋钮空间，导致调优成本高和性能欠佳。

Method: MCTuner采用MoE机制与专用LLM来识别性能关键旋钮，并引入空间分解算法以递归方式将空间划分为分层子空间，然后在这些子空间上执行贝叶斯优化。

Result: MCTuner在不同基准测试上实现了高达19.2%的性能提升，并且每次迭代的配置发现速度比最先进的方法快1.4倍。

Conclusion: MCTuner通过自适应旋钮调优框架，有效减少了无效配置空间的探索，并在数据库性能优化方面取得了显著成果。

Abstract: Database knob tuning is essential for optimizing the performance of modern
database management systems, which often expose hundreds of knobs with
continuous or categorical values. However, the large number of knobs and the
vast configuration space make it difficult to identify optimal settings
efficiently. Although learning-based tuning has shown promise, existing
approaches either ignore domain knowledge by relying solely on benchmark
feedback or struggle to explore the high-dimensional knob space, resulting in
high tuning costs and suboptimal performance. To address these challenges, we
propose MCTuner, an adaptive knob tuning framework that minimizes exploration
in ineffective regions of the configuration space. MCTuner employs a
Mixture-of-Experts (MoE) mechanism with specialized LLMs to identify
performance-critical knobs. In further, MCTuner introduces the first spatial
decomposition algorithm that recursively partitions the space into hierarchical
subspaces, on which Bayesian Optimization is performed to efficiently search
for near-optimal configurations. Evaluated on different benchmarks (OLAP, OLTP,
and HTAP), MCTuner achieves up to 19.2% performance gains and 1.4x faster
configuration discovery per iteration compared to state-of-the-art methods.

</details>


### [143] [Relational Algebras for Subset Selection and Optimisation](https://arxiv.org/abs/2509.06439)
*David Robert Pratten,Luke Mathieson,Fahimeh Ramezani*

Main category: cs.DB

TL;DR: 提出了一种统一的关系代数基础，用于子集选择和优化查询。


<details>
  <summary>Details</summary>
Motivation: 数据库社区缺乏统一的关系查询语言来进行子集选择和优化查询。

Method: 1. 扩展关系代数以完成域关系。2. 引入解集，一种在关系集上的高阶关系代数。3. 提供从解集到标准关系代数的结构保持翻译语义。

Result: 该框架实现了最强大的先前方法的功能，同时提供了先前工作中没有的理论清晰度和组合属性。

Conclusion: 通过多态SQL展示了这些代数开启的功能，其中标准子句在单个范例中无缝地表达了数据管理、子集选择和优化查询。

Abstract: The database community lacks a unified relational query language for subset
selection and optimisation queries, limiting both user expression and query
optimiser reasoning about such problems. Decades of research (latterly under
the rubric of prescriptive analytics) have produced powerful evaluation
algorithms with incompatible, ad-hoc SQL extensions that specify and filter
through distinct mechanisms. We present the first unified algebraic foundation
for these queries, introducing relational exponentiation to complete the
fundamental algebraic operations alongside union (addition) and cross product
(multiplication). First, we extend relational algebra to complete domain
relations-relations defined by characteristic functions rather than explicit
extensions-achieving the expressiveness of NP-complete/hard problems, while
simultaneously providing query safety for finite inputs. Second, we introduce
solution sets, a higher-order relational algebra over sets of relations that
naturally expresses search spaces as functions f: Base to Decision, yielding
|Decision|^|Base| candidate relations. Third, we provide structure-preserving
translation semantics from solution sets to standard relational algebra,
enabling mechanical translation to existing evaluation algorithms. This
framework achieves the expressiveness of the most powerful prior approaches
while providing the theoretical clarity and compositional properties absent in
previous work. We demonstrate the capabilities these algebras open up through a
polymorphic SQL where standard clauses seamlessly express data management,
subset selection, and optimisation queries within a single paradigm.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [144] [Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models](https://arxiv.org/abs/2509.05564)
*Chihiro Yamasaki,Kai Sugahara,Kazushi Okamoto*

Main category: cs.IR

TL;DR: 提出了一种名为KARL的框架，它结合主动学习和大型语言模型来扩展高质量的互补关系数据集，从而提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 行为标签噪声大且不可靠，功能标签依赖于昂贵的手动注释，限制了模型的泛化能力。

Method: 该框架策略性地融合了主动学习与大型语言模型（LLM），通过选择分类器认为最困难的数据点并使用LLM的标签扩展来有效地扩展高质量的FBL数据集。

Result: 在分布外（OOD）设置中，KARL将基线精度提高了高达37%。相比之下，在分布内（ID）设置中，改进小于0.5%，并且长时间学习可能会降低准确性。

Conclusion: KARL的知识扩展驱动了数据多样性，表明需要一种动态采样策略，该策略根据预测上下文（ID或OOD）调整多样性。

Abstract: Complementary recommendations play a crucial role in e-commerce by enhancing
user experience through suggestions of compatible items. Accurate
classification of complementary item relationships requires reliable labels,
but their creation presents a dilemma. Behavior-based labels are widely used
because they can be easily generated from interaction logs; however, they often
contain significant noise and lack reliability. While function-based labels
(FBLs) provide high-quality definitions of complementary relationships by
carefully articulating them based on item functions, their reliance on costly
manual annotation severely limits a model's ability to generalize to diverse
items. To resolve this trade-off, we propose Knowledge-Augmented Relation
Learning (KARL), a framework that strategically fuses active learning with
large language models (LLMs). KARL efficiently expands a high-quality FBL
dataset at a low cost by selectively sampling data points that the classifier
finds the most difficult and uses the label extension of the LLM. Our
experiments showed that in out-of-distribution (OOD) settings, an unexplored
item feature space, KARL improved the baseline accuracy by up to 37%. In
contrast, in in-distribution (ID) settings, the learned item feature space, the
improvement was less than 0.5%, with prolonged learning could degrade accuracy.
These contrasting results are due to the data diversity driven by KARL's
knowledge expansion, suggesting the need for a dynamic sampling strategy that
adjusts diversity based on the prediction context (ID or OOD).

</details>


### [145] [LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce](https://arxiv.org/abs/2509.05570)
*Yipeng Zhang,Bowen Liu,Xiaoshuang Zhang,Aritra Mandal,Zhe Wu,Canran Xu*

Main category: cs.IR

TL;DR: 本文提出了一种名为 LESER 的新框架，该框架使用实时搜索引擎反馈作为监督来微调上下文感知 LLM，以解决电子商务搜索中用户查询模糊、简短和不明确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经查询扩展和基于提示的 LLM 方法在实际环境中存在不足，难以捕捉细微的用户意图，生成的输出经常违反平台约束，并且依赖于难以在生产中扩展的工作流程。

Method: LESER 将查询扩展 формулируется 为检索优化任务，并利用组相对策略优化直接从相关性和覆盖率指标中学习。

Result: 在大型真实电子商务数据集上的评估表明，LESER 在离线和在线设置中都取得了显着改进。LESER 不仅提高了语义覆盖率和检索相关性，还带来了用户参与度的可衡量提升。

Conclusion: LESER 是一种实用且可扩展的现代搜索系统解决方案。

Abstract: User queries in e-commerce search are often vague, short, and underspecified,
making it difficult for retrieval systems to match them accurately against
structured product catalogs. This challenge is amplified by the one-to-many
nature of user intent, where a single query can imply diverse and competing
needs. Existing methods, including neural query expansion and prompting-based
LLM approaches, fall short in real-world settings: they struggle to capture
nuanced user intent, often generate outputs that violate platform constraints,
and rely on workflows that are difficult to scale in production. We propose
Learning to Expand via Search Engine-feedback Reinforcement (LESER), a novel
framework that fine-tunes a context-aware LLM using real-time search engine
feedback as supervision. LESER formulates query expansion as a retrieval
optimization task and leverages Group Relative Policy Optimization to learn
directly from relevance and coverage metrics. LESER is trained to reason over
search results and produce high quality query expansions that align with
platform rules and retrieval objectives. We evaluate LESER on large-scale,
real-world e-commerce datasets, demonstrating substantial improvements in both
offline and online settings. Our results show that LESER not only enhances
semantic coverage and retrieval relevance but also delivers measurable gains in
user engagement, making it a practical and scalable solution for modern search
systems.

</details>


### [146] [Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search](https://arxiv.org/abs/2509.05750)
*Ilias Azizi,Karima Echihab,Themis Palpanas,Vassilis Christophides*

Main category: cs.IR

TL;DR: 本文对向量搜索算法进行了全面的实验评估，比较了十二种最先进的方法在七个真实数据集上的性能，并探讨了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 向量数据在商业和科学应用中越来越普遍，其分析也变得越来越复杂。向量搜索是许多关键分析任务的支柱，基于图的方法已成为分析任务的最佳选择。

Method: 本文对十二种最先进的向量搜索方法在七个真实数据集上进行了详尽的实验评估。

Result: 实验结果表明，最好的方法通常基于增量插入和邻域多样化，并且基本图的选择会影响可扩展性。

Conclusion: 本文总结了现有方法的优势和局限性，并讨论了开放的研究方向，例如设计更复杂的数据自适应种子选择和多样化策略的重要性。

Abstract: Vector data is prevalent across business and scientific applications, and its
popularity is growing with the proliferation of learned embeddings. Vector data
collections often reach billions of vectors with thousands of dimensions, thus,
increasing the complexity of their analysis. Vector search is the backbone of
many critical analytical tasks, and graph-based methods have become the best
choice for analytical tasks that do not require guarantees on the quality of
the answers. Although several paradigms (seed selection, incremental insertion,
neighborhood propagation, neighborhood diversification, and divide-and-conquer)
have been employed to design in-memory graph-based vector search algorithms, a
systematic comparison of the key algorithmic advances is still missing. We
conduct an exhaustive experimental evaluation of twelve state-of-the-art
methods on seven real data collections, with sizes up to 1 billion vectors. We
share key insights about the strengths and limitations of these methods; e.g.,
the best approaches are typically based on incremental insertion and
neighborhood diversification, and the choice of the base graph can hurt
scalability. Finally, we discuss open research directions, such as the
importance of devising more sophisticated data adaptive seed selection and
diversification strategies.

</details>


### [147] [A Survey of Real-World Recommender Systems: Challenges, Constraints, and Industrial Perspectives](https://arxiv.org/abs/2509.06002)
*Kuan Zou,Aixin Sun*

Main category: cs.IR

TL;DR: 本文综述了工业推荐系统，对比了学术界的研究，旨在弥合两者之间的差距，促进合作。


<details>
  <summary>Details</summary>
Motivation: 学术研究受限于数据和平台，缺乏实际应用，无法充分理解推荐系统中的关键挑战。

Method: 系统性地回顾工业推荐系统，并与学术界的系统进行对比，总结实际应用场景及其挑战，并根据item特征和推荐目标，将工业推荐系统分为面向交易和面向内容两类。

Result: 强调了数据规模、实时要求和评估方法的主要差异，并概述了实际应用中如何应对这些挑战。

Conclusion: 概述了未来的研究方向，包括用户决策、经济和心理理论的整合，以及改进学术研究的具体建议。

Abstract: Recommender systems have generated tremendous value for both users and
businesses, drawing significant attention from academia and industry alike.
However, due to practical constraints, academic research remains largely
confined to offline dataset optimizations, lacking access to real user data and
large-scale recommendation platforms. This limitation reduces practical
relevance, slows technological progress, and hampers a full understanding of
the key challenges in recommender systems. In this survey, we provide a
systematic review of industrial recommender systems and contrast them with
their academic counterparts. We highlight key differences in data scale,
real-time requirements, and evaluation methodologies, and we summarize major
real-world recommendation scenarios along with their associated challenges. We
then examine how industry practitioners address these challenges in
Transaction-Oriented Recommender Systems and Content-Oriented Recommender
Systems, a new classification grounded in item characteristics and
recommendation objectives. Finally, we outline promising research directions,
including the often-overlooked role of user decision-making, the integration of
economic and psychological theories, and concrete suggestions for advancing
academic research. Our goal is to enhance academia's understanding of practical
recommender systems, bridge the growing development gap, and foster stronger
collaboration between industry and academia.

</details>


### [148] [Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs](https://arxiv.org/abs/2509.06185)
*Firas Jarboui,Issa Memari*

Main category: cs.IR

TL;DR: 本文提出了一种利用检索评分分布的熵来平衡对话式推荐系统中探索和利用的方法，该方法允许LLM驱动的代理实时了解任意大的产品目录，而无需扩大其上下文窗口。


<details>
  <summary>Details</summary>
Motivation: 在对话式推荐系统中，平衡探索（澄清用户需求）和利用（提出建议）仍然具有挑战性，尤其是在使用具有大量产品目录的大型语言模型（LLM）时。

Method: 该方法使用神经检索器来获取与用户查询相关的项目，并计算重新排序的分数的熵，以动态地路由对话策略：低熵（特定）查询触发直接推荐，而高熵（模糊）查询提示探索性问题。

Result: 该策略允许LLM驱动的代理实时了解任意大的目录，而无需扩大其上下文窗口。

Conclusion: 该方法通过对检索分数分布的熵进行建模来解决对话式推荐系统中探索和利用之间的平衡问题。

Abstract: Conversational recommender systems promise rich interactions for e-commerce,
but balancing exploration (clarifying user needs) and exploitation (making
recommendations) remains challenging, especially when deploying large language
models (LLMs) with vast product catalogs. We address this challenge by modeling
the breadth of user interest via the entropy of retrieval score distributions.
Our method uses a neural retriever to fetch relevant items for a user query and
computes the entropy of the re-ranked scores to dynamically route the dialogue
policy: low-entropy (specific) queries trigger direct recommendations, whereas
high-entropy (ambiguous) queries prompt exploratory questions. This simple yet
effective strategy allows an LLM-driven agent to remain aware of an arbitrarily
large catalog in real-time without bloating its context window.

</details>


### [149] [Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods](https://arxiv.org/abs/2509.06195)
*Jinrui Yang,Fan Jiang,Timothy Baldwin*

Main category: cs.IR

TL;DR: 论文探讨了多语言信息检索（MLIR）系统中语言公平性的问题，即语义相同的不同语言查询应返回等效的排序列表。


<details>
  <summary>Details</summary>
Motivation: 确保不同语言用户公平地获取信息至关重要。论文基于假设：语义相同的不同语言查询应在相同的多语言文档上产生等效的排序列表。

Method: 使用传统检索方法和基于mBERT及XLM-R的DPR神经排序器评估公平性。引入了名为LaKDA的新型损失函数，以减轻神经MLIR方法中的语言偏差。

Result: 揭示了当前MLIR技术中固有的语言偏差，不同检索方法之间存在显著差异。LaKDA在增强语言公平性方面有效。

Conclusion: 分析揭示了多语言信息检索技术中存在的语言偏差，并证明了LaKDA损失函数在提升语言公平性方面的有效性。

Abstract: Language fairness in multilingual information retrieval (MLIR) systems is
crucial for ensuring equitable access to information across diverse languages.
This paper sheds light on the issue, based on the assumption that queries in
different languages, but with identical semantics, should yield equivalent
ranking lists when retrieving on the same multilingual documents. We evaluate
the degree of fairness using both traditional retrieval methods, and a DPR
neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a
novel loss designed to mitigate language biases in neural MLIR approaches. Our
analysis exposes intrinsic language biases in current MLIR technologies, with
notable disparities across the retrieval methods, and the effectiveness of
LaKDA in enhancing language fairness.

</details>


### [150] [AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation](https://arxiv.org/abs/2509.06452)
*Enrico Palumbo,Gustavo Penha,Alva Liu,Marcus Eltscheminov,Jefferson Carvalho dos Santos,Alice Wang,Hugues Bouchard,Humberto Jesús Corona Pampin,Michelle Tran Luu*

Main category: cs.IR

TL;DR: Spotify introduces AudioBoost, a system using LLMs to generate synthetic queries for audiobooks, improving their searchability.


<details>
  <summary>Details</summary>
Motivation: To enable users to explore audiobooks on Spotify through broad searches by topic, genre, etc., especially in a cold-start scenario with limited user interactions.

Method: AudioBoost uses LLMs to generate synthetic queries based on audiobook metadata, indexing them in Query AutoComplete (QAC) and the Search Retrieval engine.

Result: Offline evaluation shows increased retrievability and high quality of synthetic queries. Online A/B testing shows a +0.7% increase in audiobook impressions, +1.22% in audiobook clicks, and +1.82% in audiobook exploratory query completions.

Conclusion: AudioBoost effectively improves audiobook discovery on Spotify by enhancing query formulation and retrieval.

Abstract: Spotify has recently introduced audiobooks as part of its catalog,
complementing its music and podcast offering. Search is often the first entry
point for users to access new items, and an important goal for Spotify is to
support users in the exploration of the audiobook catalog. More specifically,
we would like to enable users without a specific item in mind to broadly search
by topic, genre, story tropes, decade, and discover audiobooks, authors and
publishers they may like. To do this, we need to 1) inspire users to type more
exploratory queries for audiobooks and 2) augment our retrieval systems to
better deal with exploratory audiobook queries. This is challenging in a
cold-start scenario, where we have a retrievabiliy bias due to the little
amount of user interactions with audiobooks compared to previously available
items such as music and podcast content. To address this, we propose
AudioBoost, a system to boost audiobook retrievability in Spotify's Search via
synthetic query generation. AudioBoost leverages Large Language Models (LLMs)
to generate synthetic queries conditioned on audiobook metadata. The synthetic
queries are indexed both in the Query AutoComplete (QAC) and in the Search
Retrieval engine to improve query formulation and retrieval at the same time.
We show through offline evaluation that synthetic queries increase
retrievability and are of high quality. Moreover, results from an online A/B
test show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in
audiobook clicks, and +1.82% in audiobook exploratory query completions.

</details>


### [151] [Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking](https://arxiv.org/abs/2509.06472)
*Haoxiang Jin,Ronghan Li,Qiguang Miao,Zixiang Lu*

Main category: cs.IR

TL;DR: 该论文提出了一种新的后检索知识过滤方法，以解决检索增强生成（RAG）中知识边界感知不足的问题，通过利用LLM的内部隐藏状态构建置信度检测模型，并 fine-tune 一个 reranker 来优先排序下游 LLM 更喜欢的上下文，同时引入基于置信度的动态检索（CBDR）以自适应地触发检索，从而提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于离散标签或有限信号，忽略了 LLM 连续内部隐藏状态中的丰富信息，导致知识边界感知不足。

Method: 1. 构建基于 LLM 内部隐藏状态的置信度检测模型，以量化检索到的上下文如何增强模型的置信度。
2. 使用该模型构建偏好数据集 (NQ_Rerank) 以 fine-tune reranker，使其能够优先排序下游 LLM 更喜欢的上下文。
3. 引入基于置信度的动态检索 (CBDR)，它根据 LLM 对原始问题的初始置信度自适应地触发检索。

Result: 在上下文筛选和端到端 RAG 性能方面取得了显着改进，同时在保持竞争力的准确性的前提下显着降低了检索成本。

Conclusion: 该论文提出的方法能够有效提高 RAG 系统的准确性和效率，并降低检索成本。

Abstract: Large Language Models (LLMs) often generate inaccurate responses
(hallucinations) when faced with questions beyond their knowledge scope.
Retrieval-Augmented Generation (RAG) addresses this by leveraging external
knowledge, but a critical challenge remains: determining whether retrieved
contexts effectively enhance the model`s ability to answer specific queries.
This challenge underscores the importance of knowledge boundary awareness,
which current methods-relying on discrete labels or limited signals-fail to
address adequately, as they overlook the rich information in LLMs` continuous
internal hidden states. To tackle this, we propose a novel post-retrieval
knowledge filtering approach. First, we construct a confidence detection model
based on LLMs` internal hidden states to quantify how retrieved contexts
enhance the model`s confidence. Using this model, we build a preference dataset
(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts
preferred by the downstream LLM during reranking. Additionally, we introduce
Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval
based on the LLM`s initial confidence in the original question, reducing
knowledge conflicts and improving efficiency. Experimental results demonstrate
significant improvements in accuracy for context screening and end-to-end RAG
performance, along with a notable reduction in retrieval costs while
maintaining competitive accuracy.

</details>


### [152] [Reasoning-enhanced Query Understanding through Decomposition and Interpretation](https://arxiv.org/abs/2509.06544)
*Yunfei Zhong,Jun Yang,Yixing Fan,Jiafeng Guo,Lixin Su,Maarten de Rijke,Ruqing Zhang,Dawei Yin,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出了一种名为ReDI的，基于LLM的，增强推理的查询理解方法，用于处理长query和复杂intent。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理短query上表现良好，但在长query和复杂intent上的效果有待提高。

Method: ReDI 通过分解query，进行语义解释，并融合检索结果来理解query。

Result: 在BRIGHT和BEIR数据集上的实验表明，ReDI 优于现有方法。

Conclusion: ReDI 在稀疏和稠密检索范例中均有效。

Abstract: Accurate inference of user intent is crucial for enhancing document retrieval
in modern search engines. While large language models (LLMs) have made
significant strides in this area, their effectiveness has predominantly been
assessed with short, keyword-based queries. As AI-driven search evolves,
long-form queries with intricate intents are becoming more prevalent, yet they
remain underexplored in the context of LLM-based query understanding (QU). To
bridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query
understanding through Decomposition and Interpretation. ReDI leverages the
reasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)
it breaks down complex queries into targeted sub-queries to accurately capture
user intent; (ii) it enriches each sub-query with detailed semantic
interpretations to improve the query-document matching; and (iii) it
independently retrieves documents for each sub-query and employs a fusion
strategy to aggregate the results for the final ranking. We compiled a
large-scale dataset of real-world complex queries from a major search engine
and distilled the query understanding capabilities of teacher models into
smaller models for practical application. Experiments on BRIGHT and BEIR
demonstrate that ReDI consistently surpasses strong baselines in both sparse
and dense retrieval paradigms, affirming its effectiveness.

</details>


### [153] [UniSearch: Rethinking Search System with a Unified Generative Architecture](https://arxiv.org/abs/2509.06887)
*Jiahui Chen,Xiaoze Jiang,Zhibo Wang,Quanzhi Zhu,Junyao Zhao,Feng Hu,Kang Pan,Ao Xie,Maohua Pei,Zhiheng Qin,Hongjing Zhang,Zhixin Zhai,Xiaobo Guo,Runbin Zhou,Kefeng Wang,Mingyang Geng,Cheng Chen,Jingshan Lv,Yupeng Huang,Xiao Liang,Han Li*

Main category: cs.IR

TL;DR: 提出了一种名为UniSearch的统一生成搜索框架，用于解决传统搜索系统多模块设计和维护的复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法并非真正的端到端，通常分别训练项目编码器和生成器，导致目标不一致和泛化能力有限。

Method: UniSearch用集成了搜索生成器和视频编码器的端到端架构替换了级联pipeline，并引入了搜索偏好优化（SPO）以利用真实用户反馈。

Result: 在工业规模数据集上的大量实验以及在短视频和直播搜索场景中的在线A/B测试表明UniSearch具有强大的有效性和部署潜力。

Conclusion: UniSearch在直播搜索中的部署产生了近年来产品历史上单次实验的最大改进，突显了其在实际应用中的实用价值。

Abstract: Modern search systems play a crucial role in facilitating information
acquisition. Traditional search engines typically rely on a cascaded
architecture, where results are retrieved through recall, pre-ranking, and
ranking stages. The complexity of designing and maintaining multiple modules
makes it difficult to achieve holistic performance gains. Recent advances in
generative recommendation have motivated the exploration of unified generative
search as an alternative. However, existing approaches are not genuinely
end-to-end: they typically train an item encoder to tokenize candidates first
and then optimize a generator separately, leading to objective inconsistency
and limited generalization. To address these limitations, we propose UniSearch,
a unified generative search framework for Kuaishou Search. UniSearch replaces
the cascaded pipeline with an end-to-end architecture that integrates a Search
Generator and a Video Encoder. The Generator produces semantic identifiers of
relevant items given a user query, while the Video Encoder learns latent item
embeddings and provides their tokenized representations. A unified training
framework jointly optimizes both components, enabling mutual enhancement and
improving representation quality and generation accuracy. Furthermore, we
introduce Search Preference Optimization (SPO), which leverages a reward model
and real user feedback to better align generation with user preferences.
Extensive experiments on industrial-scale datasets, together with online A/B
testing in both short-video and live search scenarios, demonstrate the strong
effectiveness and deployment potential of UniSearch. Notably, its deployment in
live search yields the largest single-experiment improvement in recent years of
our product's history, highlighting its practical value for real-world
applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [154] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）的非学习（Unlearning）方法，发现现有基准测试和抽样方法存在局限性，并提出了一系列最佳实践。


<details>
  <summary>Details</summary>
Motivation: 现有LLM非学习研究未能充分反映现实世界数据的复杂性和关系，且常用抽样方法的有效性和稳定性未经严格检验。

Method: 系统评估了现有基准测试中单邻居集的使用以及1:1抽样和循环迭代抽样方法的有效性，并提出了一种模块化的实体级别非学习（MELU）策略。

Result: 研究表明，依赖单一邻居集是次优的，标准抽样方法会掩盖性能权衡。MELU策略结合稳健的算法，为有效的非学习提供了一条清晰且稳定的路径。

Conclusion: 本文提出并验证了一系列最佳实践，包括整合多样化的邻居集以平衡非学习效果和模型效用，以及使用MELU策略替代低效的1:1抽样方法。

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [155] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的鲁棒微调方法，该方法在函数空间中约束微调模型和预训练模型之间的距离，并引入一致性正则化以提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒微调方法不能始终提高不同模型架构的 OOD 鲁棒性，因为它们不能很好地优化函数空间。

Method: 提出一种新的正则化方法，该方法在函数空间中约束微调模型和预训练模型之间的距离，并引入一致性正则化以促进扰动样本的稳定预测。

Result: 该方法在各种 CLIP backbones 上都能持续提高下游任务的 ID 微调性能和 OOD 鲁棒性，优于现有的基于正则化的鲁棒微调方法。

Conclusion: 该方法能够有效地提高模型的 OOD 鲁棒性，同时保持 ID 性能。

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [156] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: 本文研究了图神经网络（GNNs）中的拓扑隐私风险，发现GNNs容易受到图级推理攻击。提出了拓扑推理攻击（TIAs）来重建目标训练图的结构，并发现现有的边级差分隐私机制不足以缓解风险。为此，引入了私有图重建（PGR），以保护拓扑隐私并保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: GNNs被广泛应用于图结构数据的学习，但其广泛应用引发了严重的隐私问题。之前的研究主要集中在边级别的隐私，而忽略了拓扑隐私——图的整体结构的机密性。

Method: 提出了拓扑推理攻击（TIAs），利用黑盒访问GNN模型来重建目标训练图的结构。同时，引入了私有图重建（PGR），将其表述为一个双层优化问题，使用元梯度迭代生成合成训练图，并基于不断演变的图并发更新GNN模型。

Result: 实验表明，PGR在对模型准确性影响最小的情况下，显著降低了拓扑泄漏。

Conclusion: GNNs容易受到图级推理攻击，现有的边级差分隐私机制不足以缓解风险。PGR可以有效地保护拓扑隐私并保持模型准确性。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [157] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: 本文研究了针对大型语言模型的成员推理攻击，发现通过分析模型的内部表示（隐藏状态和注意力模式）可以有效检测训练数据是否被模型记住。


<details>
  <summary>Details</summary>
Motivation: 现有的成员推理攻击对大型语言模型效果不佳，暗示大规模预训练方法可能不存在隐私泄露风险。本文旨在通过研究LLMs的内部表示，探索潜在的成员推理信号。

Method: 本文提出了名为memTrace的框架，该框架通过提取transformer隐藏状态和注意力模式中的信息信号，分析层间表示动态、注意力分布特征和跨层转换模式来检测潜在的记忆指纹。

Result: memTrace在多个模型系列上实现了强大的成员检测，在流行的MIA基准测试中平均AUC得分为0.85。

Conclusion: 研究结果表明，即使基于输出的信号受到保护，内部模型行为也可以揭示训练数据暴露的方面，强调需要进一步研究成员隐私以及为大型语言模型开发更强大的隐私保护训练技术。

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [158] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: 提出了一种利用上下文Bandit算法动态学习用户最佳内容类型分布的校准方法，以解决Spotify主页内容类型不平衡和用户偏好随情境变化的问题。


<details>
  <summary>Details</summary>
Motivation: 历史数据严重偏向音乐，难以提供平衡和个性化的内容组合；用户对不同内容类型的偏好可能随时间、日期甚至设备而变化。

Method: 利用上下文Bandit算法动态学习每个用户基于情境和偏好的最佳内容类型分布。

Result: 离线和在线结果表明，Spotify主页的精确度和用户参与度得到了提高，特别是对于代表性不足的内容类型（如播客）。

Conclusion: 该方法通过适应用户在不同情境下对不同内容类型的兴趣变化，从而提高用户参与度。

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [159] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个名为X-SQL的Text-to-SQL框架，该框架通过更有效地利用数据库schema信息，并在系统中使用Multi-LLMs，在Spider数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL框架忽略了数据库schema信息在生成高质量SQL查询中的重要性。

Method: 论文提出了一个新颖的数据库schema专家，包含X-Linking和X-Admin两个组件，分别用于Schema Linking和Schema Understanding。同时，论文还尝试使用Multi-LLMs来提升系统性能。

Result: X-SQL在Spider-Dev数据集上取得了84.9%的Execution Accuracy，在Spider-Test数据集上取得了82.5%的Execution Accuracy。

Conclusion: X-SQL是目前基于开源模型的领先Text-to-SQL框架。

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [160] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的自监督学习框架PLanTS，用于处理多元时间序列数据，该框架考虑了时间序列的周期性结构和动态演化。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在处理高维、少标签和非平稳的多元时间序列数据时面临挑战。现有的自监督学习方法忽略了时间序列的周期性结构和潜在状态的动态演化。

Method: 设计了一个周期感知的多粒度patching机制和一个广义对比损失，以保留跨多个时间分辨率的实例级别和状态级别的相似性。设计了一个next-transition prediction pretext task，鼓励表示编码关于未来状态演化的预测信息。

Result: PLanTS在各种下游任务中始终优于现有的自监督学习方法，并且与基于DTW的方法相比，具有更高的运行时效率。

Conclusion: PLanTS能够有效地提高多元时间序列数据的表示质量，并在各种下游任务中表现出色。

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [161] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: 本文提出了一种利用信号时序逻辑 (STL) 规范来定义生物分子神经网络 (BNN) 训练目标的方法，以解决 BNN 训练中缺乏目标数据的问题。


<details>
  <summary>Details</summary>
Motivation: BNN 具有超越简单生物电路的通用函数逼近能力，但由于缺乏目标数据，训练 BNN 仍然具有挑战性。

Method: 利用 STL 的定量语义，实现基于梯度的 BNN 权重优化，并引入一种学习算法，使 BNN 能够在生物系统中执行回归和控制任务。

Result: 数值实验表明，基于 STL 的学习能够有效地解决所研究的回归和控制任务。

Conclusion: 基于 STL 的学习可以有效地训练 BNN，使其能够在生物系统中执行回归和控制任务。

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [162] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: 研究训练数据分布对图像分类模型性能的影响，提出了一种无需重新训练即可理解模型对未见数据预测置信度的框架。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据分布对图像分类模型性能的影响。

Method: 通过分析训练集的嵌入，根据嵌入空间中与训练分布的距离过滤掉低置信度预测。

Result: 在多个分类模型上展示了该方法，表明在不同架构上性能均得到一致提升。使用多个嵌入模型可以更稳健地估计置信度，从而更好地检测和排除分布外样本，从而进一步提高准确性。

Conclusion: 该方法是模型无关且可推广的，具有超出计算机视觉的潜在应用，包括预测可靠性至关重要的自然语言处理等领域。

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [163] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: MambaLite-Micro is the first deployment of a Mamba-based model on microcontrollers, using a C-based runtime-free engine.


<details>
  <summary>Details</summary>
Motivation: Deploying Mamba models on MCUs is difficult due to memory limitations, lack of operator support, and absence of toolchains.

Method: The pipeline maps a trained PyTorch Mamba model to on-device execution by exporting weights and implementing a handcrafted Mamba layer in C with optimizations.

Result: MambaLite-Micro reduces memory usage by 83% with minimal numerical error and achieves 100% consistency with PyTorch baselines on KWS and HAR tasks. It was validated on ESP32S3 and STM32H7 microcontrollers.

Conclusion: The work demonstrates consistent operation across heterogeneous embedded platforms, enabling the use of Mamba in resource-constrained applications.

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [164] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: 提出了自对齐奖励（SAR），以提高大型语言模型（LLM）推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习方法在LLM推理中存在效率问题，例如过度冗长的推理和高计算成本，并且现有解决方案通常会牺牲准确性。

Method: 定义SAR为基于查询条件下的答案与独立答案之间的相对困惑度差异，从而倾向于简洁且特定于查询的响应。将SAR与PPO和GRPO等主流RL算法集成。

Result: 在7个基准测试中，SAR将准确率提高了4%，同时将推理成本降低了30%。SAR在正确性和效率之间实现了帕累托最优的权衡。

Conclusion: 自对齐奖励可以作为可验证奖励的细粒度补充，为更高效、更有效的LLM训练铺平道路。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [165] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-1.5是一种通过实例重加权来优化多模态过程奖励模型(PRM)训练的框架。


<details>
  <summary>Details</summary>
Motivation: 解决多模态过程奖励模型训练中存在的分布偏移和噪声数据问题。

Method: 提出DreamPRM-1.5框架，通过双层优化自适应地调整每个训练样本的重要性。设计了两种互补策略：Instance Table（适用于较小数据集）和Instance Net（可扩展到较大数据集）。

Result: 在MMMU基准测试中，DreamPRM-1.5实现了84.6的准确率。

Conclusion: DreamPRM-1.5框架在测试时扩展后，性能超过了GPT-5。

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


### [166] [Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning](https://arxiv.org/abs/2509.05874)
*Shao-An Yin*

Main category: cs.LG

TL;DR: 提出了一种深度强化学习框架，用于在信息有限的情况下选择阅读哪些论文，以模拟人类知识构建。


<details>
  <summary>Details</summary>
Motivation: 科学文献快速扩张，难以获取新知识，特别是在专业领域，推理复杂，全文访问受限，目标参考文献稀疏。

Method: 使用深度强化学习框架，模拟人类知识构建，优先考虑在有限时间和成本下阅读哪些论文。

Result: 在药物-基因关系发现上进行了评估，访问权限仅限于标题和摘要，结果表明，人类和机器都可以从部分信息中有效地构建知识。

Conclusion: 在信息有限的情况下，人类和机器都可以有效地构建知识。

Abstract: The rapid expansion of scientific literature makes it increasingly difficult
to acquire new knowledge, particularly in specialized domains where reasoning
is complex, full-text access is restricted, and target references are sparse
among a large set of candidates. We present a Deep Reinforcement Learning
framework for sparse reference selection that emulates human knowledge
construction, prioritizing which papers to read under limited time and cost.
Evaluated on drug--gene relation discovery with access restricted to titles and
abstracts, our approach demonstrates that both humans and machines can
construct knowledge effectively from partial information.

</details>


### [167] [Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks](https://arxiv.org/abs/2509.05545)
*Yang Yu*

Main category: cs.LG

TL;DR: 本文提出了一种新的分层强化学习框架，称为具有预期性的强化学习 (RLA)，用于解决长时程目标条件任务。


<details>
  <summary>Details</summary>
Motivation: 解决长时程目标条件任务仍然是强化学习中的一个重大挑战。分层强化学习 (HRL) 通过将任务分解为更易于管理的子任务来解决这个问题，但层级的自动发现和多层策略的联合训练通常会受到不稳定性的影响，并且可能缺乏理论保证。

Method: RLA 智能体学习两种协同模型：一种低级、目标条件策略，学习达到指定子目标；以及一种高级预期模型，其功能类似于规划器，在达到最终目标的最佳路径上提出中间子目标。RLA 的关键特征是预期模型的训练，该训练以价值几何一致性原则为指导，并进行正则化以防止退化解。

Result: 我们提供了 RLA 在各种条件下接近全局最优策略的证明，为长时程目标条件任务中的分层规划和执行建立了一种有原则且收敛的方法。

Conclusion: RLA 是一种有原则且可扩展的框架，旨在解决 HRL 的局限性。

Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge
in reinforcement learning (RL). Hierarchical reinforcement learning (HRL)
addresses this by decomposing tasks into more manageable sub-tasks, but the
automatic discovery of the hierarchy and the joint training of multi-level
policies often suffer from instability and can lack theoretical guarantees. In
this paper, we introduce Reinforcement Learning with Anticipation (RLA), a
principled and potentially scalable framework designed to address these
limitations. The RLA agent learns two synergistic models: a low-level,
goal-conditioned policy that learns to reach specified subgoals, and a
high-level anticipation model that functions as a planner, proposing
intermediate subgoals on the optimal path to a final goal. The key feature of
RLA is the training of the anticipation model, which is guided by a principle
of value geometric consistency, regularized to prevent degenerate solutions. We
present proofs that RLA approaches the globally optimal policy under various
conditions, establishing a principled and convergent method for hierarchical
planning and execution in long-horizon goal-conditioned tasks.

</details>


### [168] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: 提出了一种名为 Persona 的新颖个性化方法，使用基于原型的无反向传播参数编辑框架，以增强模型泛化能力，而无需部署后重新训练。


<details>
  <summary>Details</summary>
Motivation: 设备上的实时数据分布变化对设备上轻量级模型的泛化提出了挑战。目前的研究主要依赖于数据密集型和计算成本高的微调方法，而这个问题经常被忽视。

Method: Persona 采用云中的神经适配器来生成基于实时设备数据的参数编辑矩阵。该矩阵巧妙地使设备上的模型适应于 prevailing 数据分布，有效地将它们聚类成原型模型。原型通过参数编辑矩阵动态地细化，从而促进高效的演化。此外，交叉层知识转移的集成确保了一致的和上下文感知的多层参数变化和原型分配。

Result: 在多个数据集上的视觉任务和推荐任务上进行的大量实验证实了 Persona 的有效性和通用性。

Conclusion: Persona 是一种有效的个性化方法，可以增强模型泛化能力，而无需部署后重新训练。

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [169] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: ProfilingAgent使用大型语言模型（LLM）来自动化压缩，通过结构化剪枝和后训练动态量化，针对特定架构设计策略，实现模型优化。


<details>
  <summary>Details</summary>
Motivation: 现有压缩技术忽略了架构和运行时异构性，且很少有工具将性能分析整合到自动化流程中，导致基础模型面临日益增长的计算和内存瓶颈，限制了其在资源受限平台上的部署。

Method: 提出了一种名为ProfilingAgent的性能分析引导的agent方法，该方法使用大型语言模型（LLM）通过结构化剪枝和后训练动态量化来自动压缩。该模块化多agent系统基于静态指标（MACs，参数计数）和动态信号（延迟，内存）进行推理，以设计特定于架构的策略。

Result: 在ImageNet-1K、CIFAR-10和CIFAR-100上，使用ResNet-101、ViT-B/16、Swin-B和DeiT-B/16进行的实验表明，剪枝保持了有竞争力的或提高的准确性（ImageNet-1K上约1%的下降，较小数据集上ViT-B/16的+2%的收益），而量化实现了高达74%的内存节省，且准确性损失小于0.5%。量化还产生了一致的推理加速，高达1.74倍。

Conclusion: 这些结果表明，agent系统是性能分析引导模型优化的可扩展解决方案。

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [170] [Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2509.05615)
*Xiaoguang Zhu,Lianlong Sun,Yang Liu,Pengyi Jiang,Uma Srivatsa,Nipavan Chiamvimonvat,Vladimir Filkov*

Main category: cs.LG

TL;DR: 医学多模态表征学习旨在整合异构临床数据以支持预测建模，但现实世界的医疗数据集经常因成本、协议或患者特定约束而存在模态缺失。本文旨在解决由于数据采集过程本身引入的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要通过从原始数据空间或特征空间中的可用观察中学习来解决这个问题，但通常忽略了数据采集过程本身引入的潜在偏差。本文识别了两种阻碍模型泛化的偏差：缺失偏差和分布偏差。

Method: 本文对数据生成过程进行了结构因果分析，并提出了一个统一的框架，该框架与现有的基于直接预测的多模态学习方法兼容。该方法包括两个关键组成部分：(1)一个基于后门调整的近似因果干预的缺失去混淆模块；(2)一个双分支神经网络，它明确地将因果特征与虚假相关性分离。

Result: 在真实世界的公共和院内数据集上评估了该方法，证明了其有效性和因果洞察力。

Conclusion: 该方法在医学多模态表征学习中，有效地解决了由于数据缺失和偏差导致的模型泛化问题，并在实践中取得了良好的效果。

Abstract: Medical multimodal representation learning aims to integrate heterogeneous
clinical data into unified patient representations to support predictive
modeling, which remains an essential yet challenging task in the medical data
mining community. However, real-world medical datasets often suffer from
missing modalities due to cost, protocol, or patient-specific constraints.
Existing methods primarily address this issue by learning from the available
observations in either the raw data space or feature space, but typically
neglect the underlying bias introduced by the data acquisition process itself.
In this work, we identify two types of biases that hinder model generalization:
missingness bias, which results from non-random patterns in modality
availability, and distribution bias, which arises from latent confounders that
influence both observed features and outcomes. To address these challenges, we
perform a structural causal analysis of the data-generating process and propose
a unified framework that is compatible with existing direct prediction-based
multimodal learning methods. Our method consists of two key components: (1) a
missingness deconfounding module that approximates causal intervention based on
backdoor adjustment and (2) a dual-branch neural network that explicitly
disentangles causal features from spurious correlations. We evaluated our
method in real-world public and in-hospital datasets, demonstrating its
effectiveness and causal insights.

</details>


### [171] [OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search](https://arxiv.org/abs/2509.05656)
*Bo Lyu,Yu Cui,Tuo Shi,Ke Li*

Main category: cs.LG

TL;DR: OptiProxy-NAS: Reformulates NAS space to be continuous, differentiable, and smooth for gradient-based search.


<details>
  <summary>Details</summary>
Motivation: Computationally expensive NAS with discrete, vast, and spiky search space.

Method: Proposes OptiProxy-NAS, an optimization proxy to streamline NAS as an end-to-end optimization framework using a proxy representation for continuous, differentiable NAS space.

Result: Superior search results and efficiency on 12 NAS tasks across 4 search spaces in computer vision, NLP, and resource-constrained NAS.

Conclusion: Demonstrates the flexibility through experiments on low-fidelity scenarios.

Abstract: Neural architecture search (NAS) is a hard computationally expensive
optimization problem with a discrete, vast, and spiky search space. One of the
key research efforts dedicated to this space focuses on accelerating NAS via
certain proxy evaluations of neural architectures. Different from the prevalent
predictor-based methods using surrogate models and differentiable architecture
search via supernetworks, we propose an optimization proxy to streamline the
NAS as an end-to-end optimization framework, named OptiProxy-NAS. In
particular, using a proxy representation, the NAS space is reformulated to be
continuous, differentiable, and smooth. Thereby, any differentiable
optimization method can be applied to the gradient-based search of the relaxed
architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$
search spaces across three different domains including computer vision, natural
language processing, and resource-constrained NAS fully demonstrate the
superior search results and efficiency. Further experiments on low-fidelity
scenarios verify the flexibility.

</details>


### [172] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: 本文提出了一种基于主动学习的时间序列异常检测方法，通过选择性地查询多元时间序列的标签来优化阈值选择过程。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督时间序列异常检测方法通常阈值设置不佳，或者需要使用带标签的数据子集进行校准，但在现实世界中，带标签的数据子集通常不可用。

Method: 本文将主动学习与现有的无监督异常检测方法相结合，并引入了一种新的查询策略，称为基于差异性的查询策略 (DQS)，该策略旨在通过使用动态时间规整评估异常分数之间的相似性来最大化查询样本的多样性。

Result: DQS 在小预算场景中表现最佳，但其他方法在面对错误标记时似乎更稳健。所有查询策略都优于无监督阈值，即使在存在错误标记的情况下也是如此。

Conclusion: 只要可以查询 Oracle，建议采用基于主动学习的阈值。查询策略的选择取决于 Oracle 的专业知识和他们愿意标记的样本数量。

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [173] [GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR](https://arxiv.org/abs/2509.05671)
*Labani Halder,Tanmay Sen,Sarbani Palit*

Main category: cs.LG

TL;DR: 本文提出了一种基于图的多模态联邦学习框架 GraMFedDHAR，用于人体活动识别 (HAR) 任务。


<details>
  <summary>Details</summary>
Motivation: 由于噪声或不完整的测量、标记示例的稀缺以及隐私问题，使用多模态传感器数据进行人体活动识别 (HAR) 仍然具有挑战性。传统的集中式深度学习方法通常受到基础设施可用性、网络延迟和数据共享限制的约束。

Method: 该方法将压力垫、深度相机和多个加速度计等不同的传感器流建模为特定于模态的图，通过残差图卷积神经网络 (GCN) 进行处理，并通过基于注意力的加权融合，而不是简单的连接。

Result: 实验结果表明，所提出的 MultiModalGCN 模型优于基线 MultiModalFFN，在集中式和联邦范例中的非 DP 设置中，准确率提高了 2%。更重要的是，在差分隐私约束下观察到显着改进：MultiModalGCN 始终优于 MultiModalFFN，性能差距为 7% 到 13%，具体取决于隐私预算和设置。

Conclusion: 这些结果突出了基于图建模在多模态学习中的鲁棒性，其中 GNN 被证明更能抵抗 DP 噪声引入的性能下降。

Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains
challenging due to noisy or incomplete measurements, scarcity of labeled
examples, and privacy concerns. Traditional centralized deep learning
approaches are often constrained by infrastructure availability, network
latency, and data sharing restrictions. While federated learning (FL) addresses
privacy by training models locally and sharing only model parameters, it still
has to tackle issues arising from the use of heterogeneous multimodal data and
differential privacy requirements. In this article, a Graph-based Multimodal
Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse
sensor streams such as a pressure mat, depth camera, and multiple
accelerometers are modeled as modality-specific graphs, processed through
residual Graph Convolutional Neural Networks (GCNs), and fused via
attention-based weighting rather than simple concatenation. The fused
embeddings enable robust activity classification, while differential privacy
safeguards data during federated aggregation. Experimental results show that
the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with
up to 2 percent higher accuracy in non-DP settings in both centralized and
federated paradigms. More importantly, significant improvements are observed
under differential privacy constraints: MultiModalGCN consistently surpasses
MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on
the privacy budget and setting. These results highlight the robustness of
graph-based modeling in multimodal learning, where GNNs prove more resilient to
the performance degradation introduced by DP noise.

</details>


### [174] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: 提出了一种新的分布式训练方法，以加速深度神经网络的训练。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练耗时很长，尤其是在网络很深、数据集很大的情况下。

Method: 结合了数据并行和完全解耦的并行反向传播算法。

Result: 该方法在特定条件下可以收敛到临界点，并在CIFAR-10数据集上进行了有效性验证。

Conclusion: 该方法可以显著提高训练效率。

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [175] [Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure](https://arxiv.org/abs/2509.05697)
*Iara Cunha,Marcos Eduardo Valle*

Main category: cs.LG

TL;DR: 本文提出了一种使用凸凹过程(CCP)训练形态感知器的方法。


<details>
  <summary>Details</summary>
Motivation: 形态感知器的形态算子的不可微性使得基于梯度的优化方法不适合训练这种网络，因此通常采用不依赖于梯度信息的替代策略。

Method: 将训练问题表述为凸函数(DC)的差，并使用CCP迭代求解，从而产生一系列线性规划子问题。

Result: 计算实验表明了所提出的训练方法在解决MPCL网络分类任务中的有效性。

Conclusion: 本文提出了一种使用凸凹过程(CCP)训练形态感知器的方法，并通过实验验证了其有效性。

Abstract: A morphological perceptron is a multilayer feedforward neural network in
which neurons perform elementary operations from mathematical morphology. For
multiclass classification tasks, a morphological perceptron with a competitive
layer (MPCL) is obtained by integrating a winner-take-all output layer into the
standard morphological architecture. The non-differentiability of morphological
operators renders gradient-based optimization methods unsuitable for training
such networks. Consequently, alternative strategies that do not depend on
gradient information are commonly adopted. This paper proposes the use of the
convex-concave procedure (CCP) for training MPCL networks. The training problem
is formulated as a difference of convex (DC) functions and solved iteratively
using CCP, resulting in a sequence of linear programming subproblems.
Computational experiments demonstrate the effectiveness of the proposed
training method in addressing classification tasks with MPCL networks.

</details>


### [176] [Simulation Priors for Data-Efficient Deep Learning](https://arxiv.org/abs/2509.05732)
*Lenart Treven,Bhavya Sukhija,Jonas Rothfuss,Stelian Coros,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: SimPEL: Combines first-principles models with Bayesian deep learning for efficient real-world AI learning.


<details>
  <summary>Details</summary>
Motivation: First-principles models fail to capture real-world complexity, while deep learning needs large datasets.

Method: SimPEL uses low-fidelity simulators as priors in Bayesian deep learning.

Result: SimPEL outperforms baselines in learning complex dynamics across biological, agricultural, and robotic domains and bridges the sim-to-real gap in reinforcement learning.

Conclusion: SimPEL enables data-efficient learning and control in complex real-world environments.

Abstract: How do we enable AI systems to efficiently learn in the real-world?
First-principles models are widely used to simulate natural systems, but often
fail to capture real-world complexity due to simplifying assumptions. In
contrast, deep learning approaches can estimate complex dynamics with minimal
assumptions but require large, representative datasets. We propose SimPEL, a
method that efficiently combines first-principles models with data-driven
learning by using low-fidelity simulators as priors in Bayesian deep learning.
This enables SimPEL to benefit from simulator knowledge in low-data regimes and
leverage deep learning's flexibility when more data is available, all the while
carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse
systems, including biological, agricultural, and robotic domains, showing
superior performance in learning complex dynamics. For decision-making, we
demonstrate that SimPEL bridges the sim-to-real gap in model-based
reinforcement learning. On a high-speed RC car task, SimPEL learns a highly
dynamic parking maneuver involving drifting with substantially less data than
state-of-the-art baselines. These results highlight the potential of SimPEL for
data-efficient learning and control in complex real-world environments.

</details>


### [177] [Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies](https://arxiv.org/abs/2509.05735)
*Jiaqi Chen,Ji Shi,Cansu Sancaktar,Jonas Frey,Georg Martius*

Main category: cs.LG

TL;DR: 本文研究了基于模型的强化学习中在线和离线数据对世界模型的影响。研究发现，在线agent优于离线agent，离线agent的性能下降主要是由于遇到分布外的状态。可以通过允许额外的在线交互或加入探索数据来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 研究在线与离线数据对世界模型的影响，以及由此产生的任务表现。

Method: 在31个不同的环境进行了实验，对比在线和离线agent的表现。

Result: 在线agent优于离线agent。离线agent的性能下降主要是由于遇到分布外的状态。可以通过允许额外的在线交互或加入探索数据来缓解此问题。

Conclusion: 建议在收集大型数据集时添加探索数据，因为当前的工作主要集中在专家数据上。

Abstract: Data collection is crucial for learning robust world models in model-based
reinforcement learning. The most prevalent strategies are to actively collect
trajectories by interacting with the environment during online training or
training on offline datasets. At first glance, the nature of learning
task-agnostic environment dynamics makes world models a good candidate for
effective offline training. However, the effects of online vs. offline data on
world models and thus on the resulting task performance have not been
thoroughly studied in the literature. In this work, we investigate both
paradigms in model-based settings, conducting experiments on 31 different
environments. First, we showcase that online agents outperform their offline
counterparts. We identify a key challenge behind performance degradation of
offline agents: encountering Out-Of-Distribution states at test time. This
issue arises because, without the self-correction mechanism in online agents,
offline datasets with limited state space coverage induce a mismatch between
the agent's imagination and real rollouts, compromising policy training. We
demonstrate that this issue can be mitigated by allowing for additional online
interactions in a fixed or adaptive schedule, restoring the performance of
online training with limited interaction data. We also showcase that
incorporating exploration data helps mitigate the performance degradation of
offline agents. Based on our insights, we recommend adding exploration data
when collecting large datasets, as current efforts predominantly focus on
expert data alone.

</details>


### [178] [Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders](https://arxiv.org/abs/2509.05766)
*Jiaju Miao,Wei Zhu*

Main category: cs.LG

TL;DR: 提出了一种新的异常检测混合框架，该框架集成了 PRC-RF 和自编码器，以应对极端类不平衡和维度灾难。


<details>
  <summary>Details</summary>
Motivation: 异常检测在网络安全、入侵检测和欺诈预防等关键应用中至关重要，快速识别异常模式不可或缺。但该领域的进展通常受到极端类不平衡和维度灾难两个障碍的阻碍。

Method: 该方法集成了 PRC-RF 与自编码器，自编码器是一种学习紧凑潜在表示的无监督机器学习方法，以同时应对两个挑战。

Result: 在各种基准数据集上进行的大量实验表明，所提出的 Autoencoder-PRC-RF 模型相对于先前的方法实现了更高的准确性、可扩展性和可解释性。

Conclusion: 结果证实了该模型在高风险异常检测任务中的潜力。

Abstract: Anomaly detection underpins critical applications from network security and
intrusion detection to fraud prevention, where recognizing aberrant patterns
rapidly is indispensable. Progress in this area is routinely impeded by two
obstacles: extreme class imbalance and the curse of dimensionality. To combat
the former, we previously introduced Precision-Recall Curve (PRC)
classification trees and their ensemble extension, the PRC Random Forest
(PRC-RF). Building on that foundation, we now propose a hybrid framework that
integrates PRC-RF with autoencoders, unsupervised machine learning methods that
learn compact latent representations, to confront both challenges
simultaneously. Extensive experiments across diverse benchmark datasets
demonstrate that the resulting Autoencoder-PRC-RF model achieves superior
accuracy, scalability, and interpretability relative to prior methods,
affirming its potential for high-stakes anomaly-detection tasks.

</details>


### [179] [Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting](https://arxiv.org/abs/2509.05768)
*Chen Shao,Yue Wang,Zhenyi Zhu,Zhanbo Huang,Sebastian Pütz,Benjamin Schäfer,Tobais Käfer,Michael Färber*

Main category: cs.LG

TL;DR: 现有的能源预测基准在空间和时间范围上都有限，并且缺乏多能源特征。为了解决这个问题，我们提出了 Real-E 数据集，它涵盖了 30 多个欧洲国家/地区的 74 多个发电站，时间跨度为 10 年，并具有丰富的元数据。


<details>
  <summary>Details</summary>
Motivation: 现有的基准在空间和时间范围上仍然有限，并且缺乏多能源特征，这引起了人们对其在实际部署中的可靠性和适用性的担忧。

Method: 我们利用 Real-E 进行了广泛的数据分析和基准测试，涵盖了各种模型类型的 20 多个基线。我们引入了一种新的指标来量化相关结构的变化。

Result: 现有的方法在我们的数据集上表现不佳，该数据集表现出更复杂和非平稳的相关动态。

Conclusion: 我们的研究结果强调了当前方法的关键局限性，并为构建更强大的预测模型提供了强大的经验基础。

Abstract: Energy forecasting is vital for grid reliability and operational efficiency.
Although recent advances in time series forecasting have led to progress,
existing benchmarks remain limited in spatial and temporal scope and lack
multi-energy features. This raises concerns about their reliability and
applicability in real-world deployment. To address this, we present the Real-E
dataset, covering over 74 power stations across 30+ European countries over a
10-year span with rich metadata. Using Real- E, we conduct an extensive data
analysis and benchmark over 20 baselines across various model types. We
introduce a new metric to quantify shifts in correlation structures and show
that existing methods struggle on our dataset, which exhibits more complex and
non-stationary correlation dynamics. Our findings highlight key limitations of
current methods and offer a strong empirical basis for building more robust
forecasting models

</details>


### [180] [DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2509.05778)
*Arantxa Urrea-Castaño,Nicolás Segura-Kunsagi,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.LG

TL;DR: 本文提出了一种新的OOD检测模型评估框架，该框架通过双重交叉验证来更可靠地评估OOD检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的OOD检测方法是一个重大挑战，并且对这些技术进行严格评估对于确保其有效性至关重要。

Method: 该框架有效整合了同分布（ID）和OOD数据，同时考虑了它们的不同特征。ID数据使用传统方法进行划分，而OOD数据则通过基于其类别对样本进行分组来划分。此外，我们分析了具有类层次结构的数据的上下文，以提出一种考虑整个类层次结构的数据分割，以获得公平的ID-OOD分区，以应用所提出的评估框架。

Result: 结果表明，该方法能够非常快速地收敛到真实性能。

Conclusion: 本文提出了一个双重交叉验证框架，用于鲁棒的OOD检测评估，旨在提高评估的可靠性。

Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the
robustness of artificial intelligence systems by identifying inputs that differ
significantly from the training distribution, thereby preventing unreliable
predictions and enabling appropriate fallback mechanisms. Developing reliable
OOD detection methods is a significant challenge, and rigorous evaluation of
these techniques is essential for ensuring their effectiveness, as it allows
researchers to assess their performance under diverse conditions and to
identify potential limitations or failure modes. Cross-validation (CV) has
proven to be a highly effective tool for providing a reasonable estimate of the
performance of a learning algorithm. Although OOD scenarios exhibit particular
characteristics, an appropriate adaptation of CV can lead to a suitable
evaluation framework for this setting. This work proposes a dual CV framework
for robust evaluation of OOD detection models, aimed at improving the
reliability of their assessment. The proposed evaluation framework aims to
effectively integrate in-distribution (ID) and OOD data while accounting for
their differing characteristics. To achieve this, ID data are partitioned using
a conventional approach, whereas OOD data are divided by grouping samples based
on their classes. Furthermore, we analyze the context of data with class
hierarchy to propose a data splitting that considers the entire class hierarchy
to obtain fair ID-OOD partitions to apply the proposed evaluation framework.
This framework is called Dual Cross-Validation for Robust Out-of-Distribution
Detection (DCV-ROOD). To test the validity of the evaluation framework, we
selected a set of state-of-the-art OOD detection methods, both with and without
outlier exposure. The results show that the method achieves very fast
convergence to the true performance.

</details>


### [181] [Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.05779)
*Wei Chen,Yuqian Wu,Yuanshao Zhu,Xixuan Hao,Shiyu Wang,Yuxuan Liang*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为 \model 的新框架，用于在时空预测中对外生变量进行建模，遵循“选择，然后平衡”的范式。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案只关注使用有限数量的观测目标变量进行建模。在现实场景中，外生变量可以作为额外的输入特征整合到模型中，并与目标信号相关联，以提高预测准确性。然而，这仍然面临两个挑战：不同的外生变量对目标系统的不一致影响，以及历史变量和未来变量之间的不平衡影响。

Method: 该框架首先构建了一个潜在空间门控专家模块，其中融合的外生信息被投影到一个潜在空间，以通过专门的子专家动态选择和重组显著信号。此外，我们设计了一个 Siamese 网络架构，其中过去和未来的外生变量的重组表示被输入到双分支时空主干网络中，以捕获动态模式。输出通过上下文感知的加权机制进行整合，以在建模过程中实现动态平衡。

Result: 在真实世界数据集上的大量实验表明了我们提出的框架的有效性、通用性、鲁棒性和效率。

Conclusion: 论文提出了一种新的时空预测框架，通过选择和平衡外生变量来提高预测准确性。

Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic
systems and plays an important role in multiple fields. However, existing
solutions only focus on modeling using a limited number of observed target
variables. In real-world scenarios, exogenous variables can be integrated into
the model as additional input features and associated with the target signal to
promote forecast accuracy. Although promising, this still encounters two
challenges: the inconsistent effects of different exogenous variables to the
target system, and the imbalance effects between historical variables and
future variables. To address these challenges, this paper introduces \model, a
novel framework for modeling \underline{exo}genous variables in
\underline{s}patio-\underline{t}emporal forecasting, which follows a ``select,
then balance'' paradigm. Specifically, we first construct a latent space gated
expert module, where fused exogenous information is projected into a latent
space to dynamically select and recompose salient signals via specialized
sub-experts. Furthermore, we design a siamese network architecture in which
recomposed representations of past and future exogenous variables are fed into
dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs
are integrated through a context-aware weighting mechanism to achieve dynamic
balance during the modeling process. Extensive experiments on real-world
datasets demonstrate the effectiveness, generality, robustness, and efficiency
of our proposed framework.

</details>


### [182] [time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models](https://arxiv.org/abs/2509.05801)
*Debdeep Sanyal,Aaryan Nagpal,Dhruv Kumar,Murari Mandal,Saurabh Deshpande*

Main category: cs.LG

TL;DR: 本文研究了基于 Transformer 的基础模型在预测常规模式方面的表现，并探讨了它们是否内化了市场机制等语义概念，或者仅仅是拟合曲线。此外，还研究了它们的内部表征是否可以用于模拟罕见的高风险事件，如市场崩溃。


<details>
  <summary>Details</summary>
Motivation: 研究 Transformer 模型是否真正理解时间序列中的语义概念，以及它们是否能够模拟极端事件。

Method: 提出了一种名为激活移植的因果干预方法，该方法通过在正向传递过程中将一个事件的统计矩（例如，历史崩溃）施加到另一个事件（例如，平静时期）来操纵隐藏状态。从而确定性地引导预测。

Result: 研究表明，注入崩溃语义会导致经济衰退预测，而注入平静语义会抑制崩溃并恢复稳定。模型编码了事件严重程度的分级概念，潜在向量范数与系统性冲击的大小直接相关。在两种架构不同的 TSFM（Toto 和 Chronos）上验证了该方法。

Conclusion: 研究结果表明，可操纵的、语义接地的表征是大型时间序列 Transformer 的一个鲁棒属性。这些发现为控制模型预测的潜在概念空间提供了证据，将可解释性从事后归因转移到直接因果干预，并为战略压力测试提供语义“假设”分析。

Abstract: While transformer-based foundation models excel at forecasting routine
patterns, two questions remain: do they internalize semantic concepts such as
market regimes, or merely fit curves? And can their internal representations be
leveraged to simulate rare, high-stakes events such as market crashes? To
investigate this, we introduce activation transplantation, a causal
intervention that manipulates hidden states by imposing the statistical moments
of one event (e.g., a historical crash) onto another (e.g., a calm period)
during the forward pass. This procedure deterministically steers forecasts:
injecting crash semantics induces downturn predictions, while injecting calm
semantics suppresses crashes and restores stability. Beyond binary control, we
find that models encode a graded notion of event severity, with the latent
vector norm directly correlating with the magnitude of systemic shocks.
Validated across two architecturally distinct TSFMs, Toto (decoder only) and
Chronos (encoder-decoder), our results demonstrate that steerable, semantically
grounded representations are a robust property of large time series
transformers. Our findings provide evidence for a latent concept space that
governs model predictions, shifting interpretability from post-hoc attribution
to direct causal intervention, and enabling semantic "what-if" analysis for
strategic stress-testing.

</details>


### [183] [Simple Optimizers for Convex Aligned Multi-Objective Optimization](https://arxiv.org/abs/2509.05811)
*Ben Kretzu,Karen Ullrich,Yonathan Efroni*

Main category: cs.LG

TL;DR: 本文研究了凸对齐多目标优化 (AMOO) 的梯度下降算法，放宽了强凸性假设，并提出了新的分析工具和指标来表征凸 AMOO 中的收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖于强凸性假设，这与深度学习实践不符。本文旨在放宽这一假设，研究标准平滑或 Lipschitz 连续性条件下凸 AMOO 的梯度下降算法。

Method: 本文开发了新的分析工具，提出了可扩展的凸 AMOO 算法，并建立了它们的收敛性保证。此外，还证明了一种新的下界，表明与本文方法相比，简单的等权重方法是次优的。

Result: 本文为凸 AMOO 提出了可扩展的算法，并建立了它们的收敛性保证。此外，还证明了一种新的下界，表明与本文方法相比，简单的等权重方法是次优的。

Conclusion: 本文研究了凸对齐多目标优化 (AMOO) 的梯度下降算法，放宽了强凸性假设，并提出了新的分析工具和指标来表征凸 AMOO 中的收敛性。

Abstract: It is widely recognized in modern machine learning practice that access to a
diverse set of tasks can enhance performance across those tasks. This
observation suggests that, unlike in general multi-objective optimization, the
objectives in many real-world settings may not be inherently conflicting. To
address this, prior work introduced the Aligned Multi-Objective Optimization
(AMOO) framework and proposed gradient-based algorithms with provable
convergence guarantees. However, existing analysis relies on strong
assumptions, particularly strong convexity, which implies the existence of a
unique optimal solution. In this work, we relax this assumption and study
gradient-descent algorithms for convex AMOO under standard smoothness or
Lipschitz continuity conditions-assumptions more consistent with those used in
deep learning practice. This generalization requires new analytical tools and
metrics to characterize convergence in the convex AMOO setting. We develop such
tools, propose scalable algorithms for convex AMOO, and establish their
convergence guarantees. Additionally, we prove a novel lower bound that
demonstrates the suboptimality of naive equal-weight approaches compared to our
methods.

</details>


### [184] [Performance of Conformal Prediction in Capturing Aleatoric Uncertainty](https://arxiv.org/abs/2509.05826)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 本文研究了共形预测器量化 aleatoric 不确定性的有效性，特别是由重叠类引起的数据集中固有的模糊性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是，虽然共形预测的预测集大小有望捕捉到 aleatoric 不确定性，但缺乏关于其有效性的证据。文献表明预测集大小可以限制 aleatoric 不确定性，或者预测集对于困难的实例更大，对于容易的实例更小，但是缺少对此属性的验证。

Method: 本文通过测量预测集大小与人类注释者为每个实例分配的不同标签数量之间的相关性来评估共形预测器量化 aleatoric 不确定性的有效性。我们进一步评估了预测集和人类提供的注释之间的相似性。我们使用三种共形预测方法为在四个数据集上训练的八个深度学习模型生成预测集。这些数据集包含来自多个人的注释。

Result: 结果表明，绝大多数共形预测输出与人类注释的相关性非常弱到弱，只有少数显示出中等相关性。

Conclusion: 研究结论是，共形预测器在捕获 aleatoric 不确定性方面的能力仍然有限，需要批判性地重新评估使用共形预测器生成的预测集。

Abstract: Conformal prediction is a model-agnostic approach to generating prediction
sets that cover the true class with a high probability. Although its prediction
set size is expected to capture aleatoric uncertainty, there is a lack of
evidence regarding its effectiveness. The literature presents that prediction
set size can upper-bound aleatoric uncertainty or that prediction sets are
larger for difficult instances and smaller for easy ones, but a validation of
this attribute of conformal predictors is missing. This work investigates how
effectively conformal predictors quantify aleatoric uncertainty, specifically
the inherent ambiguity in datasets caused by overlapping classes. We perform
this by measuring the correlation between prediction set sizes and the number
of distinct labels assigned by human annotators per instance. We further assess
the similarity between prediction sets and human-provided annotations. We use
three conformal prediction approaches to generate prediction sets for eight
deep learning models trained on four datasets. The datasets contain annotations
from multiple human annotators (ranging from five to fifty participants) per
instance, enabling the identification of class overlap. We show that the vast
majority of the conformal prediction outputs show a very weak to weak
correlation with human annotations, with only a few showing moderate
correlation. These findings underscore the necessity of critically reassessing
the prediction sets generated using conformal predictors. While they can
provide a higher coverage of the true classes, their capability in capturing
aleatoric uncertainty remains limited.

</details>


### [185] [Finetuning LLMs for Human Behavior Prediction in Social Science Experiments](https://arxiv.org/abs/2509.05830)
*Akaash Kolluri,Shengguang Wu,Joon Sung Park,Michael S. Bernstein*

Main category: cs.LG

TL;DR: 大型语言模型可以通过在社会科学实验的个体层面反应上进行微调，显著提高模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在模拟社会科学实验结果方面的潜力，并提高模拟的准确性。

Method: 构建了一个包含290万个来自210个社会科学实验参与者回应的数据集SocSci210，并通过在此数据集上微调大型语言模型。

Result: 通过微调，模型在未见过的研究中，预测结果与人类反应的分布对齐度提高了26%，优于GPT-4o 13%；在新条件下泛化能力提高了71%；人口统计学上的偏差降低了10.6%。

Conclusion: 在特定主题的数据集上进行微调可以更准确地模拟实验假设筛选。

Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the
results of social science experiments. In this work, we demonstrate that
finetuning LLMs directly on individual-level responses from past experiments
meaningfully improves the accuracy of such simulations across diverse social
science domains. We construct SocSci210 via an automatic pipeline, a dataset
comprising 2.9 million responses from 400,491 participants in 210 open-source
social science experiments. Through finetuning, we achieve multiple levels of
generalization. In completely unseen studies, our strongest model,
Socrates-Qwen-14B, produces predictions that are 26% more aligned with
distributions of human responses to diverse outcome questions under varying
conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by
13%. By finetuning on a subset of conditions in a study, generalization to new
unseen conditions is particularly robust, improving by 71%. Since SocSci210
contains rich demographic information, we reduce demographic parity, a measure
of bias, by 10.6% through finetuning. Because social sciences routinely
generate rich, topic-specific datasets, our findings indicate that finetuning
on such data could enable more accurate simulations for experimental hypothesis
screening. We release our data, models and finetuning code at
stanfordhci.github.io/socrates.

</details>


### [186] [Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces](https://arxiv.org/abs/2509.05833)
*Zeyu Song,Sainyam Galhotra,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 论文提出了一个全面的基准框架，用于评估去中心化梯度市场中的梯度聚合方法。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习基准忽略了梯度市场中关键的经济和系统因素，如成本效益、对卖家的公平性以及市场稳定性，尤其是在买家依赖私有基线数据集进行评估时。

Method: 论文构建了一个模拟环境，对具有可变买家基线和多样化卖家分布的市场动态进行建模，并提出了一种评估方法，该方法利用以市场为中心的维度（如经济效率、公平性和选择动态）来增强标准的联邦学习指标。

Result: 论文对现有的分布式梯度市场框架MartFL进行了深入的实证分析，包括集成和比较评估了改进的FLTrust和SkyMask作为其中的替代聚合策略。该基准涵盖了不同的数据集、本地攻击和针对市场选择过程的Sybil攻击。

Conclusion: 该基准为社区提供了必要的工具和经验证据，以评估和设计更稳健、公平和经济上可行的去中心化梯度市场。

Abstract: The rise of distributed and privacy-preserving machine learning has sparked
interest in decentralized gradient marketplaces, where participants trade
intermediate artifacts like gradients. However, existing Federated Learning
(FL) benchmarks overlook critical economic and systemic factors unique to such
marketplaces-cost-effectiveness, fairness to sellers, and market
stability-especially when a buyer relies on a private baseline dataset for
evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate
robust gradient aggregation methods within these buyer-baseline-reliant
marketplaces. Our contributions include: (1) a simulation environment modeling
marketplace dynamics with a variable buyer baseline and diverse seller
distributions; (2) an evaluation methodology augmenting standard FL metrics
with marketplace-centric dimensions such as Economic Efficiency, Fairness, and
Selection Dynamics; (3) an in-depth empirical analysis of the existing
Distributed Gradient Marketplace framework, MartFL, including the integration
and comparative evaluation of adapted FLTrust and SkyMask as alternative
aggregation strategies within it. This benchmark spans diverse datasets, local
attacks, and Sybil attacks targeting the marketplace selection process; and (4)
actionable insights into the trade-offs between model performance, robustness,
cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical
evidence to evaluate and design more robust, equitable, and economically viable
decentralized gradient marketplaces.

</details>


### [187] [Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics](https://arxiv.org/abs/2509.05839)
*Daksh Mittal,Shunri Zheng,Jing Dong,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 提出了一种基于自回归序列模型的排队网络建模和仿真数据驱动框架，该框架通过学习事件类型和事件时间的条件分布，将建模任务转化为序列分布学习问题，从而实现高保真模拟器的自动构建。


<details>
  <summary>Details</summary>
Motivation: 传统的排队网络模型需要大量的人力和领域专业知识来构建，为了使这种建模方法更具可扩展性和可访问性。

Method: 基于在事件流数据上训练的自回归序列模型，学习事件类型和事件时间的条件分布。

Result: Transformer风格的架构可以有效地参数化这些分布，从而实现高保真模拟器的自动构建。在从不同的排队网络生成的事件表上验证了该框架，展示了其在模拟、不确定性量化和反事实评估中的效用。

Conclusion: 该框架利用人工智能的进步和日益增长的数据可用性，朝着更自动化、数据驱动的建模管道迈出了一步，以支持在服务领域更广泛地采用排队网络模型。

Abstract: While queueing network models are powerful tools for analyzing service
systems, they traditionally require substantial human effort and domain
expertise to construct. To make this modeling approach more scalable and
accessible, we propose a data-driven framework for queueing network modeling
and simulation based on autoregressive sequence models trained on event-stream
data. Instead of explicitly specifying arrival processes, service mechanisms,
or routing logic, our approach learns the conditional distributions of event
types and event times, recasting the modeling task as a problem of sequence
distribution learning. We show that Transformer-style architectures can
effectively parameterize these distributions, enabling automated construction
of high-fidelity simulators. As a proof of concept, we validate our framework
on event tables generated from diverse queueing networks, showcasing its
utility in simulation, uncertainty quantification, and counterfactual
evaluation. Leveraging advances in artificial intelligence and the growing
availability of data, our framework takes a step toward more automated,
data-driven modeling pipelines to support broader adoption of queueing network
models across service domains.

</details>


### [188] [The Measure of Deception: An Analysis of Data Forging in Machine Unlearning](https://arxiv.org/abs/2509.05865)
*Rishabh Dixit,Yuan Hui,Rayan Saab*

Main category: cs.LG

TL;DR: 本文研究了机器学习中的“遗忘学习”问题，重点关注了对抗性伪造数据以欺骗遗忘学习验证的挑战。


<details>
  <summary>Details</summary>
Motivation: 隐私法规和减轻有害数据影响的需求促使了机器学习遗忘学习的发展。验证遗忘学习的一个关键挑战是伪造数据，即对抗性地制作模仿目标点梯度的假数据，从而在不真正移除信息的情况下造成遗忘的假象。

Method: 本文提出了一个分析框架，用于研究梯度在容差 $\epsilon$ 内近似于目标梯度的伪造数据集的集合。针对线性回归和单层神经网络，分析了该集合的勒贝格测度。

Result: 研究表明，对于线性回归和单层神经网络，伪造数据集的勒贝格测度很小，其规模与 $\epsilon$ 成比例，当 $\epsilon$ 足够小时，与 $\epsilon^d$ 成比例。在温和的正则性假设下，证明了伪造集测度衰减为 $\epsilon^{(d-r)/2}$，其中 $d$ 是数据维度，$r<d$ 是由模型梯度定义的变分矩阵的零度。此外，还建立了概率界限，表明在非退化的数据分布下，随机抽样伪造点的可能性非常小。

Conclusion: 研究结果表明，对抗性伪造从根本上受到限制，并且原则上可以检测到虚假的遗忘学习声明。

Abstract: Motivated by privacy regulations and the need to mitigate the effects of
harmful data, machine unlearning seeks to modify trained models so that they
effectively ``forget'' designated data. A key challenge in verifying unlearning
is forging -- adversarially crafting data that mimics the gradient of a target
point, thereby creating the appearance of unlearning without actually removing
information. To capture this phenomenon, we consider the collection of data
points whose gradients approximate a target gradient within tolerance
$\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a
framework for its analysis. For linear regression and one-layer neural
networks, we show that the Lebesgue measure of this set is small. It scales on
the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$.
More generally, under mild regularity assumptions, we prove that the forging
set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and
$r<d$ is the nullity of a variation matrix defined by the model gradients.
Extensions to batch SGD and almost-everywhere smooth loss functions yield the
same asymptotic scaling. In addition, we establish probability bounds showing
that, under non-degenerate data distributions, the likelihood of randomly
sampling a forging point is vanishingly small. These results provide evidence
that adversarial forging is fundamentally limited and that false unlearning
claims can, in principle, be detected.

</details>


### [189] [SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework](https://arxiv.org/abs/2509.05886)
*Reza Pirayeshshirazinezhad*

Main category: cs.LG

TL;DR: 使用代理模型预测矩形微型散热器内液态钠 (Na) 流动的对流换热系数。


<details>
  <summary>Details</summary>
Motivation: 计算流体动力学 (CFD) 对液态金属湍流强制对流的高精度建模既耗时又计算成本高。因此，基于机器学习的模型为液态金属冷却微型散热器的设计和优化提供了一种强大的替代工具。

Method: 首先，将基于核的机器学习技术和浅层神经网络应用于具有 87 个矩形微型散热器中液态钠的努塞尔数的数据集。随后，使用自监督物理信息神经网络和迁移学习方法来提高估计性能。在自监督物理信息神经网络中，附加层确定损失函数中物理的权重，以根据数据和物理的不确定性来平衡数据和物理，从而获得更好的估计。对于迁移学习，适用于水的浅层神经网络适用于 Na。

Result: 验证结果表明，自监督物理信息神经网络成功估计了 Na 的传热率，误差幅度约为 +8%。仅使用物理进行回归时，误差保持在 5% 到 10% 之间。其他机器学习方法主要在 +8% 以内指定预测。

Conclusion: 基于机器学习的模型为液态金属冷却微型散热器的设计和优化提供了一种强大的替代工具

Abstract: A surrogate model is developed to predict the convective heat transfer
coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.
Initially, kernel-based machine learning techniques and shallow neural network
are applied to a dataset with 87 Nusselt numbers for liquid sodium in
rectangular miniature heat sinks. Subsequently, a self-supervised
physics-informed neural network and transfer learning approach are used to
increase the estimation performance. In the self-supervised physics-informed
neural network, an additional layer determines the weight the of physics in the
loss function to balance data and physics based on their uncertainty for a
better estimation. For transfer learning, a shallow neural network trained on
water is adapted for use with Na. Validation results show that the
self-supervised physics-informed neural network successfully estimate the heat
transfer rates of Na with an error margin of approximately +8%. Using only
physics for regression, the error remains between 5% to 10%. Other machine
learning methods specify the prediction mostly within +8%. High-fidelity
modeling of turbulent forced convection of liquid metals using computational
fluid dynamics (CFD) is both time-consuming and computationally expensive.
Therefore, machine learning based models offer a powerful alternative tool for
the design and optimization of liquid-metal-cooled miniature heat sinks.

</details>


### [190] [Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms](https://arxiv.org/abs/2509.05930)
*Ali Zeynali,Mahsa Sahebdel,Qingsong Liu,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.LG

TL;DR: 本文介绍了一种新的在线决策框架，用于在不确定性下进行目标跟踪，同时考虑跟踪成本、对抗扰动成本和切换成本。


<details>
  <summary>Details</summary>
Motivation: 在诸如AI集群中的弹性及非弹性工作负载调度等现实场景中，需要平衡长期服务水平协议与突发需求高峰。

Method: 提出了名为BEST的鲁棒算法，并设计了CoRT算法，该算法融入了来自机器学习模型的黑盒预测。

Result: 理论分析表明，当预测准确时，CoRT算法优于BEST算法，并在任意预测误差下保持鲁棒性。通过工作负载调度案例研究验证了该方法的有效性。

Conclusion: 提出的算法能够有效地平衡轨迹跟踪、决策平滑性以及对外部干扰的弹性。

Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.

</details>


### [191] [Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior](https://arxiv.org/abs/2509.06025)
*Vignesh Ethiraj,Subhash Talluri*

Main category: cs.LG

TL;DR: 提出了一种新的名为统一交互基础模型（UIFM）的基础模型，用于理解和预测复杂的事件序列。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言基础模型无法理解电信、电商和金融等领域中结构化交互的整体性质，因为它们将事件序列化为文本，导致语义碎片化并丢失关键上下文。

Method: 采用复合tokenization原则，将每个多属性事件视为一个单一的、语义连贯的单元。

Result: 该架构不仅更准确，而且代表着朝着创建更具适应性和智能化的预测系统迈出的重要一步。

Conclusion: UIFM能够学习用户行为的底层“语法”，感知整个交互过程，而不是不连贯的数据点流。

Abstract: A central goal of artificial intelligence is to build systems that can
understand and predict complex, evolving sequences of events. However, current
foundation models, designed for natural language, fail to grasp the holistic
nature of structured interactions found in domains like telecommunications,
e-commerce and finance. By serializing events into text, they disassemble them
into semantically fragmented parts, losing critical context. In this work, we
introduce the Unified Interaction Foundation Model (UIFM), a foundation model
engineered for genuine behavioral understanding. At its core is the principle
of composite tokenization, where each multi-attribute event is treated as a
single, semantically coherent unit. This allows UIFM to learn the underlying
"grammar" of user behavior, perceiving entire interactions rather than a
disconnected stream of data points. We demonstrate that this architecture is
not just more accurate, but represents a fundamental step towards creating more
adaptable and intelligent predictive systems.

</details>


### [192] [PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training](https://arxiv.org/abs/2509.06053)
*Mingrui Lv,Hangzhi Liu,Zhi Luo,Hongjie Zhang,Jie Ou*

Main category: cs.LG

TL;DR: PolicyEvolve: 使用 LLM 生成多人游戏中可解释的程序化策略，减少对人工代码的依赖。


<details>
  <summary>Details</summary>
Motivation: 在多人强化学习中，训练有效的对抗策略需要大量样本和计算资源，并且缺乏可解释性。

Method: PolicyEvolve 框架包含全局池、局部池、策略规划器和轨迹评论器四个模块，通过迭代训练生成和优化策略。

Result: PolicyEvolve 能够以最少的环境交互实现高性能策略。

Conclusion: PolicyEvolve 是一种通用的框架，用于在多人游戏中生成程序化策略。

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in solving complex multi-player games through self-play. However, training
effective adversarial policies requires millions of experience samples and
substantial computational resources. Moreover, these policies lack
interpretability, hindering their practical deployment. Recently, researchers
have successfully leveraged Large Language Models (LLMs) to generate
programmatic policies for single-agent tasks, transforming neural network-based
policies into interpretable rule-based code with high execution efficiency.
Inspired by this, we propose PolicyEvolve, a general framework for generating
programmatic policies in multi-player games. PolicyEvolve significantly reduces
reliance on manually crafted policy code, achieving high-performance policies
with minimal environmental interactions. The framework comprises four modules:
Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool
preserves elite policies accumulated during iterative training. The Local Pool
stores temporary policies for the current iteration; only sufficiently
high-performing policies from this pool are promoted to the Global Pool. The
Policy Planner serves as the core policy generation module. It samples the top
three policies from the Global Pool, generates an initial policy for the
current iteration based on environmental information, and refines this policy
using feedback from the Trajectory Critic. Refined policies are then deposited
into the Local Pool. This iterative process continues until the policy achieves
a sufficiently high average win rate against the Global Pool, at which point it
is integrated into the Global Pool. The Trajectory Critic analyzes interaction
data from the current policy, identifies vulnerabilities, and proposes
directional improvements to guide the Policy Planner

</details>


### [193] [A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation](https://arxiv.org/abs/2509.06056)
*Chun Wang*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习和计算流体动力学的生物质流化床气化耦合模型，以提高复杂热化学反应过程的预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高复杂热化学反应过程的预测精度和计算效率。

Method: 通过构建基于实验数据和高保真模拟结果的高质量数据集，训练用于描述反应动力学特征的代理模型，并将其嵌入到计算流体动力学（CFD）框架中，以实现反应速率和组成演化的实时更新。

Result: 实现了反应速率和组成演化的实时更新。

Conclusion: 该模型提高了复杂热化学反应过程的预测精度和计算效率。

Abstract: A coupling model of biomass fluidized bed gasification based on machine
learning and computational fluid dynamics is proposed to improve the prediction
accuracy and computational efficiency of complex thermochemical reaction
process. By constructing a high-quality data set based on experimental data and
high fidelity simulation results, the agent model used to describe the
characteristics of reaction kinetics was trained and embedded into the
computational fluid dynamics (CFD) framework to realize the real-time update of
reaction rate and composition evolution.

</details>


### [194] [ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting](https://arxiv.org/abs/2509.06060)
*Fei Wang,Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Zhulin An,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: ARIES是一个用于评估时间序列属性和建模策略之间关系并为实际时间序列推荐深度预测模型的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的基准数据集未能提供多样且明确的时间模式，限制了对模型性能和数据属性之间内在联系的系统评估。此外，没有有效的模型推荐方法，导致在不同下游应用中测试不同架构时花费大量时间和成本。

Method: 构建一个具有多个不同模式的合成数据集，并设计一个综合系统来计算时间序列的属性。对 50 多个预测模型进行了广泛的基准测试，并建立了时间序列属性和建模策略之间的关系。

Result: 实验结果表明存在明显的相关性。

Conclusion: ARIES是第一个建立时间序列数据属性和建模策略之间关系的研究，同时还实现了模型推荐系统。

Abstract: Recent advancements in deep learning models for time series forecasting have
been significant. These models often leverage fundamental time series
properties such as seasonality and non-stationarity, which may suggest an
intrinsic link between model performance and data properties. However, existing
benchmark datasets fail to offer diverse and well-defined temporal patterns,
restricting the systematic evaluation of such connections. Additionally, there
is no effective model recommendation approach, leading to high time and cost
expenditures when testing different architectures across different downstream
applications. For those reasons, we propose ARIES, a framework for assessing
relation between time series properties and modeling strategies, and for
recommending deep forcasting models for realistic time series. First, we
construct a synthetic dataset with multiple distinct patterns, and design a
comprehensive system to compute the properties of time series. Next, we conduct
an extensive benchmarking of over 50 forecasting models, and establish the
relationship between time series properties and modeling strategies. Our
experimental results reveal a clear correlation. Based on these findings, we
propose the first deep forecasting model recommender, capable of providing
interpretable suggestions for real-world time series. In summary, ARIES is the
first study to establish the relations between the properties of time series
data and modeling strategies, while also implementing a model recommendation
system. The code is available at: https://github.com/blisky-li/ARIES.

</details>


### [195] [A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network](https://arxiv.org/abs/2509.06067)
*Mianjun Xiao,Peng Song,Yulong Liu,Cedric Korte,Ziyang Xu,Jiale Gao,Jiaqi Lu,Haoyang Nie,Qiantong Deng,Timing Qu*

Main category: cs.LG

TL;DR: 提出了一种基于全连接残差神经网络（FCRN）的替代模型，用于预测REBCO螺线管中的时空电流密度分布。


<details>
  <summary>Details</summary>
Motivation: 有限元方法（FEM）广泛应用于高温超导（HTS）磁体，但其计算成本随着磁体尺寸的增加而增加，对于米级磁体而言非常耗时，尤其是在考虑多物理场耦合时，这限制了大型REBCO磁体系统的快速设计。

Method: 开发了一个基于全连接残差神经网络（FCRN）的替代模型，用于预测REBCO螺线管中的时空电流密度分布。训练数据集来自FEM模拟，具有不同数量的匝数和饼。

Result: 结果表明，对于更深的网络，FCRN架构比传统的全连接网络（FCN）实现了更好的收敛性，具有12个残差块和每层256个神经元的配置，在训练精度和泛化能力之间提供了最有利的平衡。外推研究表明，该模型可以可靠地预测高达训练范围50%的磁化损耗，最大误差低于10%。

Conclusion: 基于FCRN的替代模型兼具准确性和效率，为大型HTS磁体的快速分析提供了一种有前途的工具。

Abstract: Finite element method (FEM) is widely used in high-temperature
superconducting (HTS) magnets, but its computational cost increases with magnet
size and becomes time-consuming for meter-scale magnets, especially when
multi-physics couplings are considered, which limits the fast design of
large-scale REBCO magnet systems. In this work, a surrogate model based on a
fully connected residual neural network (FCRN) is developed to predict the
space-time current density distribution in REBCO solenoids. Training datasets
were generated from FEM simulations with varying numbers of turns and pancakes.
The results demonstrate that, for deeper networks, the FCRN architecture
achieves better convergence than conventional fully connected network (FCN),
with the configuration of 12 residual blocks and 256 neurons per layer
providing the most favorable balance between training accuracy and
generalization capability. Extrapolation studies show that the model can
reliably predict magnetization losses for up to 50% beyond the training range,
with maximum errors below 10%. The surrogate model achieves predictions several
orders of magnitude faster than FEM and still remains advantageous when
training costs are included. These results indicate that the proposed
FCRN-based surrogate model provides both accuracy and efficiency, offering a
promising tool for the rapid analysis of large-scale HTS magnets.

</details>


### [196] [Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs](https://arxiv.org/abs/2509.06094)
*S. R. Eshwar*

Main category: cs.LG

TL;DR: 本文研究了具有时间不一致偏好的智能体的强化学习问题，特别是Quasi-Hyperbolic (QH) 折扣。


<details>
  <summary>Details</summary>
Motivation: 研究人类和动物决策中的时间不一致偏好，并将其整合到强化学习框架中。

Method: 1. 从理论上分析了最优策略的结构，证明其可以简化为简单的单步非平稳形式。
2. 设计了首个实用的、无模型的策略评估和 Q-learning 算法，并具有可证明的收敛性保证。

Result: 证明了最优策略具有简单的单步非平稳形式，并设计了具有收敛性保证的实用算法。

Conclusion: 为在强化学习中结合 QH 偏好提供了基础性的见解。

Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over
larger-later rewards, are a key feature of human and animal decision-making.
Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this
behavior, but its integration into the reinforcement learning (RL) framework
has been limited. This paper addresses key theoretical and algorithmic gaps for
precommitted agents with QH preferences. We make two primary contributions: (i)
we formally characterize the structure of the optimal policy, proving for the
first time that it reduces to a simple one-step non-stationary form; and (ii)
we design the first practical, model-free algorithms for both policy evaluation
and Q-learning in this setting, both with provable convergence guarantees. Our
results provide foundational insights for incorporating QH preferences in RL.

</details>


### [197] [If generative AI is the answer, what is the question?](https://arxiv.org/abs/2509.06120)
*Ambuj Tewari*

Main category: cs.LG

TL;DR: 探讨生成式AI的基础，并从机器学习的角度分析其与预测、压缩和决策的关系。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI作为一种独特的机器学习任务的根本原理。

Method: 调研了五种主要的生成模型家族：自回归模型、变分自编码器、标准化流、生成对抗网络和扩散模型。引入了一个概率框架，强调密度估计和生成之间的区别。回顾了一个博弈论框架，用双人对抗学习设置来研究生成。

Result: 讨论了为部署准备生成模型的后训练修改。重点介绍了社会责任生成中的一些重要主题，如隐私、AI生成内容的检测以及版权和IP。

Conclusion: 总结了生成式AI作为一个机器学习问题的任务优先框架，侧重于生成是什么，而不仅仅是模型如何实现它。

Abstract: Beginning with text and images, generative AI has expanded to audio, video,
computer code, and molecules. Yet, if generative AI is the answer, what is the
question? We explore the foundations of generation as a distinct machine
learning task with connections to prediction, compression, and decision-making.
We survey five major generative model families: autoregressive models,
variational autoencoders, normalizing flows, generative adversarial networks,
and diffusion models. We then introduce a probabilistic framework that
emphasizes the distinction between density estimation and generation. We review
a game-theoretic framework with a two-player adversary-learner setup to study
generation. We discuss post-training modifications that prepare generative
models for deployment. We end by highlighting some important topics in socially
responsible generation such as privacy, detection of AI-generated content, and
copyright and IP. We adopt a task-first framing of generation, focusing on what
generation is as a machine learning problem, rather than only on how models
implement it.

</details>


### [198] [Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators](https://arxiv.org/abs/2509.06154)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: 图神经网络模拟器(GNS)通过对瞬时时间导数建模，并结合显式数值时间步进方案，构建精确的前向模型来学习偏微分方程(PDE)的解，提高数据效率，减少误差累积。


<details>
  <summary>Details</summary>
Motivation: 神经算子(NOs)在训练数据稀缺时表现不佳，并且许多NO公式没有明确地编码物理演化的因果、局部时间结构。自回归模型虽然保持了因果关系，但存在快速误差累积的问题。

Method: 采用图神经网络模拟器(GNS)，这是一个消息传递图神经网络框架，结合显式数值时间步进方案。

Result: GNS显著提高了数据效率，与DeepONet和FNO等神经算子基线相比，仅用少量训练轨迹就实现了更高的泛化精度。在所有三个PDE系统中，仅使用1000个样本中的30个训练样本(3%的可用数据)，GNS始终实现了低于1%的相对L2误差。它大大减少了在扩展时间范围内的误差累积：在所有情况下，GNS相对于FNO AR减少了82.48%的自回归误差，相对于DON AR减少了99.86%。

Conclusion: 将基于图的局部归纳偏置与传统时间积分器相结合，可以为时间相关的偏微分方程产生精确、物理一致且可扩展的替代模型。

Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional
function spaces but require large datasets and struggle with scarce training
data. Many NO formulations don't explicitly encode causal, local-in-time
structure of physical evolution. While autoregressive models preserve causality
by predicting next time-steps, they suffer from rapid error accumulation. We
employ Graph Neural Simulators (GNS) - a message-passing graph neural network
framework - with explicit numerical time-stepping schemes to construct accurate
forward models that learn PDE solutions by modeling instantaneous time
derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D
Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D
Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly
improves data efficiency, achieving higher generalization accuracy with
substantially fewer training trajectories compared to neural operator baselines
like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors
with only 30 training samples out of 1000 (3% of available data) across all
three PDE systems. It substantially reduces error accumulation over extended
temporal horizons: averaged across all cases, GNS reduces autoregressive error
by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a
PCA+KMeans trajectory selection strategy enhancing low-data performance.
Results indicate combining graph-based local inductive biases with conventional
time integrators yields accurate, physically consistent, and scalable surrogate
models for time-dependent PDEs.

</details>
