{"id": "2507.12504", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12504", "abs": "https://arxiv.org/abs/2507.12504", "authors": ["Vito Chan", "Lennart Ebert", "Paul-Julius Hillmann", "Christoffer Rubensson", "Stephan A. Fahrenkrog-Petersen", "Jan Mendling"], "title": "Transforming Football Data into Object-centric Event Logs with Spatial Context Information", "comment": "Accepted for the 3rd Workshop on Object-centric processes from A to Z\n  (co-locatedOBJECTS 2025) with BPM 2025", "summary": "Object-centric event logs expand the conventional single-case notion event\nlog by considering multiple objects, allowing for the analysis of more complex\nand realistic process behavior. However, the number of real-world\nobject-centric event logs remains limited, and further studies are needed to\ntest their usefulness. The increasing availability of data from team sports can\nfacilitate object-centric process mining, leveraging both real-world data and\nsuitable use cases. In this paper, we present a framework for transforming\nfootball (soccer) data into an object-centric event log, further enhanced with\na spatial dimension. We demonstrate the effectiveness of our framework by\ngenerating object-centric event logs based on real-world football data and\ndiscuss the results for varying process representations. With our paper, we\nprovide the first example for object-centric event logs in football analytics.\nFuture work should consider variant analysis and filtering techniques to better\nhandle variability", "AI": {"tldr": "This paper introduces a framework for transforming football data into object-centric event logs, providing a real-world example and demonstrating its effectiveness. ", "motivation": "The number of real-world object-centric event logs remains limited, and further studies are needed to test their usefulness. The increasing availability of data from team sports can facilitate object-centric process mining, leveraging both real-world data and suitable use cases.", "method": "The paper presents a framework for transforming football (soccer) data into an object-centric event log, further enhanced with a spatial dimension.", "result": "The paper demonstrates the effectiveness of the framework by generating object-centric event logs based on real-world football data and discusses the results for varying process representations.", "conclusion": "The paper provides the first example for object-centric event logs in football analytics. Future work should consider variant analysis and filtering techniques to better handle variability."}}
{"id": "2507.12562", "categories": ["cs.DB", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12562", "abs": "https://arxiv.org/abs/2507.12562", "authors": ["Md. Tanvir Alam", "Md. Ahasanul Alam", "Md Mahmudur Rahman", "Md. Mosaddek Khan"], "title": "Rel-HNN: Split Parallel Hypergraph Neural Network for Learning on Relational Databases", "comment": null, "summary": "Relational databases (RDBs) are ubiquitous in enterprise and real-world\napplications. Flattening the database poses challenges for deep learning models\nthat rely on fixed-size input representations to capture relational semantics\nfrom the structured nature of relational data. Graph neural networks (GNNs)\nhave been proposed to address this, but they often oversimplify relational\nstructures by modeling all the tuples as monolithic nodes and ignoring\nintra-tuple associations. In this work, we propose a novel hypergraph-based\nframework, that we call rel-HNN, which models each unique attribute-value pair\nas a node and each tuple as a hyperedge, enabling the capture of fine-grained\nintra-tuple relationships. Our approach learns explicit multi-level\nrepresentations across attribute-value, tuple, and table levels. To address the\nscalability challenges posed by large RDBs, we further introduce a\nsplit-parallel training algorithm that leverages multi-GPU execution for\nefficient hypergraph learning. Extensive experiments on real-world and\nbenchmark datasets demonstrate that rel-HNN significantly outperforms existing\nmethods in both classification and regression tasks. Moreover, our\nsplit-parallel training achieves substantial speedups -- up to 3.18x for\nlearning on relational data and up to 2.94x for hypergraph learning -- compared\nto conventional single-GPU execution.", "AI": {"tldr": "\u63d0\u51farel-HNN\u6a21\u578b\uff0c\u4f7f\u7528\u8d85\u56fe\u5efa\u6a21\u5173\u7cfb\u6570\u636e\u5e93\uff0c\u5e76\u4f7f\u7528split-parallel\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u7684\u5173\u7cfb\u8bed\u4e49\u96be\u4ee5\u88ab\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6355\u83b7\uff0c\u73b0\u6709\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u53c8\u8fc7\u5ea6\u7b80\u5316\u4e86\u5173\u7cfb\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8d85\u56fe\u7684\u6846\u67b6rel-HNN\uff0c\u5b83\u5c06\u6bcf\u4e2a\u552f\u4e00\u7684\u5c5e\u6027-\u503c\u5bf9\u5efa\u6a21\u4e3a\u4e00\u4e2a\u8282\u70b9\uff0c\u5e76\u5c06\u6bcf\u4e2a\u5143\u7ec4\u5efa\u6a21\u4e3a\u4e00\u4e2a\u8d85\u8fb9\u3002", "result": "rel-HNN\u5728\u771f\u5b9e\u4e16\u754c\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0csplit-parallel\u8bad\u7ec3\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.18x\u7684\u52a0\u901f\u3002", "conclusion": "rel-HNN\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14split-parallel\u8bad\u7ec3\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002"}}
{"id": "2507.12668", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.12668", "abs": "https://arxiv.org/abs/2507.12668", "authors": ["Shuang Liang", "Lili Chen", "Wensheng Gan", "Philip S. Yu", "Shengjie Zhao"], "title": "Targeted Mining of Time-Interval Related Patterns", "comment": "Preprint. 8 figures, 4 tables", "summary": "Compared to frequent pattern mining, sequential pattern mining emphasizes the\ntemporal aspect and finds broad applications across various fields. However,\nnumerous studies treat temporal events as single time points, neglecting their\ndurations. Time-interval-related pattern (TIRP) mining is introduced to address\nthis issue and has been applied to healthcare analytics, stock prediction, etc.\nTypically, mining all patterns is not only computationally challenging for\naccurate forecasting but also resource-intensive in terms of time and memory.\nTargeting the extraction of time-interval-related patterns based on specific\ncriteria can improve data analysis efficiency and better align with customer\npreferences. Therefore, this paper proposes a novel algorithm called TaTIRP to\ndiscover Targeted Time-Interval Related Patterns. Additionally, we develop\nmultiple pruning strategies to eliminate redundant extension operations,\nthereby enhancing performance on large-scale datasets. Finally, we conduct\nexperiments on various real-world and synthetic datasets to validate the\naccuracy and efficiency of the proposed algorithm.", "AI": {"tldr": "This paper introduces TaTIRP, a novel algorithm for targeted time-interval-related pattern mining, enhancing efficiency and accuracy on large datasets.", "motivation": "Numerous studies treat temporal events as single time points, neglecting their durations. Mining all patterns is computationally challenging and resource-intensive. Targeting the extraction of time-interval-related patterns based on specific criteria can improve data analysis efficiency and better align with customer preferences.", "method": "The paper proposes a novel algorithm called TaTIRP to discover Targeted Time-Interval Related Patterns and develops multiple pruning strategies to eliminate redundant extension operations.", "result": "The proposed TaTIRP algorithm enhances performance on large-scale datasets by eliminating redundant extension operations.", "conclusion": "The paper validates the accuracy and efficiency of the proposed TaTIRP algorithm through experiments on real-world and synthetic datasets."}}
{"id": "2507.12805", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.12805", "abs": "https://arxiv.org/abs/2507.12805", "authors": ["Hui Sun", "Yanfeng Ding", "Liping Yi", "Huidong Ma", "Gang Wang", "Xiaoguang Liu", "Cheng Zhong", "Wentong Cai"], "title": "PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database", "comment": "Accepted via KDD-25", "summary": "Learning-based lossless compressors play a crucial role in large-scale\ngenomic database backup, storage, transmission, and management. However, their\n1) inadequate compression ratio, 2) low compression \\& decompression\nthroughput, and 3) poor compression robustness limit their widespread adoption\nand application in both industry and academia. To solve those challenges, we\npropose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge\n\\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial\ndesigns: 1) We propose an automated multi-knowledge learning-based compression\nframework as compressors' backbone to enhance compression ratio and robustness;\n2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression\nthroughput and computing resource usage; 3) we introduce data block\npartitioning and Step-wise Model Passing (SMP) mechanisms for parallel\nacceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet\nthe complex application scenarios, where the former runs on a\nresource-constrained single GPU and the latter is multi-GPU accelerated. We\nbenchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15\nreal-world datasets with different species and data sizes. Compared to\nbaselines on the testing datasets, PMKLC-S/M achieve the average compression\nratio improvement up to 73.609\\% and 73.480\\%, the average throughput\nimprovement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides,\nPMKLC-S/M also achieve the best robustness and competitive memory cost,\nindicating its greater stability against datasets with different probability\ndistribution perturbations, and its strong ability to run on memory-constrained\ndevices.", "AI": {"tldr": "Propose PMKLC, a parallel multi-knowledge learning-based compressor, to improve compression ratio, throughput, and robustness for genomic data.", "motivation": "inadequate compression ratio, low compression & decompression throughput, and poor compression robustness limit their widespread adoption", "method": "novel Parallel Multi-Knowledge Learning-based Compressor (PMKLC) with four crucial designs: 1) automated multi-knowledge learning-based compression framework; 2) GPU-accelerated ($s$,$k$)-mer encoder; 3) data block partitioning and Step-wise Model Passing (SMP) mechanisms; 4) two compression modes PMKLC-S and PMKLC-M", "result": "PMKLC-S/M achieve the average compression ratio improvement up to 73.609% and 73.480%, the average throughput improvement up to 3.036x and 10.710x, respectively", "conclusion": "PMKLC-S/M achieves the best robustness and competitive memory cost, indicating its greater stability against datasets with different probability distribution perturbations, and its strong ability to run on memory-constrained devices."}}
{"id": "2507.12840", "categories": ["cs.IR", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.12840", "abs": "https://arxiv.org/abs/2507.12840", "authors": ["Muhammad Javed", "Sedigh Khademi Habibabadi", "Christopher Palmer", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila"], "title": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand Public Concerns about Vaccines", "comment": null, "summary": "Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94).", "AI": {"tldr": "Developed VaxPulse Query Corner using Retrieval Augmented Generation to address complex vaccine concern queries on social media, achieving high faithfulness and relevance in analysis.", "motivation": "Traditional methods struggle to capture nuanced opinions on social media, and Large Language Models (LLMs) often miss current events and community concerns, with hallucinations compromising public health communication.", "method": "Retrieval Augmented Generation technique", "result": "Analysing 35,103 Shingrix social media posts, the tool achieved answer faithfulness (0.96) and relevance (0.94).", "conclusion": "VaxPulse Query Corner, a tool using Retrieval Augmented Generation, effectively addresses complex queries about public vaccine concerns on social media, achieving high answer faithfulness and relevance."}}
{"id": "2507.12547", "categories": ["cs.CL", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.12547", "abs": "https://arxiv.org/abs/2507.12547", "authors": ["Lionel Wong", "Katherine M. Collins", "Lance Ying", "Cedegao E. Zhang", "Adrian Weller", "Tobias Gersternberg", "Timothy O'Donnell", "Alexander K. Lew", "Jacob D. Andreas", "Joshua B. Tenenbaum", "Tyler Brooke-Wilson"], "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "comment": "Presented at CogSci 2025", "summary": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "AI": {"tldr": "People use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations.", "motivation": "What permits us to draw in globally relevant information and reason over it coherently?", "method": "a ``Model Synthesis Architecture'' (MSA) -- using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models", "result": "MSA approach captures human judgments better than language model-only baselines, under both direct and chain-of-thought generations from the LM that supports model synthesis.", "conclusion": "MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains."}}
{"id": "2507.12484", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12484", "abs": "https://arxiv.org/abs/2507.12484", "authors": ["Jaros\u0142aw A. Chudziak", "Adam Kostka"], "title": "AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education", "comment": "8 pages, 5 figures", "summary": "The growing ubiquity of artificial intelligence (AI), in particular large\nlanguage models (LLMs), has profoundly altered the way in which learners gain\nknowledge and interact with learning material, with many claiming that AI\npositively influences their learning achievements. Despite this advancement,\ncurrent AI tutoring systems face limitations associated with their reactive\nnature, often providing direct answers without encouraging deep reflection or\nincorporating structured pedagogical tools and strategies. This limitation is\nmost apparent in the field of mathematics, in which AI tutoring systems remain\nunderdeveloped. This research addresses the question: How can AI tutoring\nsystems move beyond providing reactive assistance to enable structured,\nindividualized, and tool-assisted learning experiences? We introduce a novel\nmulti-agent AI tutoring platform that combines adaptive and personalized\nfeedback, structured course generation, and textbook knowledge retrieval to\nenable modular, tool-assisted learning processes. This system allows students\nto learn new topics while identifying and targeting their weaknesses, revise\nfor exams effectively, and practice on an unlimited number of personalized\nexercises. This article contributes to the field of artificial intelligence in\neducation by introducing a novel platform that brings together pedagogical\nagents and AI-driven components, augmenting the field with modular and\neffective systems for teaching mathematics.", "AI": {"tldr": "This research addresses the question: How can AI tutoring systems move beyond providing reactive assistance to enable structured, individualized, and tool-assisted learning experiences?", "motivation": "current AI tutoring systems face limitations associated with their reactive nature, often providing direct answers without encouraging deep reflection or incorporating structured pedagogical tools and strategies. This limitation is most apparent in the field of mathematics, in which AI tutoring systems remain underdeveloped.", "method": "We introduce a novel multi-agent AI tutoring platform that combines adaptive and personalized feedback, structured course generation, and textbook knowledge retrieval to enable modular, tool-assisted learning processes.", "result": "This system allows students to learn new topics while identifying and targeting their weaknesses, revise for exams effectively, and practice on an unlimited number of personalized exercises.", "conclusion": "This article contributes to the field of artificial intelligence in education by introducing a novel platform that brings together pedagogical agents and AI-driven components, augmenting the field with modular and effective systems for teaching mathematics."}}
{"id": "2507.12507", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12507", "abs": "https://arxiv.org/abs/2507.12507", "authors": ["Mingjie Liu", "Shizhe Diao", "Jian Hu", "Ximing Lu", "Xin Dong", "Hao Zhang", "Alexander Bukharin", "Shaokun Zhang", "Jiaqi Zeng", "Makesh Narsimhan Sreedhar", "Gerald Shen", "David Mosallanezhad", "Di Zhang", "Jonas Yang", "June Yang", "Oleksii Kuchaiev", "Guilin Liu", "Zhiding Yu", "Pavlo Molchanov", "Yejin Choi", "Jan Kautz", "Yi Dong"], "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training", "comment": "14 pages, 7 figures", "summary": "Recent advancements in reasoning-focused language models such as OpenAI's O1\nand DeepSeek-R1 have shown that scaling test-time computation-through\nchain-of-thought reasoning and iterative exploration-can yield substantial\nimprovements on complex tasks like mathematics and code generation. These\nbreakthroughs have been driven by large-scale reinforcement learning (RL),\nparticularly when combined with verifiable reward signals that provide\nobjective and grounded supervision. In this report, we investigate the effects\nof prolonged reinforcement learning on a small language model across a diverse\nset of reasoning domains. Our work identifies several key ingredients for\neffective training, including the use of verifiable reward tasks, enhancements\nto Group Relative Policy Optimization (GRPO), and practical techniques to\nimprove training stability and generalization. We introduce controlled KL\nregularization, clipping ratio, and periodic reference policy resets as\ncritical components for unlocking long-term performance gains. Our model\nachieves significant improvements over strong baselines, including +14.7% on\nmath, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate\ncontinued research, we release our model publicly.", "AI": {"tldr": "\u901a\u8fc7\u957f\u671f\u5f3a\u5316\u5b66\u4e60\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u903b\u8f91\u8c1c\u9898\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u8fed\u4ee3\u63a2\u7d22\u6765\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u590d\u6742\u4efb\u52a1\uff0c\u4f8b\u5982\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u3002", "method": "\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u4efb\u52a1\u3001\u6539\u8fdb\u7684 Group Relative Policy Optimization (GRPO) \u4ee5\u53ca\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5b9e\u7528\u6280\u672f\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5404\u79cd\u63a8\u7406\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u516c\u5f00\u4e86\u8be5\u6a21\u578b\u4ee5\u4fc3\u8fdb\u6301\u7eed\u7814\u7a76\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u903b\u8f91\u8c1c\u9898\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5206\u522b\u63d0\u9ad8\u4e86 +14.7%\u3001+13.9% \u548c +54.8%\u3002"}}
{"id": "2507.12490", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12490", "abs": "https://arxiv.org/abs/2507.12490", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "H\u00e9ctor Cerezo-Costas", "Dimosthenis Karatzas"], "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering", "comment": "This work has been accepted for presentation at the 16th Conference\n  and Labs of the Evaluation Forum (CLEF 2025) and will be published in the\n  proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that\n(1) generates natural language rationales via a vision language model, (2)\ngrounds these rationales to spatial sub-regions by computing multimodal\nembedding similarities over a configurable grid with majority voting, and (3)\nrestricts the generation of responses only from the relevant regions selected\nin the masked image. Experiments on the DocVQA dataset demonstrate that our\nbest configuration not only outperforms the base model on exact match accuracy\nand Average Normalized Levenshtein Similarity metrics but also enhances\ntransparency and reproducibility in DocVQA without additional model\nfine-tuning.", "AI": {"tldr": "EaGERS\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7406\u7531\u5e76\u5c06\u5176\u5b9a\u4f4d\u5230\u56fe\u50cf\u533a\u57df\uff0c\u4ece\u800c\u63d0\u9ad8DocVQA\u7684\u900f\u660e\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u9ad8DocVQA\u7684\u900f\u660e\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u5fae\u8c03\u3002", "method": "EaGERS\uff0c\u4e00\u4e2a\u5b8c\u5168\u514d\u8bad\u7ec3\u4e14\u6a21\u578b\u65e0\u5173\u7684\u6d41\u7a0b\uff0c\u5b83\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u7406\u7531\uff0c\u901a\u8fc7\u8ba1\u7b97\u53ef\u914d\u7f6e\u7f51\u683c\u4e0a\u7684\u591a\u6a21\u6001\u5d4c\u5165\u76f8\u4f3c\u6027\u5c06\u8fd9\u4e9b\u7406\u7531\u5b9a\u4f4d\u5230\u7a7a\u95f4\u5b50\u533a\u57df\uff0c\u5e76\u4e14\u4ec5\u4ece\u63a9\u853d\u56fe\u50cf\u4e2d\u9009\u62e9\u7684\u76f8\u5173\u533a\u57df\u9650\u5236\u54cd\u5e94\u7684\u751f\u6210\u3002", "result": "EaGERS\u7684\u6700\u4f73\u914d\u7f6e\u5728DocVQA\u6570\u636e\u96c6\u4e0a\u7684\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u548c\u5e73\u5747\u5f52\u4e00\u5316Levenshtein\u76f8\u4f3c\u5ea6\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "EaGERS\u5728DocVQA\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u900f\u660e\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u5fae\u8c03\u3002"}}
{"id": "2507.12844", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.12844", "abs": "https://arxiv.org/abs/2507.12844", "authors": ["Joel Nitu", "Heidrun M\u00fchle", "Andreas St\u00f6ckl"], "title": "Machine-Readable Ads: Accessibility and Trust Patterns for AI Web Agents interacting with Online Advertisements", "comment": null, "summary": "Autonomous multimodal language models are rapidly evolving into web agents\nthat can browse, click, and purchase items on behalf of users, posing a threat\nto display advertising designed for human eyes. Yet little is known about how\nthese agents interact with ads or which design principles ensure reliable\nengagement. To address this, we ran a controlled experiment using a faithful\nclone of the news site TT.com, seeded with diverse ads: static banners, GIFs,\ncarousels, videos, cookie dialogues, and paywalls. We ran 300 initial trials\nplus follow-ups using the Document Object Model (DOM)-centric Browser Use\nframework with GPT-4o, Claude 3.7 Sonnet, Gemini 2.0 Flash, and the pixel-based\nOpenAI Operator, across 10 realistic user tasks. Our results show these agents\ndisplay severe satisficing: they never scroll beyond two viewports and ignore\npurely visual calls to action, clicking banners only when semantic button\noverlays or off-screen text labels are present. Critically, when sweepstake\nparticipation required a purchase, GPT-4o and Claude 3.7 Sonnet subscribed in\n100% of trials, and Gemini 2.0 Flash in 70%, revealing gaps in cost-benefit\nanalysis. We identified five actionable design principles-semantic overlays,\nhidden labels, top-left placement, static frames, and dialogue replacement,\nthat make human-centric creatives machine-detectable without harming user\nexperience. We also evaluated agent trustworthiness through \"behavior patterns\"\nsuch as cookie consent handling and subscription choices, highlighting\nmodel-specific risk boundaries and the urgent need for robust trust evaluation\nframeworks in real-world advertising.", "AI": {"tldr": "Autonomous agents readily subscribe to services and ignore visual cues in ads, highlighting the need for better design and trust evaluation.", "motivation": "Little is known about how autonomous multimodal language model agents interact with ads or which design principles ensure reliable engagement, posing a threat to display advertising.", "method": "Controlled experiment using a faithful clone of the news site TT.com, seeded with diverse ads, and the Document Object Model (DOM)-centric Browser Use framework with GPT-4o, Claude 3.7 Sonnet, Gemini 2.0 Flash, and the pixel-based OpenAI Operator, across 10 realistic user tasks.", "result": "Agents display severe satisficing, never scrolling beyond two viewports and ignoring purely visual calls to action. GPT-4o and Claude 3.7 Sonnet subscribed in 100% of trials when sweepstake participation required a purchase, and Gemini 2.0 Flash in 70%. Five actionable design principles were identified.", "conclusion": "Autonomous multimodal language models exhibit satisficing behavior, ignoring visual cues and readily making purchases, revealing vulnerabilities in cost-benefit analysis. The paper identifies actionable design principles for machine-detectable ads and evaluates agent trustworthiness, highlighting the need for robust trust evaluation frameworks."}}
{"id": "2507.12553", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12553", "abs": "https://arxiv.org/abs/2507.12553", "authors": ["Michael A. Lepori", "Jennifer Hu", "Ishita Dasgupta", "Roma Patel", "Thomas Serre", "Ellie Pavlick"], "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility", "comment": null, "summary": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.", "AI": {"tldr": "\u6211\u4eec\u4f7f\u7528\u53ef\u89e3\u91ca\u6027\u6280\u672f\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u60c5\u6001\u5206\u7c7b\uff0c\u53d1\u73b0\u5b83\u4eec\u6bd4\u4ee5\u524d\u8ba4\u4e3a\u7684\u66f4\u64c5\u957f\u6b64\u4efb\u52a1\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u7528\u6765\u6a21\u62df\u4eba\u7c7b\u7684\u60c5\u6001\u5206\u7c7b\u884c\u4e3a\u3002", "motivation": "\u4e3a\u4e86\u53ef\u9760\u5730\u5b8c\u6210\u4efb\u52a1\uff0c\u8bed\u8a00\u6a21\u578b\u5fc5\u987b\u80fd\u591f\u8bc6\u522b\u53e5\u5b50\u7684\u60c5\u6001\u7c7b\u522b\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5bf9\u8bed\u8a00\u6a21\u578b\u6839\u636e\u60c5\u6001\u5bf9\u53e5\u5b50\u8fdb\u884c\u5206\u7c7b\u7684\u80fd\u529b\u63d0\u51fa\u4e86\u8d28\u7591\u3002", "method": "\u6211\u4eec\u8bc6\u522b\u4e86\u5404\u79cd\u8bed\u8a00\u6a21\u578b\u4e2d\u533a\u5206\u60c5\u6001\u7c7b\u522b\u7684\u7ebf\u6027\u8868\u793a\uff0c\u6216\u79f0\u60c5\u6001\u5dee\u5f02\u5411\u91cf\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u5411\u91cf\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u8fdb\u884c\u6bd4\u4e4b\u524d\u62a5\u9053\u7684\u66f4\u53ef\u9760\u7684\u60c5\u6001\u5206\u7c7b\u5224\u65ad\u3002\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u63d0\u9ad8\uff0c\u60c5\u6001\u5dee\u5f02\u5411\u91cf\u4ee5\u4e00\u81f4\u7684\u987a\u5e8f\u51fa\u73b0\u3002\u5728LM\u6fc0\u6d3b\u4e2d\u8bc6\u522b\u51fa\u7684\u60c5\u6001\u5dee\u5f02\u5411\u91cf\u53ef\u7528\u4e8e\u5efa\u6a21\u7ec6\u7c92\u5ea6\u7684\u4eba\u7c7b\u5206\u7c7b\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u6211\u4eec\u6df1\u5165\u4e86\u89e3\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u60c5\u6001\u5206\u7c7b\uff0c\u5e76\u6709\u53ef\u80fd\u52a0\u6df1\u6211\u4eec\u5bf9\u4eba\u7c7b\u60c5\u6001\u5206\u7c7b\u7684\u7406\u89e3\u3002"}}
{"id": "2507.12494", "categories": ["cs.AI", "cs.GT", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12494", "abs": "https://arxiv.org/abs/2507.12494", "authors": ["Dustin Holley", "Jovin D'sa", "Hossein Nourkhiz Mahjoub", "Gibran Ali"], "title": "MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents", "comment": "8 pages", "summary": "Enhancing simulation environments to replicate real-world driver behavior,\ni.e., more humanlike sim agents, is essential for developing autonomous vehicle\ntechnology. In the context of highway merging, previous works have studied the\noperational-level yielding dynamics of lag vehicles in response to a merging\ncar at highway on-ramps. Other works focusing on tactical decision modeling\ngenerally consider limited action sets or utilize payoff functions with large\nparameter sets and limited payoff bounds. In this work, we aim to improve the\nsimulation of the highway merge scenario by targeting a game theoretic model\nfor tactical decision-making with improved payoff functions and lag actions. We\ncouple this with an underlying dynamics model to have a unified decision and\ndynamics model that can capture merging interactions and simulate more\nrealistic interactions in an explainable and interpretable fashion. The\nproposed model demonstrated good reproducibility of complex interactions when\nvalidated on a real-world dataset. The model was finally integrated into a high\nfidelity simulation environment and confirmed to have adequate computation time\nefficiency for use in large-scale simulations to support autonomous vehicle\ndevelopment.", "AI": {"tldr": "This paper improves highway merge simulation by using a game theoretic model for tactical decision-making with improved payoff functions and lag actions. The model is validated on real-world data and integrated into a high-fidelity simulation environment for autonomous vehicle development.", "motivation": "Enhancing simulation environments to replicate real-world driver behavior is essential for developing autonomous vehicle technology. Previous works have limitations in action sets or utilize payoff functions with large parameter sets and limited payoff bounds.", "method": "a game theoretic model for tactical decision-making with improved payoff functions and lag actions coupled with an underlying dynamics model", "result": "The model was integrated into a high fidelity simulation environment.", "conclusion": "The proposed model demonstrated good reproducibility of complex interactions when validated on a real-world dataset and confirmed to have adequate computation time efficiency for use in large-scale simulations to support autonomous vehicle development."}}
{"id": "2507.12549", "categories": ["cs.LG", "cs.CC", "stat.ML", "68Q15, 68Q10, 68T07", "F.1.1; F.1.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.12549", "abs": "https://arxiv.org/abs/2507.12549", "authors": ["Yuxi Liu", "Konpat Preechakul", "Kananart Kuwaranancharoen", "Yutong Bai"], "title": "The Serial Scaling Hypothesis", "comment": "28 pages (13 pages main text + appendices & references), 8 figures,\n  equal-contribution first authors", "summary": "While machine learning has advanced through massive parallelization, we\nidentify a critical blind spot: some problems are fundamentally sequential.\nThese \"inherently serial\" problems-from mathematical reasoning to physical\nsimulations to sequential decision-making-require dependent computational steps\nthat cannot be parallelized. Drawing from complexity theory, we formalize this\ndistinction and demonstrate that current parallel-centric architectures face\nfundamental limitations on such tasks. We argue that recognizing the serial\nnature of computation holds profound implications on machine learning, model\ndesign, hardware development. As AI tackles increasingly complex reasoning,\ndeliberately scaling serial computation-not just parallel computation-is\nessential for continued progress.", "AI": {"tldr": "\u5927\u89c4\u6a21\u5e76\u884c\u5316\u4fc3\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u4f46\u67d0\u4e9b\u95ee\u9898\u672c\u8d28\u4e0a\u662f\u987a\u5e8f\u7684\uff0c\u5f53\u524d\u7684\u5e76\u884c\u67b6\u6784\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u9762\u4e34\u6839\u672c\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u4ee5\u5e76\u884c\u8ba1\u7b97\u4e3a\u4e2d\u5fc3\u7684\u67b6\u6784\u5728\u5904\u7406\u9700\u8981\u4f9d\u8d56\u8ba1\u7b97\u6b65\u9aa4\u7684\u95ee\u9898\uff08\u4ece\u6570\u5b66\u63a8\u7406\u5230\u7269\u7406\u6a21\u62df\u518d\u5230\u987a\u5e8f\u51b3\u7b56\uff09\u65f6\u9762\u4e34\u6839\u672c\u9650\u5236\u3002", "method": "\u4ece\u590d\u6742\u6027\u7406\u8bba\u4e2d\u5f97\u51fa", "result": "\u8bc1\u5b9e\u4e86\u5f53\u524d\u4ee5\u5e76\u884c\u8ba1\u7b97\u4e3a\u4e2d\u5fc3\u7684\u67b6\u6784\u5728\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\u65f6\u9762\u4e34\u6839\u672c\u9650\u5236\u3002", "conclusion": "\u6269\u5c55\u4e32\u884c\u8ba1\u7b97\u5bf9\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.12508", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12508", "abs": "https://arxiv.org/abs/2507.12508", "authors": ["Yuncong Yang", "Jiageng Liu", "Zheyuan Zhang", "Siyuan Zhou", "Reuben Tan", "Jianwei Yang", "Yilun Du", "Chuang Gan"], "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "comment": "Project Page: https://umass-embodied-agi.github.io/MindJourney", "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.", "AI": {"tldr": "MindJourney\u901a\u8fc7\u5c06VLM\u4e0e\u4e16\u754c\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4f7f\u5176\u5177\u5907\u4e863D\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u9884\u6d4b\u81ea\u6211\u4e2d\u5fc3\u8fd0\u52a8\u540e\u573a\u666f\u7684\u5916\u89c2\u7b49\u7b80\u5355\u4efb\u52a1\u4e2d\u7ecf\u5e38\u9047\u5230\u56f0\u96be\uff0c\u56e0\u4e3a\u5b83\u4eec\u611f\u77e52D\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f3D\u52a8\u6001\u7684\u5185\u90e8\u6a21\u578b\u3002", "method": "\u5c06VLM\u4e0e\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u53ef\u63a7\u4e16\u754c\u6a21\u578b\u76f8\u7ed3\u5408\uff0cVLM\u8fed\u4ee3\u5730\u52fe\u52d2\u51fa\u7b80\u6d01\u7684\u76f8\u673a\u8f68\u8ff9\uff0c\u800c\u4e16\u754c\u6a21\u578b\u5219\u5408\u6210\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u5bf9\u5e94\u7684\u89c6\u56fe\u3002\u7136\u540e\uff0cVLM\u6839\u636e\u4ea4\u4e92\u63a2\u7d22\u671f\u95f4\u6536\u96c6\u7684\u591a\u89c6\u56fe\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\u3002", "result": "MindJourney\u5728SAT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u5347\u4e868%\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6539\u8fdb\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406VLMs\u3002", "conclusion": "MindJourney\u5728SAT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u5347\u4e868%\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5c06VLMs\u4e0e\u4e16\u754c\u6a21\u578b\u914d\u5bf9\u8fdb\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u662f\u4e00\u79cd\u5b9e\u73b0\u9c81\u68d23D\u63a8\u7406\u7684\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u8fd8\u6539\u8fdb\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406VLMs\u3002"}}
{"id": "2507.12871", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12871", "abs": "https://arxiv.org/abs/2507.12871", "authors": ["Jinqiu Jin", "Yang Zhang", "Junwei Pan", "Fuli Feng", "Hua Lu", "Haijie Gu", "Xiangnan He"], "title": "Generative Multi-Target Cross-Domain Recommendation", "comment": null, "summary": "Recently, there has been a surge of interest in Multi-Target Cross-Domain\nRecommendation (MTCDR), which aims to enhance recommendation performance across\nmultiple domains simultaneously. Existing MTCDR methods primarily rely on\ndomain-shared entities (\\eg users or items) to fuse and transfer cross-domain\nknowledge, which may be unavailable in non-overlapped recommendation scenarios.\nSome studies model user preferences and item features as domain-sharable\nsemantic representations, which can be utilized to tackle the MTCDR task.\nNevertheless, they often require extensive auxiliary data for pre-training.\nDeveloping more effective solutions for MTCDR remains an important area for\nfurther exploration.\n  Inspired by recent advancements in generative recommendation, this paper\nintroduces GMC, a generative paradigm-based approach for multi-target\ncross-domain recommendation. The core idea of GMC is to leverage semantically\nquantized discrete item identifiers as a medium for integrating multi-domain\nknowledge within a unified generative model. GMC first employs an item\ntokenizer to generate domain-shared semantic identifiers for each item, and\nthen formulates item recommendation as a next-token generation task by training\na domain-unified sequence-to-sequence model. To further leverage the domain\ninformation to enhance performance, we incorporate a domain-aware contrastive\nloss into the semantic identifier learning, and perform domain-specific\nfine-tuning on the unified recommender. Extensive experiments on five public\ndatasets demonstrate the effectiveness of GMC compared to a range of baseline\nmethods.", "AI": {"tldr": "GMC is a generative approach for multi-target cross-domain recommendation that uses semantically quantized discrete item identifiers to integrate multi-domain knowledge. It outperforms baselines on five public datasets.", "motivation": "Existing MTCDR methods primarily rely on domain-shared entities, which may be unavailable in non-overlapped recommendation scenarios. Some studies require extensive auxiliary data for pre-training. Developing more effective solutions for MTCDR remains an important area for further exploration.", "method": "GMC leverages semantically quantized discrete item identifiers as a medium for integrating multi-domain knowledge within a unified generative model. It employs an item tokenizer to generate domain-shared semantic identifiers and formulates item recommendation as a next-token generation task. A domain-aware contrastive loss is incorporated, and domain-specific fine-tuning is performed.", "result": "GMC, a generative paradigm-based approach for multi-target cross-domain recommendation, is effective.", "conclusion": "Extensive experiments on five public datasets demonstrate the effectiveness of GMC compared to a range of baseline methods."}}
{"id": "2507.12672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12672", "abs": "https://arxiv.org/abs/2507.12672", "authors": ["Abu-Viskhan A. Umishov", "Vladislav A. Grigorian"], "title": "The first open machine translation system for the Chechen language", "comment": "7 pages", "summary": "We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.", "AI": {"tldr": "\u53d1\u5e03\u4e86\u9996\u4e2a\u5f00\u6e90\u8f66\u81e3\u8bed-\u4fc4\u8bed\u7ffb\u8bd1\u6a21\u578b\u53ca\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03 NLLB-200 \u5b9e\u73b0\u3002", "motivation": "\u6784\u5efa\u9996\u4e2a\u5f00\u6e90\u7684\u8f66\u81e3\u8bed\u548c\u4fc4\u8bed\u4e92\u8bd1\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b NLLB-200 \u8fdb\u884c\u5fae\u8c03\uff0c\u5c06\u65b0\u8bed\u8a00\uff08\u8f66\u81e3\u8bed\uff09\u52a0\u5165\u5230\u591a\u8bed\u79cd\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u3002", "result": "\u4fc4\u8bed\u5230\u8f66\u81e3\u8bed\u7684 BLEU / ChrF++ \u5206\u6570\u4e3a 8.34 / 34.69\uff0c\u53cd\u5411\u7ffb\u8bd1\u4e3a 20.89 / 44.55\u3002", "conclusion": "\u53d1\u5e03\u4e86\u8f66\u81e3\u8bed\u548c\u4fc4\u8bed\u4e4b\u95f4\u7684\u7ffb\u8bd1\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e73\u884c\u8bed\u6599\u5e93\u548c\u591a\u8bed\u79cd\u53e5\u5b50\u7f16\u7801\u5668\u3002"}}
{"id": "2507.12599", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12599", "abs": "https://arxiv.org/abs/2507.12599", "authors": ["L\u00e9o Sauli\u00e8res"], "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs", "comment": "69 pages, 19 figures", "summary": "The success of recent Artificial Intelligence (AI) models has been\naccompanied by the opacity of their internal mechanisms, due notably to the use\nof deep neural networks. In order to understand these internal mechanisms and\nexplain the output of these AI models, a set of methods have been proposed,\ngrouped under the domain of eXplainable AI (XAI). This paper focuses on a\nsub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims\nto explain the actions of an agent that has learned by reinforcement learning.\nWe propose an intuitive taxonomy based on two questions \"What\" and \"How\". The\nfirst question focuses on the target that the method explains, while the second\nrelates to the way the explanation is provided. We use this taxonomy to provide\na state-of-the-art review of over 250 papers. In addition, we present a set of\ndomains close to XRL, which we believe should get attention from the community.\nFinally, we identify some needs for the field of XRL.", "AI": {"tldr": "This paper proposes a taxonomy for XRL, reviews 250+ papers, and identifies future needs in the field.", "motivation": "The opacity of AI models' internal mechanisms, especially deep neural networks, motivates the need for eXplainable AI (XAI), specifically eXplainable Reinforcement Learning (XRL).", "method": "The authors propose a taxonomy based on \"What\" and \"How\" questions to review XRL papers.", "result": "The authors review over 250 papers using their taxonomy and present related domains.", "conclusion": "This paper identifies needs for the field of XRL."}}
{"id": "2507.12555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12555", "abs": "https://arxiv.org/abs/2507.12555", "authors": ["Slimane Larabi"], "title": "Can Mental Imagery Improve the Thinking Capabilities of AI Systems?", "comment": "15 pages, 8 figures", "summary": "Although existing models can interact with humans and provide satisfactory\nresponses, they lack the ability to act autonomously or engage in independent\nreasoning. Furthermore, input data in these models is typically provided as\nexplicit queries, even when some sensory data is already acquired.\n  In addition, AI agents, which are computational entities designed to perform\ntasks and make decisions autonomously based on their programming, data inputs,\nand learned knowledge, have shown significant progress. However, they struggle\nwith integrating knowledge across multiple domains, unlike humans.\n  Mental imagery plays a fundamental role in the brain's thinking process,\nwhich involves performing tasks based on internal multisensory data, planned\nactions, needs, and reasoning capabilities. In this paper, we investigate how\nto integrate mental imagery into a machine thinking framework and how this\ncould be beneficial in initiating the thinking process. Our proposed machine\nthinking framework integrates a Cognitive thinking unit supported by three\nauxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery\nUnit. Within this framework, data is represented as natural language sentences\nor drawn sketches, serving both informative and decision-making purposes. We\nconducted validation tests for this framework, and the results are presented\nand discussed.", "AI": {"tldr": "This paper introduces a machine thinking framework that uses mental imagery to enhance autonomous reasoning and decision-making in AI agents.", "motivation": "Existing models lack autonomous action and independent reasoning, struggling to integrate knowledge across domains, unlike humans who use mental imagery.", "method": "The proposed framework integrates a Cognitive thinking unit with Input Data, Needs, and Mental Imagery Units. Data is represented as natural language or sketches.", "result": "Validation tests for the proposed framework are conducted, and the results are presented and discussed.", "conclusion": "This paper proposes and validates a machine thinking framework integrating mental imagery to improve autonomous reasoning and decision-making."}}
{"id": "2507.12566", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12566", "abs": "https://arxiv.org/abs/2507.12566", "authors": ["Gen Luo", "Wenhan Dou", "Wenhao Li", "Zhaokai Wang", "Xue Yang", "Changyao Tian", "Hao Li", "Weiyun Wang", "Wenhai Wang", "Xizhou Zhu", "Yu Qiao", "Jifeng Dai"], "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models", "comment": null, "summary": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies for monolithic MLLMs often\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a new visual parameter space into a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nvia delta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through a\nmultimodal mixture-of-experts architecture. In addition, we design an\ninnovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++\nintroduces additional visual attention experts to Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fused CUDA kernel to speed up its MoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive performance with Mono-InternVL. To evaluate\nour approach, we conduct extensive experiments across 15 benchmarks. Results\ndemonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out\nof 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared\nto its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves\nsimilar multimodal performance while reducing first-token latency by up to 69%.\nCode and models are released at https://github.com/OpenGVLab/Mono-InternVL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u4f53 MLLM\uff0c\u901a\u8fc7\u5d4c\u5165\u65b0\u7684\u89c6\u89c9\u53c2\u6570\u7a7a\u95f4\u548c\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u4f53\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7ed3\u6784\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u7ecf\u5e38\u906d\u53d7\u4e0d\u7a33\u5b9a\u7684\u4f18\u5316\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u5f15\u5165 Mono-InternVL\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u6574\u5408\u89c6\u89c9\u4e13\u5bb6\uff0c\u5e76\u8bbe\u8ba1\u5185\u751f\u89c6\u89c9\u9884\u8bad\u7ec3 (EViP) \u548c\u6539\u8fdb\u7684 EViP++\u3002", "result": "Mono-InternVL \u5728 15 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684 12 \u4e2a\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u4f53 MLLM\uff0cMono-InternVL-1.5 \u5b9e\u73b0\u4e86\u7c7b\u4f3c\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002", "conclusion": "Mono-InternVL \u548c Mono-InternVL-1.5 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u6709\u6240\u6539\u8fdb\u3002"}}
{"id": "2507.13336", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13336", "abs": "https://arxiv.org/abs/2507.13336", "authors": ["Weizhi Zhang", "Liangwei Yang", "Zihe Song", "Henrry Peng Zou", "Ke Xu", "Yuanjie Zhu", "Philip S. Yu"], "title": "SGCL: Unifying Self-Supervised and Supervised Learning for Graph Recommendation", "comment": "Accepted in RecSys 2025. arXiv admin note: substantial text overlap\n  with arXiv:2404.15954", "summary": "Recommender systems (RecSys) are essential for online platforms, providing\npersonalized suggestions to users within a vast sea of information.\nSelf-supervised graph learning seeks to harness high-order collaborative\nfiltering signals through unsupervised augmentation on the user-item bipartite\ngraph, primarily leveraging a multi-task learning framework that includes both\nsupervised recommendation loss and self-supervised contrastive loss. However,\nthis separate design introduces additional graph convolution processes and\ncreates inconsistencies in gradient directions due to disparate losses,\nresulting in prolonged training times and sub-optimal performance. In this\nstudy, we introduce a unified framework of Supervised Graph Contrastive\nLearning for recommendation (SGCL) to address these issues. SGCL uniquely\ncombines the training of recommendation and unsupervised contrastive losses\ninto a cohesive supervised contrastive learning loss, aligning both tasks\nwithin a single optimization direction for exceptionally fast training.\nExtensive experiments on three real-world datasets show that SGCL outperforms\nstate-of-the-art methods, achieving superior accuracy and efficiency.", "AI": {"tldr": "SGCL\u7ed3\u5408\u4e86\u63a8\u8350\u548c\u65e0\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u7684\u8bad\u7ec3\uff0c\u4f18\u4e8e\u5f53\u524d\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u6211\u76d1\u7763\u56fe\u5b66\u4e60\u65e8\u5728\u901a\u8fc7\u7528\u6237-\u9879\u76ee\u4e8c\u5206\u56fe\u4e0a\u7684\u65e0\u76d1\u7763\u589e\u5f3a\u6765\u5229\u7528\u9ad8\u9636\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\uff0c\u4e3b\u8981\u5229\u7528\u5305\u62ec\u76d1\u7763\u63a8\u8350\u635f\u5931\u548c\u81ea\u6211\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5206\u79bb\u7684\u8bbe\u8ba1\u5f15\u5165\u4e86\u989d\u5916\u7684\u56fe\u5377\u79ef\u8fc7\u7a0b\uff0c\u5e76\u7531\u4e8e\u4e0d\u540c\u7684\u635f\u5931\u800c\u5bfc\u81f4\u68af\u5ea6\u65b9\u5411\u7684\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u5ef6\u957f\u548c\u6027\u80fd\u6b20\u4f73\u3002", "method": "SGCL\u5c06\u63a8\u8350\u548c\u65e0\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u7684\u8bad\u7ec3\u72ec\u7279\u5730\u7ed3\u5408\u5230\u4e00\u4e2a\u6709\u51dd\u805a\u529b\u7684\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u4e2d\uff0c\u4ece\u800c\u5728\u5355\u4e2a\u4f18\u5316\u65b9\u5411\u4e0a\u5bf9\u9f50\u4e86\u8fd9\u4e24\u4e2a\u4efb\u52a1\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u975e\u5e38\u5feb\u901f\u7684\u8bad\u7ec3\u3002", "result": "SGCL\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "SGCL\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8868\u660e\u5176\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.12679", "categories": ["cs.CL", "q-bio.QM", "I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.12679", "abs": "https://arxiv.org/abs/2507.12679", "authors": ["Arthur J. Funnell", "Panayiotis Petousis", "Fabrice Harel-Canada", "Ruby Romero", "Alex A. T. Bui", "Adam Koncsol", "Hritika Chaturvedi", "Chelsea Shover", "David Goodman-Meza"], "title": "Improving Drug Identification in Overdose Death Surveillance using Large Language Models", "comment": "30 pages, 1 figure, 4 tables, 2 supplemental figures, 4 supplemental\n  tables, submitted to Journal of Forensic Sciences (JFS)", "summary": "The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.", "AI": {"tldr": "This paper uses NLP models to classify overdose deaths from free-text reports, achieving high accuracy with fine-tuned BioClinicalBERT, which can improve surveillance workflows.", "motivation": "The rising rate of drug-related deaths in the United States, largely driven by fentanyl, requires timely and accurate surveillance. However, critical overdose data are often buried in free-text coroner reports, leading to delays and information loss when coded into ICD (International Classification of Disease)-10 classifications.", "method": "Multiple NLP approaches were evaluated for classifying specific drug involvement from unstructured death certificate text. These included traditional single- and multi-label classifiers, as well as fine-tuned encoder-only language models such as Bidirectional Encoder Representations from Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large language models such as Qwen 3 and Llama 3.", "result": "Fine-tuned BioClinicalBERT models achieved near-perfect performance, with macro F1 scores >=0.998 on the internal test set. External validation confirmed robustness (macro F1=0.966), outperforming conventional machine learning, general-domain BERT models, and various decoder-only large language models.", "conclusion": "NLP models, particularly fine-tuned clinical variants like BioClinicalBERT, offer a highly accurate and scalable solution for overdose death classification from free-text reports. These methods can significantly accelerate surveillance workflows, overcoming the limitations of manual ICD-10 coding and supporting near real-time detection of emerging substance use trends."}}
{"id": "2507.12666", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12666", "abs": "https://arxiv.org/abs/2507.12666", "authors": ["Alex Zook", "Josef Spjut", "Jonathan Tremblay"], "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models", "comment": "Published at Reinforcement Learning and Video Games workshop\n  https://sites.google.com/view/rlvg-workshop-2025/home", "summary": "Game design hinges on understanding how static rules and content translate\ninto dynamic player behavior - something modern generative systems that inspect\nonly a game's code or assets struggle to capture. We present an automated\ndesign iteration framework that closes this gap by pairing a reinforcement\nlearning (RL) agent, which playtests the game, with a large multimodal model\n(LMM), which revises the game based on what the agent does. In each loop the RL\nplayer completes several episodes, producing (i) numerical play metrics and/or\n(ii) a compact image strip summarising recent video frames. The LMM designer\nreceives a gameplay goal and the current game configuration, analyses the play\ntraces, and edits the configuration to steer future behaviour toward the goal.\nWe demonstrate results that LMMs can reason over behavioral traces supplied by\nRL agents to iteratively refine game mechanics, pointing toward practical,\nscalable tools for AI-assisted game design.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u8bbe\u8ba1\u8fed\u4ee3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09agent\uff08\u8d1f\u8d23\u6e38\u620f\u6d4b\u8bd5\uff09\u4e0e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\uff08\u8d1f\u8d23\u4fee\u6539\u6e38\u620f\uff09\u914d\u5bf9\uff0c\u4ece\u800c\u5f25\u8865\u4e86\u9759\u6001\u89c4\u5219\u548c\u5185\u5bb9\u8f6c\u5316\u4e3a\u52a8\u6001\u73a9\u5bb6\u884c\u4e3a\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u7cfb\u7edf\u53ea\u80fd\u68c0\u67e5\u6e38\u620f\u7684\u4ee3\u7801\u6216\u8d44\u6e90\uff0c\u96be\u4ee5\u6355\u6349\u9759\u6001\u89c4\u5219\u548c\u5185\u5bb9\u5982\u4f55\u8f6c\u5316\u4e3a\u52a8\u6001\u73a9\u5bb6\u884c\u4e3a\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09agent\uff08\u8d1f\u8d23\u6e38\u620f\u6d4b\u8bd5\uff09\u548c\u4e00\u4e2a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\uff08\u8d1f\u8d23\u6839\u636eagent\u7684\u884c\u4e3a\u4fee\u6539\u6e38\u620f\uff09\u3002", "result": "LMMs\u53ef\u4ee5\u901a\u8fc7RL agent\u63d0\u4f9b\u7684\u884c\u4e3a\u8f68\u8ff9\u8fdb\u884c\u63a8\u7406\uff0c\u8fed\u4ee3\u5730\u6539\u8fdb\u6e38\u620f\u673a\u5236\u3002", "conclusion": "LMMs\u53ef\u4ee5\u57fa\u4e8eRL agent\u7684\u884c\u4e3a\u8f68\u8ff9\u8fdb\u884c\u63a8\u7406\uff0c\u8fed\u4ee3\u5730\u6539\u8fdb\u6e38\u620f\u673a\u5236\uff0c\u4e3aAI\u8f85\u52a9\u6e38\u620f\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2507.12573", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12573", "abs": "https://arxiv.org/abs/2507.12573", "authors": ["Eduardo V. L. Barboza", "Paulo R. Lisboa de Almeida", "Alceu de Souza Britto Jr.", "Robert Sabourin", "Rafael M. O. Cruz"], "title": "IncA-DES: An incremental and adaptive dynamic ensemble selection approach using online K-d tree neighborhood search for data streams with concept drift", "comment": "Preprint of article published to Information Fusion", "summary": "Data streams pose challenges not usually encountered in batch-based ML. One\nof them is concept drift, which is characterized by the change in data\ndistribution over time. Among many approaches explored in literature, the\nfusion of classifiers has been showing good results and is getting growing\nattention. DS methods, due to the ensemble being instance-based, seem to be an\nefficient choice under drifting scenarios. However, some attention must be paid\nto adapting such methods for concept drift. The training must be done in order\nto create local experts, and the commonly used neighborhood-search DS may\nbecome prohibitive with the continuous arrival of data. In this work, we\npropose IncA-DES, which employs a training strategy that promotes the\ngeneration of local experts with the assumption that different regions of the\nfeature space become available with time. Additionally, the fusion of a concept\ndrift detector supports the maintenance of information and adaptation to a new\nconcept. An overlap-based classification filter is also employed in order to\navoid using the DS method when there is a consensus in the neighborhood, a\nstrategy that we argue every DS method should employ, as it was shown to make\nthem more applicable and quicker. Moreover, aiming to reduce the processing\ntime of the kNN, we propose an Online K-d tree algorithm, which can quickly\nremove instances without becoming inconsistent and deals with unbalancing\nconcerns that may occur in data streams. Experimental results showed that the\nproposed framework got the best average accuracy compared to seven\nstate-of-the-art methods considering different levels of label availability and\npresented the smaller processing time between the most accurate methods.\nAdditionally, the fusion with the Online K-d tree has improved processing time\nwith a negligible loss in accuracy. We have made our framework available in an\nonline repository.", "AI": {"tldr": "This paper introduces IncA-DES, a framework for handling concept drift in data streams. It combines local expert training, concept drift detection, and an Online K-d tree for efficient processing. Results show improved accuracy and speed compared to existing methods.", "motivation": "Concept drift in data streams poses challenges for batch-based ML. Distance-based (DS) methods are suitable for drifting scenarios but need adaptation for concept drift and efficient neighborhood search.", "method": "The paper proposes IncA-DES, which uses a training strategy for local expert generation, a concept drift detector for maintenance and adaptation, and an overlap-based classification filter. It also introduces an Online K-d tree algorithm for faster kNN processing.", "result": "Experimental results demonstrate that IncA-DES achieves the best average accuracy compared to seven state-of-the-art methods and presents smaller processing time among the most accurate methods. The Online K-d tree improves processing time with negligible accuracy loss.", "conclusion": "The proposed framework, IncA-DES, achieves high accuracy and efficient processing time compared to state-of-the-art methods. The fusion with the Online K-d tree further improves processing time with minimal accuracy loss. The framework is available in an online repository."}}
{"id": "2507.12590", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12590", "abs": "https://arxiv.org/abs/2507.12590", "authors": ["Judy Long", "Tao Liu", "Sean Alexander Woznicki", "Miljana Markovi\u0107", "Oskar Marko", "Molly Sears"], "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows", "comment": "A review article. 41 pages, 22 figures. Preprint", "summary": "Crop mapping involves identifying and classifying crop types using spatial\ndata, primarily derived from remote sensing imagery. This study presents the\nfirst comprehensive review of large-scale, pixel-wise crop mapping workflows,\nencompassing both conventional supervised methods and emerging transfer\nlearning approaches. To identify the optimal supervised crop mapping workflows,\nwe conducted systematic experiments, comparing six widely adopted satellite\nimage-based preprocessing methods, alongside eleven supervised pixel-wise\nclassification models. Additionally, we assessed the synergistic impact of\nvaried training sample sizes and variable combinations. Moreover, we identified\noptimal transfer learning techniques for different magnitudes of domain shift.\nThe evaluation of best methods was conducted across five diverse agricultural\nsites. Landsat 8 served as the primary satellite data source. Labels come from\nCDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval\npreprocessing paired with Transformer models consistently delivered optimal\nperformance for both supervised and transferable workflows. RF offered rapid\ntraining and competitive performance in conventional supervised learning and\ndirect transfer to similar domains. Second, transfer learning techniques\nenhanced workflow adaptability, with UDA being effective for homogeneous crop\nclasses while fine-tuning remains robust across diverse scenarios. Finally,\nworkflow choice depends heavily on the availability of labeled samples. With a\nsufficient sample size, supervised training typically delivers more accurate\nand generalizable results. Below a certain threshold, transfer learning that\nmatches the level of domain shift is a viable alternative to achieve crop\nmapping. Repository:\nBest-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows", "AI": {"tldr": "\u672c\u7814\u7a76\u56de\u987e\u4e86\u5927\u89c4\u6a21\u50cf\u7d20\u7ea7\u4f5c\u7269mapping\u5de5\u4f5c\u6d41\u7a0b\uff0c\u53d1\u73b0Transformer\u6a21\u578b\u548c\u7cbe\u7ec6\u5c3a\u5ea6\u9884\u5904\u7406\u6548\u679c\u6700\u4f73\uff0cRF\u6a21\u578b\u8bad\u7ec3\u5feb\uff0c\u8fc1\u79fb\u5b66\u4e60\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u3002\u5de5\u4f5c\u6d41\u9009\u62e9\u53d6\u51b3\u4e8e\u6807\u7b7e\u6837\u672c\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u5229\u7528\u9065\u611f\u56fe\u50cf\u8fdb\u884c\u5927\u89c4\u6a21\u3001\u50cf\u7d20\u7ea7\u4f5c\u7269mapping\u662f\u91cd\u8981\u7684\u3002\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u56de\u987e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u6700\u4f73\u7684\u76d1\u7763\u548c\u8fc1\u79fb\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u516d\u79cd\u536b\u661f\u56fe\u50cf\u9884\u5904\u7406\u65b9\u6cd5\u548c\u5341\u4e00\u79cd\u76d1\u7763\u50cf\u7d20\u5206\u7c7b\u6a21\u578b\uff0c\u8bc4\u4f30\u4e0d\u540c\u8bad\u7ec3\u6837\u672c\u5927\u5c0f\u548c\u53d8\u91cf\u7ec4\u5408\u7684\u534f\u540c\u5f71\u54cd\uff0c\u8bc6\u522b\u9002\u7528\u4e8e\u4e0d\u540c\u9886\u57df\u8fc1\u79fb\u7a0b\u5ea6\u7684\u6700\u4f73\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\u3002\u5728\u4e94\u4e2a\u519c\u4e1a\u5730\u70b9\u8bc4\u4f30\u6700\u4f73\u65b9\u6cd5\u3002Landsat 8\u662f\u4e3b\u8981\u7684\u536b\u661f\u6570\u636e\u6e90\uff0c\u6807\u7b7e\u6765\u81eaCDL\u7f6e\u4fe1\u50cf\u7d20\u548c\u5b9e\u5730\u8c03\u67e5\u3002", "result": "\u7cbe\u7ec6\u5c3a\u5ea6\u9884\u5904\u7406\u4e0eTransformer\u6a21\u578b\u7ed3\u5408\u6548\u679c\u6700\u4f73\u3002RF\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u5feb\uff0c\u6027\u80fd\u6709\u7ade\u4e89\u529b\u3002UDA\u9002\u7528\u4e8e\u540c\u7c7b\u4f5c\u7269\u7c7b\u522b\uff0c\u5fae\u8c03\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\u3002\u6837\u672c\u5145\u8db3\u65f6\uff0c\u76d1\u7763\u5b66\u4e60\u66f4\u51c6\u786e\uff0c\u6837\u672c\u4e0d\u8db3\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u662f\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "Transformer\u6a21\u578b\u548c\u7cbe\u7ec6\u5c3a\u5ea6\u9884\u5904\u7406\u5728\u76d1\u7763\u548c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u8868\u73b0\u6700\u4f73\u3002RF\u6a21\u578b\u8bad\u7ec3\u5feb\uff0c\u5728\u76d1\u7763\u5b66\u4e60\u548c\u540c\u7c7b\u9886\u57df\u8fc1\u79fb\u4e2d\u8868\u73b0\u6709\u7ade\u4e89\u529b\u3002\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u4e86\u5de5\u4f5c\u6d41\u7684\u9002\u5e94\u6027\uff0cUDA\u9002\u7528\u4e8e\u540c\u7c7b\u4f5c\u7269\uff0c\u5fae\u8c03\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3002\u5de5\u4f5c\u6d41\u9009\u62e9\u53d6\u51b3\u4e8e\u6807\u7b7e\u6837\u672c\u7684\u53ef\u7528\u6027\uff1b\u6837\u672c\u5145\u8db3\u65f6\uff0c\u76d1\u7763\u5b66\u4e60\u6548\u679c\u66f4\u597d\uff1b\u6837\u672c\u4e0d\u8db3\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u662f\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.12704", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.12704", "abs": "https://arxiv.org/abs/2507.12704", "authors": ["Xiangyi Chen", "Kousik Rajesh", "Matthew Lawhon", "Zelun Wang", "Hanyu Li", "Haomiao Li", "Saurabh Vishwas Joshi", "Pong Eksombatchai", "Jaewon Yang", "Yi-Ping Hsu", "Jiajing Xu", "Charles Rosenberg"], "title": "PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform", "comment": "RecSys 2025", "summary": "User activity sequences have emerged as one of the most important signals in\nrecommender systems. We present a foundational model, PinFM, for understanding\nuser activity sequences across multiple applications at a billion-scale visual\ndiscovery platform. We pretrain a transformer model with 20B+ parameters using\nextensive user activity data, then fine-tune it for specific applications,\nefficiently coupling it with existing models. While this\npretraining-and-fine-tuning approach has been popular in other domains, such as\nVision and NLP, its application in industrial recommender systems presents\nnumerous challenges. The foundational model must be scalable enough to score\nmillions of items every second while meeting tight cost and latency constraints\nimposed by these systems. Additionally, it should capture the interactions\nbetween user activities and other features and handle new items that were not\npresent during the pretraining stage.\n  We developed innovative techniques to address these challenges. Our\ninfrastructure and algorithmic optimizations, such as the Deduplicated\nCross-Attention Transformer (DCAT), improved our throughput by 600% on\nPinterest internal data. We demonstrate that PinFM can learn interactions\nbetween user sequences and candidate items by altering input sequences, leading\nto a 20% increase in engagement with new items. PinFM is now deployed to help\nimprove the experience of more than a half billion users across various\napplications.", "AI": {"tldr": "PinFM, a foundational model for user activity sequences, overcomes challenges in industrial recommender systems, improving throughput and engagement.", "motivation": "Understanding user activity sequences is important for recommender systems, but applying pretraining-and-fine-tuning in industrial recommender systems has challenges like scalability, capturing interactions, and handling new items.", "method": "Pretrains a transformer model and fine-tunes it for specific applications, using Deduplicated Cross-Attention Transformer (DCAT).", "result": "DCAT improves throughput by 600%; PinFM increases engagement with new items by 20%.", "conclusion": "PinFM is deployed to improve user experience across applications."}}
{"id": "2507.12695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12695", "abs": "https://arxiv.org/abs/2507.12695", "authors": ["S M Rafiuddin", "Sadia Kamal", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen"], "title": "AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis", "comment": "12 pages (including references), 2 figures (Fig. 1 overview, Fig. 2\n  hyperparameter sensitivity with two subplots), 6 tables (performance,\n  ablation, dataset stats, case studies, etc.), accepted at ASONAM 2025 (Social\n  Network Analysis and Mining)", "summary": "We introduce AdaptiSent, a new framework for Multimodal Aspect-Based\nSentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms\nto improve sentiment classification and aspect term extraction from both text\nand images. Our model integrates dynamic modality weighting and\ncontext-adaptive attention, enhancing the extraction of sentiment and\naspect-related information by focusing on how textual cues and visual context\ninteract. We tested our approach against several baselines, including\ntraditional text-based models and other multimodal methods. Results from\nstandard Twitter datasets show that AdaptiSent surpasses existing models in\nprecision, recall, and F1 score, and is particularly effective in identifying\nnuanced inter-modal relationships that are crucial for accurate sentiment and\naspect term extraction. This effectiveness comes from the model's ability to\nadjust its focus dynamically based on the context's relevance, improving the\ndepth and accuracy of sentiment analysis across various multimodal data sets.\nAdaptiSent sets a new standard for MABSA, significantly outperforming current\nmethods, especially in understanding complex multimodal information.", "AI": {"tldr": "AdaptiSent, a new framework for Multimodal Aspect-Based Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms to improve sentiment classification", "motivation": "improve sentiment classification and aspect term extraction from both text and images", "method": "a new framework for Multimodal Aspect-Based Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms to improve sentiment classification and aspect term extraction from both text and images. Our model integrates dynamic modality weighting and context-adaptive attention", "result": "AdaptiSent surpasses existing models in precision, recall, and F1 score, and is particularly effective in identifying nuanced inter-modal relationships that are crucial for accurate sentiment and aspect term extraction.", "conclusion": "AdaptiSent sets a new standard for MABSA, significantly outperforming current methods, especially in understanding complex multimodal information."}}
{"id": "2507.12691", "categories": ["cs.AI", "cs.LG", "I.2.7; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.12691", "abs": "https://arxiv.org/abs/2507.12691", "authors": ["Avi Parrack", "Carlo Leonardo Attubato", "Stefan Heimersheim"], "title": "Benchmarking Deception Probes via Black-to-White Performance Boosts", "comment": "Preprint. 37 pages, 10 figures, 7 tables", "summary": "AI assistants will occasionally respond deceptively to user queries.\nRecently, linear classifiers (called \"deception probes\") have been trained to\ndistinguish the internal activations of a language model during deceptive\nversus honest responses. However, it's unclear how effective these probes are\nat detecting deception in practice, nor whether such probes are resistant to\nsimple counter strategies from a deceptive assistant who wishes to evade\ndetection. In this paper, we compare white-box monitoring (where the monitor\nhas access to token-level probe activations) to black-box monitoring (without\nsuch access). We benchmark deception probes by the extent to which the white\nbox monitor outperforms the black-box monitor, i.e. the black-to-white\nperformance boost. We find weak but encouraging black-to-white performance\nboosts from existing deception probes.", "AI": {"tldr": "The paper analyzes the effectiveness and robustness of deception probes in detecting deceptive AI responses, finding weak but encouraging results.", "motivation": "AI assistants will occasionally respond deceptively to user queries. It's unclear how effective deception probes are at detecting deception in practice, or whether they are resistant to simple counter strategies.", "method": "Compare white-box monitoring (with token-level probe activations) to black-box monitoring (without such access). Benchmark deception probes by the extent to which the white box monitor outperforms the black-box monitor (black-to-white performance boost).", "result": "Find weak but encouraging black-to-white performance boosts from existing deception probes.", "conclusion": "Existing deception probes show weak but encouraging black-to-white performance boosts."}}
{"id": "2507.12574", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.12574", "abs": "https://arxiv.org/abs/2507.12574", "authors": ["Yifan Deng", "Spencer S. Ericksen", "Anthony Gitter"], "title": "Assay2Mol: large language model-based drug design using BioAssay context", "comment": "23 pages, 10 figures", "summary": "Scientific databases aggregate vast amounts of quantitative data alongside\ndescriptive text. In biochemistry, molecule screening assays evaluate the\nfunctional responses of candidate molecules against disease targets.\nUnstructured text that describes the biological mechanisms through which these\ntargets operate, experimental screening protocols, and other attributes of\nassays offer rich information for new drug discovery campaigns but has been\nuntapped because of that unstructured format. We present Assay2Mol, a large\nlanguage model-based workflow that can capitalize on the vast existing\nbiochemical screening assays for early-stage drug discovery. Assay2Mol\nretrieves existing assay records involving targets similar to the new target\nand generates candidate molecules using in-context learning with the retrieved\nassay screening data. Assay2Mol outperforms recent machine learning approaches\nthat generate candidate ligand molecules for target protein structures, while\nalso promoting more synthesizable molecule generation.", "AI": {"tldr": "Assay2Mol, a large language model, leverages biochemical screening assays for early-stage drug discovery, outperforming existing methods.", "motivation": "Unstructured text in biochemical screening assays offers rich information for new drug discovery but has been untapped.", "method": "A large language model-based workflow that uses in-context learning with retrieved assay screening data.", "result": "Assay2Mol can capitalize on biochemical screening assays for early-stage drug discovery by retrieving existing assay records and generating candidate molecules.", "conclusion": "Assay2Mol outperforms existing machine learning approaches in generating candidate ligand molecules and promotes more synthesizable molecule generation."}}
{"id": "2507.12591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12591", "abs": "https://arxiv.org/abs/2507.12591", "authors": ["Trong-Thang Pham", "Akash Awasthi", "Saba Khan", "Esteban Duran Marti", "Tien-Phat Nguyen", "Khoa Vo", "Minh Tran", "Ngoc Son Nguyen", "Cuong Tran Van", "Yuki Ikebe", "Anh Totti Nguyen", "Anh Nguyen", "Zhigang Deng", "Carol C. Wu", "Hien Van Nguyen", "Ngan Le"], "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling", "comment": "ICCV 2025", "summary": "Understanding radiologists' eye movement during Computed Tomography (CT)\nreading is crucial for developing effective interpretable computer-aided\ndiagnosis systems. However, CT research in this area has been limited by the\nlack of publicly available eye-tracking datasets and the three-dimensional\ncomplexity of CT volumes. To address these challenges, we present the first\npublicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we\nintroduce CT-Searcher, a novel 3D scanpath predictor designed specifically to\nprocess CT volumes and generate radiologist-like 3D fixation sequences,\novercoming the limitations of current scanpath predictors that only handle 2D\ninputs. Since deep learning models benefit from a pretraining step, we develop\na pipeline that converts existing 2D gaze datasets into 3D gaze data to\npretrain CT-Searcher. Through both qualitative and quantitative evaluations on\nCT-ScanGaze, we demonstrate the effectiveness of our approach and provide a\ncomprehensive assessment framework for 3D scanpath prediction in medical\nimaging.", "AI": {"tldr": "This paper introduces CT-ScanGaze, a new CT eye-tracking dataset, and CT-Searcher, a 3D scanpath predictor for CT volumes. The model is pre-trained using converted 2D gaze data and shows promising results.", "motivation": "Understanding radiologists' eye movement during Computed Tomography (CT) reading is crucial for developing effective interpretable computer-aided diagnosis systems. CT research in this area has been limited by the lack of publicly available eye-tracking datasets and the three-dimensional complexity of CT volumes.", "method": "The authors introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to process CT volumes and generate radiologist-like 3D fixation sequences. A pipeline converts existing 2D gaze datasets into 3D gaze data to pretrain CT-Searcher.", "result": "The authors present CT-ScanGaze, the first publicly available eye gaze dataset on CT. Through both qualitative and quantitative evaluations on CT-ScanGaze, the authors demonstrate the effectiveness of their approach.", "conclusion": "The study demonstrates the effectiveness of the proposed CT-Searcher and provides a comprehensive assessment framework for 3D scanpath prediction in medical imaging."}}
{"id": "2507.13105", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13105", "abs": "https://arxiv.org/abs/2507.13105", "authors": ["Marc Brinner", "Sina Zarriess"], "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts", "comment": null, "summary": "We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.", "AI": {"tldr": "SemCSE, an unsupervised method using LLM-generated summaries and contrastive learning, outperforms citation-based methods by capturing semantic content, achieving state-of-the-art results on SciRepEval.", "motivation": "Traditional citation-based approaches do not necessarily reflect semantic similarity.", "method": "An unsupervised method leveraging LLM-generated summaries of scientific abstracts to train a model using contrastive learning.", "result": "SemCSE enforces a stronger semantic separation within the embedding space, validated by a novel benchmark.", "conclusion": "SemCSE achieves state-of-the-art performance on the SciRepEval benchmark, demonstrating the benefits of a semantically focused training approach."}}
{"id": "2507.12705", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12705", "abs": "https://arxiv.org/abs/2507.12705", "authors": ["Potsawee Manakul", "Woody Haosheng Gan", "Michael J. Ryan", "Ali Sartaz Khan", "Warit Sirichotedumrong", "Kunat Pipatanakul", "William Held", "Diyi Yang"], "title": "AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation", "comment": null, "summary": "Current speech evaluation suffers from two critical limitations: the need and\ndifficulty of designing specialized systems targeting individual audio\ncharacteristics, and poor correlation between automatic evaluation methods and\nhuman preferences. This work presents a systematic study of Large Audio Model\n(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified\nevaluation framework that addresses both challenges. We systematically explore\nAudioJudge across audio characteristic detection tasks, including\npronunciation, speaking rate, speaker identification and speech quality, and\nsystem-level human preference simulation for automated benchmarking. We\ninvestigate different prompt engineering strategies, finding that audio\nconcatenation combined with in-context learning significantly improves\nperformance across both audio characteristic detection and human preference\nsimulation tasks. We further introduce a multi-aspect ensemble AudioJudge to\nenable general-purpose multi-aspect audio evaluation. This method decomposes\nspeech assessment into specialized judges for lexical content, speech quality,\nand paralinguistic features, achieving up to 0.91 Spearman correlation with\nhuman preferences on our system ranking benchmark. Robustness analysis reveals\nthat while LAMs maintain strong performance under acoustic noise, they exhibit\nsignificant verbosity and positional biases that require careful mitigation.", "AI": {"tldr": "LAMs can be used for speech evaluation, but prompt engineering is needed to improve performance and address biases.", "motivation": "Current speech evaluation suffers from the need and difficulty of designing specialized systems and poor correlation between automatic evaluation methods and human preferences.", "method": "Audio concatenation combined with in-context learning and multi-aspect ensemble AudioJudge.", "result": "Achieves up to 0.91 Spearman correlation with human preferences on system ranking benchmark. Maintains strong performance under acoustic noise.", "conclusion": "LAMs are effective for speech evaluation but have verbosity and positional biases."}}
{"id": "2507.12801", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12801", "abs": "https://arxiv.org/abs/2507.12801", "authors": ["Sosui Moribe", "Taketoshi Ushiama"], "title": "Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning", "comment": "This is the preprint version of the paper published in IMCOM 2025,\n  IEEE Xplore (DOI: 10.1109/IMCOM64595.2025.10857528)", "summary": "In recent years, peer learning has gained attention as a method that promotes\nspontaneous thinking among learners, and its effectiveness has been confirmed\nby numerous studies. This study aims to develop an AI Agent as a learning\ncompanion that enables peer learning anytime and anywhere. However, peer\nlearning between humans has various limitations, and it is not always\neffective. Effective peer learning requires companions at the same proficiency\nlevels. In this study, we assume that a learner's peers with the same\nproficiency level as the learner make the same mistakes as the learner does and\nfocus on English composition as a specific example to validate this approach.", "AI": {"tldr": "This study develops an AI peer for English composition, assuming similar learners make similar mistakes, to enable effective peer learning anytime and anywhere.", "motivation": "Peer learning promotes spontaneous thinking, but human peer learning has limitations, requiring companions at the same proficiency levels. The study aims to create an AI agent to enable peer learning anytime and anywhere.", "method": "Development of an AI Agent as a learning companion focusing on English composition.", "result": "The study validates the approach.", "conclusion": "This study validates the approach of using AI agents as learning companions in English composition, assuming learners with similar proficiency make similar mistakes."}}
{"id": "2507.12583", "categories": ["cs.LG", "cs.CC", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.12583", "abs": "https://arxiv.org/abs/2507.12583", "authors": ["Ali Fattahi", "Ali Eshragh", "Babak Aslani", "Meysam Rabiee"], "title": "Ranking Vectors Clustering: Theory and Applications", "comment": null, "summary": "We study the problem of clustering ranking vectors, where each vector\nrepresents preferences as an ordered list of distinct integers. Specifically,\nwe focus on the k-centroids ranking vectors clustering problem (KRC), which\naims to partition a set of ranking vectors into k clusters and identify the\ncentroid of each cluster. Unlike classical k-means clustering (KMC), KRC\nconstrains both the observations and centroids to be ranking vectors. We\nestablish the NP-hardness of KRC and characterize its feasible set. For the\nsingle-cluster case, we derive a closed-form analytical solution for the\noptimal centroid, which can be computed in linear time. To address the\ncomputational challenges of KRC, we develop an efficient approximation\nalgorithm, KRCA, which iteratively refines initial solutions from KMC, referred\nto as the baseline solution. Additionally, we introduce a branch-and-bound\n(BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a\ndecision tree framework to reduce computational time while incorporating a\ncontrolling parameter to balance solution quality and efficiency. We establish\ntheoretical error bounds for KRCA and BnB. Through extensive numerical\nexperiments on synthetic and real-world datasets, we demonstrate that KRCA\nconsistently outperforms baseline solutions, delivering significant\nimprovements in solution quality with fast computational times. This work\nhighlights the practical significance of KRC for personalization and\nlarge-scale decision making, offering methodological advancements and insights\nthat can be built upon in future studies.", "AI": {"tldr": "\u7814\u7a76\u4e86\u805a\u7c7b\u6392\u5e8f\u5411\u91cf\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86KRCA\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5feb\u901f\u8ba1\u7b97\u65f6\u95f4\u5185\u663e\u7740\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u805a\u7c7b\u6392\u5e8f\u5411\u91cf\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5411\u91cf\u8868\u793a\u504f\u597d\u4e3a\u4e0d\u540c\u6574\u6570\u7684\u6709\u5e8f\u5217\u8868\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5173\u6ce8 k-\u4e2d\u5fc3\u70b9\u6392\u5e8f\u5411\u91cf\u805a\u7c7b\u95ee\u9898 \uff08KRC\uff09\uff0c\u8be5\u95ee\u9898\u65e8\u5728\u5c06\u4e00\u7ec4\u6392\u5e8f\u5411\u91cf\u5212\u5206\u4e3a k \u4e2a\u96c6\u7fa4\uff0c\u5e76\u786e\u5b9a\u6bcf\u4e2a\u96c6\u7fa4\u7684\u4e2d\u5fc3\u70b9\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fd1\u4f3c\u7b97\u6cd5 KRCA\uff0c\u8be5\u7b97\u6cd5\u8fed\u4ee3\u5730\u6539\u8fdb\u6765\u81ea KMC \u7684\u521d\u59cb\u89e3\uff0c\u79f0\u4e3a\u57fa\u7ebf\u89e3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u652f\u5b9a\u754c \uff08BnB\uff09 \u7b97\u6cd5\uff0c\u7528\u4e8e\u5728 KRCA \u4e2d\u8fdb\u884c\u9ad8\u6548\u7684\u96c6\u7fa4\u91cd\u5efa\uff0c\u5229\u7528\u51b3\u7b56\u6811\u6846\u67b6\u6765\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\uff0c\u540c\u65f6\u7ed3\u5408\u4e00\u4e2a\u63a7\u5236\u53c2\u6570\u6765\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u6548\u7387\u3002", "result": "\u5efa\u7acb\u4e86 KRC \u7684 NP \u96be\u5ea6\u5e76\u8868\u5f81\u4e86\u5176\u53ef\u884c\u96c6\u3002\u5bf9\u4e8e\u5355\u96c6\u7fa4\u60c5\u51b5\uff0c\u6211\u4eec\u63a8\u5bfc\u4e86\u6700\u4f18\u4e2d\u5fc3\u70b9\u7684\u95ed\u5f0f\u89e3\u6790\u89e3\uff0c\u8be5\u89e3\u53ef\u4ee5\u5728\u7ebf\u6027\u65f6\u95f4\u5185\u8ba1\u7b97\u3002\u6211\u4eec\u5efa\u7acb\u4e86 KRCA \u548c BnB \u7684\u7406\u8bba\u8bef\u5dee\u754c\u9650\u3002", "conclusion": "KRCA\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5feb\u901f\u8ba1\u7b97\u65f6\u95f4\u5185\u663e\u7740\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86 KRC \u5728\u4e2a\u6027\u5316\u548c\u5927\u89c4\u6a21\u51b3\u7b56\u4e2d\u7684\u5b9e\u9645\u610f\u4e49\uff0c\u63d0\u4f9b\u4e86\u65b9\u6cd5\u4e0a\u7684\u8fdb\u6b65\u548c\u89c1\u89e3\uff0c\u53ef\u4ee5\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\u52a0\u4ee5\u5229\u7528\u3002"}}
{"id": "2507.12602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12602", "abs": "https://arxiv.org/abs/2507.12602", "authors": ["Said Ohamouddou", "Abdellatif El Afia", "Hanaa El Afia", "Raddouane Chiheb"], "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification", "comment": null, "summary": "Tree species classification from terrestrial LiDAR point clouds is\nchallenging because of the complex multi-scale geometric structures in forest\nenvironments. Existing approaches using multi-scale dynamic graph convolutional\nneural networks (MS-DGCNN) employ parallel multi-scale processing, which fails\nto capture the semantic relationships between the hierarchical levels of the\ntree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion\ndynamic graph convolutional network that uses semantically meaningful feature\nextraction at local, branch, and canopy scales with cross-scale information\npropagation. Our method employs scale-specific feature engineering, including\nstandard geometric features for the local scale, normalized relative vectors\nfor the branch scale, and distance information for the canopy scale. This\nhierarchical approach replaces uniform parallel processing with semantically\ndifferentiated representations that are aligned with the natural tree\nstructure. Under the same proposed tree species data augmentation strategy for\nall experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS,\noutperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On\nFOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to\nMS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN\nand MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on\nModelNet10. With lower parameters and reduced complexity compared to\nstate-of-the-art transformer approaches, our method is suitable for\nresource-constrained applications while maintaining a competitive accuracy.\nBeyond tree classification, the method generalizes to standard 3D object\nrecognition, establishing it as a versatile solution for diverse point cloud\nprocessing applications. The implementation code is publicly available at\nhttps://github.com/said-ohamouddou/MS-DGCNN2.", "AI": {"tldr": "MS-DGCNN++ is a hierarchical multiscale fusion dynamic graph convolutional network that improves tree species classification and generalizes to standard 3D object recognition with competitive accuracy and reduced complexity.", "motivation": "Existing approaches using multi-scale dynamic graph convolutional neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails to capture the semantic relationships between the hierarchical levels of the tree architecture.", "method": "A hierarchical multiscale fusion dynamic graph convolutional network that uses semantically meaningful feature extraction at local, branch, and canopy scales with cross-scale information propagation. It employs scale-specific feature engineering, including standard geometric features for the local scale, normalized relative vectors for the branch scale, and distance information for the canopy scale.", "result": "MS-DGCNN++ achieved an accuracy of 94.96 % on STPCTLS, outperforming DGCNN, MS-DGCNN, and PPT. On FOR-species20K, it achieves 67.25% accuracy (6.1% improvement compared to MS-DGCNN). For standard 3D object recognition, it outperformed DGCNN and MS-DGCNN with overall accuracies of 93.15% on ModelNet40 and 94.05% on ModelNet10.", "conclusion": "MS-DGCNN++ is a versatile solution for diverse point cloud processing applications, generalizing beyond tree classification to standard 3D object recognition."}}
{"id": "2507.13255", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13255", "abs": "https://arxiv.org/abs/2507.13255", "authors": ["Lyucheng Wu", "Mengru Wang", "Ziwen Xu", "Tri Cao", "Nay Oo", "Bryan Hooi", "Shumin Deng"], "title": "Automating Steering for Safe Multimodal Large Language Models", "comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.", "AI": {"tldr": "AutoSteer, a modular intervention technology, improves the safety of MLLMs during inference by reducing the attack success rate on various threats without fine-tuning the model.", "motivation": "To improve the safety of MLLMs during inference, particularly when faced with adversarial multimodal inputs.", "method": "a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS); (2) an adaptive safety prober; and (3) a lightweight Refusal Head.", "result": "AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities.", "conclusion": "AutoSteer is a practical, interpretable, and effective framework for safer deployment of multimodal AI systems."}}
{"id": "2507.12720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12720", "abs": "https://arxiv.org/abs/2507.12720", "authors": ["Abraham Toluase Owodunni", "Orevaoghene Ahia", "Sachin Kumar"], "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models", "comment": null, "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens", "AI": {"tldr": "introduce FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation, consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers.", "motivation": "LMs are challenging to adapt to new data distributions by simple finetuning, due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts.", "method": "develop byte-level LMs with learnable tokenizers to make tokenization adaptive. include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation.", "result": "Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers.", "conclusion": "FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers."}}
{"id": "2507.12806", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12806", "abs": "https://arxiv.org/abs/2507.12806", "authors": ["Zhiwei Liu", "Jielin Qiu", "Shiyu Wang", "Jianguo Zhang", "Zuxin Liu", "Roshan Ram", "Haolin Chen", "Weiran Yao", "Huan Wang", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong"], "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models", "comment": "https://github.com/SalesforceAIResearch/MCPEval", "summary": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation.", "AI": {"tldr": "MCPEval automates end-to-end task generation and deep evaluation of LLM agents across diverse domains, standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines.", "motivation": "The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment.", "method": "an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains", "result": "Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance.", "conclusion": "MCPEval is effective in revealing nuanced, domain-specific performance."}}
{"id": "2507.12584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12584", "abs": "https://arxiv.org/abs/2507.12584", "authors": ["Yinan Li", "Kwang-Sung Jun"], "title": "Second-Order Bounds for [0,1]-Valued Regression via Betting Loss", "comment": null, "summary": "We consider the $[0,1]$-valued regression problem in the i.i.d. setting. In a\nrelated problem called cost-sensitive classification, \\citet{foster21efficient}\nhave shown that the log loss minimizer achieves an improved generalization\nbound compared to that of the squared loss minimizer in the sense that the\nbound scales with the cost of the best classifier, which can be arbitrarily\nsmall depending on the problem at hand. Such a result is often called a\nfirst-order bound. For $[0,1]$-valued regression, we first show that the log\nloss minimizer leads to a similar first-order bound. We then ask if there\nexists a loss function that achieves a variance-dependent bound (also known as\na second order bound), which is a strict improvement upon first-order bounds.\nWe answer this question in the affirmative by proposing a novel loss function\ncalled the betting loss. Our result is ``variance-adaptive'' in the sense that\nthe bound is attained \\textit{without any knowledge about the variance}, which\nis in contrast to modeling label (or reward) variance or the label distribution\nitself explicitly as part of the function class such as distributional\nreinforcement learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86 [0,1] \u503c\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570 betting loss\uff0c\u5b9e\u73b0\u4e86\u65b9\u5dee\u76f8\u5173\u7684\u754c\u9650\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u7684\u4e00\u9636\u754c\u9650\u3002", "motivation": "\u5728 i.i.d. \u8bbe\u7f6e\u4e2d\u8003\u8651 [0,1] \u503c\u56de\u5f52\u95ee\u9898\u3002\u5728\u76f8\u5173\u7684 cost-sensitive \u5206\u7c7b\u95ee\u9898\u4e2d\uff0clog loss \u6700\u5c0f\u5316\u5668\u5b9e\u73b0\u4e86\u6539\u8fdb\u7684\u6cdb\u5316\u754c\u9650\uff0c\u672c\u6587\u7814\u7a76\u662f\u5426\u5b58\u5728\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u53ef\u4ee5\u5b9e\u73b0\u65b9\u5dee\u76f8\u5173\u7684\u754c\u9650\uff08\u4e5f\u79f0\u4e3a\u4e8c\u9636\u754c\u9650\uff09\uff0c\u8fd9\u662f\u5bf9\u4e00\u9636\u754c\u9650\u7684\u4e25\u683c\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u635f\u5931\u51fd\u6570 betting loss\u3002", "result": "log loss \u6700\u5c0f\u5316\u5668\u5bfc\u81f4\u7c7b\u4f3c\u7684\u4e00\u9636\u754c\u9650\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u635f\u5931\u51fd\u6570 betting loss\uff0c\u5b9e\u73b0\u4e86\u65b9\u5dee\u76f8\u5173\u7684\u754c\u9650\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u79f0\u4e3a betting loss\uff0c\u5b9e\u73b0\u4e86\u65b9\u5dee\u76f8\u5173\u7684\u754c\u9650\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u7684\u4e00\u9636\u754c\u9650\u3002"}}
{"id": "2507.12617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12617", "abs": "https://arxiv.org/abs/2507.12617", "authors": ["David Freire-Obreg\u00f3n", "Oliverio J. Santana", "Javier Lorenzo-Navarro", "Daniel Hern\u00e1ndez-Sosa", "Modesto Castrill\u00f3n-Santana"], "title": "Predicting Soccer Penalty Kick Direction Using Human Action Recognition", "comment": "Accepted at 23rd International Conference on Image Analysis and\n  Processing (ICIAP 2025)", "summary": "Action anticipation has become a prominent topic in Human Action Recognition\n(HAR). However, its application to real-world sports scenarios remains limited\nby the availability of suitable annotated datasets. This work presents a novel\ndataset of manually annotated soccer penalty kicks to predict shot direction\nbased on pre-kick player movements. We propose a deep learning classifier to\nbenchmark this dataset that integrates HAR-based feature embeddings with\ncontextual metadata. We evaluate twenty-two backbone models across seven\narchitecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),\nachieving up to 63.9% accuracy in predicting shot direction (left or right),\noutperforming the real goalkeepers' decisions. These results demonstrate the\ndataset's value for anticipatory action recognition and validate our model's\npotential as a generalizable approach for sports-based predictive tasks.", "AI": {"tldr": "A novel dataset of soccer penalty kicks is presented to predict shot direction based on pre-kick player movements. A deep learning classifier is proposed to benchmark this dataset, achieving up to 63.9% accuracy, outperforming real goalkeepers' decisions.", "motivation": "The application of action anticipation to real-world sports scenarios remains limited by the availability of suitable annotated datasets.", "method": "A deep learning classifier is proposed to benchmark this dataset that integrates HAR-based feature embeddings with contextual metadata. Twenty-two backbone models across seven architecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D) are evaluated.", "result": "Up to 63.9% accuracy is achieved in predicting shot direction (left or right), outperforming the real goalkeepers' decisions.", "conclusion": "The dataset's value for anticipatory action recognition is demonstrated, and the model's potential as a generalizable approach for sports-based predictive tasks is validated."}}
{"id": "2507.13275", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13275", "abs": "https://arxiv.org/abs/2507.13275", "authors": ["Luis Gasco", "Hermenegildo Fabregat", "Laura Garc\u00eda-Sardi\u00f1a", "Paula Estrella", "Daniel Deniz", "Alvaro Rodrigo", "Rabih Zbib"], "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management", "comment": null, "summary": "Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.", "AI": {"tldr": "TalentCLEF 2025 \u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u6280\u80fd\u548c\u804c\u4f4d\u540d\u79f0\u667a\u80fd\u7684\u8bc4\u4f30\u6d3b\u52a8\uff0c\u65e8\u5728\u586b\u8865\u8be5\u9886\u57df\u516c\u5171\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u5e76\u4fc3\u8fdb\u52b3\u52a8\u529b\u5e02\u573a\u4e2d\u7a33\u5065\u3001\u516c\u5e73\u7684\u8bed\u8a00\u6280\u672f\u53d1\u5c55\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u6b63\u5728\u63a8\u52a8\u4eba\u529b\u8d44\u672c\u7ba1\u7406\u9886\u57df\u7684\u91cd\u5927\u8f6c\u578b\uff0c\u4f46\u8fd9\u4e9b\u6280\u672f\u7684\u91c7\u7528\u548c\u8fdb\u6b65\u5173\u952e\u53d6\u51b3\u4e8e\u53ef\u9760\u548c\u516c\u5e73\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u4ee5\u53ca\u5728\u516c\u5171\u6570\u636e\u548c\u5f00\u653e\u57fa\u51c6\u4e0a\u7684\u9002\u5f53\u8bc4\u4f30\uff0c\u800c\u8fd9\u4e9b\u5728\u76ee\u524d\u4e3a\u6b62\u5728\u8be5\u9886\u57df\u5c1a\u4e0d\u53ef\u7528\u3002", "method": "\u4fe1\u606f\u68c0\u7d22\u6280\u672f\uff0c\u4f7f\u7528\u57fa\u4e8e\u591a\u8bed\u8a00\u7f16\u7801\u5668\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u6216\u91cd\u6392\u5e8f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8bad\u7ec3\u7b56\u7565\u6bd4\u6a21\u578b\u672c\u8eab\u7684\u5927\u5c0f\u5177\u6709\u66f4\u5927\u7684\u5f71\u54cd\u3002TalentCLEF\u5438\u5f15\u4e8676\u4e2a\u6ce8\u518c\u56e2\u961f\uff0c\u63d0\u4ea4\u4e86\u8d85\u8fc7280\u4efd\u65b9\u6848\u3002", "conclusion": "TalentCLEF\u4f5c\u4e3a\u9996\u4e2a\u516c\u5f00\u57fa\u51c6\uff0c\u9f13\u52b1\u4e3a\u52b3\u52a8\u529b\u5e02\u573a\u5f00\u53d1\u7a33\u5065\u3001\u516c\u5e73\u548c\u53ef\u8f6c\u79fb\u7684\u8bed\u8a00\u6280\u672f\u3002"}}
{"id": "2507.12724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12724", "abs": "https://arxiv.org/abs/2507.12724", "authors": ["Richard Sproat", "Tianyu Zhao", "Llion Jones"], "title": "TransEvalnia: Reasoning-based Evaluation and Ranking of Translations", "comment": null, "summary": "We present TransEvalnia, a prompting-based translation evaluation and ranking\nsystem that uses reasoning in performing its evaluations and ranking. This\nsystem presents fine-grained evaluations based on a subset of the\nMultidimensional Quality Metrics (https://themqm.org/), returns an assessment\nof which translation it deems the best, and provides numerical scores for the\nvarious dimensions and for the overall translation. We show that TransEvalnia\nperforms as well as or better than the state-of-the-art MT-Ranker (Moosa et al.\n2024) on our own English-Japanese data as well as several language pairs from\nvarious WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and\nQwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations\nreturned are deemed highly acceptable to human raters, and that the scores\nassigned to the translations by Sonnet, as well as other LLMs, correlate well\nwith scores assigned by the human raters. We also note the sensitivity of our\nsystem -- as well as MT-Ranker -- to the order in which the translations are\npresented, and we propose methods to address this position bias. All data,\nincluding the system's evaluation and reasoning, human assessments, as well as\ncode is released.", "AI": {"tldr": "TransEvalnia \u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u7ffb\u8bd1\u8bc4\u4f30\u548c\u6392\u5e8f\u7cfb\u7edf\uff0c\u5176\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684 MT-Ranker \u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5e76\u4e14\u5176\u8bc4\u4f30\u7ed3\u679c\u5bf9\u4eba\u7c7b\u8bc4\u4f30\u8005\u6765\u8bf4\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\u3002", "motivation": "\u8bba\u6587\u63d0\u51fa\u4e86 TransEvalnia\uff0c\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u7ffb\u8bd1\u8bc4\u4f30\u548c\u6392\u5e8f\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u7ffb\u8bd1\u8bc4\u4f30\u548c\u6392\u5e8f\u95ee\u9898\u3002", "method": "TransEvalnia \u662f\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u7ffb\u8bd1\u8bc4\u4f30\u548c\u6392\u5e8f\u7cfb\u7edf\uff0c\u5b83\u4f7f\u7528\u63a8\u7406\u6765\u8fdb\u884c\u8bc4\u4f30\u548c\u6392\u5e8f\u3002\u8be5\u7cfb\u7edf\u57fa\u4e8e\u591a\u7ef4\u8d28\u91cf\u6307\u6807\u7684\u4e00\u4e2a\u5b50\u96c6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u3002", "result": "TransEvalnia \u7684\u8bc4\u4f30\u7ed3\u679c\u88ab\u8ba4\u4e3a\u5bf9\u4eba\u7c7b\u8bc4\u4f30\u8005\u6765\u8bf4\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\uff0c\u5e76\u4e14 Sonnet \u4ee5\u53ca\u5176\u4ed6 LLM \u7ed9\u7ffb\u8bd1\u7684\u5206\u6570\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u7ed9\u51fa\u7684\u5206\u6570\u76f8\u5173\u6027\u5f88\u597d\u3002", "conclusion": "TransEvalnia \u7684\u8868\u73b0\u4e0e\u6700\u5148\u8fdb\u7684 MT-Ranker \u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5e76\u4e14\u5176\u8bc4\u4f30\u7ed3\u679c\u5bf9\u4eba\u7c7b\u8bc4\u4f30\u8005\u6765\u8bf4\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\uff0c\u5e76\u4e14\u5206\u6570\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u7ed9\u51fa\u7684\u5206\u6570\u76f8\u5173\u6027\u5f88\u597d\u3002\u8be5\u7cfb\u7edf\u5bf9\u7ffb\u8bd1\u7684\u5448\u73b0\u987a\u5e8f\u654f\u611f\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u79cd\u4f4d\u7f6e\u504f\u5dee\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.12820", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12820", "abs": "https://arxiv.org/abs/2507.12820", "authors": ["Shiquan Wang", "Ruiyu Fang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "Emotional Support with LLM-based Empathetic Dialogue Generation", "comment": null, "summary": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.", "AI": {"tldr": "This paper explores using LLMs with prompt engineering and finetuning for emotional support conversations, achieving second place in a competition.", "motivation": "Addresses the growing demand for mental health support through dialogue by providing empathetic and effective emotional assistance.", "method": "The authors leverage large-scale language models enhanced by prompt engineering and finetuning techniques, exploring both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning.", "result": "The study highlights the potential of combining LLMs with effective adaptation methods for ESC tasks.", "conclusion": "The paper's best model ranked second in the NLPCC 2025 Task 8 ESC competition, demonstrating the potential of combining LLMs with adaptation methods for ESC tasks."}}
{"id": "2507.12604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12604", "abs": "https://arxiv.org/abs/2507.12604", "authors": ["Antoni Zajko", "Katarzyna Wo\u017anica"], "title": "Are encoders able to learn landmarkers for warm-starting of Hyperparameter Optimization?", "comment": null, "summary": "Effectively representing heterogeneous tabular datasets for meta-learning\npurposes is still an open problem. Previous approaches rely on representations\nthat are intended to be universal. This paper proposes two novel methods for\ntabular representation learning tailored to a specific meta-task -\nwarm-starting Bayesian Hyperparameter Optimization. Both follow the specific\nrequirement formulated by ourselves that enforces representations to capture\nthe properties of landmarkers. The first approach involves deep metric\nlearning, while the second one is based on landmarkers reconstruction. We\nevaluate the proposed encoders in two ways. Next to the gain in the target\nmeta-task, we also use the degree of fulfillment of the proposed requirement as\nthe evaluation metric. Experiments demonstrate that while the proposed encoders\ncan effectively learn representations aligned with landmarkers, they may not\ndirectly translate to significant performance gains in the meta-task of HPO\nwarm-starting.", "AI": {"tldr": "This paper introduces two new tabular representation learning methods for Bayesian Hyperparameter Optimization warm-starting, focusing on capturing landmarker properties. While the encoders align with landmarkers, they don't guarantee significant HPO performance gains.", "motivation": "Effectively representing heterogeneous tabular datasets for meta-learning purposes is still an open problem. Previous approaches rely on representations that are intended to be universal.", "method": "This paper proposes two novel methods for tabular representation learning tailored to a specific meta-task - warm-starting Bayesian Hyperparameter Optimization. The first approach involves deep metric learning, while the second one is based on landmarkers reconstruction.", "result": "Experiments demonstrate that the proposed encoders can effectively learn representations aligned with landmarkers.", "conclusion": "The proposed encoders effectively learn representations aligned with landmarkers, but may not directly translate to significant performance gains in the meta-task of HPO warm-starting."}}
{"id": "2507.12628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12628", "abs": "https://arxiv.org/abs/2507.12628", "authors": ["Sandipan Sarma", "Agney Talwarr", "Arijit Sur"], "title": "Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection", "comment": "10 pages, 6 figures", "summary": "Human-object interaction detection (HOID) refers to localizing interactive\nhuman-object pairs in images and identifying the interactions. Since there\ncould be an exponential number of object-action combinations, labeled data is\nlimited - leading to a long-tail distribution problem. Recently, zero-shot\nlearning emerged as a solution, with end-to-end transformer-based object\ndetectors adapted for HOID becoming successful frameworks. However, their\nprimary focus is designing improved decoders for learning entangled or\ndisentangled interpretations of interactions. We advocate that HOI-specific\ncues must be anticipated at the encoder stage itself to obtain a stronger scene\ninterpretation. Consequently, we build a top-down framework named Funnel-HOI\ninspired by the human tendency to grasp well-defined concepts first and then\nassociate them with abstract concepts during scene understanding. We first\nprobe an image for the presence of objects (well-defined concepts) and then\nprobe for actions (abstract concepts) associated with them. A novel asymmetric\nco-attention mechanism mines these cues utilizing multimodal information\n(incorporating zero-shot capabilities) and yields stronger interaction\nrepresentations at the encoder level. Furthermore, a novel loss is devised that\nconsiders objectaction relatedness and regulates misclassification penalty\nbetter than existing loss functions for guiding the interaction classifier.\nExtensive experiments on the HICO-DET and V-COCO datasets across\nfully-supervised and six zero-shot settings reveal our state-of-the-art\nperformance, with up to 12.4% and 8.4% gains for unseen and rare HOI\ncategories, respectively.", "AI": {"tldr": "This paper introduces Funnel-HOI, a new framework for human-object interaction detection that uses a top-down approach and a novel co-attention mechanism to improve scene interpretation at the encoder level. It achieves state-of-the-art results, especially in zero-shot settings.", "motivation": "Since there could be an exponential number of object-action combinations, labeled data is limited - leading to a long-tail distribution problem. Recently, zero-shot learning emerged as a solution, with end-to-end transformer-based object detectors adapted for HOID becoming successful frameworks. However, their primary focus is designing improved decoders for learning entangled or disentangled interpretations of interactions. We advocate that HOI-specific cues must be anticipated at the encoder stage itself to obtain a stronger scene interpretation.", "method": "a top-down framework named Funnel-HOI inspired by the human tendency to grasp well-defined concepts first and then associate them with abstract concepts during scene understanding. We first probe an image for the presence of objects (well-defined concepts) and then probe for actions (abstract concepts) associated with them. A novel asymmetric co-attention mechanism mines these cues utilizing multimodal information (incorporating zero-shot capabilities) and yields stronger interaction representations at the encoder level. Furthermore, a novel loss is devised that considers objectaction relatedness and regulates misclassification penalty better than existing loss functions for guiding the interaction classifier.", "result": "state-of-the-art performance, with up to 12.4% and 8.4% gains for unseen and rare HOI categories, respectively.", "conclusion": "Extensive experiments on the HICO-DET and V-COCO datasets across fully-supervised and six zero-shot settings reveal our state-of-the-art performance, with up to 12.4% and 8.4% gains for unseen and rare HOI categories, respectively."}}
{"id": "2507.12732", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12732", "abs": "https://arxiv.org/abs/2507.12732", "authors": ["Fuya Nakamori", "Yin Jou Huang", "Fei Cheng"], "title": "Strategy Adaptation in Large Language Model Werewolf Agents", "comment": "7 pages, 2 figures", "summary": "This study proposes a method to improve the performance of Werewolf agents by\nswitching between predefined strategies based on the attitudes of other players\nand the context of conversations. While prior works of Werewolf agents using\nprompt engineering have employed methods where effective strategies are\nimplicitly defined, they cannot adapt to changing situations. In this research,\nwe propose a method that explicitly selects an appropriate strategy based on\nthe game context and the estimated roles of other players. We compare the\nstrategy adaptation Werewolf agents with baseline agents using implicit or\nfixed strategies and verify the effectiveness of our proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6839\u636e\u6e38\u620f\u73af\u5883\u548c\u4f30\u8ba1\u7684\u5176\u4ed6\u73a9\u5bb6\u89d2\u8272\u663e\u5f0f\u9009\u62e9\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u72fc\u4eba\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u7684\u72fc\u4eba\u4ee3\u7406\u65e0\u6cd5\u9002\u5e94\u53d8\u5316\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5176\u4ed6\u73a9\u5bb6\u6001\u5ea6\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5728\u9884\u5b9a\u4e49\u7b56\u7565\u4e4b\u95f4\u5207\u6362\u7684\u65b9\u6cd5\u3002", "result": "\u7b56\u7565\u8c03\u6574\u72fc\u4eba\u4ee3\u7406\u4f18\u4e8e\u4f7f\u7528\u9690\u5f0f\u6216\u56fa\u5b9a\u7b56\u7565\u7684\u57fa\u7ebf\u4ee3\u7406\u3002", "conclusion": "\u901a\u8fc7\u7b56\u7565\u8c03\u6574\uff0c\u72fc\u4eba\u4ee3\u7406\u7684\u8868\u73b0\u5f97\u5230\u4e86\u63d0\u5347\u3002"}}
{"id": "2507.12821", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12821", "abs": "https://arxiv.org/abs/2507.12821", "authors": ["Lance Ying", "Katherine M. Collins", "Prafull Sharma", "Cedric Colas", "Kaiya Ivy Zhao", "Adrian Weller", "Zenna Tavares", "Phillip Isola", "Samuel J. Gershman", "Jacob D. Andreas", "Thomas L. Griffiths", "Francois Chollet", "Kelsey R. Allen", "Joshua B. Tenenbaum"], "title": "Assessing adaptive world models in machines with novel games", "comment": "17 pages, 4 figures", "summary": "Human intelligence exhibits a remarkable capacity for rapid adaptation and\neffective problem-solving in novel and unfamiliar contexts. We argue that this\nprofound adaptability is fundamentally linked to the efficient construction and\nrefinement of internal representations of the environment, commonly referred to\nas world models, and we refer to this adaptation mechanism as world model\ninduction. However, current understanding and evaluation of world models in\nartificial intelligence (AI) remains narrow, often focusing on static\nrepresentations learned from training on a massive corpora of data, instead of\nthe efficiency and efficacy of models in learning these representations through\ninteraction and exploration within a novel environment. In this Perspective, we\nprovide a view of world model induction drawing on decades of research in\ncognitive science on how humans learn and adapt so efficiently; we then call\nfor a new evaluation framework for assessing adaptive world models in AI.\nConcretely, we propose a new benchmarking paradigm based on suites of carefully\ndesigned games with genuine, deep and continually refreshing novelty in the\nunderlying game structures -- we refer to this kind of games as novel games. We\ndetail key desiderata for constructing these games and propose appropriate\nmetrics to explicitly challenge and evaluate the agent's ability for rapid\nworld model induction. We hope that this new evaluation framework will inspire\nfuture evaluation efforts on world models in AI and provide a crucial step\ntowards developing AI systems capable of the human-like rapid adaptation and\nrobust generalization -- a critical component of artificial general\nintelligence.", "AI": {"tldr": "The paper proposes a new evaluation framework using 'novel games' to test AI's ability to quickly learn and adapt world models, mimicking human intelligence.", "motivation": "Current AI world models lack the rapid adaptation seen in human intelligence, which relies on efficient internal representations (world models).", "method": "The paper proposes a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures.", "result": "The paper details key desiderata for constructing these games and proposes appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction.", "conclusion": "This paper advocates for a new evaluation framework for AI world models, using novel games to assess their ability for rapid world model induction, aiming for human-like adaptation and generalization."}}
