{"id": "2509.03661", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03661", "abs": "https://arxiv.org/abs/2509.03661", "authors": ["Daryl Chang", "Yi Wu", "Jennifer She", "Li Wei", "Lukasz Heldt"], "title": "ACT: Automated Constraint Targeting for Multi-Objective Recommender Systems", "comment": null, "summary": "Recommender systems often must maximize a primary objective while ensuring\nsecondary ones satisfy minimum thresholds, or \"guardrails.\" This is critical\nfor maintaining a consistent user experience and platform ecosystem, but\nenforcing these guardrails despite orthogonal system changes is challenging and\noften requires manual hyperparameter tuning. We introduce the Automated\nConstraint Targeting (ACT) framework, which automatically finds the minimal set\nof hyperparameter changes needed to satisfy these guardrails. ACT uses an\noffline pairwise evaluation on unbiased data to find solutions and continuously\nretrains to adapt to system and user behavior changes. We empirically\ndemonstrate its efficacy and describe its deployment in a large-scale\nproduction environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u52a8\u5316\u7ea6\u675f\u76ee\u6807\uff08ACT\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u81ea\u52a8\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u4ee5\u6ee1\u8db3\u6b21\u8981\u76ee\u6807\uff08\u201c\u62a4\u680f\u201d\uff09\u7684\u6700\u4f4e\u9608\u503c\u3002", "motivation": "\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u9700\u8981\u5728\u6700\u5927\u5316\u4e3b\u8981\u76ee\u6807\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u6b21\u8981\u76ee\u6807\u6ee1\u8db3\u6700\u4f4e\u9608\u503c\uff0c\u4f46\u6b63\u4ea4\u7cfb\u7edf\u53d8\u66f4\u4f7f\u5f97\u6267\u884c\u8fd9\u4e9b\u62a4\u680f\u5177\u6709\u6311\u6218\u6027\uff0c\u901a\u5e38\u9700\u8981\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u3002", "method": "ACT\u6846\u67b6\u4f7f\u7528\u79bb\u7ebf\u6210\u5bf9\u8bc4\u4f30\u6765\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6301\u7eed\u91cd\u65b0\u8bad\u7ec3\u4ee5\u9002\u5e94\u7cfb\u7edf\u548c\u7528\u6237\u884c\u4e3a\u7684\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86ACT\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u63cf\u8ff0\u4e86\u5176\u5728\u5927\u578b\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "conclusion": "ACT\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u627e\u5230\u6ee1\u8db3\u62a4\u680f\u6240\u9700\u7684\u6700\u5c0f\u8d85\u53c2\u6570\u66f4\u6539\u96c6\u3002"}}
{"id": "2509.03692", "categories": ["cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03692", "abs": "https://arxiv.org/abs/2509.03692", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann"], "title": "lifeXplore at the Lifelog Search Challenge 2021", "comment": null, "summary": "Since its first iteration in 2018, the Lifelog Search Challenge (LSC)\ncontinues to rise in popularity as an interactive lifelog data retrieval\ncompetition, co-located at the ACM International Conference on Multimedia\nRetrieval (ICMR). The goal of this annual live event is to search a large\ncorpus of lifelogging data for specifically announced memories using a\npurposefully developed tool within a limited amount of time. As long-standing\nparticipants, we present our improved lifeXplore - a retrieval system combining\nchronologic day summary browsing with interactive combinable concept filtering.\nCompared to previous versions, the tool is improved by incorporating temporal\nqueries, advanced day summary features as well as usability improvements.", "AI": {"tldr": "\u751f\u547d\u8bb0\u5f55\u641c\u7d22\u6311\u6218\u8d5b (LSC) \u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u751f\u547d\u8bb0\u5f55\u6570\u636e\u68c0\u7d22\u7ade\u8d5b\uff0c\u4e0e ACM \u56fd\u9645\u591a\u5a92\u4f53\u68c0\u7d22\u4f1a\u8bae (ICMR) \u540c\u5730\u4e3e\u529e\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a lifeXplore \u7684\u68c0\u7d22\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u6309\u65f6\u95f4\u987a\u5e8f\u6392\u5217\u7684\u6bcf\u65e5\u6458\u8981\u6d4f\u89c8\u548c\u4ea4\u4e92\u5f0f\u53ef\u7ec4\u5408\u6982\u5ff5\u8fc7\u6ee4\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u67e5\u8be2\u3001\u9ad8\u7ea7\u6bcf\u65e5\u6458\u8981\u529f\u80fd\u4ee5\u53ca\u53ef\u7528\u6027\u6539\u8fdb\u8fdb\u884c\u4e86\u6539\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u5728\u751f\u547d\u8bb0\u5f55\u641c\u7d22\u6311\u6218\u8d5b\u4e2d\u68c0\u7d22\u751f\u547d\u8bb0\u5f55\u6570\u636e\u3002", "method": "\u7ed3\u5408\u4e86\u6309\u65f6\u95f4\u987a\u5e8f\u6392\u5217\u7684\u6bcf\u65e5\u6458\u8981\u6d4f\u89c8\u548c\u4ea4\u4e92\u5f0f\u53ef\u7ec4\u5408\u6982\u5ff5\u8fc7\u6ee4\u7684 lifeXplore \u68c0\u7d22\u7cfb\u7edf\u3002", "result": "\u8be5\u5de5\u5177\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u67e5\u8be2\u3001\u9ad8\u7ea7\u6bcf\u65e5\u6458\u8981\u529f\u80fd\u4ee5\u53ca\u53ef\u7528\u6027\u6539\u8fdb\u5f97\u5230\u4e86\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6539\u8fdb\u7684 lifeXplore \u68c0\u7d22\u7cfb\u7edf\u3002"}}
{"id": "2509.03696", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.03696", "abs": "https://arxiv.org/abs/2509.03696", "authors": ["Aleksandr V. Petrov", "Michael Murtagh", "Karthik Nagesh"], "title": "LLMs for estimating positional bias in logged interaction data", "comment": "Accepted at the CONSEQUENCES Workshop @ RecSys'25", "summary": "Recommender and search systems commonly rely on Learning To Rank models\ntrained on logged user interactions to order items by predicted relevance.\nHowever, such interaction data is often subject to position bias, as users are\nmore likely to click on items that appear higher in the ranking, regardless of\ntheir actual relevance. As a result, newly trained models may inherit and\nreinforce the biases of prior ranking models rather than genuinely improving\nrelevance. A standard approach to mitigate position bias is Inverse Propensity\nScoring (IPS), where the model's loss is weighted by the inverse of a\npropensity function, an estimate of the probability that an item at a given\nposition is examined. However, accurate propensity estimation is challenging,\nespecially in interfaces with complex non-linear layouts. In this paper, we\npropose a novel method for estimating position bias using Large Language Models\n(LLMs) applied to logged user interaction data. This approach offers a\ncost-effective alternative to online experimentation. Our experiments show that\npropensities estimated with our LLM-as-a-judge approach are stable across score\nbuckets and reveal the row-column effects of Viator's grid layout that simpler\nheuristics overlook. An IPS-weighted reranker trained with these propensities\nmatches the production model on standard NDCG@10 while improving weighted\nNDCG@10 by roughly 2%. We will verify these offline gains in forthcoming\nlive-traffic experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u4f30\u8ba1\u4f4d\u7f6e\u504f\u5dee\uff0c\u4ee5\u89e3\u51b3\u63a8\u8350\u548c\u641c\u7d22\u7cfb\u7edf\u4e2d\u5b66\u4e60\u6392\u5e8f\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u4f4d\u7f6e\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u7528\u6237\u66f4\u6709\u53ef\u80fd\u70b9\u51fb\u6392\u540d\u8f83\u9ad8\u7684\u9879\u76ee\uff0c\u800c\u4e0d\u7ba1\u5b83\u4eec\u7684\u5b9e\u9645\u76f8\u5173\u6027\u3002\u8fd9\u5bfc\u81f4\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u80fd\u7ee7\u627f\u5e76\u52a0\u5f3a\u5148\u524d\u6392\u540d\u6a21\u578b\u7684\u504f\u5dee\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u63d0\u9ad8\u76f8\u5173\u6027\u3002\u51c6\u786e\u7684\u503e\u5411\u4f30\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u590d\u6742\u975e\u7ebf\u6027\u5e03\u5c40\u7684\u754c\u9762\u4e2d\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u8bb0\u5f55\u7684\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u7528\u4e8e\u4f30\u8ba1\u4f4d\u7f6e\u504f\u5dee\u3002", "result": "\u4f7f\u7528LLM\u4f30\u8ba1\u7684\u503e\u5411\u5728\u5206\u6570\u6876\u4e2d\u662f\u7a33\u5b9a\u7684\uff0c\u5e76\u63ed\u793a\u4e86Viator\u7f51\u683c\u5e03\u5c40\u7684\u884c-\u5217\u6548\u5e94\uff0c\u800c\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5ffd\u7565\u4e86\u8fd9\u4e9b\u6548\u5e94\u3002\u4f7f\u7528\u8fd9\u4e9b\u503e\u5411\u8bad\u7ec3\u7684IPS\u52a0\u6743\u91cd\u6392\u5e8f\u5668\u5728\u6807\u51c6NDCG@10\u4e0a\u4e0e\u751f\u4ea7\u6a21\u578b\u5339\u914d\uff0c\u540c\u65f6\u5c06\u52a0\u6743NDCG@10\u63d0\u9ad8\u4e86\u5927\u7ea62%\u3002", "conclusion": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u4f4d\u7f6e\u504f\u5dee\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03746", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.03746", "abs": "https://arxiv.org/abs/2509.03746", "authors": ["Anushya Subbiah", "Vikram Aggarwal", "James Pine", "Steffen Rendle", "Krishna Sayana", "Kun Su"], "title": "Efficient Item ID Generation for Large-Scale LLM-based Recommendation", "comment": null, "summary": "Integrating product catalogs and user behavior into LLMs can enhance\nrecommendations with broad world knowledge, but the scale of real-world item\ncatalogs, often containing millions of discrete item identifiers (Item IDs),\nposes a significant challenge. This contrasts with the smaller, tokenized text\nvocabularies typically used in LLMs. The predominant view within the LLM-based\nrecommendation literature is that it is infeasible to treat item ids as a first\nclass citizen in the LLM and instead some sort of tokenization of an item into\nmultiple tokens is required. However, this creates a key practical bottleneck\nin serving these models for real-time low-latency applications.\n  Our paper challenges this predominant practice and integrates item ids as\nfirst class citizens into the LLM. We provide simple, yet highly effective,\nnovel training and inference modifications that enable single-token\nrepresentations of items and single-step decoding. Our method shows\nimprovements in recommendation quality (Recall and NDCG) over existing\ntechniques on the Amazon shopping datasets while significantly improving\ninference efficiency by 5x-14x. Our work offers an efficiency perspective\ndistinct from that of other popular approaches within LLM-based recommendation,\npotentially inspiring further research and opening up a new direction for\nintegrating IDs into LLMs. Our code is available here\nhttps://drive.google.com/file/d/1cUMj37rV0Z1bCWMdhQ6i4q4eTRQLURtC", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5c06\u5546\u54c1ID\u4f5c\u4e3aLLM\u4e2d\u7684\u4e00\u7b49\u516c\u6c11\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684LLM\u63a8\u8350\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u5546\u54c1\u76ee\u5f55\uff0c\u901a\u5e38\u9700\u8981\u5c06\u5546\u54c1tokenize\uff0c\u8fd9\u5bfc\u81f4\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u74f6\u9888\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u4fee\u6539\u65b9\u6cd5\uff0c\u5141\u8bb8\u5546\u54c1\u4f7f\u7528\u5355token\u8868\u793a\uff0c\u5e76\u8fdb\u884c\u5355\u6b65\u89e3\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Amazon\u8d2d\u7269\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u63a8\u8350\u8d28\u91cf\uff08\u53ec\u56de\u7387\u548cNDCG\uff09\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e865-14\u500d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0d\u540c\u4e8e\u5176\u4ed6LLM\u63a8\u8350\u65b9\u6cd5\u7684\u6548\u7387\u89c6\u89d2\uff0c\u53ef\u80fd\u6fc0\u53d1\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u5e76\u4e3a\u5c06ID\u96c6\u6210\u5230LLM\u4e2d\u5f00\u8f9f\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.03525", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03525", "abs": "https://arxiv.org/abs/2509.03525", "authors": ["Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sepehr Karimi", "Sina Rashidi", "Ali Zolnour", "Maryam Dadkhah", "Yasaman Haghbin", "Hossein AzadMaleki", "Maryam Zolnoori"], "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies", "comment": null, "summary": "Over half of US adults with Alzheimer disease and related dementias remain\nundiagnosed, and speech-based screening offers a scalable detection approach.\nWe compared large language model adaptation strategies for dementia detection\nusing the DementiaBank speech corpus, evaluating nine text-only models and\nthree multimodal audio-text models on recordings from DementiaBank speech\ncorpus. Adaptations included in-context learning with different demonstration\nselection policies, reasoning-augmented prompting, parameter-efficient\nfine-tuning, and multimodal integration. Results showed that class-centroid\ndemonstrations achieved the highest in-context learning performance, reasoning\nimproved smaller models, and token-level fine-tuning generally produced the\nbest scores. Adding a classification head substantially improved\nunderperforming models. Among multimodal models, fine-tuned audio-text systems\nperformed well but did not surpass the top text-only models. These findings\nhighlight that model adaptation strategies, including demonstration selection,\nreasoning design, and tuning method, critically influence speech-based dementia\ndetection, and that properly adapted open-weight models can match or exceed\ncommercial systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u8bed\u97f3\u7684\u75f4\u5446\u75c7\u7b5b\u67e5\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u79cd\u9002\u5e94\u7b56\u7565\uff0c\u5728DementiaBank\u8bed\u97f3\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u65e8\u5728\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7f8e\u56fd\u8d85\u8fc7\u4e00\u534a\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u76f8\u5173\u75f4\u5446\u75c7\u60a3\u8005\u672a\u88ab\u8bca\u65ad\uff0c\u800c\u57fa\u4e8e\u8bed\u97f3\u7684\u7b5b\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e5d\u4e2a\u7eaf\u6587\u672c\u6a21\u578b\u548c\u4e09\u4e2a\u591a\u6a21\u6001\u97f3\u9891-\u6587\u672c\u6a21\u578b\uff0c\u91c7\u7528\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u63a8\u7406\u589e\u5f3a\u63d0\u793a\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u591a\u6a21\u6001\u96c6\u6210\u7b49\u9002\u5e94\u7b56\u7565\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7c7b\u4e2d\u5fc3\u6f14\u793a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\uff0c\u63a8\u7406\u6539\u8fdb\u4e86\u8f83\u5c0f\u7684\u6a21\u578b\uff0ctoken\u7ea7\u5fae\u8c03\u901a\u5e38\u4ea7\u751f\u6700\u4f73\u5206\u6570\u3002\u6dfb\u52a0\u5206\u7c7b\u5934\u663e\u7740\u6539\u5584\u4e86\u8868\u73b0\u4e0d\u4f73\u7684\u6a21\u578b\u3002\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u8d85\u8fc7\u6700\u4f73\u7eaf\u6587\u672c\u6a21\u578b\u3002", "conclusion": "\u6a21\u578b\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ec\u6f14\u793a\u9009\u62e9\u3001\u63a8\u7406\u8bbe\u8ba1\u548c\u8c03\u6574\u65b9\u6cd5\uff0c\u5bf9\u57fa\u4e8e\u8bed\u97f3\u7684\u75f4\u5446\u75c7\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u9002\u5f53\u8c03\u6574\u7684\u5f00\u653e\u6743\u91cd\u6a21\u578b\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8fc7\u5546\u4e1a\u7cfb\u7edf\u3002"}}
{"id": "2509.03609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03609", "abs": "https://arxiv.org/abs/2509.03609", "authors": ["Shengkai Sun", "Zefan Zhang", "Jianfeng Dong", "Zhiyong Cheng", "Xiaojun Chang", "Meng Wang"], "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling", "comment": "Accepted by ICCV 2025", "summary": "Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u901a\u7528\u7279\u5f81\u9884\u6d4b\u6846\u67b6\uff08GFP\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u63a9\u853d\u9aa8\u9abc\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u91cd\u5efa\u76ee\u6807\u9650\u5236\u4e3a\u539f\u59cb\u5173\u8282\u5750\u6807\u6216\u5176\u7b80\u5355\u53d8\u4f53\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u6709\u9650\u7684\u8bed\u4e49\u8868\u793a\u3002", "method": "\u7528\u9ad8\u7ea7\u7279\u5f81\u9884\u6d4b\u4ee3\u66ff\u4f20\u7edf\u7684\u4f4e\u7ea7\u91cd\u5efa\uff0c\u5f15\u5165\u4e00\u4e2a\u534f\u540c\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u52a8\u6001\u751f\u6210\u8de8\u65f6\u7a7a\u5c42\u6b21\u7684\u591a\u6837\u5316\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u7ea6\u675f\u4f18\u5316\u4ee5\u786e\u4fdd\u7279\u5f81\u591a\u6837\u6027\u3002", "result": "\u5728NTU RGB+D 60\u3001NTU RGB+D 120\u548cPKU-MMD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u8ba1\u7b97\u6548\u7387\uff08\u6bd4\u6807\u51c6\u7684\u63a9\u853d\u9aa8\u9abc\u5efa\u6a21\u65b9\u6cd5\u5feb6.2\u500d\uff09\u548c\u5353\u8d8a\u7684\u8868\u793a\u8d28\u91cf\uff0c\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u901a\u7528\u7279\u5f81\u9884\u6d4b\u6846\u67b6\u80fd\u591f\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8d28\u91cf\uff0c\u5e76\u5728\u9aa8\u9abc\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03536", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03536", "abs": "https://arxiv.org/abs/2509.03536", "authors": ["Weizhi Chen", "Ziwei Wang", "Leyang Yang", "Sheng Zhou", "Xiaoxuan Tang", "Jiajun Bu", "Yong Li", "Wei Jiang"], "title": "PG-Agent: An Agent Powered by Page Graph", "comment": "Paper accepted to ACM MM 2025", "summary": "Graphical User Interface (GUI) agents possess significant commercial and\nsocial value, and GUI agents powered by advanced multimodal large language\nmodels (MLLMs) have demonstrated remarkable potential. Currently, existing GUI\nagents usually utilize sequential episodes of multi-step operations across\npages as the prior GUI knowledge, which fails to capture the complex transition\nrelationship between pages, making it challenging for the agents to deeply\nperceive the GUI environment and generalize to new scenarios. Therefore, we\ndesign an automated pipeline to transform the sequential episodes into page\ngraphs, which explicitly model the graph structure of the pages that are\nnaturally connected by actions. To fully utilize the page graphs, we further\nintroduce Retrieval-Augmented Generation (RAG) technology to effectively\nretrieve reliable perception guidelines of GUI from them, and a tailored\nmulti-agent framework PG-Agent with task decomposition strategy is proposed to\nbe injected with the guidelines so that it can generalize to unseen scenarios.\nExtensive experiments on various benchmarks demonstrate the effectiveness of\nPG-Agent, even with limited episodes for page graph construction.", "AI": {"tldr": "\u73b0\u6709\u7684GUI Agent\u65e0\u6cd5\u6355\u6349\u9875\u9762\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "motivation": "\u73b0\u6709\u7684GUI Agent\u65e0\u6cd5\u6355\u6349\u9875\u9762\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u5c06\u987a\u5e8f episodes \u8f6c\u6362\u6210 page graphs\uff0c\u5229\u7528 RAG \u6280\u672f\u68c0\u7d22 GUI \u7684\u53ef\u9760\u8ba4\u77e5\u6307\u5357\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u5b9a\u5236\u7684\u591a\u4ee3\u7406\u6846\u67b6 PG-Agent\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPG-Agent \u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "PG-Agent \u5373\u4f7f\u5728\u9875\u9762\u56fe\u6784\u5efa\u7684 episodes \u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u573a\u666f\u3002"}}
{"id": "2509.03594", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.03594", "abs": "https://arxiv.org/abs/2509.03594", "authors": ["Thomas R. Harvey"], "title": "The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric", "comment": "https://github.com/harveyThomas4692/Induced-Metric-Optimiser", "summary": "We present a class of novel optimisers for training neural networks that\nmakes use of the Riemannian metric naturally induced when the loss landscape is\nembedded in higher-dimensional space. This is the same metric that underlies\ncommon visualisations of loss landscapes. By taking this geometric perspective\nliterally and using the induced metric, we develop a new optimiser and compare\nit to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of\ntasks and architectures. Empirically, we conclude that this new class of\noptimisers is highly effective in low dimensional examples, and provides slight\nimprovement over state-of-the-art methods for training neural networks. These\nnew optimisers have theoretically desirable properties. In particular, the\neffective learning rate is automatically decreased in regions of high curvature\nacting as a smoothed out form of gradient clipping. Similarly, one variant of\nthese optimisers can also be viewed as inducing an effective scheduled learning\nrate and decoupled weight decay is the natural choice from our geometric\nperspective. The basic method can be used to modify any existing\npreconditioning method. The new optimiser has a computational complexity\ncomparable to that of Adam.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u7c7b\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5668\uff0c\u8be5\u4f18\u5316\u5668\u5229\u7528\u635f\u5931\u666f\u89c2\u5d4c\u5165\u5230\u9ad8\u7ef4\u7a7a\u95f4\u65f6\u81ea\u7136\u4ea7\u751f\u7684\u9ece\u66fc\u5ea6\u91cf\u3002", "motivation": "\u901a\u8fc7\u91c7\u7528\u51e0\u4f55\u89c6\u89d2\u5e76\u4f7f\u7528\u8bf1\u5bfc\u5ea6\u91cf\uff0c\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08SGD\u3001Adam\u3001AdamW \u548c Muon\uff09\u5728\u4e00\u7cfb\u5217\u4efb\u52a1\u548c\u67b6\u6784\u4e2d\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u5229\u7528\u635f\u5931\u666f\u89c2\u5d4c\u5165\u5230\u9ad8\u7ef4\u7a7a\u95f4\u65f6\u81ea\u7136\u4ea7\u751f\u7684\u9ece\u66fc\u5ea6\u91cf\u3002", "result": "\u8fd9\u79cd\u65b0\u7684\u4f18\u5316\u5668\u7c7b\u5728\u4f4e\u7ef4\u793a\u4f8b\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65b9\u9762\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7565\u6709\u6539\u8fdb\u3002\u6709\u6548\u5b66\u4e60\u7387\u5728\u66f2\u7387\u8f83\u9ad8\u7684\u533a\u57df\u81ea\u52a8\u964d\u4f4e\uff0c\u5145\u5f53\u5e73\u6ed1\u5f62\u5f0f\u7684\u68af\u5ea6\u88c1\u526a\u3002\u540c\u6837\uff0c\u8fd9\u4e9b\u4f18\u5316\u5668\u7684\u4e00\u79cd\u53d8\u4f53\u4e5f\u53ef\u4ee5\u88ab\u89c6\u4e3a\u8bf1\u5bfc\u6709\u6548\u7684\u9884\u5b9a\u5b66\u4e60\u7387\uff0c\u5e76\u4e14\u4ece\u6211\u4eec\u7684\u51e0\u4f55\u89d2\u5ea6\u6765\u770b\uff0c\u89e3\u8026\u6743\u91cd\u8870\u51cf\u662f\u81ea\u7136\u7684\u9009\u62e9\u3002\u57fa\u672c\u65b9\u6cd5\u53ef\u7528\u4e8e\u4fee\u6539\u4efb\u4f55\u73b0\u6709\u7684\u9884\u5904\u7406\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u4f18\u5316\u5668\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e Adam \u76f8\u5f53\u3002"}}
{"id": "2509.03764", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03764", "abs": "https://arxiv.org/abs/2509.03764", "authors": ["Han Wang", "Alex Whitworth", "Pak Ming Cheung", "Zhenjie Zhang", "Krishna Kamath"], "title": "LLM-based Relevance Assessment for Web-Scale Search Evaluation at Pinterest", "comment": "RecSys 2025 EARL Workshop", "summary": "Relevance evaluation plays a crucial role in personalized search systems to\nensure that search results align with a user's queries and intent. While human\nannotation is the traditional method for relevance evaluation, its high cost\nand long turnaround time limit its scalability. In this work, we present our\napproach at Pinterest Search to automate relevance evaluation for online\nexperiments using fine-tuned LLMs. We rigorously validate the alignment between\nLLM-generated judgments and human annotations, demonstrating that LLMs can\nprovide reliable relevance measurement for experiments while greatly improving\nthe evaluation efficiency. Leveraging LLM-based labeling further unlocks the\nopportunities to expand the query set, optimize sampling design, and\nefficiently assess a wider range of search experiences at scale. This approach\nleads to higher-quality relevance metrics and significantly reduces the Minimum\nDetectable Effect (MDE) in online experiment measurements.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5fae\u8c03\u7684LLM\u6765\u81ea\u52a8\u5316\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\u5e76\u6269\u5927\u8bc4\u4f30\u8303\u56f4\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u5468\u671f\u957f\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684LLM\u6765\u751f\u6210\u5224\u65ad\uff0c\u5e76\u9a8c\u8bc1LLM\u751f\u6210\u7684\u5224\u65ad\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "LLM\u53ef\u4ee5\u4e3a\u5b9e\u9a8c\u63d0\u4f9b\u53ef\u9760\u7684\u76f8\u5173\u6027\u6d4b\u91cf\uff0c\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\uff0c\u6269\u5927\u67e5\u8be2\u96c6\uff0c\u4f18\u5316\u62bd\u6837\u8bbe\u8ba1\uff0c\u5e76\u6709\u6548\u8bc4\u4f30\u66f4\u5927\u8303\u56f4\u7684\u641c\u7d22\u4f53\u9a8c\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u76f8\u5173\u6027\u6307\u6807\u7684\u8d28\u91cf\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5728\u7ebf\u5b9e\u9a8c\u6d4b\u91cf\u4e2d\u7684\u6700\u5c0f\u53ef\u68c0\u6d4b\u6548\u5e94\uff08MDE\uff09\u3002", "conclusion": "\u4f7f\u7528LLM\u8fdb\u884c\u76f8\u5173\u6027\u8bc4\u4f30\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
